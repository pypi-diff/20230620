# Comparing `tmp/verticapy-0.9.0.tar.gz` & `tmp/verticapy-1.0.0b1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "verticapy-0.9.0.tar", last modified: Fri May  6 16:56:43 2022, max compression
+gzip compressed data, was "verticapy-1.0.0b1.tar", last modified: Tue Jun 20 21:34:24 2023, max compression
```

## Comparing `verticapy-0.9.0.tar` & `verticapy-1.0.0b1.tar`

### file list

```diff
@@ -1,151 +1,578 @@
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.176155 verticapy-0.9.0/
--rw-r--r--   0 runner    (1001) docker     (121)    11342 2022-05-06 16:56:33.000000 verticapy-0.9.0/LICENSE.txt
--rw-r--r--   0 runner    (1001) docker     (121)    11132 2022-05-06 16:56:43.176155 verticapy-0.9.0/PKG-INFO
--rwxr-xr-x   0 runner    (1001) docker     (121)    10222 2022-05-06 16:56:33.000000 verticapy-0.9.0/README.md
--rw-r--r--   0 runner    (1001) docker     (121)       38 2022-05-06 16:56:43.176155 verticapy-0.9.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (121)     2171 2022-05-06 16:56:33.000000 verticapy-0.9.0/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.152154 verticapy-0.9.0/verticapy/
--rwxr-xr-x   0 runner    (1001) docker     (121)     2925 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    15271 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/connect.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.156154 verticapy-0.9.0/verticapy/data/
--rwxr-xr-x   0 runner    (1001) docker     (121)     1925 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2617 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/airline_passengers.csv
--rw-r--r--   0 runner    (1001) docker     (121)   174821 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/amazon.csv
--rw-r--r--   0 runner    (1001) docker     (121)    10701 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/cities.csv
--rw-r--r--   0 runner    (1001) docker     (121)    40378 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/commodities.csv
--rw-r--r--   0 runner    (1001) docker     (121)    82078 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/gapminder.csv
--rw-r--r--   0 runner    (1001) docker     (121)     5107 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/iris.csv
--rw-r--r--   0 runner    (1001) docker     (121)    11566 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/market.csv
--rw-r--r--   0 runner    (1001) docker     (121)   400878 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/pop_growth.csv
--rw-r--r--   0 runner    (1001) docker     (121)   367185 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/smart_meters.csv
--rw-r--r--   0 runner    (1001) docker     (121)   116752 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/titanic.csv
--rw-r--r--   0 runner    (1001) docker     (121)   406287 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/winequality.csv
--rw-r--r--   0 runner    (1001) docker     (121)   390866 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/data/world.csv
--rwxr-xr-x   0 runner    (1001) docker     (121)    27148 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/datasets.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     2655 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/errors.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    12488 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/geo.py
--rw-r--r--   0 runner    (1001) docker     (121)     5023 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/hchart.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    57505 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/highchart.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/learn/
--rwxr-xr-x   0 runner    (1001) docker     (121)     1925 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    20255 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/cluster.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    11046 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/decomposition.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    43268 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/delphi.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    31105 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/ensemble.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    10635 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/linear_model.py
--rwxr-xr-x   0 runner    (1001) docker     (121)   122133 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/memmodel.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    43608 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/metrics.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    41508 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/mlplot.py
--rwxr-xr-x   0 runner    (1001) docker     (121)   113151 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/model_selection.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7341 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/naive_bayes.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    66019 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/neighbors.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    15094 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    15466 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/preprocessing.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7318 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/svm.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    33228 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/tools.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    10381 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/tree.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    67249 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/tsa.py
--rwxr-xr-x   0 runner    (1001) docker     (121)   224120 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/learn/vmodel.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    75195 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/logo.py
--rwxr-xr-x   0 runner    (1001) docker     (121)   142380 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/plot.py
--rw-r--r--   0 runner    (1001) docker     (121)    10419 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/sql.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/stats/
--rwxr-xr-x   0 runner    (1001) docker     (121)     2032 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/stats/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    45135 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/stats/math.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    43815 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/stats/tools.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7385 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/base.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     2357 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/connect/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/connect/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     1939 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/connect/test_connect.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/datasets/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/datasets/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     3678 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/datasets/test_datasets.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/geo/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/geo/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)   153913 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/geo/test_geo.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/hchart/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/hchart/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     4871 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/hchart/test_hchart.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/sql/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/sql/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     5979 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/sql/test_sql.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/stats/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/stats/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    22258 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/stats/test_stats.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.164154 verticapy-0.9.0/verticapy/tests/udf/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/udf/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)       91 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/udf/pmath.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     2475 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/udf/test_udf.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.168155 verticapy-0.9.0/verticapy/tests/utilities/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/utilities/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    17086 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/utilities/test_utilities.py
--rw-r--r--   0 runner    (1001) docker     (121)   315639 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/utilities/titanic-passengers.json
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.168155 verticapy-0.9.0/verticapy/tests/vDataFrame/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     8979 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_combine_join_sort.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    20798 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_correlation.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     1518 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_create.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    41130 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_descriptive_statistics.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    31474 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_feature_engineering.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     6977 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_filter_sample.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    28821 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_plot.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    15824 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_preprocessing.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    21414 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_utilities.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.176155 verticapy-0.9.0/verticapy/tests/vModel/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     2906 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_balance.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     8912 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_bisecting_kmeans.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     5539 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_countvectorizer.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     3288 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_dbscan.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    19363 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_decision_tree_classifier.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    12425 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_decision_tree_regressor.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     2917 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_delphi.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    19276 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_dummy_tree_classifier.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    12337 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_dummy_tree_regressor.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    11897 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_elastic_net.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     4027 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_kde.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7850 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_kmeans.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     9547 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_knn_classifier.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7911 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_knn_regressor.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    11853 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_lasso.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    11997 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_linear_regression.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    16315 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_linear_svc.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    11534 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_linear_svr.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     3840 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_lof.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    17117 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_logistic_regression.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7790 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_mca.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    65231 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_memmodel.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    13450 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_model_selection.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    18617 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_naive_bayes.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    11063 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_nearestcentroid.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    15723 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_normalizer.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     8838 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_onehotencoder.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     9240 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_pca.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     9624 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    19558 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_randomforest_classifier.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    13032 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_randomforest_regressor.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    12007 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_ridge.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7633 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_sarimax.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     8740 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_svd.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    14038 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_tools.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     7683 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_var.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    22347 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_xgb_classifier.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    17409 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/tests/vModel/test_xgb_regressor.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    45318 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/toolbox.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    16094 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/udf.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.176155 verticapy-0.9.0/verticapy/util/
--rwxr-xr-x   0 runner    (1001) docker     (121)      606 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/util/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (121)     1493 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/util/log.py
--rw-r--r--   0 runner    (1001) docker     (121)     2276 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/util/os_utils.py
--rw-r--r--   0 runner    (1001) docker     (121)    86831 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/utilities.py
--rwxr-xr-x   0 runner    (1001) docker     (121)   139277 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/vcolumn.py
--rwxr-xr-x   0 runner    (1001) docker     (121)   411515 2022-05-06 16:56:33.000000 verticapy-0.9.0/verticapy/vdataframe.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-05-06 16:56:43.156154 verticapy-0.9.0/verticapy.egg-info/
--rw-r--r--   0 runner    (1001) docker     (121)    11132 2022-05-06 16:56:43.000000 verticapy-0.9.0/verticapy.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     4480 2022-05-06 16:56:43.000000 verticapy-0.9.0/verticapy.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-05-06 16:56:43.000000 verticapy-0.9.0/verticapy.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (121)      190 2022-05-06 16:56:43.000000 verticapy-0.9.0/verticapy.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (121)       10 2022-05-06 16:56:43.000000 verticapy-0.9.0/verticapy.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/
+-rw-r--r--   0 runner    (1001) docker     (123)    11342 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/LICENSE.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    20105 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/PKG-INFO
+-rwxr-xr-x   0 runner    (1001) docker     (123)    19328 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     2137 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.361272 verticapy-1.0.0b1/verticapy/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3693 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.361272 verticapy-1.0.0b1/verticapy/_config/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_config/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6293 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_config/config.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2493 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_config/validators.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3917 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_help.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2160 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_typing.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.361272 verticapy-1.0.0b1/verticapy/_utils/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12398 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_display.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1680 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_gen.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    73985 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_logo.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4401 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1758 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_object.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2292 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_parsers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.365272 verticapy-1.0.0b1/verticapy/_utils/_sql/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3787 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_cast.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1272 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_check.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5564 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_collect.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3847 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_dblink.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1797 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_display.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    11105 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_format.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9319 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_merge.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1399 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_random.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2652 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_sys.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4935 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/_utils/_sql/_vertica_version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.365272 verticapy-1.0.0b1/verticapy/connection/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1079 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/connection/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6355 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/connection/connect.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1598 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/connection/external.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3244 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/connection/global_connection.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5113 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/connection/read.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1734 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/connection/utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5715 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/connection/write.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.365272 verticapy-1.0.0b1/verticapy/core/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.365272 verticapy-1.0.0b1/verticapy/core/parsers/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2152 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10436 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/all.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5484 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/avro.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    20468 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/csv.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18799 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/json.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6878 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/pandas.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2563 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/parsers/shp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.365272 verticapy-1.0.0b1/verticapy/core/string_sql/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      667 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/string_sql/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9906 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/string_sql/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.365272 verticapy-1.0.0b1/verticapy/core/tablesample/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      670 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/tablesample/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22392 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/tablesample/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.369272 verticapy-1.0.0b1/verticapy/core/vdataframe/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      681 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    86125 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_aggregate.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    74311 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_corr.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    24436 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_encoding.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5346 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_eval.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    20773 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_fill.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    28658 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_filter.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    30554 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_io.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    11984 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_join_union_sort.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    35973 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_machine_learning.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    33120 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_math.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1716 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_multiprocessing.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    17839 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_normalize.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9053 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_pivot.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    75362 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_plotting.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    16513 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_plotting_animated.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21629 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_read.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    15755 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_rolling.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    27363 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_sys.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7350 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_text.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13021 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_typing.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7342 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    15648 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/core/vdataframe/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.369272 verticapy-1.0.0b1/verticapy/datasets/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1093 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/datasets/data/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2617 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/airline_passengers.csv
+-rw-r--r--   0 runner    (1001) docker     (123)   174821 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/amazon.csv
+-rw-r--r--   0 runner    (1001) docker     (123)    10701 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/cities.csv
+-rw-r--r--   0 runner    (1001) docker     (123)    40378 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/commodities.csv
+-rw-r--r--   0 runner    (1001) docker     (123)    82078 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/gapminder.csv
+-rw-r--r--   0 runner    (1001) docker     (123)     5107 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/iris.csv
+-rw-r--r--   0 runner    (1001) docker     (123)    11566 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/market.csv
+-rw-r--r--   0 runner    (1001) docker     (123)   400878 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/pop_growth.csv
+-rw-r--r--   0 runner    (1001) docker     (123)   367185 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/smart_meters.csv
+-rw-r--r--   0 runner    (1001) docker     (123)   116752 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/titanic.csv
+-rw-r--r--   0 runner    (1001) docker     (123)   406287 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/winequality.csv
+-rw-r--r--   0 runner    (1001) docker     (123)   390866 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/data/world.csv
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9821 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/generators.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18855 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/loaders.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4390 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/datasets/tests_loaders.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/errors/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1159 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/errors/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/geo/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      817 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/geo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/hchart/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      690 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/hchart/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/jupyter/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/jupyter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8964 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/jupyter/_javascript.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/jupyter/extensions/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/jupyter/extensions/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1862 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/jupyter/extensions/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9823 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/jupyter/extensions/chart_magic.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11909 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/jupyter/extensions/sql_magic.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/cluster/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      735 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/cluster/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/decomposition/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      689 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/decomposition/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/delphi/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      842 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/delphi/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/ensemble/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      787 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/ensemble/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/linear_model/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      762 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/linear_model/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/memmodel/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      681 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/memmodel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/metrics/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2933 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/metrics/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/model_selection/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1281 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/model_selection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/naive_bayes/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      764 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/naive_bayes/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/neighbors/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      840 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/neighbors/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/pipeline/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      679 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/pipeline/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/preprocessing/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      800 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/preprocessing/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/svm/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      686 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/svm/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/tools/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      689 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/tools/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/tree/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      774 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/tree/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/tsa/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/tsa/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/learn/vmodel/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/learn/vmodel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.373272 verticapy-1.0.0b1/verticapy/machine_learning/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1522 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4229 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    23033 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/cluster.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7028 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/decomposition.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13555 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/ensemble.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5733 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/linear_model.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7937 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/naive_bayes.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7846 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/preprocessing.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    32737 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/memmodel/tree.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/machine_learning/metrics/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4202 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/metrics/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    76793 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/metrics/classification.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9212 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/metrics/plotting.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    15560 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/metrics/regression.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1335 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/hp_tuning/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1015 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/hp_tuning/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    32727 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/hp_tuning/cv.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    27013 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/hp_tuning/param_gen.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9934 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/hp_tuning/plotting.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9541 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/kmeans.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    19250 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/model_validation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/statistical_tests/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1145 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/statistical_tests/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4364 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/statistical_tests/norm.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10547 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/statistical_tests/ols.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    26038 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/statistical_tests/tsa.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21097 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/model_selection/variables_selection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/machine_learning/vertica/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2015 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/machine_learning/vertica/automl/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      842 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/automl/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7742 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/automl/clustering.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    15037 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/automl/dataprep.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    28843 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/automl/supervised.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)   102564 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    39245 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/cluster.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    26739 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/decomposition.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    49160 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/ensemble.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    23024 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/linear_model.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6681 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/model_management.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7977 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/naive_bayes.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    49833 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/neighbors.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12316 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    31328 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/preprocessing.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8861 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/svm.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7188 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/machine_learning/vertica/tree.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.377272 verticapy-1.0.0b1/verticapy/plotting/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.381272 verticapy-1.0.0b1/verticapy/plotting/_highcharts/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2708 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4804 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/acf.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6048 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/bar.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6711 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/barh.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1852 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3164 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/boxplot.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2828 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/candlestick.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3726 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/contour.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4224 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/density.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5530 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/heatmap.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6877 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/line.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.381272 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3201 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/champion_challenger.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2406 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/elbow.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2898 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/importance.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4062 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/lof.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3047 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/logistic_reg.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5761 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/model_evaluation.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9000 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/pca.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3899 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/regression.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2145 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/regression_tree.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5058 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/stepwise.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4680 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/machine_learning/svm.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4046 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/outliers.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8272 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/pie.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2910 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/range.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5549 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/scatter.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2857 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_highcharts/spider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.381272 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3120 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5810 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/acf.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.381272 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/animated/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/animated/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7746 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/animated/bar.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1214 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/animated/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12197 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/animated/bubble.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4330 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/animated/line.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3273 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/animated/pie.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4727 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/bar.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6737 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/barh.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2963 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3602 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/boxplot.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3001 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/contour.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5108 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/density.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4164 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/heatmap.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3102 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/hexbin.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2369 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/hist.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7993 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/line.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.385272 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8289 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/champion_challenger.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1902 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/elbow.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2824 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/importance.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3986 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/kmeans.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4719 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/lof.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6360 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/logistic_reg.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7402 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/model_evaluation.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8952 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/pca.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4184 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/regression.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2595 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/regression_tree.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4928 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/stepwise.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6883 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/machine_learning/svm.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4846 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/outliers.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10252 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/pie.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3849 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/range.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6893 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/scatter.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3238 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_matplotlib/spider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.385272 verticapy-1.0.0b1/verticapy/plotting/_plotly/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2761 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3573 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/acf.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2124 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/bar.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2169 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/barh.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3687 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7408 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/boxplot.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3287 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/contour.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2812 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/density.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4714 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2072 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/hist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3596 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/line.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.385272 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5175 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/champion_challenger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/elbow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2856 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/importance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6804 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/kmeans.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5301 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/lof.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5965 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/logistic_reg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6764 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/model_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2752 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/pca.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2753 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/regression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2820 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/regression_tree.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4588 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/stepwise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6756 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/machine_learning/svm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6022 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/outliers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4011 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/pie.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2749 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/range.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3654 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/scatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2701 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_plotly/spider.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4961 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    63291 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    11132 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/plotting/sql.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.385272 verticapy-1.0.0b1/verticapy/sdk/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sdk/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.385272 verticapy-1.0.0b1/verticapy/sdk/vertica/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sdk/vertica/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.389272 verticapy-1.0.0b1/verticapy/sdk/vertica/udf/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      745 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sdk/vertica/udf/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    11279 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sdk/vertica/udf/gen.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3376 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sdk/vertica/udf/load.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2937 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sdk/vertica/udf/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.389272 verticapy-1.0.0b1/verticapy/sql/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      935 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3629 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/create.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6474 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/drop.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6418 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/dtypes.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6144 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/flex.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.393272 verticapy-1.0.0b1/verticapy/sql/functions/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2244 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8563 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/analytic.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2994 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/conditional.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9095 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/date.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    14963 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/math.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2085 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/null_handling.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1478 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/random.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5681 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/regexp.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5182 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/functions/string.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.393272 verticapy-1.0.0b1/verticapy/sql/geo/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      817 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/geo/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5887 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/geo/functions.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5225 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/geo/index.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4698 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/insert.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1183 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/sql/sys.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.393272 verticapy-1.0.0b1/verticapy/stats/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2776 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/stats/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7393 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2467 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/connect/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/connect/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2395 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/connect/test_connect.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/datasets/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/datasets/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3745 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/datasets/test_datasets.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/geo/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/geo/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)   153945 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/geo/test_geo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/hchart/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/hchart/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5088 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/hchart/test_hchart.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/sql/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/sql/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7742 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/sql/test_sql.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/stats/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/stats/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    23086 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/stats/test_stats.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.397272 verticapy-1.0.0b1/verticapy/tests/udf/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/udf/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       91 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/udf/pmath.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2489 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/udf/test_udf.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.401273 verticapy-1.0.0b1/verticapy/tests/utilities/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utilities/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       39 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utilities/csv_test1.csv
+-rw-r--r--   0 runner    (1001) docker     (123)       39 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utilities/csv_test2.csv
+-rw-r--r--   0 runner    (1001) docker     (123)       39 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utilities/csv_test3.csv
+-rwxr-xr-x   0 runner    (1001) docker     (123)    38029 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utilities/test_utilities.py
+-rw-r--r--   0 runner    (1001) docker     (123)   315639 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utilities/titanic-passengers.json
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.401273 verticapy-1.0.0b1/verticapy/tests/utils/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utils/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1501 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utils/log.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2273 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/utils/os_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.401273 verticapy-1.0.0b1/verticapy/tests/vDataFrame/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13815 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_combine_join_sort.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21816 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_correlation.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3778 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_create.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    39755 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_descriptive_statistics.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    31699 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_feature_engineering.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6817 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_filter_sample.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    28742 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_plot.py
+-rw-r--r--   0 runner    (1001) docker     (123)    89282 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_plot_plotly.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18302 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_preprocessing.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    23013 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_utilities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.405273 verticapy-1.0.0b1/verticapy/tests/vModel/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2931 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_balance.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9028 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_bisecting_kmeans.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3817 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_countvectorizer.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2453 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_dbscan.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18976 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_decision_tree_classifier.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12640 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_decision_tree_regressor.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3056 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_delphi.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18569 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_dummy_tree_classifier.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12224 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_dummy_tree_regressor.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12627 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_elastic_net.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10335 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_iforest.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3120 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_kde.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7926 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_kmeans.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8857 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_knn_classifier.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7173 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_knn_regressor.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7991 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_kprototypes.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12588 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_lasso.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12726 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_linear_regression.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    16380 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_linear_svc.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    11726 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_linear_svr.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3121 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_lof.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    17732 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_logistic_regression.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7714 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_mca.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    62066 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_memmodel.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13757 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_model_selection.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    19003 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_naive_bayes.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9956 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_nearestcentroid.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    15627 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_normalizer.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8791 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_onehotencoder.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9202 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_pca.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10022 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    19395 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_randomforest_classifier.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13755 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_randomforest_regressor.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12741 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_ridge.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8734 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_svd.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10874 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_tools.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    22751 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_xgb_classifier.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    18106 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests/vModel/test_xgb_regressor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.405273 verticapy-1.0.0b1/verticapy/tests_new/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.405273 verticapy-1.0.0b1/verticapy/tests_new/config/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/config/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    11348 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.405273 verticapy-1.0.0b1/verticapy/tests_new/connection/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/connection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/datasets/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/jupyter/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/jupyter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/jupyter/extensions/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/jupyter/extensions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/memmodel/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/memmodel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/metrics/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18299 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/metrics/test_classification_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10239 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/metrics/test_regression_metrics.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/model_selection/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/model_selection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/vertica/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/machine_learning/vertica/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/plotting/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    55249 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/base_test_files.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1644 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1183 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.409273 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      868 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_champion_challenger.py
+-rw-r--r--   0 runner    (1001) docker     (123)      836 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_elbow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1137 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_importance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1045 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_lof.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1341 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_logistic_reg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1598 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_model_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      832 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_pca.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1188 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_regression.py
+-rw-r--r--   0 runner    (1001) docker     (123)      856 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_regression_tree.py
+-rw-r--r--   0 runner    (1001) docker     (123)      827 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_stepwise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1303 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/machine_learning/test_highcharts_svm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1683 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_acf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2551 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_bar.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1948 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_barh.py
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1478 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_boxplot.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1909 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_candlestick.py
+-rw-r--r--   0 runner    (1001) docker     (123)      831 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_contour.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1594 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_density.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1055 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1178 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_hist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1382 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_line.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1550 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_outliers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1563 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_pie.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1045 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_range.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2639 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1195 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/highcharts/test_highcharts_spider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.413273 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1151 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.413273 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      860 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_champion_challenger.py
+-rw-r--r--   0 runner    (1001) docker     (123)      836 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_elbow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1464 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_importance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3228 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_kmeans.py
+-rw-r--r--   0 runner    (1001) docker     (123)      950 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_lof.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1638 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_logistic_reg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_model_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      832 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_pca.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_regression.py
+-rw-r--r--   0 runner    (1001) docker     (123)      856 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_regression_tree.py
+-rw-r--r--   0 runner    (1001) docker     (123)      827 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_stepwise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1392 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/machine_learning/test_matplotlib_svm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1896 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_acf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2949 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_bar.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2860 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_barh.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1950 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1433 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_boxplot.py
+-rw-r--r--   0 runner    (1001) docker     (123)      831 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_contour.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1519 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_density.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1653 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4109 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_hexbin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1012 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_hist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1510 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_line.py
+-rw-r--r--   0 runner    (1001) docker     (123)      979 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_outliers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1890 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_pie.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1490 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_range.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2675 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1352 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/matplotlib/test_matplotlib_spider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.413273 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      864 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_champion_challenger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_elbow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_importance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1155 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_kmeans.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1696 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_lof.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1626 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_logistic_reg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2297 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_model_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1066 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_pca.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1472 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_regression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2092 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_regression_tree.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1569 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_stepwise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2846 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/machine_learning/test_plotly_svm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2707 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_acf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2801 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_bar.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3683 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_barh.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4706 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6427 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_boxplot.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2063 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_contour.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2174 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_density.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3151 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_hist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2647 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_line.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2869 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_outliers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3011 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_pie.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2779 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_range.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5600 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1873 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/plotting/plotly/test_plotly_spider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/tests_new/sdk/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/sdk/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/tests_new/sql/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/sql/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/tests_new/sql/functions/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/sql/functions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/tests_new/sql/geo/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/sql/geo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/tests_new/utils/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      614 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/tests_new/utils/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/udf/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      685 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/udf/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/utilities/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1542 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/utilities/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.417273 verticapy-1.0.0b1/verticapy/vdataframe/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      681 2023-06-20 21:34:11.000000 verticapy-1.0.0b1/verticapy/vdataframe/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-20 21:34:24.361272 verticapy-1.0.0b1/verticapy.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    20105 2023-06-20 21:34:24.000000 verticapy-1.0.0b1/verticapy.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    22430 2023-06-20 21:34:24.000000 verticapy-1.0.0b1/verticapy.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-20 21:34:24.000000 verticapy-1.0.0b1/verticapy.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      207 2023-06-20 21:34:24.000000 verticapy-1.0.0b1/verticapy.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       10 2023-06-20 21:34:24.000000 verticapy-1.0.0b1/verticapy.egg-info/top_level.txt
```

### Comparing `verticapy-0.9.0/LICENSE.txt` & `verticapy-1.0.0b1/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/setup.py` & `verticapy-1.0.0b1/setup.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,64 +1,67 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 #!/usr/bin/env python
 import setuptools
 
 with open("README.md", "r") as fh:
     long_description = fh.read()
 
 setuptools.setup(
     name="verticapy",
-    version="0.9.0",
+    version="1.0.0-beta.1",
     author="Badr Ouali",
     author_email="badr.ouali@vertica.com",
     url="https://github.com/vertica/VerticaPy",
     keywords="vertica python ml data science machine learning statistics database",
     description=(
         "VerticaPy simplifies data exploration, data cleaning, and machine"
         " learning in Vertica."
     ),
     long_description=long_description,
     long_description_content_type="text/markdown",
     packages=setuptools.find_packages(),
-    python_requires=">=3.6",
+    python_requires=">=3.8",
     install_requires=[
-        "matplotlib>=2.0",
+        "matplotlib<=3.5.3",
         "numpy>=1.11.0",
         "pandas>=0.23.0",
-        "python-highcharts>=0.4.1",
+        "vertica-highcharts>=0.1.2",
         "scipy>=1.0.0",
         "tqdm>=4.0.0",
-        "vertica-python>=0.11.0",
+        "vertica-python>=1.2.0",
+        "plotly>=5.10.0",
     ],
     extras_require={
         "all": [
             "descartes>=1.0.0",
             "geopandas>=0.8.0",
             "graphviz>=0.9.0",
             "shapely>=1.6.0",
         ],
     },
-    package_data={"": ["*.csv", "*.json"]},
+    package_data={"": ["*.csv", "*.json", "*.css", "*.html"]},
     classifiers=[
         "Intended Audience :: Science/Research",
         "Intended Audience :: Developers",
-        "Programming Language :: Python :: 3",
-        "Programming Language :: Python :: 3.6",
-        "Programming Language :: Python :: 3.7",
         "Programming Language :: Python :: 3.8",
+        "Programming Language :: Python :: 3.9",
         "Topic :: Database",
         "License :: OSI Approved :: Apache Software License",
         "Operating System :: OS Independent",
     ],
 )
```

### Comparing `verticapy-0.9.0/verticapy/data/airline_passengers.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/airline_passengers.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/amazon.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/amazon.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/cities.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/cities.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/commodities.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/commodities.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/gapminder.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/gapminder.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/iris.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/iris.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/market.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/market.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/pop_growth.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/pop_growth.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/smart_meters.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/smart_meters.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/titanic.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/titanic.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/winequality.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/winequality.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/data/world.csv` & `verticapy-1.0.0b1/verticapy/datasets/data/world.csv`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/learn/cluster.py` & `verticapy-1.0.0b1/verticapy/machine_learning/memmodel/cluster.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,522 +1,725 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# Standard Python Modules
-import os
-
-# VerticaPy Modules
-import vertica_python, verticapy
-from verticapy import vDataFrame
-from verticapy.connect import current_cursor
-from verticapy.utilities import *
-from verticapy.toolbox import *
-from verticapy.errors import *
-from verticapy.learn.vmodel import *
-from verticapy.learn.tools import *
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import copy
+from typing import Literal, Optional, Union
+
+import numpy as np
 
-# ---#
-class BisectingKMeans(Clustering, Tree):
+from verticapy._typing import ArrayLike, NoneType
+from verticapy._utils._sql._format import clean_query, format_magic, format_type
+
+from verticapy.machine_learning.memmodel.base import InMemoryModel
+from verticapy.machine_learning.memmodel.tree import Tree
+
+
+class Clustering(InMemoryModel):
     """
----------------------------------------------------------------------------
-Creates a BisectingKMeans object using the Vertica bisecting k-means 
-algorithm on the data. k-means clustering is a method of vector quantization, 
-originally from signal processing, that aims to partition n observations into 
-k clusters. Each observation belongs to the cluster with the nearest 
-mean (cluster centers or cluster centroid), which serves as a prototype of 
-the cluster. This results in a partitioning of the data space into Voronoi
-cells. Bisecting k-means combines k-means and hierarchical clustering.
-
-Parameters
-----------
-name: str
-    Name of the the model. The model will be stored in the DB.
-n_cluster: int, optional
-    Number of clusters
-bisection_iterations: int, optional
-    The number of iterations the bisecting KMeans algorithm performs for each 
-    bisection step. This corresponds to how many times a standalone KMeans 
-    algorithm runs in each bisection step. Setting to more than 1 allows 
-    the algorithm to run and choose the best KMeans run within each bisection 
-    step. Note that if you are using kmeanspp the bisection_iterations value is 
-    always 1, because kmeanspp is more costly to run but also better than the 
-    alternatives, so it does not require multiple runs.
-split_method: str, optional
-    The method used to choose a cluster to bisect/split.
-        size        : Choose the largest cluster to bisect.
-        sum_squares : Choose the cluster with the largest withInSS to bisect.
-min_divisible_cluster_size: int, optional
-    The minimum number of points of a divisible cluster. Must be greater than or 
-    equal to 2.
-distance_method: str, optional
-    The measure for distance between two data points. Only Euclidean distance 
-    is supported at this time.
-init: str/list, optional
-    The method to use to find the initial KMeans cluster centers.
-        kmeanspp : Uses the KMeans++ method to initialize the centers.
-        pseudo   : Uses "pseudo center" approach used by Spark, bisects given 
-            center without iterating over points.
-    It can be also a list with the initial cluster centers to use.
-max_iter: int, optional
-    The maximum number of iterations the KMeans algorithm performs.
-tol: float, optional
-    Determines whether the KMeans algorithm has converged. The algorithm is 
-    considered converged after no center has moved more than a distance of 
-    'tol' from the previous iteration.
+    InMemoryModel implementation of clustering algorithms.
+
+    Parameters
+    ----------
+    clusters: ArrayLike
+        ArrayLike   of   the   model's  cluster   centers.
+    p: int, optional
+        The p corresponding to one of the p-distances.
+    clusters_names: ArrayLike, optional
+        Names of the clusters.
     """
 
+    # Properties.
+
+    @property
+    def object_type(self) -> Literal["Clustering"]:
+        return "Clustering"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["clusters_", "p_"]
+
+    # System & Special Methods.
+
     def __init__(
         self,
-        name: str,
-        n_cluster: int = 8,
-        bisection_iterations: int = 1,
-        split_method: str = "sum_squares",
-        min_divisible_cluster_size: int = 2,
-        distance_method: str = "euclidean",
-        init: str = "kmeanspp",
-        max_iter: int = 300,
-        tol: float = 1e-4,
-    ):
-        check_types([("name", name, [str])])
-        self.type, self.name = "BisectingKMeans", name
-        self.set_params(
-            {
-                "n_cluster": n_cluster,
-                "bisection_iterations": bisection_iterations,
-                "split_method": split_method,
-                "min_divisible_cluster_size": min_divisible_cluster_size,
-                "distance_method": distance_method,
-                "init": init,
-                "max_iter": max_iter,
-                "tol": tol,
-            }
+        clusters: ArrayLike,
+        p: int = 2,
+        clusters_names: Optional[ArrayLike] = None,
+    ) -> None:
+        clusters_names = format_type(clusters_names, dtype=list)
+        self.clusters_ = np.array(clusters)
+        self.classes_ = np.array(clusters_names)
+        self.p_ = p
+
+    # Prediction / Transformation Methods - IN MEMORY.
+
+    def predict(self, X: ArrayLike) -> np.ndarray:
+        """
+        Predicts  clusters  using  the input  matrix.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The data on which to make the prediction.
+
+        Returns
+        -------
+        numpy.array
+            Predicted values.
+        """
+        distances = self.transform(X)
+        clusters_pred_id = np.argmin(distances, axis=1).astype(object)
+        if hasattr(self, "classes_") and len(self.classes_) > 0:
+            for idx, c in enumerate(self.classes_):
+                clusters_pred_id[clusters_pred_id == idx] = c
+        return clusters_pred_id
+
+    def predict_proba(self, X: ArrayLike) -> np.ndarray:
+        """
+        Predicts the probability of each input to belong
+        to the model clusters.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The data on which to make the prediction.
+
+        Returns
+        -------
+        numpy.array
+            Probabilities.
+        """
+        distances = self.transform(X)
+        return (
+            1 / (distances + 1e-99) / np.sum(1 / (distances + 1e-99), axis=1)[:, None]
         )
-        version(condition=[9, 3, 1])
 
-    # ---#
-    def get_tree(self):
+    def transform(self, X: ArrayLike) -> np.ndarray:
+        """
+        Transforms and returns the distance to each cluster.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The data on which to make the transformation.
+
+        Returns
+        -------
+        numpy.array
+            Transformed values.
+        """
+        result = []
+        for centroid in self.clusters_:
+            result += [
+                np.sum((np.array(centroid) - X) ** self.p_, axis=1) ** (1 / self.p_)
+            ]
+        return np.column_stack(result)
+
+    # Prediction / Transformation Methods - IN DATABASE.
+
+    def predict_sql(self, X: ArrayLike) -> str:
         """
-    ---------------------------------------------------------------------------
-    Returns a table containing information about the BK-tree.
+        Returns the SQL code needed to deploy the model using
+        its attributes.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The names or values of the input predictors.
+
+        Returns
+        -------
+        str
+            SQL code.
+        """
+        if hasattr(self, "classes_"):
+            n = len(self.classes_)
+        else:
+            n = 0
+        clusters_distance = self.transform_sql(X)
+        sql = []
+        k = len(clusters_distance)
+        for i in range(k):
+            list_tmp = []
+            for j in range(i):
+                list_tmp += [f"{clusters_distance[i]} <= {clusters_distance[j]}"]
+            sql += [" AND ".join(list_tmp)]
+        sql = sql[1:]
+        sql.reverse()
+        is_null_x = " OR ".join([f"{x} IS NULL" for x in X])
+        sql_final = f"CASE WHEN {is_null_x} THEN NULL"
+        for i in range(k - 1):
+            if n == 0:
+                c = k - i - 1
+            else:
+                c = format_magic(self.classes_[k - i - 1])
+            sql_final += f" WHEN {sql[i]} THEN {c}"
+        if n == 0:
+            c = 0
+        else:
+            c = format_magic(self.classes_[0])
+        sql_final += f" ELSE {c} END"
+        return sql_final
+
+    def predict_proba_sql(self, X: ArrayLike) -> list[str]:
+        """
+        Returns  the SQL code needed to deploy the model
+        probabilities.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The names or values of the input predictors.
+
+        Returns
+        -------
+        list
+            SQL code.
+        """
+        clusters_distance = self.transform_sql(X)
+        sum_distance = " + ".join([f"1 / ({d})" for d in clusters_distance])
+        proba = [
+            f"""
+            (CASE 
+                WHEN {clusters_distance[i]} = 0 
+                    THEN 1.0 
+                ELSE 1 / ({clusters_distance[i]}) 
+                      / ({sum_distance})
+            END)"""
+            for i in range(len(clusters_distance))
+        ]
+        return [clean_query(p) for p in proba]
+
+    def transform_sql(self, X: ArrayLike) -> list[str]:
+        """
+        Transforms  and returns the SQL distance to each
+        cluster.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The names or values of the input predictors.
+
+        Returns
+        -------
+        list
+            SQL code.
         """
-        return self.cluster_centers_
+        for c in self.clusters_:
+            if len(X) != len(c):
+                raise ValueError(
+                    "The length of parameter 'X' must be the same as "
+                    "the length of each cluster."
+                )
+        clusters_distance = []
+        for c in self.clusters_:
+            list_tmp = []
+            for idx in range(len(X)):
+                list_tmp += [f"POWER({X[idx]} - {c[idx]}, {self.p_})"]
+            clusters_distance += ["POWER(" + " + ".join(list_tmp) + f", 1 / {self.p_})"]
+        return clusters_distance
+
+
+class KMeans(Clustering):
+    """
+    InMemoryModel implementation of KMeans.
+
+    Parameters
+    ----------
+    clusters: ArrayLike
+        List of the model's cluster centers.
+    p: int, optional
+        The p corresponding to one of the p-distances.
+    """
+
+    # Properties.
+
+    @property
+    def object_type(self) -> Literal["KMeans"]:
+        return "KMeans"
+
+    # System & Special Methods.
+
+    def __init__(self, clusters: ArrayLike, p: int = 2) -> None:
+        self.clusters_ = np.array(clusters)
+        self.p_ = p
+
+
+class NearestCentroid(Clustering):
+    """
+    InMemoryModel   implementation  of   NearestCentroid
+    algorithm.
+
+    Parameters
+    ----------
+    clusters: ArrayLike
+        List of the model's cluster centers.
+    classes: ArrayLike
+        Names of the classes.
+    p: int, optional
+        The p corresponding to  one of the p-distances.
+    """
+
+    # Properties.
+
+    @property
+    def object_type(self) -> Literal["NearestCentroid"]:
+        return "NearestCentroid"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["clusters_", "classes_", "p_"]
+
+    # System & Special Methods.
+
+    def __init__(
+        self,
+        clusters: ArrayLike,
+        classes: ArrayLike,
+        p: int = 2,
+    ) -> None:
+        self.clusters_ = np.array(clusters)
+        self.classes_ = np.array(classes)
+        self.p_ = p
 
 
-# ---#
-class DBSCAN(vModel):
+class BisectingKMeans(Clustering, Tree):
     """
----------------------------------------------------------------------------
-[Beta Version]
-Creates a DBSCAN object by using the DBSCAN algorithm as defined by Martin 
-Ester, Hans-Peter Kriegel, Jrg Sander, and Xiaowei Xu. This object uses 
-pure SQL to compute the distances and neighbors and uses Python to compute 
-the cluster propagation (non-scalable phase).
-
-\u26A0 Warning : This algorithm uses a CROSS JOIN during computation and
-                 is therefore computationally expensive at O(n * n), where
-                 n is the total number of elements.
-                 This algorithm indexes elements of the table in order to be optimal 
-                 (the CROSS JOIN will happen only with IDs which are integers). 
-                 Since DBSCAN is uses the p-distance, it is highly sensitive to 
-                 unnormalized data. However, DBSCAN is robust to outliers and can 
-                 find non-linear clusters. It is a very powerful algorithm for 
-                 outliers detection and clustering. A table will be created 
-                 at the end of the learning phase.
-
-Parameters
-----------
-name: str
-	Name of the the model. This is not a built-in model, so this name will be used
-    to build the final table.
-eps: float, optional
-	The radius of a neighborhood with respect to some point.
-min_samples: int, optional
-	Minimum number of points required to form a dense region.
-p: int, optional
-	The p of the p-distance (distance metric used during the model computation).
-	"""
-
-    def __init__(self, name: str, eps: float = 0.5, min_samples: int = 5, p: int = 2):
-        check_types([("name", name, [str])])
-        self.type, self.name = "DBSCAN", name
-        self.set_params({"eps": eps, "min_samples": min_samples, "p": p})
+    InMemoryModel implementation of BisectingKMeans.
 
-    # ---#
-    def fit(
+    Parameters
+    ----------
+    clusters: ArrayLike
+        List of the model's cluster centers.
+    children_left: ArrayLike
+        A list  of node IDs, where  children_left[i] is
+        the node ID of the left child of node i.
+    children_right: ArrayLike
+        A list of node IDs, where  children_right[i] is
+        the node ID of the right child of node i.
+    cluster_size: ArrayLike
+        A list of sizes,  where  cluster_size[i] is the
+        number of elements in node i.
+    cluster_score: ArrayLike
+        A list of scores, where cluster_score[i] is the
+        score  for internal  node i.  The score is  the
+        ratio between the within-cluster sum of squares
+        of the node and the total within-cluster sum of
+        squares.
+    p: int, optional
+        The p corresponding to one of the p-distances.
+    """
+
+    # Properties.
+
+    @property
+    def object_type(self) -> Literal["BisectingKMeans"]:
+        return "BisectingKMeans"
+
+    @property
+    def _attributes(
         self,
-        input_relation: (str, vDataFrame),
-        X: list = [],
-        key_columns: list = [],
-        index: str = "",
-    ):
-        """
-	---------------------------------------------------------------------------
-	Trains the model.
-
-	Parameters
-	----------
-	input_relation: str/vDataFrame
-		Training relation.
-	X: list, optional
-		List of the predictors. If empty, all the numerical vcolumns will be used.
-	key_columns: list, optional
-		Columns not used during the algorithm computation but which will be used
-		to create the final relation.
-	index: str, optional
-		Index used to identify each row separately. It is highly recommanded to
-        have one already in the main table to avoid creating temporary tables.
-
-	Returns
-	-------
-	object
- 		self
-		"""
-        if isinstance(key_columns, str):
-            key_columns = [key_columns]
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [
-                ("input_relation", input_relation, [str, vDataFrame]),
-                ("X", X, [list]),
-                ("key_columns", key_columns, [list]),
-                ("index", index, [str]),
-            ]
+    ) -> Literal[
+        "clusters_",
+        "children_left_",
+        "children_left_",
+        "children_right_",
+        "cluster_size_",
+        "cluster_score_",
+        "p_",
+    ]:
+        return [
+            "clusters_",
+            "children_left_",
+            "children_left_",
+            "children_right_",
+            "cluster_size_",
+            "cluster_score_",
+            "p_",
+        ]
+
+    # System & Special Methods.
+
+    def __init__(
+        self,
+        clusters: ArrayLike,
+        children_left: ArrayLike,
+        children_right: ArrayLike,
+        cluster_size: Optional[ArrayLike] = None,
+        cluster_score: Optional[ArrayLike] = None,
+        p: int = 2,
+    ) -> None:
+        cluster_size, cluster_score = format_type(
+            cluster_size, cluster_score, dtype=list
         )
-        if verticapy.options["overwrite_model"]:
-            self.drop()
-        else:
-            does_model_exist(name=self.name, raise_error=True)
-        if isinstance(input_relation, vDataFrame):
-            if not (X):
-                X = input_relation.numcol()
-            input_relation = input_relation.__genSQL__()
+        self.clusters_ = np.array(clusters)
+        self.children_left_ = np.array(children_left)
+        self.children_right_ = np.array(children_right)
+        self.cluster_size_ = np.array(cluster_size)
+        self.cluster_score_ = np.array(cluster_score)
+        self.p_ = p
+
+    # Prediction / Transformation Methods - IN MEMORY.
+
+    def _predict_tree(
+        self,
+        X: ArrayLike,
+        node_id: int,
+    ) -> int:
+        """
+        Function used recursively to get the tree prediction
+        starting at the input node.
+        """
+        if isinstance(self.children_left_[node_id], NoneType) and isinstance(
+            self.children_right_[node_id], NoneType
+        ):
+            return int(node_id)
         else:
-            if not (X):
-                X = vDataFrame(input_relation).numcol()
-        X = [quote_ident(column) for column in X]
-        self.X = X
-        self.key_columns = [quote_ident(column) for column in key_columns]
-        self.input_relation = input_relation
-        schema, relation = schema_relation(input_relation)
-        name_main, name_dbscan_clusters = (
-            gen_tmp_name(name="main"),
-            gen_tmp_name(name="clusters"),
-        )
-        try:
-            if not (index):
-                index = "id"
-                drop(f"v_temp_schema.{name_main}", method="table")
-                sql = """CREATE LOCAL TEMPORARY TABLE {0} 
-                         ON COMMIT PRESERVE ROWS AS 
-                         SELECT 
-                            ROW_NUMBER() OVER() AS id, 
-                            {1} 
-                         FROM {2} 
-                         WHERE {3}""".format(
-                    name_main,
-                    ", ".join(X + key_columns),
-                    self.input_relation,
-                    " AND ".join([f"{item} IS NOT NULL" for item in X]),
-                )
-                executeSQL(sql, title="Computing the DBSCAN Table [Step 0]")
+            right_node = int(self.children_right_[node_id])
+            left_node = int(self.children_left_[node_id])
+            if np.sum((X - self.clusters_[left_node]) ** self.p_) < np.sum(
+                (X - self.clusters_[right_node]) ** self.p_
+            ):
+                return self._predict_tree(X, left_node)
             else:
-                executeSQL(
-                    "SELECT {0} FROM {1} LIMIT 10".format(
-                        ", ".join(X + key_columns + [index]), self.input_relation
-                    ),
-                    print_time_sql=False,
-                )
-                name_main = self.input_relation
-            sql = [
-                "POWER(ABS(x.{0} - y.{0}), {1})".format(X[i], self.parameters["p"])
-                for i in range(len(X))
-            ]
-            distance = "POWER({0}, 1 / {1})".format(
-                " + ".join(sql), self.parameters["p"]
-            )
-            sql = """SELECT 
-                        x.{0} AS node_id, 
-                        y.{0} AS nn_id, 
-                        {1} AS distance 
-                     FROM {2} AS x 
-                     CROSS JOIN {2} AS y""".format(
-                index, distance, name_main
-            )
-            sql = """SELECT 
-                        node_id, 
-                        nn_id, 
-                        SUM(CASE WHEN distance <= {0} THEN 1 ELSE 0 END) 
-                            OVER (PARTITION BY node_id) AS density, 
-                        distance 
-                     FROM ({1}) distance_table""".format(
-                self.parameters["eps"], sql
-            )
-            if isinstance(verticapy.options["random_state"], int):
-                order_by = "ORDER BY node_id, nn_id"
-            else:
-                order_by = ""
-            sql = """SELECT 
-                        node_id, 
-                        nn_id 
-                     FROM ({0}) VERTICAPY_SUBTABLE 
-                     WHERE density > {1} 
-                        AND distance < {2} 
-                        AND node_id != nn_id {3}""".format(
-                sql, self.parameters["min_samples"], self.parameters["eps"], order_by,
-            )
-            graph = executeSQL(
-                sql, title="Computing the DBSCAN Table [Step 1]", method="fetchall"
+                return self._predict_tree(X, right_node)
+
+    def _predict_row(self, X: ArrayLike) -> int:
+        """
+        Function used recursively to get the tree prediction.
+        """
+        return self._predict_tree(X, 0)
+
+    def predict(self, X: ArrayLike) -> np.ndarray:
+        """
+        Predicts using the bisecting k-means model.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The data on which to make the prediction.
+
+        Returns
+        -------
+        numpy.array
+            Predicted values.
+        """
+        return np.apply_along_axis(self._predict_row, 1, X)
+
+    # Prediction / Transformation Methods - IN DATABASE.
+
+    def _predict_tree_sql(
+        self,
+        children_right: ArrayLike,
+        children_left: ArrayLike,
+        node_id: int,
+        clusters_distance: ArrayLike,
+    ) -> Union[int, str]:
+        """
+        Function used recursively to do the final SQL code
+        generation.
+        """
+        if isinstance(children_left[node_id], NoneType) and isinstance(
+            children_right[node_id], NoneType
+        ):
+            return int(node_id)
+        else:
+            right_node = int(children_right[node_id])
+            left_node = int(children_left[node_id])
+            x = clusters_distance[left_node]
+            th = clusters_distance[right_node]
+            y0 = self._predict_tree_sql(
+                children_right, children_left, left_node, clusters_distance
             )
-            main_nodes = list(
-                dict.fromkeys([elem[0] for elem in graph] + [elem[1] for elem in graph])
+            y1 = self._predict_tree_sql(
+                children_right, children_left, right_node, clusters_distance
             )
-            clusters = {}
-            for elem in main_nodes:
-                clusters[elem] = None
-            i = 0
-            while graph:
-                node = graph[0][0]
-                node_neighbor = graph[0][1]
-                if (clusters[node] == None) and (clusters[node_neighbor] == None):
-                    clusters[node] = i
-                    clusters[node_neighbor] = i
-                    i = i + 1
-                else:
-                    if clusters[node] != None and clusters[node_neighbor] == None:
-                        clusters[node_neighbor] = clusters[node]
-                    elif clusters[node_neighbor] != None and clusters[node] == None:
-                        clusters[node] = clusters[node_neighbor]
-                del graph[0]
-            try:
-                f = open("{}.csv".format(name_dbscan_clusters), "w")
-                for elem in clusters:
-                    f.write("{}, {}\n".format(elem, clusters[elem]))
-                f.close()
-                drop("v_temp_schema.{}".format(name_dbscan_clusters), method="table")
-                executeSQL(
-                    (
-                        f"CREATE LOCAL TEMPORARY TABLE {name_dbscan_clusters}"
-                        "(node_id int, cluster int) ON COMMIT PRESERVE ROWS"
-                    ),
-                    print_time_sql=False,
+            return f"(CASE WHEN {x} < {th} THEN {y0} ELSE {y1} END)"
+
+    def predict_sql(self, X: ArrayLike) -> str:
+        """
+        Returns the SQL code needed to deploy the bisecting
+        k-means model using its attributes.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The names or values of the input predictors.
+
+        Returns
+        -------
+        str
+            SQL code.
+        """
+        for c in self.clusters_:
+            if len(X) != len(c):
+                ValueError(
+                    "The length of parameter 'X' must be the same as "
+                    "the length of each cluster."
                 )
-                if isinstance(current_cursor(), vertica_python.vertica.cursor.Cursor):
-                    executeSQL(
-                        (
-                            f"COPY v_temp_schema.{name_dbscan_clusters}(node_id, cluster)"
-                            " FROM STDIN DELIMITER ',' ESCAPE AS '\\';"
-                        ),
-                        method="copy",
-                        print_time_sql=False,
-                        path=f"./{name_dbscan_clusters}.csv",
-                    )
-                else:
-                    executeSQL(
-                        """COPY v_temp_schema.{0}(node_id, cluster) 
-                           FROM LOCAL './{0}.csv' DELIMITER ',' ESCAPE AS '\\';""".format(
-                            name_dbscan_clusters
-                        ),
-                        print_time_sql=False,
-                    )
-                executeSQL("COMMIT;", print_time_sql=False)
-                os.remove(f"{name_dbscan_clusters}.csv")
-            except:
-                os.remove(f"{name_dbscan_clusters}.csv")
-                raise
-            self.n_cluster_ = i
-            executeSQL(
-                """CREATE TABLE {0} AS 
-                   SELECT 
-                        {1}, 
-                        COALESCE(cluster, -1) AS dbscan_cluster 
-                   FROM v_temp_schema.{2} AS x 
-                   LEFT JOIN v_temp_schema.{3} AS y 
-                   ON x.{4} = y.node_id""".format(
-                    self.name,
-                    ", ".join(self.X + self.key_columns),
-                    name_main,
-                    name_dbscan_clusters,
-                    index,
-                ),
-                title="Computing the DBSCAN Table [Step 2]",
-            )
-            self.n_noise_ = executeSQL(
-                "SELECT COUNT(*) FROM {0} WHERE dbscan_cluster = -1".format(self.name),
-                method="fetchfirstelem",
-                print_time_sql=False,
-            )
-        except:
-            drop(f"v_temp_schema.{name_main}", method="table")
-            drop(f"v_temp_schema.{name_dbscan_clusters}", method="table")
-            raise
-        drop(f"v_temp_schema.{name_main}", method="table")
-        drop(f"v_temp_schema.{name_dbscan_clusters}", method="table")
-        model_save = {
-            "type": "DBSCAN",
-            "input_relation": self.input_relation,
-            "key_columns": self.key_columns,
-            "X": self.X,
-            "p": self.parameters["p"],
-            "eps": self.parameters["eps"],
-            "min_samples": self.parameters["min_samples"],
-            "n_cluster": self.n_cluster_,
-            "n_noise": self.n_noise_,
-        }
-        insert_verticapy_schema(
-            model_name=self.name, model_type="DBSCAN", model_save=model_save,
+        clusters_distance = []
+        for c in self.clusters_:
+            list_tmp = []
+            for idx in range(len(X)):
+                list_tmp += [f"POWER({X[idx]} - {c[idx]}, {self.p_})"]
+            clusters_distance += [f"POWER({' + '.join(list_tmp)}, 1/{self.p_})"]
+        is_null_x = " OR ".join([f"{x} IS NULL" for x in X])
+        res = self._predict_tree_sql(
+            self.children_right_, self.children_left_, 0, clusters_distance
         )
-        return self
+        sql_final = f"""
+            (CASE 
+                WHEN {is_null_x} 
+                    THEN NULL 
+                ELSE {res} 
+            END)"""
+        return clean_query(sql_final)
+
+    # Trees Representation Methods.
 
-    # ---#
-    def predict(self):
+    def to_graphviz(
+        self,
+        round_score: int = 2,
+        percent: bool = False,
+        vertical: bool = True,
+        node_style: Optional[dict] = None,
+        arrow_style: Optional[dict] = None,
+        leaf_style: Optional[dict] = None,
+    ) -> str:
         """
-	---------------------------------------------------------------------------
-	Creates a vDataFrame of the model.
+        Returns the code for a Graphviz tree.
 
-	Returns
-	-------
-	vDataFrame
- 		the vDataFrame including the prediction.
-		"""
-        return vDataFrame(self.name)
+        Parameters
+        ----------
+        round_score: int, optional
+            The number of decimals to round the node's score to 0
+            rounds to an integer.
+        percent: bool, optional
+            If set to True, the scores are returned as a percent.
+        vertical: bool, optional
+            If  set to True,  the function  generates a  vertical
+            tree.
+        node_style: dict, optional
+            Dictionary  of options to customize each node of  the
+            tree. For a list of options, see the Graphviz API:
+            https://graphviz.org/doc/info/attrs.html
+        arrow_style: dict, optional
+            Dictionary  of options to customize each arrow of the
+            tree. For a list of options, see the Graphviz API:
+            https://graphviz.org/doc/info/attrs.html
+        leaf_style: dict, optional
+            Dictionary  of options to customize each leaf of  the
+            tree. For a list of options, see the Graphviz API:
+            https://graphviz.org/doc/info/attrs.html
+
+        Returns
+        -------
+        str
+            Graphviz code.
+        """
+        node_style, leaf_style = format_type(
+            node_style, leaf_style, dtype=dict, na_out={"shape": "none"}
+        )
+        arrow_style = format_type(arrow_style, dtype=dict)
+        n = len(self.children_left_)
+        vertical = ""
+        if not vertical:
+            position = '\ngraph [rankdir = "LR"];'
+        res = "digraph Tree{" + position
+        for i in range(n):
+            if (len(self.cluster_size_) == n) and (len(self.cluster_score_) == n):
+                if "bgcolor" in node_style and (
+                    self.children_left_[i] != self.children_right_[i]
+                ):
+                    color = node_style["bgcolor"]
+                elif "color" in node_style and (
+                    self.children_left_[i] != self.children_right_[i]
+                ):
+                    color = node_style["color"]
+                elif self.children_left_[i] != self.children_right_[i]:
+                    color = "#87cefa"
+                elif "bgcolor" in leaf_style:
+                    color = node_style["bgcolor"]
+                elif "color" in leaf_style:
+                    color = node_style["color"]
+                else:
+                    color = "#efc5b5"
+                label = (
+                    '<<table border="0" cellspacing="0"> <tr><td port="port1" '
+                    f'border="1" bgcolor="{color}"><b> cluster_id: {i} </b></td></tr>'
+                )
+                if len(self.cluster_size_) == n:
+                    label += '<tr><td port="port2" border="1" align="left">'
+                    label += f" size: {self.cluster_size_[i]} </td></tr>"
+                if len(self.cluster_score_) == n:
+                    val = (
+                        round(self.cluster_score_[i] * 100, round_score)
+                        if percent
+                        else round(self.cluster_score_[i], round_score)
+                    )
+                    if percent:
+                        val = str(val) + "%"
+                    label += '<tr><td port="port3" border="1" align="left"> '
+                    label += f"score: {val} </td></tr>"
+                label += "</table>>"
+            else:
+                label = f'"{i}"'
+            if self.children_left_[i] != self.children_right_[i]:
+                flat_dict_str = self._flat_dict(node_style)
+            else:
+                flat_dict_str = self._flat_dict(leaf_style)
+            res += f"\n{i} [label={label}{flat_dict_str}]"
+            if self.children_left_[i] != self.children_right_[i]:
+                res += f'\n{i} -> {self.children_left_[i]} [label=""{self._flat_dict(arrow_style)}]'
+                res += f'\n{i} -> {self.children_right_[i]} [label=""{self._flat_dict(arrow_style)}]'
+        return res + "\n}"
 
 
-# ---#
-class KMeans(Clustering):
+class KPrototypes(Clustering):
+    """
+    InMemoryModel implementation of KPrototypes.
+
+    Parameters
+    ----------
+    clusters: ArrayLike
+        List of the model's cluster centers.
+    p: int, optional
+        The p corresponding to  one of the p-distances.
+    gamma: float, optional
+        Weighting  factor  for  categorical columns.  This
+        determines  relative  importance of numerical  and
+        categorical attributes.
+    is_categorical: list / numpy.array, optional
+        ArrayLike  of booleans to indicate whether  X[idx]
+        is  a categorical  variable, where True  indicates
+        categorical  and  False numerical.  If empty,  all
+        the variables are considered categorical.
     """
----------------------------------------------------------------------------
-Creates a KMeans object using the Vertica k-means algorithm on the data. 
-k-means clustering is a method of vector quantization, originally from signal 
-processing, that aims to partition n observations into k clusters in which 
-each observation belongs to the cluster with the nearest mean (cluster centers 
-or cluster centroid), serving as a prototype of the cluster. This results in 
-a partitioning of the data space into Voronoi cells.
-
-Parameters
-----------
-name: str
-	Name of the the model. The model will be stored in the database.
-n_cluster: int, optional
-	Number of clusters
-init: str/list, optional
-	The method to use to find the initial cluster centers.
-		kmeanspp : Uses the KMeans++ method to initialize the centers.
-		random   : The initial centers.
-	It can be also a list with the initial cluster centers to use.
-max_iter: int, optional
-	The maximum number of iterations the algorithm performs.
-tol: float, optional
-	Determines whether the algorithm has converged. The algorithm is considered 
-	converged after no center has moved more than a distance of 'tol' from the 
-	previous iteration.
-	"""
+
+    # Properties.
+
+    @property
+    def object_type(self) -> Literal["KPrototypes"]:
+        return "KPrototypes"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["clusters_", "p_", "gamma_", "is_categorical_"]
+
+    # System & Special Methods.
 
     def __init__(
         self,
-        name: str,
-        n_cluster: int = 8,
-        init: str = "kmeanspp",
-        max_iter: int = 300,
-        tol: float = 1e-4,
-    ):
-        check_types([("name", name, [str])])
-        self.type, self.name = "KMeans", name
-        self.set_params(
-            {
-                "n_cluster": n_cluster,
-                "init": init.lower() if isinstance(init, str) else init,
-                "max_iter": max_iter,
-                "tol": tol,
-            }
-        )
-        version(condition=[8, 0, 0])
+        clusters: ArrayLike,
+        p: int = 2,
+        gamma: float = 1.0,
+        is_categorical: Optional[ArrayLike] = None,
+    ) -> None:
+        is_categorical = format_type(is_categorical, dtype=list)
+        self.clusters_ = np.array(clusters)
+        self.p_ = p
+        self.gamma_ = gamma
+        self.is_categorical_ = np.array(is_categorical)
+
+    # Prediction / Transformation Methods - IN MEMORY.
 
-    # ---#
-    def plot_voronoi(
-        self, max_nb_points: int = 50, plot_crosses: bool = True, ax=None, **style_kwds
-    ):
+    def _transform_row(self, X: ArrayLike) -> list:
+        """
+        Transforms and returns the distance to each cluster
+        for one row.
         """
-    ---------------------------------------------------------------------------
-    Draws the Voronoi Graph of the model.
+        distance = []
+        for centroid in self.clusters_:
+            distance_num, distance_cat = 0, 0
+            for idx in range(len(X)):
+                val, centroid_val = X[idx], centroid[idx]
+                try:
+                    val = float(val)
+                    centroid_val = float(centroid_val)
+                except (TypeError, ValueError):
+                    pass
+                if isinstance(centroid_val, str) or isinstance(centroid_val, NoneType):
+                    distance_cat += abs(int(val == centroid_val) - 1)
+                else:
+                    distance_num += (val - centroid_val) ** self.p_
+            distance_final = distance_num + self.gamma_ * distance_cat
+            distance += [distance_final]
+        return distance
 
-    Parameters
-    ----------
-    max_nb_points: int, optional
-        Maximum number of points to display.
-    plot_crosses: bool, optional
-        If set to True, the centers are represented by white crosses.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    Figure
-        Matplotlib Figure
+    def transform(self, X: ArrayLike) -> np.ndarray:
         """
-        if len(self.X) == 2:
-            from verticapy.learn.mlplot import voronoi_plot
+        Transforms and returns the distance to each cluster.
 
-            query = "SELECT GET_MODEL_ATTRIBUTE(USING PARAMETERS model_name = '{}', attr_name = 'centers')".format(
-                self.name
-            )
-            clusters = executeSQL(query, print_time_sql=False, method="fetchall")
-            return voronoi_plot(
-                clusters=clusters,
-                columns=self.X,
-                input_relation=self.input_relation,
-                plot_crosses=plot_crosses,
-                ax=ax,
-                max_nb_points=max_nb_points,
-                **style_kwds,
-            )
+        Parameters
+        ----------
+        X: ArrayLike
+            The data on which to make the transformation.
+
+        Returns
+        -------
+        numpy.array
+            Transformed values.
+        """
+        return np.apply_along_axis(self._transform_row, 1, X)
+
+    # Prediction / Transformation Methods - IN DATABASE.
+
+    def transform_sql(self, X: ArrayLike) -> list[str]:
+        """
+        Transforms and returns the SQL distance to each cluster.
+
+        Parameters
+        ----------
+        X: ArrayLike
+            The names or values of the input predictors.
+
+        Returns
+        -------
+        list
+            SQL code.
+        """
+        if len(self.is_categorical_) == 0:
+            is_categorical = np.array([True for i in range(len(X))])
         else:
-            raise Exception("Voronoi Plots are only available in 2D")
+            is_categorical = copy.deepcopy(self.is_categorical_)
+
+        for c in self.clusters_:
+            if not len(X) == len(c) == len(is_categorical):
+                raise ValueError(
+                    "The length of parameter 'X' must be the same as "
+                    "the length of each cluster AND the categorical vector."
+                )
+        clusters_distance = []
+        for c in self.clusters_:
+            clusters_distance_num, clusters_distance_cat = [], []
+            for idx in range(len(X)):
+                if is_categorical[idx]:
+                    c_i = str(c[idx]).replace("'", "''")
+                    clusters_distance_cat += [f"ABS(({X[idx]} = '{c_i}')::int - 1)"]
+                else:
+                    clusters_distance_num += [f"POWER({X[idx]} - {c[idx]}, {self.p_})"]
+            final_cluster_distance = ""
+            if clusters_distance_num:
+                final_cluster_distance += (
+                    f"POWER({' + '.join(clusters_distance_num)}, 1 / {self.p_})"
+                )
+            if clusters_distance_cat:
+                if clusters_distance_num:
+                    final_cluster_distance += " + "
+                final_cluster_distance += (
+                    f"{self.gamma_} * ({' + '.join(clusters_distance_cat)})"
+                )
+            clusters_distance += [final_cluster_distance]
+        return clusters_distance
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/learn/delphi.py` & `verticapy-1.0.0b1/verticapy/machine_learning/vertica/automl/supervised.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,725 +1,281 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# Standard Python Modules
-import random, datetime
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import copy
+from typing import Literal, Optional, Union
+from tqdm.auto import tqdm
 import numpy as np
-from typing import Union
 
-# VerticaPy Modules
-from verticapy import vDataFrame
-from verticapy.utilities import *
-from verticapy.toolbox import *
-from verticapy.errors import *
-from verticapy.learn.ensemble import *
-from verticapy.learn.naive_bayes import *
-from verticapy.learn.linear_model import *
-from verticapy.learn.decomposition import *
-from verticapy.learn.cluster import *
-from verticapy.learn.neighbors import *
-from verticapy.learn.svm import *
-from verticapy.learn.mlplot import plot_bubble_ml
-from verticapy.learn.vmodel import *
-from verticapy.learn.tools import get_model_category
-
-
-class vAuto(vModel):
-    # ---#
-    def set_params(self, parameters: dict = {}):
-        """
-    ---------------------------------------------------------------------------
-    Sets the parameters of the model.
+import verticapy._config.config as conf
+from verticapy._typing import (
+    PlottingObject,
+    PythonScalar,
+    NoneType,
+    SQLRelation,
+    SQLColumns,
+)
+from verticapy._utils._gen import gen_tmp_name
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import format_type, schema_relation
+from verticapy._utils._sql._vertica_version import vertica_version
+
+
+from verticapy.core.tablesample.base import TableSample
+from verticapy.core.vdataframe.base import vDataFrame
+
+import verticapy.machine_learning.memmodel as mm
+from verticapy.machine_learning.model_selection.hp_tuning.cv import grid_search_cv
+from verticapy.machine_learning.model_selection.hp_tuning.param_gen import (
+    gen_params_grid,
+)
+from verticapy.machine_learning.model_selection.variables_selection import stepwise
+from verticapy.machine_learning.vertica.automl.dataprep import AutoDataPrep
+from verticapy.machine_learning.vertica.base import VerticaModel
+from verticapy.machine_learning.vertica.cluster import NearestCentroid
+from verticapy.machine_learning.vertica.ensemble import (
+    RandomForestRegressor,
+    RandomForestClassifier,
+    XGBClassifier,
+    XGBRegressor,
+)
+from verticapy.machine_learning.vertica.naive_bayes import NaiveBayes
+from verticapy.machine_learning.vertica.linear_model import (
+    LogisticRegression,
+    LinearRegression,
+    ElasticNet,
+    Lasso,
+    Ridge,
+)
+from verticapy.machine_learning.vertica.neighbors import (
+    KNeighborsClassifier,
+    KNeighborsRegressor,
+)
+from verticapy.machine_learning.vertica.svm import LinearSVC, LinearSVR
 
-    Parameters
-    ----------
-    parameters: dict, optional
-        New parameters.
-        """
-        for elem in self.parameters:
-            if elem not in parameters:
-                parameters[elem] = self.parameters[elem]
-        self.__init__(self.name, **parameters)
 
-
-class AutoDataPrep(vAuto):
-    """
----------------------------------------------------------------------------
-Automatically find relations between the different features to preprocess
-the data according to each column type.
-
-Parameters
-----------
-name: str, optional
-    Name of the model in which to store the output relation in the
-    Vertica database.
-cat_method: str, optional
-    Method for encoding categorical features. This can be set to 'label' for
-    label encoding and 'ooe' for One-Hot Encoding.
-num_method: str, optional
-    [Only used for non-time series datasets]
-    Method for encoding numerical features. This can be set to 'same_freq' to
-    encode using frequencies, 'same_width' to encode using regular bins, or
-    'none' to not encode numerical features.
-nbins: int, optional
-    [Only used for non-time series datasets]
-    Number of bins used to discretize numerical features.
-outliers_threshold: float, optional
-    [Only used for non-time series datasets]
-    How to deal with outliers. If a number is used, all elements with an absolute 
-    z-score greater than the threshold will be converted to NULL values. Otherwise,
-    outliers are treated as regular values.
-na_method: str, optional
-    Method for handling missing values.
-        auto: Mean for the numerical features and creates a new category for the 
-              categorical vColumns. For time series datasets, 'constant' interpolation 
-              is used for categorical features and 'linear' for the others.
-        drop: Drops the missing values.
-cat_topk: int, optional
-    Keeps the top-k most frequent categories and merges the others into one unique 
-    category. If unspecified, all categories are kept.
-normalize: bool, optional
-    If True, the data will be normalized using the z-score. The 'num_method' parameter
-    must be set to 'none'.
-normalize_min_cat: int, optional
-    Minimum feature cardinality before using normalization.
-id_method: str, optional
-    Method for handling ID features.
-        drop: Drops any feature detected as ID.
-        none: Does not change ID features.
-apply_pca: bool, optional
-    [Only used for non-time series datasets]
-    If True, a PCA is applied at the end of the preprocessing.
-rule: str / time, optional
-    [Only used for time series datasets]
-    Interval to use to slice the time. For example, '5 minutes' will create records
-    separated by '5 minutes' time interval.
-    If set to auto, the rule will be detected using aggregations.
-identify_ts: bool, optional
-    If True and parameter 'ts' is undefined when fitting the model, the function will
-    try to automatically detect the parameter 'ts'.
-save: bool, optional
-    If True, saves the final relation inside the database.
-
-Attributes
-----------
-X_in: list
-    Variables used to fit the model.
-X_out: list
-    Variables created by the model.
-ts: str
-    TS component.
-by: list
-    vcolumns used in the partition.
-sql_: str
-    SQL needed to deploy the model.
-final_relation_: vDataFrame
-    Relation created after fitting the model.
+class AutoML(VerticaModel):
     """
-
-    # ---#
-    def __init__(
-        self,
-        name: str = "",
-        cat_method: str = "ooe",
-        num_method: str = "none",
-        nbins: int = 20,
-        outliers_threshold: float = 4.0,
-        na_method: str = "auto",
-        cat_topk: int = 10,
-        normalize: bool = True,
-        normalize_min_cat: int = 6,
-        id_method: int = "drop",
-        apply_pca: bool = False,
-        rule: Union[str, datetime.timedelta] = "auto",
-        identify_ts: bool = True,
-        save: bool = True,
-    ):
-        check_types(
-            [
-                ("name", name, [str]),
-                ("cat_method", cat_method, ["label", "ooe"]),
-                ("num_method", num_method, ["same_freq", "same_width", "none"]),
-                ("nbins", nbins, [int, float]),
-                ("outliers_threshold", outliers_threshold, [int, float]),
-                ("na_method", na_method, ["auto", "drop"]),
-                ("cat_topk", cat_topk, [int, float]),
-                ("rule", rule, [str, datetime.timedelta]),
-                ("normalize", normalize, [bool]),
-                ("id_method", id_method, ["none", "drop"]),
-                ("apply_pca", apply_pca, [bool]),
-                ("normalize_min_cat", normalize_min_cat, [int, float]),
-                ("identify_ts", identify_ts, [bool]),
-                ("save", save, [bool]),
-            ]
-        )
-        self.type, self.name = "AutoDataPrep", name
-        if not (self.name):
-            self.name = gen_tmp_name(
-                schema=verticapy.options["temp_schema"], name="autodataprep"
-            )
-        self.parameters = {
-            "cat_method": cat_method,
-            "num_method": num_method,
-            "nbins": nbins,
-            "outliers_threshold": outliers_threshold,
-            "na_method": na_method,
-            "cat_topk": cat_topk,
-            "rule": rule,
-            "normalize": normalize,
-            "normalize_min_cat": normalize_min_cat,
-            "apply_pca": apply_pca,
-            "id_method": id_method,
-            "identify_ts": identify_ts,
-            "save": save,
-        }
-
-    # ---#
-    def fit(
-        self,
-        input_relation: Union[str, vDataFrame],
-        X: list = [],
-        ts: str = "",
-        by: list = [],
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Trains the model.
+    Tests multiple models to find those that maximize
+    the input score.
 
     Parameters
     ----------
-    input_relation: str/vDataFrame
-        Training Relation.
-    X: list, optional
-        List of the features to preprocess.
-    ts: str, optional
-        Time series vcolumn to use to order the data. The vcolumn type must be
-        date-like (date, datetime, timestamp...)
-    by: list, optional
-        vcolumns used in the partition.
-
-    Returns
-    -------
-    object
-        the cleaned relation
-        """
-        if verticapy.options["overwrite_model"]:
-            self.drop()
-        else:
-            does_model_exist(name=self.name, raise_error=True)
-        current_print_info = verticapy.options["print_info"]
-        verticapy.options["print_info"] = False
-        assert not (by) or (ts), ParameterError(
-            "Parameter 'by' must be empty if 'ts' is not defined."
-        )
-        if isinstance(input_relation, str):
-            vdf = vDataFrameSQL(input_relation)
-        else:
-            vdf = input_relation.copy()
-        if not (X):
-            X = vdf.get_columns()
-        if not (ts) and self.parameters["identify_ts"]:
-            nb_date, nb_num, nb_others = 0, 0, 0
-            for elem in X:
-                if vdf[elem].isnum() and not (vdf[elem].isbool()):
-                    nb_num += 1
-                elif vdf[elem].isdate():
-                    nb_date += 1
-                    ts_tmp = elem
-                else:
-                    nb_others += 1
-                    cat_tmp = elem
-            if nb_date == 1 and nb_others <= 1:
-                ts = ts_tmp
-            if nb_date == 1 and nb_others == 1:
-                by = [cat_tmp]
-        vdf.are_namecols_in(X)
-        X = vdf.format_colnames(X)
-        X_diff = vdf.get_columns(exclude_columns=X)
-        columns_to_drop = []
-        n = vdf.shape()[0]
-        for elem in X:
-            is_id = (
-                not (vdf[elem].isnum())
-                and not (vdf[elem].isdate())
-                and 0.9 * n <= vdf[elem].nunique()
-            )
-            if (
-                self.parameters["id_method"] == "drop"
-                and is_id
-                and (not (by) or elem not in by)
-            ):
-                columns_to_drop += [elem]
-                X_diff += [elem]
-            elif not (is_id) and (not (by) or elem not in by):
-                if not (vdf[elem].isdate()):
-                    if vdf[elem].isnum():
-                        if (self.parameters["outliers_threshold"]) and self.parameters[
-                            "outliers_threshold"
-                        ] > 0:
-                            vdf[elem].fill_outliers(
-                                method="null",
-                                threshold=self.parameters["outliers_threshold"],
-                            )
-                        if (
-                            self.parameters["num_method"] == "none"
-                            and (self.parameters["normalize"])
-                            and (
-                                self.parameters["normalize_min_cat"] < 2
-                                or (
-                                    vdf[elem].nunique()
-                                    > self.parameters["normalize_min_cat"]
-                                )
-                            )
-                        ):
-                            vdf[elem].normalize(method="zscore")
-                        if self.parameters["na_method"] == "auto":
-                            vdf[elem].fillna(method="mean")
-                        else:
-                            vdf[elem].dropna()
-                    if (
-                        vdf[elem].isnum()
-                        and not (ts)
-                        and self.parameters["num_method"] in ("same_width", "same_freq")
-                    ):
-                        vdf[elem].discretize(
-                            method=self.parameters["num_method"],
-                            nbins=self.parameters["nbins"],
-                        )
-                    elif vdf[elem].nunique() > self.parameters["cat_topk"] and not (
-                        vdf[elem].isnum()
-                    ):
-                        if self.parameters["na_method"] == "auto":
-                            vdf[elem].fillna("NULL")
-                        else:
-                            vdf[elem].dropna()
-                        vdf[elem].discretize(
-                            method="topk", k=self.parameters["cat_topk"]
-                        )
-                    if (
-                        self.parameters["cat_method"] == "ooe"
-                        and not (vdf[elem].isnum())
-                    ) or (
-                        vdf[elem].isnum()
-                        and not (ts)
-                        and self.parameters["num_method"] in ("same_width", "same_freq")
-                    ):
-                        vdf[elem].get_dummies(drop_first=False)
-                        columns_to_drop += [elem]
-                    elif (
-                        self.parameters["cat_method"] == "label"
-                        and not (vdf[elem].isnum())
-                    ) or (
-                        vdf[elem].isnum()
-                        and not (ts)
-                        and self.parameters["num_method"] in ("same_width", "same_freq")
-                    ):
-                        vdf[elem].label_encode()
-                elif not (ts):
-                    vdf[elem.replace('"', "") + "_year"] = f"YEAR({elem})"
-                    vdf[elem.replace('"', "") + "_dayofweek"] = f"DAYOFWEEK({elem})"
-                    vdf[elem.replace('"', "") + "_month"] = f"MONTH({elem})"
-                    vdf[elem.replace('"', "") + "_hour"] = f"HOUR({elem})"
-                    vdf[elem.replace('"', "") + "_quarter"] = f"QUARTER({elem})"
-                    vdf[
-                        elem.replace('"', "") + "_trend"
-                    ] = f"({elem}::timestamp - MIN({elem}::timestamp) OVER ()) / '1 second'::interval"
-                    columns_to_drop += [elem]
-        if columns_to_drop:
-            vdf.drop(columns_to_drop)
-        if ts:
-            vdf.are_namecols_in([ts] + by)
-            ts = vdf.format_colnames(ts)
-            by = vdf.format_colnames(by)
-            if self.parameters["rule"] == "auto":
-                vdf_tmp = vdf[[ts] + by]
-                by_tmp = "PARTITION BY {} ".format(", ".join(by)) if (by) else ""
-                vdf_tmp[
-                    "verticapy_time_delta"
-                ] = f"({ts}::timestamp - (LAG({ts}) OVER ({by_tmp}ORDER BY {ts}))::timestamp) / '00:00:01'"
-                vdf_tmp = vdf_tmp.groupby(["verticapy_time_delta"], ["COUNT(*) AS cnt"])
-                rule = executeSQL(
-                    "SELECT verticapy_time_delta FROM {} ORDER BY cnt DESC LIMIT 1".format(
-                        vdf_tmp.__genSQL__()
-                    ),
-                    method="fetchfirstelem",
-                    print_time_sql=False,
-                )
-                rule = datetime.timedelta(seconds=rule)
-            method = {}
-            X_tmp = []
-            for elem in X:
-                if elem != ts and elem not in by:
-                    if vdf[elem].isnum() and not (vdf[elem].isbool()):
-                        method[elem] = "linear"
-                    else:
-                        method[elem] = "ffill"
-            vdf = vdf.interpolate(ts=ts, rule=rule, method=method, by=by)
-            vdf.dropna()
-        self.X_in = [elem for elem in X]
-        self.X_out = vdf.get_columns(
-            exclude_columns=by + [ts] + X_diff if ts else by + X_diff
-        )
-        self.by = by
-        self.ts = ts
-        if self.parameters["apply_pca"] and not (ts):
-            model_pca = PCA(self.name + "_pca")
-            model_pca.drop()
-            model_pca.fit(vdf, self.X_out)
-            vdf = model_pca.transform()
-            self.X_out = vdf.get_columns(
-                exclude_columns=by + [ts] + X_diff if ts else by + X_diff
-            )
-        self.sql_ = vdf.__genSQL__()
-        if self.parameters["save"]:
-            vdf.to_db(name=self.name, relation_type="table", inplace=True)
-        self.final_relation_ = vdf
-        verticapy.options["print_info"] = current_print_info
-        return self.final_relation_
+    name: str
+        Name of the model.
+    estimator: list / 'native' / 'all' / 'fast' / object
+        List  of Vertica  estimators with a fit  method.
+        Alternatively,  you can specify 'native' for all
+        native  Vertica models, 'all' for all  VerticaPy
+        models, and 'fast' for quick modeling.
+    estimator_type: str, optional
+        Estimator Type.
+            auto      : Automatically     detects     the
+                        estimator type.
+            regressor : The estimator  is  used  to
+                        perform a regression.
+            binary    : The  estimator is used  to
+                        perform  a binary classification.
+            multi     : The  estimator  is  used  to
+                        perform  a multiclass
+                        classification.
+    metric: str, optional
+        Metric used for the model evaluation.
+            auto: logloss for  classification & RMSE for
+                  regression.
+        For Classification:
+            accuracy    : Accuracy
+            auc         : Area Under the Curve
+                          (ROC)
+            ba          : Balanced Accuracy
+                          = (tpr + tnr) / 2
+            bm          : Informedness
+                          = tpr + tnr - 1
+            csi         : Critical Success Index
+                          = tp / (tp + fn + fp)
+            f1          : F1 Score
+            fdr         : False Discovery Rate = 1 - ppv
+            fm          : FowlkesMallows index
+                          = sqrt(ppv * tpr)
+            fnr         : False Negative Rate
+                          = fn / (fn + tp)
+            for         : False Omission Rate = 1 - npv
+            fpr         : False Positive Rate
+                          = fp / (fp + tn)
+            logloss     : Log Loss
+            lr+         : Positive Likelihood Ratio
+                          = tpr / fpr
+            lr-         : Negative Likelihood Ratio
+                          = fnr / tnr
+            dor         : Diagnostic Odds Ratio
+            mcc         : Matthews Correlation Coefficient
+            mk          : Markedness
+                          = ppv + npv - 1
+            npv         : Negative Predictive Value
+                          = tn / (tn + fn)
+            prc_auc     : Area Under the Curve
+                          (PRC)
+            precision   : Precision
+                          = tp / (tp + fp)
+            pt          : Prevalence Threshold
+                          = sqrt(fpr) / (sqrt(tpr) + sqrt(fpr))
+            recall      : Recall
+                          = tp / (tp + fn)
+            specificity : Specificity
+                          = tn / (tn + fp)
+        For Regression:
+            max    : Max error
+            mae    : Mean absolute error
+            median : Median absolute error
+            mse    : Mean squared error
+            msle   : Mean squared log error
+            r2     : R-squared coefficient
+            r2a    : R2 adjusted
+            rmse   : Root-mean-squared error
+            var    : Explained variance
+    cv: int, optional
+        Number of folds.
+    pos_label: PythonScalar, optional
+        The main class to  be considered as positive
+        (classification only).
+    cutoff: float, optional
+        The model cutoff (classification only).
+    nbins: int, optional
+        Number of bins used to compute the different
+        parameter categories.
+    lmax: int, optional
+        Maximum length of each parameter list.
+    optimized_grid: int, optional
+        If set to zero, the randomness is based on the
+        input parameters.
+        If set to one, the randomness  is limited  to
+        some parameters while others are picked based
+        on a default grid.
+        If  set to two, no randomness is used  and  a
+        default grid is returned.
+    stepwise: bool, optional
+        If True, the stepwise algorithm is used to
+        determine the final model list of parameters.
+    stepwise_criterion: str, optional
+        Criterion used when performing the final
+        estimator stepwise.
+            aic : Akaikes information criterion
+            bic : Bayesian information criterion
+    stepwise_direction: str, optional
+        Direction to start the stepwise search,
+        either 'backward' or 'forward'.
+    stepwise_max_steps: int, optional
+        The maximum number of steps to be considered
+        when performing the final estimator stepwise.
+    x_order: str, optional
+        Method for preprocessing  X before using the
+        stepwise algorithm.
+            pearson  : X  is ordered  based  on  the
+                       Pearson's         correlation
+                       coefficient.
+            spearman : X   is   ordered   based   on
+                       Spearman's  rank  correlation
+                       coefficient.
+            random   : Shuffles the  vector X before
+                       applying     the     stepwise
+                       algorithm.
+            none     : Does not  change the order of
+                       X.
+    preprocess_data: bool, optional
+        If True, the data will be preprocessed.
+    preprocess_dict: dict, optional
+        Dictionary to pass to the AutoDataPrep class
+        in  order to preprocess the data before
+        clustering.
+    print_info: bool
+        If  True,  prints the model  information  at
+        each step.
 
-
-class AutoClustering(vAuto):
+    Attributes
+    ----------
+    preprocess_: object
+        Model used to preprocess the data.
+    best_model_: object
+        Most  efficient   models  found  during  the
+        search.
+    model_grid_ : TableSample
+        Grid   containing   the   different   models
+        information.
     """
----------------------------------------------------------------------------
-Automatically creates k different groups with which to generalize the data.
 
-Parameters
-----------
-name: str
-    Name of the model.
-n_cluster: int, optional
-    Number of clusters. If empty, an optimal number of clusters will be
-    determined using multiple k-means models.
-init: str/list, optional
-    The method for finding the initial cluster centers.
-        kmeanspp : Uses the k-means++ method to initialize the centers.
-        random   : Randomly subsamples the data to find initial centers.
-    Alternatively, you can specify a list with the initial custer centers.
-max_iter: int, optional
-    The maximum number of iterations for the algorithm.
-tol: float, optional
-    Determines whether the algorithm has converged. The algorithm is considered 
-    converged after no center has moved more than a distance of 'tol' from the 
-    previous iteration.
-preprocess_data: bool, optional
-    If True, the data will be preprocessed.
-preprocess_dict: dict, optional
-    Dictionary to pass to the AutoDataPrep class in order to 
-    preprocess the data before the clustering.
-print_info: bool
-    If True, prints the model information at each step.
-
-Attributes
-----------
-preprocess_: object
-    Model used to preprocess the data.
-model_: object
-    Final model used for the clustering.
-    """
+    # Properties.
 
-    # ---#
-    def __init__(
-        self,
-        name: str,
-        n_cluster: int = None,
-        init: str = "kmeanspp",
-        max_iter: int = 300,
-        tol: float = 1e-4,
-        preprocess_data: bool = True,
-        preprocess_dict: dict = {
-            "identify_ts": False,
-            "normalize_min_cat": 0,
-            "outliers_threshold": 3.0,
-            "na_method": "drop",
-        },
-        print_info: bool = True,
-    ):
-        check_types(
-            [
-                ("name", name, [str]),
-                ("n_cluster", n_cluster, [int]),
-                ("init", init, [str, list]),
-                ("max_iter", max_iter, [int]),
-                ("tol", tol, [float]),
-                ("preprocess_data", preprocess_data, [bool]),
-                ("preprocess_dict", preprocess_dict, [dict]),
-                ("print_info", print_info, [bool]),
-            ]
-        )
-        self.type, self.name = "AutoClustering", name
-        self.parameters = {
-            "n_cluster": n_cluster,
-            "init": init,
-            "max_iter": max_iter,
-            "tol": tol,
-            "print_info": print_info,
-            "preprocess_data": preprocess_data,
-            "preprocess_dict": preprocess_dict,
-        }
+    @property
+    def _is_native(self) -> Literal[False]:
+        return False
 
-    # ---#
-    def fit(self, input_relation: Union[str, vDataFrame], X: list = []):
-        """
-    ---------------------------------------------------------------------------
-    Trains the model.
+    @property
+    def _vertica_fit_sql(self) -> Literal[""]:
+        return ""
 
-    Parameters
-    ----------
-    input_relation: str/vDataFrame
-        Training Relation.
-    X: list, optional
-        List of the predictors.
-
-    Returns
-    -------
-    object
-        clustering model
-        """
-        if verticapy.options["overwrite_model"]:
-            self.drop()
-        else:
-            does_model_exist(name=self.name, raise_error=True)
-        if self.parameters["print_info"]:
-            print(f"\033[1m\033[4mStarting AutoClustering\033[0m\033[0m\n")
-        if self.parameters["preprocess_data"]:
-            model_preprocess = AutoDataPrep(**self.parameters["preprocess_dict"])
-            input_relation = model_preprocess.fit(input_relation, X=X)
-            X = [elem for elem in model_preprocess.X_out]
-            self.preprocess_ = model_preprocess
-        else:
-            self.preprocess_ = None
-        if not (self.parameters["n_cluster"]):
-            if self.parameters["print_info"]:
-                print(
-                    f"\033[1m\033[4mFinding a suitable number of clusters\033[0m\033[0m\n"
-                )
-            self.parameters["n_cluster"] = best_k(
-                input_relation=input_relation,
-                X=X,
-                n_cluster=(1, 100),
-                init=self.parameters["init"],
-                max_iter=self.parameters["max_iter"],
-                tol=self.parameters["tol"],
-                elbow_score_stop=0.9,
-                tqdm=self.parameters["print_info"],
-            )
-        if self.parameters["print_info"]:
-            print(f"\033[1m\033[4mBuilding the Final Model\033[0m\033[0m\n")
-        if verticapy.options["tqdm"] and self.parameters["print_info"]:
-            from tqdm.auto import tqdm
+    @property
+    def _vertica_predict_sql(self) -> Literal[""]:
+        return ""
 
-            loop = tqdm(range(1))
-        else:
-            loop = range(1)
-        for i in loop:
-            self.model_ = KMeans(
-                self.name,
-                n_cluster=self.parameters["n_cluster"],
-                init=self.parameters["init"],
-                max_iter=self.parameters["max_iter"],
-                tol=self.parameters["tol"],
-            )
-            self.model_.fit(input_relation, X=X)
-        return self.model_
+    @property
+    def _model_category(self) -> Literal["SUPERVISED"]:
+        return "SUPERVISED"
 
+    @property
+    def _model_subcategory(self) -> Literal[""]:
+        return ""
 
-class AutoML(vAuto):
-    """
----------------------------------------------------------------------------
-Tests multiple models to find those that maximize the input score.
+    @property
+    def _model_type(self) -> Literal["AutoML"]:
+        return "AutoML"
 
-Parameters
-----------
-name: str
-    Name of the model.
-estimator: list / 'native' / 'all' / 'fast' / object
-    List of Vertica estimators with a fit method.
-    Alternatively, you can specify 'native' for all native Vertica models,
-    'all' for all VerticaPy models and 'fast' for quick modeling.
-estimator_type: str, optional
-    Estimator Type.
-        auto      : Automatically detects the estimator type.
-        regressor : The estimator will be used to perform a regression.
-        binary    : The estimator will be used to perform a binary classification.
-        multi     : The estimator will be used to perform a multiclass classification.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-pos_label: int/float/str, optional
-    The main class to be considered as positive (classification only).
-cutoff: float, optional
-    The model cutoff (classification only).
-nbins: int, optional
-    Number of bins used to compute the different parameter categories.
-lmax: int, optional
-    Maximum length of each parameter list.
-optimized_grid: int, optional
-    If set to 0, the randomness is based on the input parameters.
-    If set to 1, the randomness is limited to some parameters while others
-    are picked based on a default grid.
-    If set to 2, no randomness is used and a default grid is returned.
-stepwise: bool, optional
-    If True, the stepwise algorithm will be used to determine the
-    final model list of parameters.
-stepwise_criterion: str, optional
-    Criterion used when doing the final estimator stepwise.
-        aic : Akaikes information criterion
-        bic : Bayesian information criterion
-stepwise_direction: str, optional
-    Which direction to start the stepwise search. Can be done 'backward' or 'forward'.
-stepwise_max_steps: int, optional
-    The maximum number of steps to be considered when doing the final estimator 
-    stepwise.
-x_order: str, optional
-    Method for preprocessing X before using the stepwise algorithm.
-        pearson  : X is ordered based on the Pearson's correlation coefficient.
-        spearman : X is ordered based on Spearman's rank correlation coefficient.
-        random   : Shuffles the vector X before applying the stepwise algorithm.
-        none     : Does not change the order of X.
-preprocess_data: bool, optional
-    If True, the data will be preprocessed.
-preprocess_dict: dict, optional
-    Dictionary to pass to the AutoDataPrep class in order to 
-    preprocess the data before the clustering.
-print_info: bool
-    If True, prints the model information at each step.
-
-Attributes
-----------
-preprocess_: object
-    Model used to preprocess the data.
-best_model_: object
-    Most efficient models found during the search.
-model_grid_ : tablesample
-    Grid containing the different models information.
-    """
+    @property
+    def _attributes(self) -> list[str]:
+        return ["preprocess_", "best_model_", "model_grid_"]
 
-    # ---#
+    # System & Special Methods.
+
+    @save_verticapy_logs
     def __init__(
         self,
         name: str,
         estimator: Union[list, str] = "fast",
-        estimator_type: str = "auto",
+        estimator_type: Literal["auto", "regressor", "binary", "multi"] = "auto",
         metric: str = "auto",
         cv: int = 3,
-        pos_label: Union[int, float, str] = None,
+        pos_label: Optional[PythonScalar] = None,
         cutoff: float = -1,
         nbins: int = 100,
         lmax: int = 5,
         optimized_grid: int = 2,
         stepwise: bool = True,
-        stepwise_criterion: str = "aic",
-        stepwise_direction: str = "backward",
+        stepwise_criterion: Literal["aic", "bic"] = "aic",
+        stepwise_direction: Literal["forward", "backward"] = "backward",
         stepwise_max_steps: int = 100,
-        stepwise_x_order: str = "pearson",
+        stepwise_x_order: Literal["pearson", "spearman", "random", "none"] = "pearson",
         preprocess_data: bool = True,
         preprocess_dict: dict = {"identify_ts": False},
         print_info: bool = True,
-    ):
-        check_types(
-            [
-                ("name", name, [str]),
-                ("estimator_type", estimator_type, [str]),
-                ("metric", metric, [str]),
-                ("cv", cv, [int]),
-                ("pos_label", pos_label, [int, float, str]),
-                ("cutoff", cutoff, [int, float]),
-                ("nbins", nbins, [int]),
-                ("optimized_grid", optimized_grid, [int]),
-                ("print_info", print_info, [bool]),
-                ("stepwise", stepwise, [bool]),
-                ("stepwise_criterion", stepwise_criterion, ["aic", "bic"]),
-                ("stepwise_direction", stepwise_direction, ["forward", "backward"]),
-                ("stepwise_max_steps", stepwise_max_steps, [int, float]),
-                (
-                    "stepwise_x_order",
-                    stepwise_x_order,
-                    ["pearson", "spearman", "random", "none"],
-                ),
-                ("preprocess_data", preprocess_data, [bool]),
-                ("preprocess_dict", preprocess_dict, [dict]),
-            ]
-        )
-        assert optimized_grid in [0, 1, 2], ParameterError(
-            "Optimized Grid must be an integer between 0 and 2."
-        )
-        self.type, self.name = "AutoML", name
+    ) -> None:
+        if optimized_grid not in [0, 1, 2]:
+            raise ValueError("Optimized Grid must be an integer between 0 and 2.")
+        self.model_name = name
         self.parameters = {
             "estimator": estimator,
             "estimator_type": estimator_type,
             "metric": metric,
             "cv": cv,
             "pos_label": pos_label,
             "cutoff": cutoff,
@@ -732,128 +288,172 @@
             "stepwise_direction": stepwise_direction,
             "stepwise_max_steps": stepwise_max_steps,
             "stepwise_x_order": stepwise_x_order,
             "preprocess_data": preprocess_data,
             "preprocess_dict": preprocess_dict,
         }
 
-    # ---#
-    def fit(self, input_relation: Union[str, vDataFrame], X: list = [], y: str = ""):
+    # Attributes Methods.
+
+    def get_vertica_attributes(self, attr_name: Optional[str] = None) -> TableSample:
         """
-    ---------------------------------------------------------------------------
-    Trains the model.
+        Returns the model attribute.
 
-    Parameters
-    ----------
-    input_relation: str/vDataFrame
-        Training Relation.
-    X: list, optional
-        List of the predictors.
-    y: str, optional
-        Response column.
-    Returns
-    -------
-    object
-        model grid
+        Parameters
+        ----------
+        attr_name: str, optional
+            Attribute Name.
+
+        Returns
+        -------
+        TableSample
+            model attributes.
+        """
+        return self.best_model_.get_vertica_attributes(attr_name)
+
+    # I/O Methods.
+
+    def deploySQL(self, X: Optional[SQLColumns] = None) -> str:
+        """
+        Returns the SQL code needed to deploy the model.
+
+        Parameters
+        ----------
+        X: SQLColumns, optional
+            List of the columns used to deploy the model.
+            If empty, the model  predictors are used.
+
+        Returns
+        -------
+        str
+            the SQL code needed to deploy the model.
+        """
+        return self.best_model_.deploySQL(X)
+
+    def to_memmodel(self) -> mm.InMemoryModel:
+        """
+        Converts  the model to  an InMemory object  that
+        can be used for different types of predictions.
+        """
+        return self.best_model_.to_memmodel()
+
+    # Model Fitting Method.
+
+    def fit(
+        self,
+        input_relation: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        y: Optional[str] = None,
+    ) -> None:
         """
-        if verticapy.options["overwrite_model"]:
+        Trains the model.
+
+        Parameters
+        ----------
+        input_relation: SQLRelation
+            Training Relation.
+        X: SQLColumns, optional
+            List of the predictors.
+        y: str, optional
+            Response column.
+        """
+        if conf.get_option("overwrite_model"):
             self.drop()
         else:
-            does_model_exist(name=self.name, raise_error=True)
-        if not (X):
-            if not (y):
+            self._is_already_stored(raise_error=True)
+        if isinstance(X, NoneType):
+            if not y:
                 exclude_columns = []
             else:
                 exclude_columns = [y]
-            if not (isinstance(input_relation, vDataFrame)):
-                X = vDataFrameSQL(input_relation).get_columns(
+            if not isinstance(input_relation, vDataFrame):
+                X = vDataFrame(input_relation).get_columns(
                     exclude_columns=exclude_columns
                 )
             else:
                 X = input_relation.get_columns(exclude_columns=exclude_columns)
+        X = format_type(X, dtype=list)
         if isinstance(self.parameters["estimator"], str):
-            v = version()
+            v = vertica_version()
             self.parameters["estimator"] = self.parameters["estimator"].lower()
-            check_types(
-                [("estimator", self.parameters["estimator"], ["native", "all", "fast"])]
-            )
-            modeltype = None
             estimator_method = self.parameters["estimator"]
-            if not (isinstance(input_relation, vDataFrame)):
-                vdf = vDataFrameSQL(input_relation)
+            if not isinstance(input_relation, vDataFrame):
+                vdf = vDataFrame(input_relation)
             else:
                 vdf = input_relation
             if self.parameters["estimator_type"].lower() == "binary" or (
                 self.parameters["estimator_type"].lower() == "auto"
                 and sorted(vdf[y].distinct()) == [0, 1]
             ):
                 self.parameters["estimator_type"] = "binary"
                 self.parameters["estimator"] = [
-                    LogisticRegression(self.name),
-                    NaiveBayes(self.name),
+                    LogisticRegression(self.model_name),
+                    NaiveBayes(self.model_name),
                 ]
                 if estimator_method in ("native", "all"):
                     if v[0] > 10 or (v[0] == 10 and v[1] >= 1):
-                        self.parameters["estimator"] += [XGBoostClassifier(self.name)]
+                        self.parameters["estimator"] += [XGBClassifier(self.model_name)]
                     if v[0] >= 9:
                         self.parameters["estimator"] += [
-                            LinearSVC(self.name),
-                            RandomForestClassifier(self.name),
+                            LinearSVC(self.model_name),
+                            RandomForestClassifier(self.model_name),
                         ]
                 if estimator_method == "all":
                     self.parameters["estimator"] += [
-                        KNeighborsClassifier(self.name),
-                        NearestCentroid(self.name),
+                        KNeighborsClassifier(self.model_name),
+                        NearestCentroid(self.model_name),
                     ]
             elif self.parameters["estimator_type"].lower() == "regressor" or (
                 self.parameters["estimator_type"].lower() == "auto" and vdf[y].isnum()
             ):
                 self.parameters["estimator_type"] = "regressor"
                 self.parameters["estimator"] = [
-                    LinearRegression(self.name),
-                    ElasticNet(self.name),
-                    Ridge(self.name),
-                    Lasso(self.name),
+                    LinearRegression(self.model_name),
+                    ElasticNet(self.model_name),
+                    Ridge(self.model_name),
+                    Lasso(self.model_name),
                 ]
                 if estimator_method in ("native", "all"):
                     if v[0] > 10 or (v[0] == 10 and v[1] >= 1):
-                        self.parameters["estimator"] += [XGBoostRegressor(self.name)]
+                        self.parameters["estimator"] += [XGBRegressor(self.model_name)]
                     if v[0] >= 9:
                         self.parameters["estimator"] += [
-                            LinearSVR(self.name),
-                            RandomForestRegressor(self.name),
+                            LinearSVR(self.model_name),
+                            RandomForestRegressor(self.model_name),
                         ]
                 if estimator_method == "all":
-                    self.parameters["estimator"] += [KNeighborsRegressor(self.name)]
+                    self.parameters["estimator"] += [
+                        KNeighborsRegressor(self.model_name)
+                    ]
             elif self.parameters["estimator_type"].lower() in ("multi", "auto"):
                 self.parameters["estimator_type"] = "multi"
-                self.parameters["estimator"] = [NaiveBayes(self.name)]
+                self.parameters["estimator"] = [NaiveBayes(self.model_name)]
                 if estimator_method in ("native", "all"):
                     if v[0] >= 10 and v[1] >= 1:
-                        self.parameters["estimator"] += [XGBoostClassifier(self.name)]
+                        self.parameters["estimator"] += [XGBClassifier(self.model_name)]
                     if v[0] >= 9:
                         self.parameters["estimator"] += [
-                            RandomForestClassifier(self.name)
+                            RandomForestClassifier(self.model_name)
                         ]
                 if estimator_method == "all":
                     self.parameters["estimator"] += [
-                        KNeighborsClassifier(self.name),
-                        NearestCentroid(self.name),
+                        KNeighborsClassifier(self.model_name),
+                        NearestCentroid(self.model_name),
                     ]
             else:
-                raise ParameterError(
+                raise ValueError(
                     f"Parameter 'estimator_type' must be in auto|binary|multi|regressor. Found {estimator_type}."
                 )
         elif isinstance(
             self.parameters["estimator"],
             (
                 RandomForestRegressor,
                 RandomForestClassifier,
-                XGBoostRegressor,
-                XGBoostClassifier,
+                XGBRegressor,
+                XGBClassifier,
                 NaiveBayes,
                 LinearRegression,
                 ElasticNet,
                 Lasso,
                 Ridge,
                 LogisticRegression,
                 KNeighborsRegressor,
@@ -867,82 +467,78 @@
         else:
             for elem in self.parameters["estimator"]:
                 assert isinstance(
                     elem,
                     (
                         RandomForestRegressor,
                         RandomForestClassifier,
-                        XGBoostRegressor,
-                        XGBoostClassifier,
+                        XGBRegressor,
+                        XGBClassifier,
                         NaiveBayes,
                         LinearRegression,
                         ElasticNet,
                         Lasso,
                         Ridge,
                         LogisticRegression,
                         KNeighborsRegressor,
                         KNeighborsClassifier,
                         NearestCentroid,
                         LinearSVC,
                         LinearSVR,
                     ),
-                ), ParameterError(
-                    "estimator must be a list of VerticaPy estimators. Found {}.".format(
-                        type(elem)
-                    )
+                ), ValueError(
+                    f"estimator must be a list of VerticaPy estimators. Found {elem}."
                 )
         if self.parameters["estimator_type"] == "auto":
-            self.parameters["estimator_type"] = self.parameters["estimator"][0].type
+            self.parameters["estimator_type"] = self.parameters["estimator"][
+                0
+            ]._model_type
         for elem in self.parameters["estimator"]:
-            cat = get_model_category(elem.type)
             assert (
                 self.parameters["estimator_type"] in ("binary", "multi")
-                and cat[0] == "classifier"
+                and elem._model_subcategory == "CLASSIFIER"
                 or self.parameters["estimator_type"] == "regressor"
-                and cat[0] == "regressor"
-            ), ParameterError(
-                "Incorrect list for parameter 'estimator'. Expected type '{}', found type '{}'.".format(
-                    self.parameters["estimator_type"], cat[0]
-                )
+                and elem._model_subcategory == "REGRESSOR"
+            ), ValueError(
+                f"Incorrect list for parameter 'estimator'. Expected type '{self.parameters['estimator_type']}', found type '{elem._model_subcategory}'."
             )
         if (
             self.parameters["estimator_type"] == "regressor"
             and self.parameters["metric"] == "auto"
         ):
             self.parameters["metric"] = "rmse"
         elif self.parameters["metric"] == "auto":
             self.parameters["metric"] = "logloss"
-        result = tablesample(
+        result = TableSample(
             {
                 "model_type": [],
                 "parameters": [],
                 "avg_score": [],
                 "avg_train_score": [],
                 "avg_time": [],
                 "score_std": [],
                 "score_train_std": [],
                 "model_class": [],
             }
         )
         if self.parameters["preprocess_data"]:
-            schema, name = schema_relation(self.name)
+            schema, name = schema_relation(self.model_name)
             name = gen_tmp_name(schema=schema, name="autodataprep")
             model_preprocess = AutoDataPrep(
                 name=name, **self.parameters["preprocess_dict"]
             )
-            input_relation = model_preprocess.fit(input_relation, X=X)
-            X = [elem for elem in model_preprocess.X_out]
+            model_preprocess.fit(input_relation, X=X)
+            input_relation = model_preprocess.final_relation_
+            X = copy.deepcopy(model_preprocess.X_out_)
             self.preprocess_ = model_preprocess
         else:
             self.preprocess_ = None
         if self.parameters["print_info"]:
             print(f"\033[1m\033[4mStarting AutoML\033[0m\033[0m\n")
-        if verticapy.options["tqdm"] and self.parameters["print_info"]:
-            from tqdm.auto import tqdm
-
+        if conf.get_option("tqdm") and self.parameters["print_info"]:
             loop = tqdm(self.parameters["estimator"])
         else:
             loop = self.parameters["estimator"]
         for elem in loop:
             if self.parameters["print_info"]:
                 print(
                     f"\n\033[1m\033[4mTesting Model - {str(elem.__class__).split('.')[-1][:-2]}\033[0m\033[0m\n"
@@ -956,21 +552,21 @@
             )
             gs = grid_search_cv(
                 elem,
                 param_grid,
                 input_relation,
                 X,
                 y,
-                self.parameters["metric"],
-                self.parameters["cv"],
-                self.parameters["pos_label"],
-                self.parameters["cutoff"],
-                True,
-                "no_print",
-                self.parameters["print_info"],
+                metric=self.parameters["metric"],
+                cv=self.parameters["cv"],
+                pos_label=self.parameters["pos_label"],
+                cutoff=self.parameters["cutoff"],
+                training_score=True,
+                skip_error="no_print",
+                print_info=self.parameters["print_info"],
             )
             if (
                 gs["parameters"] != []
                 and gs["avg_score"] != []
                 and gs["avg_train_score"] != []
                 and gs["avg_time"] != []
                 and gs["score_std"] != []
@@ -996,17 +592,30 @@
                 result["avg_time"][i],
                 result["score_std"][i],
                 result["score_train_std"][i],
                 result["model_class"][i],
             )
             for i in range(len(result["score_train_std"]))
         ]
-        reverse = reverse_score(self.parameters["metric"])
+        reverse = True
+        if self.parameters["metric"] in [
+            "logloss",
+            "max",
+            "mae",
+            "median",
+            "mse",
+            "msle",
+            "rmse",
+            "aic",
+            "bic",
+            "auto",
+        ]:
+            reverse = False
         data.sort(key=lambda tup: tup[2], reverse=reverse)
-        result = tablesample(
+        result = TableSample(
             {
                 "model_type": [elem[0] for elem in data],
                 "parameters": [elem[1] for elem in data],
                 "avg_score": [elem[2] for elem in data],
                 "avg_train_score": [elem[3] for elem in data],
                 "avg_time": [elem[4] for elem in data],
                 "score_std": [elem[5] for elem in data],
@@ -1015,15 +624,15 @@
             }
         )
         if self.parameters["print_info"]:
             print(f"\033[1m\033[4mFinal Model\033[0m\033[0m\n")
             print(
                 f"{result['model_type'][0]}; Best_Parameters: {result['parameters'][0]}; \033[91mBest_Test_score: {result['avg_score'][0]}\033[0m; \033[92mTrain_score: {result['avg_train_score'][0]}\033[0m; \033[94mTime: {result['avg_time'][0]}\033[0m;\n\n"
             )
-        best_model = result["model_class"][0](self.name)
+        best_model = result["model_class"][0](self.model_name)
         best_model.set_params(result["parameters"][0])
         self.stepwise_ = None
         if self.parameters["stepwise"]:
             self.stepwise_ = stepwise(
                 best_model,
                 input_relation,
                 X,
@@ -1037,61 +646,119 @@
                 show=False,
                 criterion_threshold=2,
             )
         else:
             best_model.fit(input_relation, X, y)
         self.best_model_ = best_model
         self.model_grid_ = result
-        self.parameters["reverse"] = not (reverse)
-        if self.preprocess_ != None:
+        self.parameters["reverse"] = not reverse
+        if not isinstance(self.preprocess_, NoneType):
             self.preprocess_.drop()
-            self.preprocess_.final_relation_ = vDataFrameSQL(self.preprocess_.sql_)
-        return self.model_grid_
+            self.preprocess_.final_relation_ = vDataFrame(self.preprocess_.sql_)
+
+    # Features Importance Methods.
 
-    # ---#
-    def plot(self, mltype: str = "champion", ax=None, **style_kwds):
+    def features_importance(
+        self, chart: Optional[PlottingObject] = None, **style_kwargs
+    ) -> PlottingObject:
         """
-    ---------------------------------------------------------------------------
-    Draws the AutoML plot.
+        Computes the model's features importance.
 
-    Parameters
-    ----------
-    mltype: str, optional
-        The plot type.
-            champion: champion challenger plot.
-            step    : stepwise plot.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    ax
-        Matplotlib axes object
+        Parameters
+        ----------
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            features importance.
         """
-        if mltype == "champion":
-            return plot_bubble_ml(
-                self.model_grid_["avg_time"],
-                self.model_grid_["avg_score"],
-                self.model_grid_["score_std"],
-                self.model_grid_["model_type"],
-                x_label="time",
-                y_label="score",
-                title="Model Type",
-                ax=ax,
-                reverse=(True, self.parameters["reverse"]),
-                **style_kwds,
-            )
-        else:
-            return plot_stepwise_ml(
-                [len(elem) for elem in self.stepwise_["features"]],
-                self.stepwise_[self.parameters["stepwise_criterion"]],
-                self.stepwise_["variable"],
-                self.stepwise_["change"],
-                [self.stepwise_["features"][0], self.stepwise_.best_list_],
-                x_label="n_features",
-                y_label=self.parameters["stepwise_criterion"],
-                direction=self.parameters["stepwise_direction"],
-                ax=ax,
-                **style_kwds,
+        if self.stepwise_:
+            data = {
+                "importance": self.stepwise_["importance"],
+            }
+            layout = {"columns": copy.deepcopy(self.stepwise_["variable"])}
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="ImportanceBarChart",
+                chart=chart,
+                style_kwargs=style_kwargs,
             )
+            vpy_plt.ImportanceBarChart(data=data, layout=layout).draw(**kwargs)
+        return self.best_model_.features_importance(**kwargs)
+
+    # Plotting Methods.
+
+    def plot(
+        self,
+        mltype: Literal["champion", "step"] = "champion",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the AutoML plot.
+
+        Parameters
+        ----------
+        mltype: str, optional
+            The plot type.
+                champion : champion challenger plot.
+                step     : stepwise plot.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional  parameter  to pass to  the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        if mltype == "champion":
+            data = {
+                "x": np.array(self.model_grid_["avg_time"]).astype(float),
+                "y": np.array(self.model_grid_["avg_score"]).astype(float),
+                "s": np.array(self.model_grid_["score_std"]).astype(float),
+                "c": np.array(self.model_grid_["model_type"]),
+            }
+            layout = {
+                "x_label": "time",
+                "y_label": self.parameters["metric"],
+                "z_label": self.parameters["metric"] + "_std",
+                "title": "Model Type",
+                "reverse": (True, self.parameters["reverse"]),
+            }
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="ChampionChallengerPlot",
+                chart=chart,
+                matplotlib_kwargs={"plt_text": True},
+                style_kwargs=style_kwargs,
+            )
+            return vpy_plt.ChampionChallengerPlot(data=data, layout=layout).draw(
+                **kwargs
+            )
+        else:
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="StepwisePlot",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            data = {
+                "x": np.array([len(x) for x in self.stepwise_["features"]]).astype(int),
+                "y": np.array(
+                    self.stepwise_[self.parameters["stepwise_criterion"]]
+                ).astype(float),
+                "c": np.array(self.stepwise_["variable"]),
+                "sign": np.array(self.stepwise_["change"]),
+            }
+            layout = {
+                "in_variables": self.stepwise_["features"][0],
+                "out_variables": self.stepwise_.best_list_,
+                "x_label": "n_features",
+                "y_label": self.parameters["stepwise_criterion"],
+                "direction": self.parameters["stepwise_direction"],
+            }
+            return vpy_plt.StepwisePlot(data=data, layout=layout).draw(**kwargs)
```

### Comparing `verticapy-0.9.0/verticapy/learn/ensemble.py` & `verticapy-1.0.0b1/verticapy/machine_learning/vertica/decomposition.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,722 +1,843 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# Standard Python Modules
-from typing import Union
-import random
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+from typing import Literal, Optional
 import numpy as np
 
-# VerticaPy Modules
-from verticapy.learn.metrics import *
-from verticapy.learn.mlplot import *
-from verticapy.utilities import *
-from verticapy.toolbox import *
-from verticapy import vDataFrame
-from verticapy.errors import *
-from verticapy.learn.vmodel import *
-from verticapy.learn.tree import get_tree_list_of_arrays
-
-# ---#
-class XGBoost_utils:
-    # Class:
-    # - to export Vertica XGBoost to the Python XGBoost JSON format.
-    # - to get the XGB priors
-
-    def to_json(self, path: str = ""):
-        """
-        ---------------------------------------------------------------------------
-        Creates a Python XGBoost JSON file that can be imported into the Python
-        XGBoost API.
-        
-        \u26A0 Warning : For multiclass classifiers, the probabilities returned 
-        by the VerticaPy and exported model may differ slightly because of 
-        normalization; while Vertica uses multinomial logistic regression,  
-        XGBoost Python uses Softmax. This difference does not affect the model's 
-        final predictions. Categorical predictors must be encoded.
+from verticapy._typing import (
+    NoneType,
+    PlottingObject,
+    PythonNumber,
+    SQLColumns,
+    SQLRelation,
+)
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import clean_query, format_type, quote_ident
+from verticapy._utils._sql._vertica_version import check_minimum_version
+
+from verticapy.core.tablesample.base import TableSample
+from verticapy.core.vdataframe.base import vDataFrame
+
+import verticapy.machine_learning.memmodel as mm
+from verticapy.machine_learning.vertica.preprocessing import Preprocessing
+
+"""
+General Classes.
+"""
+
+
+class Decomposition(Preprocessing):
+    # I/O Methods.
+
+    def deploySQL(
+        self,
+        X: Optional[SQLColumns] = None,
+        n_components: int = 0,
+        cutoff: PythonNumber = 1,
+        key_columns: Optional[SQLColumns] = None,
+        exclude_columns: Optional[SQLColumns] = None,
+    ) -> str:
+        """
+        Returns the SQL code needed to deploy the model.
 
         Parameters
         ----------
-        path: str, optional
-            The path and name of the output file. If a file with the same name 
-            already exists, the function returns an error.
-            
+        X: SQLColumns, optional
+            List of the columns used to deploy the model.
+            If empty,  the model predictors are used.
+        n_components: int, optional
+            Number  of  components to return.  If  set to
+            0,  all  the  components  are deployed.
+        cutoff: PythonNumber, optional
+            Specifies  the minimum accumulated  explained
+            variance.  Components  are  taken  until  the
+            accumulated  explained  variance reaches this
+            value.
+        key_columns: SQLColumns, optional
+            Predictors   used    during   the   algorithm
+            computation  that will be deployed with  the
+            principal components.
+        exclude_columns: SQLColumns, optional
+            Columns to exclude from the prediction.
+
         Returns
         -------
         str
-            The content of the JSON file if variable 'path' is empty. Otherwise,
-            nothing is returned.
+            the SQL code needed to deploy the model.
         """
+        exclude_columns, key_columns = format_type(
+            exclude_columns, key_columns, dtype=list
+        )
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        sql = f"""{self._vertica_transform_sql}({', '.join(X)} 
+                                            USING PARAMETERS
+                                            model_name = '{self.model_name}',
+                                            match_by_pos = 'true'"""
+        if key_columns:
+            key_columns = ", ".join(quote_ident(key_columns))
+            sql += f", key_columns = '{key_columns}'"
+        if exclude_columns:
+            exclude_columns = ", ".join(quote_ident(exclude_columns))
+            sql += f", exclude_columns = '{exclude_columns}'"
+        if n_components:
+            sql += f", num_components = {n_components}"
+        else:
+            sql += f", cutoff = {cutoff}"
+        sql += ")"
+        return clean_query(sql)
 
-        def xgboost_to_json(model):
-            def xgboost_dummy_tree_dict(model, i: int = 0):
-                # Dummy trees are used to store the prior probabilities.
-                # The Python XGBoost API do not use those information and start
-                # the training with priors = 0
-                result = {
-                    "base_weights": [0.0],
-                    "categories": [],
-                    "categories_nodes": [],
-                    "categories_segments": [],
-                    "categories_sizes": [],
-                    "default_left": [True],
-                    "id": -1,
-                    "left_children": [-1],
-                    "loss_changes": [0.0],
-                    "parents": [random.randint(2, 999999999)],
-                    "right_children": [-1],
-                    "split_conditions": [model.prior_[i]],
-                    "split_indices": [0],
-                    "split_type": [0],
-                    "sum_hessian": [0.0],
-                    "tree_param": {
-                        "num_deleted": "0",
-                        "num_feature": str(len(model.X)),
-                        "num_nodes": "1",
-                        "size_leaf_vector": "0",
-                    },
-                }
-                return result
-
-            def xgboost_tree_dict(model, tree_id: int, c: str = None):
-                tree = model.get_tree(tree_id)
-                attributes = get_tree_list_of_arrays(tree, model.X, model.type)
-                n_nodes = len(attributes[0])
-                split_conditions = []
-                parents = [0 for i in range(n_nodes)]
-                parents[0] = random.randint(n_nodes + 1, 999999999)
-                for i in range(n_nodes):
-                    left_child = attributes[0][i]
-                    right_child = attributes[1][i]
-                    if left_child != right_child:
-                        parents[left_child] = i
-                        parents[right_child] = i
-                    if attributes[5][i]:
-                        split_conditions += [attributes[3][i]]
-                    elif attributes[5][i] == None:
-                        if model.type == "XGBoostRegressor":
-                            split_conditions += [
-                                float(attributes[4][i])
-                                * model.parameters["learning_rate"]
-                            ]
-                        elif (
-                            len(model.classes_) == 2
-                            and model.classes_[1] == 1
-                            and model.classes_[0] == 0
-                        ):
-                            split_conditions += [
-                                model.parameters["learning_rate"]
-                                * float(attributes[6][i]["1"])
-                            ]
-                        else:
-                            split_conditions += [
-                                model.parameters["learning_rate"]
-                                * float(attributes[6][i][c])
-                            ]
-                    else:
-                        split_conditions += [float(attributes[3][i])]
-                result = {
-                    "base_weights": [0.0 for i in range(n_nodes)],
-                    "categories": [],
-                    "categories_nodes": [],
-                    "categories_segments": [],
-                    "categories_sizes": [],
-                    "default_left": [True for i in range(n_nodes)],
-                    "id": tree_id,
-                    "left_children": [-1 if x is None else x for x in attributes[0]],
-                    "loss_changes": [0.0 for i in range(n_nodes)],
-                    "parents": parents,
-                    "right_children": [-1 if x is None else x for x in attributes[1]],
-                    "split_conditions": split_conditions,
-                    "split_indices": [0 if x is None else x for x in attributes[2]],
-                    "split_type": [
-                        int(x) if isinstance(x, bool) else int(attributes[5][0])
-                        for x in attributes[5]
-                    ],
-                    "sum_hessian": [0.0 for i in range(n_nodes)],
-                    "tree_param": {
-                        "num_deleted": "0",
-                        "num_feature": str(len(model.X)),
-                        "num_nodes": str(n_nodes),
-                        "size_leaf_vector": "0",
-                    },
-                }
-                return result
-
-            def xgboost_tree_dict_list(model):
-                n = model.get_attr("tree_count")["tree_count"][0]
-                if model.type == "XGBoostClassifier" and (
-                    len(model.classes_) > 2
-                    or model.classes_[1] != 1
-                    or model.classes_[0] != 0
-                ):
-                    trees = []
-                    for i in range(n):
-                        for c in model.classes_:
-                            trees += [xgboost_tree_dict(model, i, str(c))]
-                    v = version()
-                    v = v[0] > 11 or (v[0] == 11 and (v[1] >= 1 or v[2] >= 1))
-                    if not (v):
-                        for i in range(len(model.classes_)):
-                            trees += [xgboost_dummy_tree_dict(model, i)]
-                    tree_info = [i for i in range(len(model.classes_))] * (
-                        n + int(not (v))
-                    )
-                    for idx, tree in enumerate(trees):
-                        tree["id"] = idx
-                else:
-                    trees = [xgboost_tree_dict(model, i) for i in range(n)]
-                    tree_info = [0 for i in range(n)]
-                return {
-                    "model": {
-                        "trees": trees,
-                        "tree_info": tree_info,
-                        "gbtree_model_param": {
-                            "num_trees": str(len(trees)),
-                            "size_leaf_vector": "0",
-                        },
-                    },
-                    "name": "gbtree",
-                }
-
-            def xgboost_learner(model):
-                v = version()
-                v = v[0] > 11 or (v[0] == 11 and (v[1] >= 1 or v[2] >= 1))
-                if v:
-                    col_sample_by_tree = model.parameters["col_sample_by_tree"]
-                    col_sample_by_node = model.parameters["col_sample_by_node"]
-                else:
-                    col_sample_by_tree = "null"
-                    col_sample_by_node = "null"
-                condition = ["{} IS NOT NULL".format(elem) for elem in model.X] + [
-                    "{} IS NOT NULL".format(model.y)
-                ]
-                n = model.get_attr("tree_count")["tree_count"][0]
-                if model.type == "XGBoostRegressor" or (
-                    len(model.classes_) == 2
-                    and model.classes_[1] == 1
-                    and model.classes_[0] == 0
-                ):
-                    bs, num_class, param, param_val = (
-                        model.prior_,
-                        "0",
-                        "reg_loss_param",
-                        {"scale_pos_weight": "1"},
-                    )
-                    if model.type == "XGBoostRegressor":
-                        objective = "reg:squarederror"
-                        attributes_dict = {
-                            "scikit_learn": '{"n_estimators": '
-                            + str(n)
-                            + ', "objective": "reg:squarederror", "max_depth": '
-                            + str(model.parameters["max_depth"])
-                            + ', "learning_rate": '
-                            + str(model.parameters["learning_rate"])
-                            + ', "verbosity": null, "booster": null, "tree_method": null,'
-                            + ' "gamma": null, "min_child_weight": null, "max_delta_step":'
-                            + ' null, "subsample": null, "colsample_bytree": '
-                            + str(col_sample_by_tree)
-                            + ', "colsample_bylevel": null, "colsample_bynode": '
-                            + str(col_sample_by_node)
-                            + ', "reg_alpha": null, "reg_lambda": null, "scale_pos_weight":'
-                            + ' null, "base_score": null, "missing": NaN, "num_parallel_tree"'
-                            + ': null, "kwargs": {}, "random_state": null, "n_jobs": null, '
-                            + '"monotone_constraints": null, "interaction_constraints": null,'
-                            + ' "importance_type": "gain", "gpu_id": null, "validate_parameters"'
-                            + ': null, "_estimator_type": "regressor"}'
-                        }
-                    else:
-                        objective = "binary:logistic"
-                        attributes_dict = {
-                            "scikit_learn": '{"use_label_encoder": true, "n_estimators": '
-                            + str(n)
-                            + ', "objective": "binary:logistic", "max_depth": '
-                            + str(model.parameters["max_depth"])
-                            + ', "learning_rate": '
-                            + str(model.parameters["learning_rate"])
-                            + ', "verbosity": null, "booster": null, "tree_method": null,'
-                            + ' "gamma": null, "min_child_weight": null, "max_delta_step":'
-                            + ' null, "subsample": null, "colsample_bytree": '
-                            + str(col_sample_by_tree)
-                            + ', "colsample_bylevel": null, "colsample_bynode": '
-                            + str(col_sample_by_node)
-                            + ', "reg_alpha": null, "reg_lambda": null, "scale_pos_weight":'
-                            + ' null, "base_score": null, "missing": NaN, "num_parallel_tree"'
-                            + ': null, "kwargs": {}, "random_state": null, "n_jobs": null,'
-                            + ' "monotone_constraints": null, "interaction_constraints": null,'
-                            + ' "importance_type": "gain", "gpu_id": null, "validate_parameters"'
-                            + ': null, "classes_": [0, 1], "n_classes_": 2, "_le": {"classes_": '
-                            + '[0, 1]}, "_estimator_type": "classifier"}'
-                        }
-                else:
-                    objective, bs, num_class, param, param_val = (
-                        "multi:softprob",
-                        0.5,
-                        str(len(model.classes_)),
-                        "softmax_multiclass_param",
-                        {"num_class": str(len(model.classes_))},
-                    )
-                    attributes_dict = {
-                        "scikit_learn": '{"use_label_encoder": true, "n_estimators": '
-                        + str(n)
-                        + ', "objective": "multi:softprob", "max_depth": '
-                        + str(model.parameters["max_depth"])
-                        + ', "learning_rate": '
-                        + str(model.parameters["learning_rate"])
-                        + ', "verbosity": null, "booster": null, "tree_method": null, '
-                        + '"gamma": null, "min_child_weight": null, "max_delta_step": '
-                        + 'null, "subsample": null, "colsample_bytree": '
-                        + str(col_sample_by_tree)
-                        + ', "colsample_bylevel": null, "colsample_bynode": '
-                        + str(col_sample_by_node)
-                        + ', "reg_alpha": null, "reg_lambda": null, "scale_pos_weight":'
-                        + ' null, "base_score": null, "missing": NaN, "num_parallel_tree":'
-                        + ' null, "kwargs": {}, "random_state": null, "n_jobs": null, '
-                        + '"monotone_constraints": null, "interaction_constraints": null, '
-                        + '"importance_type": "gain", "gpu_id": null, "validate_parameters":'
-                        + ' null, "classes_": '
-                        + str(model.classes_)
-                        + ', "n_classes_": '
-                        + str(len(model.classes_))
-                        + ', "_le": {"classes_": '
-                        + str(model.classes_)
-                        + '}, "_estimator_type": "classifier"}'
-                    }
-                attributes_dict["scikit_learn"] = attributes_dict[
-                    "scikit_learn"
-                ].replace('"', "++++")
-                gradient_booster = xgboost_tree_dict_list(model)
-                return {
-                    "attributes": attributes_dict,
-                    "feature_names": [],
-                    "feature_types": [],
-                    "gradient_booster": gradient_booster,
-                    "learner_model_param": {
-                        "base_score": np.format_float_scientific(
-                            bs, precision=7
-                        ).upper(),
-                        "num_class": num_class,
-                        "num_feature": str(len(model.X)),
-                    },
-                    "objective": {"name": objective, param: param_val},
-                }
-
-            res = {"learner": xgboost_learner(model), "version": [1, 4, 2]}
-            res = str(res)
-            res = (
-                res.replace("'", '"')
-                .replace("True", "true")
-                .replace("False", "false")
-                .replace("++++", '\\"')
-            )
-            return res
-
-        result = xgboost_to_json(self)
-        if path:
-            f = open(path, "w+")
-            f.write(result)
+    # Model Evaluation Methods.
+
+    def score(
+        self,
+        X: Optional[SQLColumns] = None,
+        input_relation: Optional[str] = None,
+        metric: Literal["avg", "median"] = "avg",
+        p: int = 2,
+    ) -> TableSample:
+        """
+        Returns  the  decomposition  score  on  a  dataset
+        for  each  transformed column.  It is the  average
+        / median of the p-distance between the real column
+        and  its  result after applying the  decomposition
+        model and its inverse.
+
+        Parameters
+        ----------
+        X: SQLColumns, optional
+            List of the columns used to deploy the model.
+            If empty, the model  predictors are used.
+        input_relation: str, optional
+            Input  Relation.  If  empty,  the model input
+            relation are used.
+        metric: str, optional
+            Distance metric used to do the scoring.
+                avg    : The average is used as
+                         aggregation.
+                median : The median  is used as
+                         aggregation.
+        p: int, optional
+            The p of the p-distance.
+
+        Returns
+        -------
+        TableSample
+            PCA scores.
+        """
+        if isinstance(X, NoneType):
+            X = self.X
+        X = format_type(X, dtype=list)
+        if not input_relation:
+            input_relation = self.input_relation
+        metric = str(metric).upper()
+        if metric == "MEDIAN":
+            metric = "APPROXIMATE_MEDIAN"
+        if self._model_type in ("PCA", "SVD"):
+            n_components = self.parameters["n_components"]
+            if not n_components:
+                n_components = len(X)
         else:
-            return result
+            n_components = len(X)
+        col_init_1 = [f"{X[idx]} AS col_init{idx}" for idx in range(len(X))]
+        col_init_2 = [f"col_init{idx}" for idx in range(len(X))]
+        cols = [f"col{idx + 1}" for idx in range(n_components)]
+        query = f"""SELECT 
+                        {self._vertica_transform_sql}
+                        ({', '.join(self.X)} 
+                            USING PARAMETERS 
+                            model_name = '{self.model_name}', 
+                            key_columns = '{', '.join(self.X)}', 
+                            num_components = {n_components}) OVER () 
+                    FROM {input_relation}"""
+        query = f"""
+            SELECT 
+                {', '.join(col_init_1 + cols)} 
+            FROM ({query}) VERTICAPY_SUBTABLE"""
+        query = f"""
+            SELECT 
+                {self._vertica_inverse_transform_sql}
+                ({', '.join(col_init_2 + cols)} 
+                    USING PARAMETERS 
+                    model_name = '{self.model_name}', 
+                    key_columns = '{', '.join(col_init_2)}', 
+                    exclude_columns = '{', '.join(col_init_2)}', 
+                    num_components = {n_components}) OVER () 
+            FROM ({query}) y"""
+        p_distances = [
+            f"""{metric}(POWER(ABS(POWER({X[idx]}, {p}) 
+                         - POWER(col_init{idx}, {p})), {1 / p})) 
+                         AS {X[idx]}"""
+            for idx in range(len(X))
+        ]
+        query = f"""
+            SELECT 
+                'Score' AS 'index', 
+                {', '.join(p_distances)} 
+            FROM ({query}) z"""
+        return TableSample.read_sql(query, title="Getting Model Score.").transpose()
+
+    # Prediction / Transformation Methods.
 
-    # ---#
-    def get_prior(self):
+    def transform(
+        self,
+        vdf: SQLRelation = None,
+        X: Optional[SQLColumns] = None,
+        n_components: int = 0,
+        cutoff: PythonNumber = 1,
+    ) -> vDataFrame:
         """
-        ---------------------------------------------------------------------------
-        Returns the XGB Priors.
-            
+        Applies the model on a vDataFrame.
+
+        Parameters
+        ----------
+        vdf: SQLRelation, optional
+            Input  vDataFrame.   You can  also  specify
+            a  customized   relation,   but   you  must
+            enclose  it  with  an  alias.  For example:
+            "(SELECT 1) x"    is     valid    whereas
+            "(SELECT 1)" and "SELECT 1" are invalid.
+        X: SQLColumns, optional
+            List of the input vDataColumns.
+        n_components: int, optional
+            Number  of components to return.  If set to
+            0, all the components are deployed.
+        cutoff: PythonNumber, optional
+            Specifies the minimum accumulated explained
+            variance.  Components  are taken until  the
+            accumulated explained variance reaches this
+            value.
+
         Returns
         -------
-        list
-            XGB Priors.
+        vDataFrame
+            object result of the model transformation.
         """
-        from verticapy.utilities import version
+        if isinstance(X, NoneType):
+            X = self.X
+        X = format_type(X, dtype=list)
+        if not vdf:
+            vdf = self.input_relation
+        if isinstance(vdf, str):
+            vdf = vDataFrame(vdf)
+        X = vdf.format_colnames(X)
+        exclude_columns = vdf.get_columns(exclude_columns=X)
+        all_columns = vdf.get_columns()
+        columns = self.deploySQL(
+            all_columns,
+            n_components,
+            cutoff,
+            exclude_columns,
+            exclude_columns,
+        )
+        main_relation = f"(SELECT {columns} FROM {vdf}) VERTICAPY_SUBTABLE"
+        return vDataFrame(main_relation)
 
-        condition = ["{} IS NOT NULL".format(elem) for elem in self.X] + [
-            "{} IS NOT NULL".format(self.y)
-        ]
-        v = version()
-        v = v[0] > 11 or (v[0] == 11 and (v[1] >= 1 or v[2] >= 1))
-        if self.type == "XGBoostRegressor" or (
-            len(self.classes_) == 2 and self.classes_[1] == 1 and self.classes_[0] == 0
-        ):
-            prior_ = executeSQL(
-                "SELECT AVG({}) FROM {} WHERE {}".format(
-                    self.y, self.input_relation, " AND ".join(condition)
-                ),
-                method="fetchfirstelem",
-                print_time_sql=False,
-            )
-        elif not (v):
-            prior_ = []
-            for elem in self.classes_:
-                avg = executeSQL(
-                    "SELECT COUNT(*) FROM {} WHERE {} AND {} = '{}'".format(
-                        self.input_relation, " AND ".join(condition), self.y, elem
-                    ),
-                    method="fetchfirstelem",
-                    print_time_sql=False,
-                )
-                avg /= executeSQL(
-                    "SELECT COUNT(*) FROM {} WHERE {}".format(
-                        self.input_relation, " AND ".join(condition)
-                    ),
-                    method="fetchfirstelem",
-                    print_time_sql=False,
-                )
-                logodds = np.log(avg / (1 - avg))
-                prior_ += [logodds]
-        else:
-            prior_ = [0.0 for elem in self.classes_]
-        return prior_
+    # Plotting Methods.
 
+    def plot(
+        self,
+        dimensions: tuple = (1, 2),
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws a decomposition scatter plot.
 
-# ---#
-class RandomForestClassifier(MulticlassClassifier, Tree):
-    """
----------------------------------------------------------------------------
-Creates a RandomForestClassifier object using the Vertica RF_CLASSIFIER 
-function. It is one of the ensemble learning methods for classification 
-that operates by constructing a multitude of decision trees at 
-training-time and outputting a class with the mode.
-Parameters
-----------
-name: str
-  Name of the the model. The model will be stored in the DB.
-n_estimators: int, optional
-  The number of trees in the forest, an integer between 0 and 1000, inclusive.
-max_features: int/str, optional
-  The number of randomly chosen features from which to pick the best feature 
-  to split on a given tree node. It can be an integer or one of the two following
-  methods.
-    auto : square root of the total number of predictors.
-    max  : number of predictors.
-max_leaf_nodes: int, optional
-  The maximum number of leaf nodes a tree in the forest can have, an integer 
-  between 1 and 1e9, inclusive.
-sample: float, optional
-  The portion of the input data set that is randomly picked for training each tree, 
-  a float between 0.0 and 1.0, inclusive. 
-max_depth: int, optional
-  The maximum depth for growing each tree, an integer between 1 and 100, inclusive.
-min_samples_leaf: int, optional
-  The minimum number of samples each branch must have after splitting a node, an 
-  integer between 1 and 1e6, inclusive. A split that causes fewer remaining samples 
-  is discarded. 
-min_info_gain: float, optional
-  The minimum threshold for including a split, a float between 0.0 and 1.0, inclusive. 
-  A split with information gain less than this threshold is discarded.
-nbins: int, optional 
-  The number of bins to use for continuous features, an integer between 2 and 1000, 
-  inclusive.
-  """
+        Parameters
+        ----------
+        dimensions: tuple, optional
+            Tuple of two elements representing the
+            IDs of the model's components.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional  parameter to pass to the
+            Plotting functions.
 
-    def __init__(
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vdf = self.transform(vDataFrame(self.input_relation))
+        dim_perc = []
+        for d in dimensions:
+            if not self.explained_variance_[d - 1]:
+                dim_perc += [""]
+            else:
+                dim_perc += [f" ({round(self.explained_variance_[d - 1] * 100, 1)}%)"]
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="ScatterPlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.ScatterPlot(
+            vdf=vdf,
+            columns=[f"col{dimensions[0]}", f"col{dimensions[1]}"],
+            max_nb_points=100000,
+            misc_layout={
+                "columns": [
+                    f"Dim{dimensions[0]}{dim_perc[0]}",
+                    f"Dim{dimensions[1]}{dim_perc[1]}",
+                ]
+            },
+        ).draw(**kwargs)
+
+    def plot_circle(
         self,
-        name: str,
-        n_estimators: int = 10,
-        max_features: Union[int, str] = "auto",
-        max_leaf_nodes: int = 1e9,
-        sample: float = 0.632,
-        max_depth: int = 5,
-        min_samples_leaf: int = 1,
-        min_info_gain: float = 0.0,
-        nbins: int = 32,
-    ):
-        version(condition=[8, 1, 1])
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "RandomForestClassifier", name
-        self.set_params(
-            {
-                "n_estimators": n_estimators,
-                "max_features": max_features,
-                "max_leaf_nodes": max_leaf_nodes,
-                "sample": sample,
-                "max_depth": max_depth,
-                "min_samples_leaf": min_samples_leaf,
-                "min_info_gain": min_info_gain,
-                "nbins": nbins,
-            }
+        dimensions: tuple = (1, 2),
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws a decomposition circle.
+
+        Parameters
+        ----------
+        dimensions: tuple, optional
+            Tuple of two elements representing the IDs
+            of the model's components.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass to  the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        if self._model_type == "SVD":
+            x = self.vectors_[:, dimensions[0] - 1]
+            y = self.vectors_[:, dimensions[1] - 1]
+        else:
+            x = self.principal_components_[:, dimensions[0] - 1]
+            y = self.principal_components_[:, dimensions[1] - 1]
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="PCACirclePlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        data = {
+            "x": x,
+            "y": y,
+            "explained_variance": [
+                self.explained_variance_[dimensions[0] - 1],
+                self.explained_variance_[dimensions[1] - 1],
+            ],
+            "dim": dimensions,
+        }
+        layout = {
+            "columns": self.X,
+        }
+        return vpy_plt.PCACirclePlot(data=data, layout=layout).draw(**kwargs)
+
+    def plot_scree(
+        self, chart: Optional[PlottingObject] = None, **style_kwargs
+    ) -> PlottingObject:
+        """
+        Draws a decomposition scree plot.
+
+        Parameters
+        ----------
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="PCAScreePlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
         )
+        n = len(self.explained_variance_)
+        data = {
+            "x": np.array([i + 1 for i in range(n)]),
+            "y": 100 * self.explained_variance_,
+            "adj_width": 0.94,
+        }
+        layout = {
+            "labels": [i + 1 for i in range(n)],
+            "x_label": "dimensions",
+            "y_label": "percentage_explained_variance (%)",
+            "title": None,
+            "plot_scree": True,
+            "plot_line": False,
+        }
+        return vpy_plt.PCAScreePlot(data=data, layout=layout).draw(**kwargs)
+
 
+"""
+Algorithms used for decomposition.
+"""
 
-# ---#
-class RandomForestRegressor(Regressor, Tree):
+
+class PCA(Decomposition):
+    """
+    Creates a PCA  (Principal Component Analysis) object
+    using the Vertica PCA algorithm.
+
+    Parameters
+    ----------
+    name: str
+        Name  of the  model. The model is stored in the
+        database.
+    n_components: int, optional
+        The  number of  components to keep in the model.
+        If  this value  is not provided,  all components
+        are kept.  The  maximum number of components  is
+        the number of  non-zero singular values returned
+        by the internal call to SVD. This number is less
+        than or equal to SVD  (number of columns, number
+        of rows).
+    scale: bool, optional
+        A  Boolean  value  that  specifies   whether  to
+        standardize  the columns during the  preparation
+        step.
+    method: str, optional
+        The method used to calculate PCA.
+                lapack: Lapack definition.
     """
----------------------------------------------------------------------------
-Creates a RandomForestRegressor object using the Vertica RF_REGRESSOR 
-function. It is one of the ensemble learning methods for regression that 
-operates by constructing a multitude of decision trees at training-time 
-and outputting a class with the mode.
-Parameters
-----------
-name: str
-  Name of the the model. The model will be stored in the DB.
-n_estimators: int, optional
-  The number of trees in the forest, an integer between 0 and 1000, inclusive.
-max_features: int/str, optional
-  The number of randomly chosen features from which to pick the best feature 
-  to split on a given tree node. It can be an integer or one of the two following
-  methods.
-    auto : square root of the total number of predictors.
-    max  : number of predictors.
-max_leaf_nodes: int, optional
-  The maximum number of leaf nodes a tree in the forest can have, an integer 
-  between 1 and 1e9, inclusive.
-sample: float, optional
-  The portion of the input data set that is randomly picked for training each tree, 
-  a float between 0.0 and 1.0, inclusive. 
-max_depth: int, optional
-  The maximum depth for growing each tree, an integer between 1 and 100, inclusive.
-min_samples_leaf: int, optional
-  The minimum number of samples each branch must have after splitting a node, an 
-  integer between 1 and 1e6, inclusive. A split that causes fewer remaining samples 
-  is discarded. 
-min_info_gain: float, optional
-  The minimum threshold for including a split, a float between 0.0 and 1.0, inclusive. 
-  A split with information gain less than this threshold is discarded.
-nbins: int, optional 
-  The number of bins to use for continuous features, an integer between 2 and 1000, 
-  inclusive.
-  """
 
+    # Properties.
+
+    @property
+    def _vertica_fit_sql(self) -> Literal["PCA"]:
+        return "PCA"
+
+    @property
+    def _vertica_transform_sql(self) -> Literal["APPLY_PCA"]:
+        return "APPLY_PCA"
+
+    @property
+    def _vertica_inverse_transform_sql(self) -> Literal["APPLY_INVERSE_PCA"]:
+        return "APPLY_INVERSE_PCA"
+
+    @property
+    def _model_category(self) -> Literal["UNSUPERVISED"]:
+        return "UNSUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["DECOMPOSITION"]:
+        return "DECOMPOSITION"
+
+    @property
+    def _model_type(self) -> Literal["PCA"]:
+        return "PCA"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["principal_components_", "mean_", "cos2_", "explained_variance_"]
+
+    # System & Special Methods.
+
+    @check_minimum_version
+    @save_verticapy_logs
     def __init__(
         self,
         name: str,
-        n_estimators: int = 10,
-        max_features: Union[int, str] = "auto",
-        max_leaf_nodes: int = 1e9,
-        sample: float = 0.632,
-        max_depth: int = 5,
-        min_samples_leaf: int = 1,
-        min_info_gain: float = 0.0,
-        nbins: int = 32,
-    ):
-        version(condition=[9, 0, 1])
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "RandomForestRegressor", name
-        self.set_params(
-            {
-                "n_estimators": n_estimators,
-                "max_features": max_features,
-                "max_leaf_nodes": max_leaf_nodes,
-                "sample": sample,
-                "max_depth": max_depth,
-                "min_samples_leaf": min_samples_leaf,
-                "min_info_gain": min_info_gain,
-                "nbins": nbins,
-            }
+        n_components: int = 0,
+        scale: bool = False,
+        method: Literal["lapack"] = "lapack",
+    ) -> None:
+        super().__init__()
+        self.model_name = name
+        self.parameters = {
+            "n_components": n_components,
+            "scale": scale,
+            "method": str(method).lower(),
+        }
+
+    # Attributes Methods.
+
+    def _compute_attributes(self) -> None:
+        """
+        Computes the model's attributes.
+        """
+        self.principal_components_ = self.get_vertica_attributes(
+            "principal_components"
+        ).to_numpy()
+        self.mean_ = np.array(self.get_vertica_attributes("columns")["mean"])
+        self.explained_variance_ = np.array(
+            self.get_vertica_attributes("singular_values")["explained_variance"]
         )
+        cos2 = self.get_vertica_attributes("principal_components").to_list()
+        for i in range(len(cos2)):
+            cos2[i] = [v**2 for v in cos2[i]]
+            total = sum(cos2[i])
+            cos2[i] = [v / total for v in cos2[i]]
+        values = {"index": self.X}
+        for idx, v in enumerate(
+            self.get_vertica_attributes("principal_components").values
+        ):
+            if v != "index":
+                values[v] = [c[idx - 1] for c in cos2]
+        self.cos2_ = TableSample(values).to_numpy()
 
+    # I/O Methods.
+
+    def to_memmodel(self) -> mm.PCA:
+        """
+        Converts  the model  to an InMemory object  that
+        can be used for different types of predictions.
+        """
+        return mm.PCA(self.principal_components_, self.mean_)
 
-# ---#
-class XGBoostClassifier(MulticlassClassifier, Tree, XGBoost_utils):
+
+class MCA(PCA):
     """
----------------------------------------------------------------------------
-Creates an XGBoostClassifier object using the Vertica XGB_CLASSIFIER 
-algorithm.
-Parameters
-----------
-name: str
-    Name of the the model. The model will be stored in the DB.
-max_ntree: int, optional
-    Maximum number of trees that will be created.
-max_depth: int, optional
-    Maximum depth of each tree.
-nbins: int, optional
-    Number of bins to use for finding splits in each column, more 
-    splits leads to longer runtime but more fine-grained and possibly 
-    better splits.
-split_proposal_method: str, optional
-    approximate splitting strategy. Can be 'global' or 'local'
-    (not yet supported)
-tol: float, optional
-    approximation error of quantile summary structures used in the 
-    approximate split finding method.
-learning_rate: float, optional
-    weight applied to each tree's prediction, reduces each tree's 
-    impact allowing for later trees to contribute, keeping earlier 
-    trees from 'hogging' all the improvements.
-min_split_loss: float, optional
-    Each split must improve the objective function value of the model 
-    by at least this much in order to not be pruned. Value of 0 is the 
-    same as turning off this parameter (trees will still be pruned based 
-    on positive/negative objective function values).
-weight_reg: float, optional
-    Regularization term that is applied to the weights of the leaves in 
-    the regression tree. The higher this value is, the more sparse/smooth 
-    the weights will be, which often helps prevent overfitting.
-sample: float, optional
-    Fraction of rows to use in training per iteration.
-col_sample_by_tree: float, optional
-    Float in the range (0,1] that specifies the fraction of columns (features), 
-    chosen at random, to use when building each tree.
-col_sample_by_node: float, optional
-    Float in the range (0,1] that specifies the fraction of columns (features), 
-    chosen at random, to use when evaluating each split.
+    Creates a MCA  (multiple correspondence analysis) object
+    using  the Vertica PCA  algorithm. MCA is a PCA applied
+    to a complete disjunctive table.  The  input relation is
+    transformed to a TCDT (transformed  complete  disjunctive
+    table) before applying the PCA.
+
+    Parameters
+    ----------
+    name: str
+        Name of the model.  The model is stored in the
+        database.
     """
 
-    def __init__(
+    # Properties.
+
+    @property
+    def _is_native(self) -> Literal[False]:
+        return False
+
+    @property
+    def _is_using_native(self) -> Literal[True]:
+        return True
+
+    @property
+    def _vertica_fit_sql(self) -> Literal["PCA"]:
+        return "PCA"
+
+    @property
+    def _vertica_transform_sql(self) -> Literal["APPLY_PCA"]:
+        return "APPLY_PCA"
+
+    @property
+    def _vertica_inverse_transform_sql(self) -> Literal["APPLY_INVERSE_PCA"]:
+        return "APPLY_INVERSE_PCA"
+
+    @property
+    def _model_category(self) -> Literal["UNSUPERVISED"]:
+        return "UNSUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["DECOMPOSITION"]:
+        return "DECOMPOSITION"
+
+    @property
+    def _model_type(self) -> Literal["MCA"]:
+        return "MCA"
+
+    # System & Special Methods.
+
+    @check_minimum_version
+    @save_verticapy_logs
+    def __init__(self, name: str) -> None:
+        super().__init__(name)
+        self.parameters = {}
+
+    # Plotting Methods.
+
+    def plot_contrib(
+        self, dimension: int = 1, chart: Optional[PlottingObject] = None, **style_kwargs
+    ) -> PlottingObject:
+        """
+        Draws a decomposition  contribution plot of the input
+        dimension.
+
+        Parameters
+        ----------
+        dimension: int, optional
+            Integer  representing  the  IDs  of the  model's
+            component.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the Plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        contrib = self.principal_components_[:, dimension - 1] ** 2
+        contrib = 100 * contrib / contrib.sum()
+        variables, contribution = zip(
+            *sorted(zip(self.X, contrib), key=lambda t: t[1], reverse=True)
+        )
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="PCAScreePlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        n = len(contribution)
+        data = {
+            "x": np.array([i + 1 for i in range(n)]),
+            "y": contribution,
+            "adj_width": 0.94,
+        }
+        layout = {
+            "labels": variables,
+            "x_label": None,
+            "y_label": "Contribution (%)",
+            "title": f"Contribution of variables to Dim {dimension}",
+            "plot_scree": True,
+            "plot_line": True,
+        }
+        return vpy_plt.PCAScreePlot(data=data, layout=layout).draw(**kwargs)
+
+    def plot_cos2(
         self,
-        name: str,
-        max_ntree: int = 10,
-        max_depth: int = 5,
-        nbins: int = 32,
-        split_proposal_method: str = "global",
-        tol: float = 0.001,
-        learning_rate: float = 0.1,
-        min_split_loss: float = 0.0,
-        weight_reg: float = 0.0,
-        sample: float = 1.0,
-        col_sample_by_tree: float = 1.0,
-        col_sample_by_node: float = 1.0,
-    ):
-        version(condition=[10, 1, 0])
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "XGBoostClassifier", name
-        params = {
-            "max_ntree": max_ntree,
-            "max_depth": max_depth,
-            "nbins": nbins,
-            "split_proposal_method": split_proposal_method,
-            "tol": tol,
-            "learning_rate": learning_rate,
-            "min_split_loss": min_split_loss,
-            "weight_reg": weight_reg,
-            "sample": sample,
-        }
-        v = version()
-        v = v[0] > 11 or (v[0] == 11 and (v[1] >= 1 or v[2] >= 1))
-        if v:
-            params["col_sample_by_tree"] = col_sample_by_tree
-            params["col_sample_by_node"] = col_sample_by_node
-        self.set_params(params)
+        dimensions: tuple = (1, 2),
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws a MCA (multiple correspondence analysis) cos2
+        plot of the two input dimensions.
+
+        Parameters
+        ----------
+        dimensions: tuple, optional
+            Tuple of two IDs of the model's components.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the Plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        cos2_1 = self.cos2_[:, dimensions[0] - 1]
+        cos2_2 = self.cos2_[:, dimensions[1] - 1]
+        n = len(cos2_1)
+        quality = []
+        for i in range(n):
+            quality += [cos2_1[i] + cos2_2[i]]
+        variables, quality = zip(
+            *sorted(zip(self.X, quality), key=lambda t: t[1], reverse=True)
+        )
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="PCAScreePlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        n = len(self.explained_variance_)
+        data = {
+            "x": np.array([i + 1 for i in range(n)]),
+            "y": 100 * np.array(quality),
+            "adj_width": 1.0,
+        }
+        layout = {
+            "labels": variables,
+            "x_label": None,
+            "y_label": "Cos2 - Quality of Representation (%)",
+            "title": f"Cos2 of variables to Dim {dimensions[0]}-{dimensions[1]}",
+            "plot_scree": False,
+            "plot_line": False,
+        }
+        return vpy_plt.PCAScreePlot(data=data, layout=layout).draw(**kwargs)
 
+    def plot_var(
+        self,
+        dimensions: tuple = (1, 2),
+        method: Literal["auto", "cos2", "contrib"] = "auto",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws  the  MCA  (multiple correspondence analysis)
+        graph.
 
-# ---#
-class XGBoostRegressor(Regressor, Tree, XGBoost_utils):
+        Parameters
+        ----------
+        dimensions: tuple, optional
+            Tuple  of  two IDs  of  the model's  components.
+        method: str, optional
+            Method used to draw the plot.
+                auto    : Only the  variables are displayed.
+                cos2    : The cos2 is used as CMAP.
+                contrib : The feature  contribution is  used
+                          as CMAP.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the Plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        x = self.principal_components_[:, dimensions[0] - 1]
+        y = self.principal_components_[:, dimensions[1] - 1]
+        n = len(self.cos2_[:, dimensions[0] - 1])
+        c = None
+        has_category = False
+        if method in ("cos2", "contrib"):
+            has_category = True
+            if method == "cos2":
+                c = np.array(
+                    [
+                        self.cos2_[:, dimensions[0] - 1][i]
+                        + self.cos2_[:, dimensions[1] - 1][i]
+                        for i in range(n)
+                    ]
+                )
+            else:
+                sum_1, sum_2 = (
+                    sum(self.cos2_[:, dimensions[0] - 1]),
+                    sum(self.cos2_[:, dimensions[1] - 1]),
+                )
+                c = np.array(
+                    [
+                        0.5
+                        * 100
+                        * (
+                            self.cos2_[:, dimensions[0] - 1][i] / sum_1
+                            + self.cos2_[:, dimensions[1] - 1][i] / sum_2
+                        )
+                        for i in range(n)
+                    ]
+                )
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="PCAVarPlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        data = {
+            "x": x,
+            "y": y,
+            "c": c,
+            "explained_variance": [
+                self.explained_variance_[dimensions[0] - 1],
+                self.explained_variance_[dimensions[1] - 1],
+            ],
+            "dim": dimensions,
+        }
+        layout = {
+            "columns": self.X,
+            "method": method,
+            "has_category": has_category,
+        }
+        return vpy_plt.PCAVarPlot(data=data, layout=layout).draw(**kwargs)
+
+
+class SVD(Decomposition):
     """
----------------------------------------------------------------------------
-Creates an XGBoostRegressor object using the Vertica XGB_REGRESSOR 
-algorithm.
-Parameters
-----------
-name: str
-    Name of the the model. The model will be stored in the DB.
-max_ntree: int, optional
-    Maximum number of trees that will be created.
-max_depth: int, optional
-    Maximum depth of each tree.
-nbins: int, optional
-    Number of bins to use for finding splits in each column, more 
-    splits leads to longer runtime but more fine-grained and possibly 
-    better splits.
-split_proposal_method: str, optional
-    approximate splitting strategy. Can be 'global' or 'local'
-    (not yet supported)
-tol: float, optional
-    approximation error of quantile summary structures used in the 
-    approximate split finding method.
-learning_rate: float, optional
-    weight applied to each tree's prediction, reduces each tree's 
-    impact allowing for later trees to contribute, keeping earlier 
-    trees from 'hogging' all the improvements.
-min_split_loss: float, optional
-    Each split must improve the objective function value of the model 
-    by at least this much in order to not be pruned. Value of 0 is the 
-    same as turning off this parameter (trees will still be pruned based 
-    on positive/negative objective function values).
-weight_reg: float, optional
-    Regularization term that is applied to the weights of the leaves in 
-    the regression tree. The higher this value is, the more sparse/smooth 
-    the weights will be, which often helps prevent overfitting.
-sample: float, optional
-    Fraction of rows to use in training per iteration.
-col_sample_by_tree: float, optional
-    Float in the range (0,1] that specifies the fraction of columns (features), 
-    chosen at random, to use when building each tree.
-col_sample_by_node: float, optional
-    Float in the range (0,1] that specifies the fraction of columns (features), 
-    chosen at random, to use when evaluating each split.
+    Creates  an  SVD  (Singular  Value  Decomposition)
+    object using the Vertica SVD algorithm.
+
+    Parameters
+    ----------
+    name: str
+        Name  of the model. The model is stored in the
+        database.
+    n_components: int, optional
+        The number  of components to keep in the model.
+        If this value  is not provided,  all components
+        are kept.  The maximum number of  components is
+        the number of non-zero singular values returned
+        by  the  internal call to SVD. This  number  is
+        less  than or equal to SVD (number of  columns,
+        number of rows).
+    method: str, optional
+        The method used to calculate SVD.
+                lapack: Lapack definition.
     """
 
+    # Properties.
+
+    @property
+    def _vertica_fit_sql(self) -> Literal["SVD"]:
+        return "SVD"
+
+    @property
+    def _vertica_transform_sql(self) -> Literal["APPLY_SVD"]:
+        return "APPLY_SVD"
+
+    @property
+    def _vertica_inverse_transform_sql(self) -> Literal["APPLY_INVERSE_SVD"]:
+        return "APPLY_INVERSE_SVD"
+
+    @property
+    def _model_category(self) -> Literal["UNSUPERVISED"]:
+        return "UNSUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["DECOMPOSITION"]:
+        return "DECOMPOSITION"
+
+    @property
+    def _model_type(self) -> Literal["SVD"]:
+        return "SVD"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["vectors_", "values_", "explained_variance_"]
+
+    # System & Special Methods.
+
+    @check_minimum_version
+    @save_verticapy_logs
     def __init__(
-        self,
-        name: str,
-        max_ntree: int = 10,
-        max_depth: int = 5,
-        nbins: int = 32,
-        split_proposal_method: str = "global",
-        tol: float = 0.001,
-        learning_rate: float = 0.1,
-        min_split_loss: float = 0.0,
-        weight_reg: float = 0.0,
-        sample: float = 1.0,
-        col_sample_by_tree: float = 1.0,
-        col_sample_by_node: float = 1.0,
-    ):
-        version(condition=[10, 1, 0])
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "XGBoostRegressor", name
-        params = {
-            "max_ntree": max_ntree,
-            "max_depth": max_depth,
-            "nbins": nbins,
-            "split_proposal_method": split_proposal_method,
-            "tol": tol,
-            "learning_rate": learning_rate,
-            "min_split_loss": min_split_loss,
-            "weight_reg": weight_reg,
-            "sample": sample,
-        }
-        v = version()
-        v = v[0] > 11 or (v[0] == 11 and (v[1] >= 1 or v[2] >= 1))
-        if v:
-            params["col_sample_by_tree"] = col_sample_by_tree
-            params["col_sample_by_node"] = col_sample_by_node
-        self.set_params(params)
+        self, name: str, n_components: int = 0, method: Literal["lapack"] = "lapack"
+    ) -> None:
+        super().__init__()
+        self.model_name = name
+        self.parameters = {
+            "n_components": n_components,
+            "method": str(method).lower(),
+        }
+
+    # Attributes Methods.
+
+    def _compute_attributes(self) -> None:
+        """
+        Computes the model's attributes.
+        """
+        self.vectors_ = self.get_vertica_attributes("right_singular_vectors").to_numpy()
+        self.values_ = np.array(self.get_vertica_attributes("singular_values")["value"])
+        self.explained_variance_ = np.array(
+            self.get_vertica_attributes("singular_values")["explained_variance"]
+        )
+
+    # I/O Methods.
+
+    def to_memmodel(self) -> mm.SVD:
+        """
+        Converts  the model  to an InMemory object  that
+        can be used for different types of predictions.
+        """
+        return mm.SVD(self.vectors_, self.values_)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/learn/linear_model.py` & `verticapy-1.0.0b1/verticapy/machine_learning/metrics/plotting.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,328 +1,299 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# VerticaPy Modules
-from verticapy import vDataFrame
-from verticapy.utilities import *
-from verticapy.toolbox import *
-from verticapy.errors import *
-from verticapy.learn.vmodel import *
-
-# ---#
-class ElasticNet(Regressor):
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+from typing import Optional
+
+import numpy as np
+
+from verticapy._typing import PlottingObject, PythonScalar, SQLRelation
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._vertica_version import check_minimum_version
+
+from verticapy.core.tablesample.base import TableSample
+
+from verticapy.machine_learning.metrics.classification import (
+    _compute_area,
+    _compute_function_metrics,
+)
+
+from verticapy.plotting._utils import PlottingUtils
+
+
+@check_minimum_version
+@save_verticapy_logs
+def lift_chart(
+    y_true: str,
+    y_score: str,
+    input_relation: SQLRelation,
+    pos_label: PythonScalar = 1,
+    nbins: int = 30,
+    show: bool = True,
+    chart: Optional[PlottingObject] = None,
+    **style_kwargs,
+) -> PlottingObject:
     """
----------------------------------------------------------------------------
-Creates a ElasticNet object using the Vertica Linear Regression algorithm 
-on the data. The Elastic Net is a regularized regression method that 
-linearly combines the L1 and L2 penalties of the Lasso and Ridge methods.
-
-Parameters
-----------
-name: str
-	Name of the the model. The model will be stored in the DB.
-tol: float, optional
-	Determines whether the algorithm has reached the specified accuracy 
-    result.
-C: float, optional
-	The regularization parameter value. The value must be zero or 
-    non-negative.
-max_iter: int, optional
-	Determines the maximum number of iterations the algorithm performs 
-    before achieving the specified accuracy result.
-solver: str, optional
-	The optimizer method to use to train the model. 
-		Newton : Newton Method
-		BFGS   : Broyden Fletcher Goldfarb Shanno
-		CGD    : Coordinate Gradient Descent
-l1_ratio: float, optional
-	ENet mixture parameter that defines how much L1 versus L2 
-    regularization to provide.
-	"""
-
-    def __init__(
-        self,
-        name: str,
-        tol: float = 1e-6,
-        C: float = 1.0,
-        max_iter: int = 100,
-        solver: str = "CGD",
-        l1_ratio: float = 0.5,
-    ):
-        version(condition=[8, 0, 0])
-        check_types([("name", name, [str])])
-        self.type, self.name = "LinearRegression", name
-        self.set_params(
-            {
-                "penalty": "enet",
-                "tol": tol,
-                "C": C,
-                "max_iter": max_iter,
-                "solver": str(solver).lower(),
-                "l1_ratio": l1_ratio,
-            }
-        )
-
+    Draws the Lift Chart.
 
-# ---#
-class Lasso(Regressor):
+    Parameters
+    ----------
+    y_true: str
+        Response column.
+    y_score: str
+        Prediction probability.
+    input_relation: SQLRelation
+        Relation used for scoring. This relation can
+        be a view, table, or a customized relation (if
+        an alias  is used at the end of the relation).
+        For example: (SELECT ... FROM ...) x
+    pos_label: PythonScalar, optional
+        To compute the Lift Chart, one of the response
+        column classes must be the positive class. The
+        parameter  'pos_label' represents this  class.
+    nbins: int, optional
+        An integer value that determines the number of
+        decision  boundaries.  Decision boundaries are
+        set at equally-spaced intervals  between 0 and
+        1, inclusive.
+    show: bool, optional
+        If set to True,  the  Plotting object  will be
+        returned.
+    chart: PlottingObject, optional
+       The chart object to plot on.
+    **style_kwargs
+        Any   optional  parameter  to  pass  to   the
+        Plotting functions.
+
+    Returns
+    -------
+    obj
+        decision_boundary, positive_prediction_ratio, lift
     """
----------------------------------------------------------------------------
-Creates a Lasso object using the Vertica Linear Regression algorithm on the 
-data. The Lasso is a regularized regression method which uses an L1 penalty.
-
-Parameters
-----------
-name: str
-	Name of the the model. The model will be stored in the DB.
-tol: float, optional
-	Determines whether the algorithm has reached the specified accuracy 
-    result.
-C: float, optional
-    The regularization parameter value. The value must be zero or 
-    non-negative.
-max_iter: int, optional
-	Determines the maximum number of iterations the algorithm performs 
-    before achieving the specified accuracy result.
-solver: str, optional
-	The optimizer method to use to train the model. 
-		Newton : Newton Method
-		BFGS   : Broyden Fletcher Goldfarb Shanno
-		CGD    : Coordinate Gradient Descent
-	"""
-
-    def __init__(
-        self,
-        name: str,
-        tol: float = 1e-6,
-        C: float = 1.0,
-        max_iter: int = 100,
-        solver: str = "CGD",
-    ):
-        version(condition=[8, 0, 0])
-        check_types([("name", name, [str])])
-        self.type, self.name = "LinearRegression", name
-        self.set_params(
-            {
-                "penalty": "l1",
-                "tol": tol,
-                "C": C,
-                "max_iter": max_iter,
-                "solver": str(solver).lower(),
+    decision_boundary, positive_prediction_ratio, lift = _compute_function_metrics(
+        y_true=y_true,
+        y_score=y_score,
+        input_relation=input_relation,
+        pos_label=pos_label,
+        nbins=nbins,
+        fun_sql_name="lift_table",
+    )
+    lift = np.nan_to_num(lift, nan=np.nanmax(lift))
+    decision_boundary.reverse()
+    if not show:
+        return TableSample(
+            values={
+                "decision_boundary": decision_boundary,
+                "positive_prediction_ratio": positive_prediction_ratio,
+                "lift": list(lift),
             }
         )
-        for elem in ["l1_ratio"]:
-            if elem in self.parameters:
-                del self.parameters[elem]
-
+    vpy_plt, kwargs = PlottingUtils().get_plotting_lib(
+        class_name="LiftChart",
+        chart=chart,
+        style_kwargs=style_kwargs,
+    )
+    data = {
+        "x": np.array(decision_boundary),
+        "y": np.array(positive_prediction_ratio),
+        "z": np.array(lift),
+    }
+    layout = {
+        "title": "Lift Table",
+        "x_label": "Cumulative Data Fraction",
+        "y_label": "Cumulative Capture Rate",
+        "z_label": "Cumulative Lift",
+    }
+    return vpy_plt.LiftChart(data=data, layout=layout).draw(**kwargs)
+
+
+@check_minimum_version
+@save_verticapy_logs
+def prc_curve(
+    y_true: str,
+    y_score: str,
+    input_relation: SQLRelation,
+    pos_label: PythonScalar = 1,
+    nbins: int = 30,
+    show: bool = True,
+    chart: Optional[PlottingObject] = None,
+    **style_kwargs,
+) -> PlottingObject:
+    """
+    Draws the PRC Curve.
 
-# ---#
-class LinearRegression(Regressor):
+    Parameters
+    ----------
+    y_true: str
+        Response column.
+    y_score: str
+        Prediction probability.
+    input_relation: SQLRelation
+        Relation used for scoring. This relation can
+        be a view, table, or a customized relation (if
+        an alias  is used at the end of the relation).
+        For example: (SELECT ... FROM ...) x
+    pos_label: PythonScalar, optional
+        To compute the PRC Curve,  one of the response
+        column classes must be the positive class. The
+        parameter  'pos_label' represents this  class.
+    nbins: int, optional
+        An integer value that determines the number of
+        decision  boundaries.  Decision boundaries are
+        set at equally-spaced intervals  between 0 and
+        1, inclusive.
+    show: bool, optional
+        If set to True, the Plotting object is
+        returned.
+    chart: PlottingObject, optional
+       The chart object to plot on.
+    **style_kwargs
+        Any   optional  parameter  to  pass  to   the
+        Plotting functions.
+
+    Returns
+    -------
+    obj
+        threshold, recall, precision
     """
----------------------------------------------------------------------------
-Creates a LinearRegression object using the Vertica Linear Regression 
-algorithm on the data.
-
-Parameters
-----------
-name: str
-	Name of the the model. The model will be stored in the DB.
-tol: float, optional
-	Determines whether the algorithm has reached the specified accuracy 
-    result.
-max_iter: int, optional
-	Determines the maximum number of iterations the algorithm performs 
-    before achieving the specified accuracy result.
-solver: str, optional
-	The optimizer method to use to train the model. 
-		Newton : Newton Method
-		BFGS   : Broyden Fletcher Goldfarb Shanno
-	"""
-
-    def __init__(
-        self, name: str, tol: float = 1e-6, max_iter: int = 100, solver: str = "Newton"
-    ):
-        version(condition=[8, 0, 0])
-        check_types(
-            [("name", name, [str]), ("solver", solver.lower(), ["newton", "bfgs"])]
-        )
-        self.type, self.name = "LinearRegression", name
-        self.set_params(
-            {
-                "penalty": "none",
-                "tol": tol,
-                "max_iter": max_iter,
-                "solver": str(solver).lower(),
+    threshold, recall, precision = _compute_function_metrics(
+        y_true=y_true,
+        y_score=y_score,
+        input_relation=input_relation,
+        pos_label=pos_label,
+        nbins=nbins,
+        fun_sql_name="prc",
+    )
+    if not show:
+        return TableSample(
+            values={
+                "threshold": threshold,
+                "recall": recall,
+                "precision": precision,
             }
         )
-        for elem in ["l1_ratio", "C"]:
-            if elem in self.parameters:
-                del self.parameters[elem]
-
+    auc = _compute_area(precision, recall)
+    vpy_plt, kwargs = PlottingUtils().get_plotting_lib(
+        class_name="PRCCurve",
+        chart=chart,
+        style_kwargs=style_kwargs,
+    )
+    data = {"x": np.array(recall), "y": np.array(precision), "auc": auc}
+    layout = {
+        "title": "PRC Curve",
+        "x_label": "Recall",
+        "y_label": "Precision",
+    }
+    return vpy_plt.PRCCurve(data=data, layout=layout).draw(**kwargs)
+
+
+@check_minimum_version
+@save_verticapy_logs
+def roc_curve(
+    y_true: str,
+    y_score: str,
+    input_relation: SQLRelation,
+    pos_label: PythonScalar = 1,
+    nbins: int = 30,
+    cutoff_curve: bool = False,
+    show: bool = True,
+    chart: Optional[PlottingObject] = None,
+    **style_kwargs,
+) -> PlottingObject:
+    """
+    Draws the ROC Curve.
 
-# ---#
-class LogisticRegression(BinaryClassifier):
+    Parameters
+    ----------
+    y_true: str
+        Response column.
+    y_score: str
+        Prediction probability.
+    input_relation: SQLRelation
+        Relation used for scoring. This relation can
+        be a view, table, or a customized relation (if
+        an alias  is used at the end of the relation).
+        For example: (SELECT ... FROM ...) x
+    pos_label: PythonScalar, optional
+        To compute the ROC Curve,  one of the response
+        column classes must be the positive class. The
+        parameter  'pos_label' represents this  class.
+    nbins: int, optional
+        An integer value that determines the number of
+        decision  boundaries.  Decision boundaries are
+        set at equally-spaced intervals  between 0 and
+        1, inclusive.
+    show: bool, optional
+        If set to True,  the  Plotting object  is
+        returned.
+    chart: PlottingObject, optional
+       The chart object to plot on.
+    **style_kwargs
+        Any   optional  parameter  to  pass  to   the
+        Plotting functions.
+
+    Returns
+    -------
+    obj
+        threshold, false_positive, true_positive
     """
----------------------------------------------------------------------------
-Creates a LogisticRegression object using the Vertica Logistic Regression
-algorithm on the data.
-
-Parameters
-----------
-name: str
-	Name of the the model. The model will be stored in the DB.
-penalty: str, optional
-	Determines the method of regularization.
-		None : No Regularization
-		L1   : L1 Regularization
-		L2   : L2 Regularization
-		ENet : Combination between L1 and L2
-tol: float, optional
-	Determines whether the algorithm has reached the specified accuracy result.
-C: float, optional
-	The regularization parameter value. The value must be zero or non-negative.
-max_iter: int, optional
-	Determines the maximum number of iterations the algorithm performs before 
-	achieving the specified accuracy result.
-solver: str, optional
-	The optimizer method to use to train the model. 
-		Newton : Newton Method
-		BFGS   : Broyden Fletcher Goldfarb Shanno
-		CGD    : Coordinate Gradient Descent
-l1_ratio: float, optional
-	ENet mixture parameter that defines how much L1 versus L2 regularization 
-	to provide.
-	"""
-
-    def __init__(
-        self,
-        name: str,
-        penalty: str = "None",
-        tol: float = 1e-6,
-        C: int = 1,
-        max_iter: int = 100,
-        solver: str = "Newton",
-        l1_ratio: float = 0.5,
-    ):
-        version(condition=[8, 0, 0])
-        check_types([("name", name, [str])])
-        self.type, self.name = "LogisticRegression", name
-        self.set_params(
-            {
-                "penalty": str(penalty).lower(),
-                "tol": tol,
-                "C": C,
-                "max_iter": max_iter,
-                "solver": str(solver).lower(),
-                "l1_ratio": l1_ratio,
+    threshold, false_positive, true_positive = _compute_function_metrics(
+        y_true=y_true,
+        y_score=y_score,
+        input_relation=input_relation,
+        pos_label=pos_label,
+        nbins=nbins,
+        fun_sql_name="roc",
+    )
+    if not show:
+        return TableSample(
+            values={
+                "threshold": threshold,
+                "false_positive": false_positive,
+                "true_positive": true_positive,
             }
         )
-        if penalty.lower() == "none":
-            for elem in ["l1_ratio", "C"]:
-                if elem in self.parameters:
-                    del self.parameters[elem]
-            check_types([("solver", solver.lower(), ["bfgs", "newton"])])
-        elif penalty.lower() in ("l1", "l2"):
-            for elem in ["l1_ratio"]:
-                if elem in self.parameters:
-                    del self.parameters[elem]
-            check_types([("solver", solver.lower(), ["bfgs", "newton", "cgd"])])
-
-
-# ---#
-class Ridge(Regressor):
-    """
----------------------------------------------------------------------------
-Creates a Ridge object using the Vertica Linear Regression algorithm on the 
-data. The Ridge is a regularized regression method which uses an L2 penalty. 
-
-Parameters
-----------
-name: str
-	Name of the the model. The model will be stored in the DB.
-tol: float, optional
-	Determines whether the algorithm has reached the specified 
-    accuracy result.
-C: float, optional
-    The regularization parameter value. The value must be zero 
-    or non-negative.
-max_iter: int, optional
-	Determines the maximum number of iterations the algorithm 
-    performs before achieving the specified accuracy result.
-solver: str, optional
-	The optimizer method to use to train the model. 
-		Newton : Newton Method
-		BFGS   : Broyden Fletcher Goldfarb Shanno
-	"""
-
-    def __init__(
-        self,
-        name: str,
-        tol: float = 1e-6,
-        C: float = 1.0,
-        max_iter: int = 100,
-        solver: str = "Newton",
-    ):
-        version(condition=[8, 0, 0])
-        check_types(
-            [("name", name, [str], ("solver", solver.lower(), ["newton", "bfgs"]))]
+    auc = _compute_area(true_positive, false_positive)
+    if cutoff_curve:
+        vpy_plt, kwargs = PlottingUtils().get_plotting_lib(
+            class_name="CutoffCurve",
+            chart=chart,
+            style_kwargs=style_kwargs,
         )
-        self.type, self.name = "LinearRegression", name
-        self.set_params(
-            {
-                "penalty": "l2",
-                "tol": tol,
-                "C": C,
-                "max_iter": max_iter,
-                "solver": str(solver).lower(),
-            }
+        data = {
+            "x": np.array(threshold),
+            "y": 1 - np.array(false_positive),
+            "z": np.array(true_positive),
+            "auc": auc,
+        }
+        layout = {
+            "title": "Cutoff Curve",
+            "x_label": "Decision Boundary",
+            "y_label": "Specificity",
+            "z_label": "Sensitivity",
+        }
+        return vpy_plt.CutoffCurve(data=data, layout=layout).draw(**kwargs)
+    else:
+        vpy_plt, kwargs = PlottingUtils().get_plotting_lib(
+            class_name="ROCCurve",
+            chart=chart,
+            style_kwargs=style_kwargs,
         )
-        for elem in ["l1_ratio"]:
-            if elem in self.parameters:
-                del self.parameters[elem]
+        data = {"x": np.array(false_positive), "y": np.array(true_positive), "auc": auc}
+        layout = {
+            "title": "ROC Curve",
+            "x_label": "False Positive Rate (1-Specificity)",
+            "y_label": "True Positive Rate (Sensitivity)",
+        }
+        return vpy_plt.ROCCurve(data=data, layout=layout).draw(**kwargs)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/learn/model_selection.py` & `verticapy-1.0.0b1/verticapy/machine_learning/vertica/base.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,3164 +1,2998 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# Standard Python Modules
-import statistics, random, time
-from collections.abc import Iterable
-from itertools import product
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import copy
+import warnings
+from abc import abstractmethod
+from typing import Any, Callable, Literal, Optional, Union, get_type_hints
 import numpy as np
-from typing import Union
 
-# VerticaPy Modules
-import verticapy
-from verticapy import vDataFrame
-from verticapy.utilities import *
-from verticapy.toolbox import *
-from verticapy.errors import *
-from verticapy.plot import gen_colors
-from verticapy.learn.tools import does_model_exist, get_model_category
-from verticapy.learn.mlplot import plot_bubble_ml, plot_stepwise_ml, plot_importance
-
-# Other Python Modules
-import matplotlib.pyplot as plt
-import matplotlib.patches as mpatches
-
-# ---#
-def bayesian_search_cv(
-    estimator,
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    metric: str = "auto",
-    cv: int = 3,
-    pos_label: Union[int, float, str] = None,
-    cutoff: float = -1,
-    param_grid: Union[dict, list] = {},
-    random_nbins: int = 16,
-    bayesian_nbins: int = None,
-    random_grid: bool = False,
-    lmax: int = 15,
-    nrows: int = 100000,
-    k_tops: int = 10,
-    RFmodel_params: dict = {},
-    print_info: bool = True,
-    **kwargs,
-):
-    """
----------------------------------------------------------------------------
-Computes the k-fold bayesian search of an estimator using a random
-forest model to estimate a probable optimal set of parameters.
-
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-pos_label: int/float/str, optional
-    The main class to be considered as positive (classification only).
-cutoff: float, optional
-    The model cutoff (classification only).
-param_grid: dict/list, optional
-    Dictionary of the parameters to test. It can also be a list of the
-    different combinations. If empty, a parameter grid will be generated.
-random_nbins: int, optional
-    Number of bins used to compute the different parameters categories
-    in the random parameters generation.
-bayesian_nbins: int, optional
-    Number of bins used to compute the different parameters categories
-    in the bayesian table generation.
-random_grid: bool, optional
-    If True, the rows used to find the optimal function will be
-    used randomnly. Otherwise, they will be regularly spaced. 
-lmax: int, optional
-    Maximum length of each parameter list.
-nrows: int, optional
-    Number of rows to use when performing the bayesian search.
-k_tops: int, optional
-    When performing the bayesian search, the final stage will be to retrain the top
-    possible combinations. 'k_tops' represents the number of models to train at
-    this stage to find the most efficient model.
-RFmodel_params: dict, optional
-    Dictionary of the random forest model parameters used to estimate a probable 
-    optimal set of parameters.
-print_info: bool, optional
-    If True, prints the model information at each step.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    if print_info:
-        print(f"\033[1m\033[4mStarting Bayesian Search\033[0m\033[0m\n")
-        print(
-            f"\033[1m\033[4mStep 1 - Computing Random Models using Grid Search\033[0m\033[0m\n"
-        )
-    if not (param_grid):
-        param_grid = gen_params_grid(estimator, random_nbins, len(X), lmax, 0)
-    param_gs = grid_search_cv(
-        estimator,
-        param_grid,
-        input_relation,
-        X,
-        y,
-        metric,
-        cv,
-        pos_label,
-        cutoff,
-        True,
-        "no_print",
-        print_info,
-        final_print="no_print",
-    )
-    if "enet" not in kwargs:
-        params = []
-        for param_grid in param_gs["parameters"]:
-            params += [elem for elem in param_grid]
-        all_params = list(dict.fromkeys(params))
-    else:
-        all_params = ["C", "l1_ratio"]
-    if not (bayesian_nbins):
-        bayesian_nbins = max(int(np.exp(np.log(nrows) / len(all_params))), 1)
-    result = {}
-    for elem in all_params:
-        result[elem] = []
-    for param_grid in param_gs["parameters"]:
-        for item in all_params:
-            if item in param_grid:
-                result[item] += [param_grid[item]]
-            else:
-                result[item] += [None]
-    result["score"] = param_gs["avg_score"]
-    if "max_features" in result:
-        for idx, elem in enumerate(result["max_features"]):
-            if elem == "auto":
-                result["max_features"][idx] = int(np.floor(np.sqrt(len(X))) + 1)
-            elif elem == "max":
-                result["max_features"][idx] = int(len(X))
-    result = tablesample(result).to_sql()
-    schema = verticapy.options["temp_schema"]
-    relation = gen_tmp_name(schema=schema, name="bayesian")
-    model_name = gen_tmp_name(schema=schema, name="rf")
-    drop(relation, method="table")
-    executeSQL("CREATE TABLE {} AS {}".format(relation, result), print_time_sql=False)
-    if print_info:
-        print(
-            f"\033[1m\033[4mStep 2 - Fitting the RF model with the hyperparameters data\033[0m\033[0m\n"
-        )
-    if verticapy.options["tqdm"] and print_info:
-        from tqdm.auto import tqdm
-
-        loop = tqdm(range(1))
-    else:
-        loop = range(1)
-    for j in loop:
-        if "enet" not in kwargs:
-            model_grid = gen_params_grid(
-                estimator,
-                nbins=bayesian_nbins,
-                max_nfeatures=len(all_params),
-                optimized_grid=-666,
-            )
-        else:
-            model_grid = {
-                "C": {"type": float, "range": [0.0, 10], "nbins": bayesian_nbins},
-                "l1_ratio": {
-                    "type": float,
-                    "range": [0.0, 1.0],
-                    "nbins": bayesian_nbins,
-                },
-            }
-        all_params = list(dict.fromkeys(model_grid))
-        from verticapy.learn.ensemble import RandomForestRegressor
+from vertica_python.errors import QueryError
+
+import verticapy._config.config as conf
+from verticapy._typing import (
+    ArrayLike,
+    NoneType,
+    PlottingObject,
+    PythonNumber,
+    PythonScalar,
+    SQLColumns,
+    SQLRelation,
+    SQLExpression,
+)
+from verticapy._utils._gen import gen_name, gen_tmp_name
+from verticapy._utils._sql._format import (
+    clean_query,
+    format_type,
+    quote_ident,
+    schema_relation,
+)
+from verticapy._utils._sql._sys import _executeSQL
+from verticapy._utils._sql._vertica_version import (
+    check_minimum_version,
+    vertica_version,
+)
+from verticapy.errors import (
+    ConversionError,
+    VersionError,
+)
+
+from verticapy.core.tablesample.base import TableSample
+from verticapy.core.vdataframe.base import vDataFrame
+
+import verticapy.machine_learning.metrics as mt
+
+from verticapy.plotting._utils import PlottingUtils
+
+from verticapy.sql.drop import drop
+
+if conf.get_import_success("graphviz"):
+    from graphviz import Source
+
+##
+#  ___      ___  ___      ___     ______    ________    _______  ___
+# |"  \    /"  ||"  \    /"  |   /    " \  |"      "\  /"     "||"  |
+#  \   \  //  /  \   \  //   |  // ____  \ (.  ___  :)(: ______)||  |
+#   \\  \/. ./   /\\  \/.    | /  /    ) :)|: \   ) || \/    |  |:  |
+#    \.    //   |: \.        |(: (____/ // (| (___\ || // ___)_  \  |___
+#     \\   /    |.  \    /:  | \        /  |:       :)(:      "|( \_|:  \
+#      \__/     |___|\__/|___|  \"_____/   (________/  \_______) \_______)
+#
+##
 
-        hyper_param_estimator = RandomForestRegressor(
-            name=estimator.name, **RFmodel_params
-        )
-        hyper_param_estimator.fit(relation, all_params, "score")
-        from verticapy.datasets import gen_meshgrid, gen_dataset
 
-        if random_grid:
-            vdf = gen_dataset(model_grid, nrows=nrows)
-        else:
-            vdf = gen_meshgrid(model_grid)
-        drop(relation, method="table")
-        vdf.to_db(relation, relation_type="table", inplace=True)
-        vdf = hyper_param_estimator.predict(vdf, name="score")
-        reverse = reverse_score(metric)
-        vdf.sort({"score": "desc" if reverse else "asc"})
-        result = vdf.head(limit=k_tops)
-        new_param_grid = []
-        for i in range(k_tops):
-            param_tmp_grid = {}
-            for elem in result.values:
-                if elem != "score":
-                    param_tmp_grid[elem] = result[elem][i]
-            new_param_grid += [param_tmp_grid]
-    if print_info:
-        print(
-            f"\033[1m\033[4mStep 3 - Computing Most Probable Good Models using Grid Search\033[0m\033[0m\n"
-        )
-    result = grid_search_cv(
-        estimator,
-        new_param_grid,
-        input_relation,
-        X,
-        y,
-        metric,
-        cv,
-        pos_label,
-        cutoff,
-        True,
-        "no_print",
-        print_info,
-        final_print="no_print",
-    )
-    for elem in result.values:
-        result.values[elem] += param_gs[elem]
-    data = []
-    keys = [elem for elem in result.values]
-    for i in range(len(result[keys[0]])):
-        data += [tuple([result[elem][i] for elem in result.values])]
-    data.sort(key=lambda tup: tup[1], reverse=reverse)
-    for idx, elem in enumerate(result.values):
-        result.values[elem] = [item[idx] for item in data]
-    hyper_param_estimator.drop()
-    if print_info:
-        print("\033[1mBayesian Search Selected Model\033[0m")
-        print(
-            f"Parameters: {result['parameters'][0]}; \033[91mTest_score: {result['avg_score'][0]}\033[0m; \033[92mTrain_score: {result['avg_train_score'][0]}\033[0m; \033[94mTime: {result['avg_time'][0]}\033[0m;"
-        )
-    drop(relation, method="table")
-    return result
-
-
-# ---#
-def best_k(
-    input_relation: Union[str, vDataFrame],
-    X: list = [],
-    n_cluster: Union[tuple, list] = (1, 100),
-    init: Union[str, list] = "kmeanspp",
-    max_iter: int = 50,
-    tol: float = 1e-4,
-    elbow_score_stop: float = 0.8,
-    **kwargs,
-):
+class VerticaModel(PlottingUtils):
+    """
+    Base Class for Vertica Models.
     """
----------------------------------------------------------------------------
-Finds the k-means k based on a score.
 
-Parameters
-----------
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list, optional
-	List of the predictor columns. If empty, all numerical columns will
-    be used.
-n_cluster: tuple/list, optional
-	Tuple representing the number of clusters to start and end with.
-    This can also be customized list with various k values to test.
-init: str/list, optional
-	The method to use to find the initial cluster centers.
-		kmeanspp : Use the k-means++ method to initialize the centers.
-        random   : Randomly subsamples the data to find initial centers.
-	It can be also a list with the initial cluster centers to use.
-max_iter: int, optional
-	The maximum number of iterations for the algorithm.
-tol: float, optional
-	Determines whether the algorithm has converged. The algorithm is considered 
-	converged after no center has moved more than a distance of 'tol' from the 
-	previous iteration.
-elbow_score_stop: float, optional
-	Stops searching for parameters when the specified elbow score is reached.
-
-Returns
--------
-int
-	the KMeans K
-	"""
-    if isinstance(X, str):
-        X = [X]
-    check_types(
-        [
-            ("X", X, [list]),
-            ("input_relation", input_relation, [str, vDataFrame]),
-            ("n_cluster", n_cluster, [list]),
-            ("init", init, ["kmeanspp", "random"]),
-            ("max_iter", max_iter, [int, float]),
-            ("tol", tol, [int, float]),
-            ("elbow_score_stop", elbow_score_stop, [int, float]),
-        ]
-    )
+    # Properties.
+
+    @property
+    def _is_native(self) -> Literal[True]:
+        return True
+
+    @property
+    def _is_using_native(self) -> Literal[False]:
+        return False
+
+    @property
+    def object_type(self) -> Literal["VerticaModel"]:
+        return "VerticaModel"
+
+    @property
+    @abstractmethod
+    def _vertica_fit_sql(self) -> str:
+        """Must be overridden in child class"""
+        raise NotImplementedError
+
+    @property
+    @abstractmethod
+    def _model_category(self) -> str:
+        """Must be overridden in child class"""
+        raise NotImplementedError
+
+    @property
+    @abstractmethod
+    def _model_subcategory(self) -> str:
+        """Must be overridden in child class"""
+        raise NotImplementedError
+
+    @property
+    @abstractmethod
+    def _model_type(self) -> str:
+        """Must be overridden in child class"""
+        raise NotImplementedError
+
+    @property
+    def _attributes(self) -> list:
+        """Must be overridden in the final class"""
+        return []
+
+    # Formatting Methods.
+
+    @staticmethod
+    def _array_to_int(object_: np.ndarray) -> np.ndarray:
+        """
+        Converts the input numpy.array values to integer,
+        if possible.
+        """
+        res = copy.deepcopy(object_)
+        try:
+            return res.astype(int)
+        except ValueError:
+            return res
+
+    @staticmethod
+    def _format_vector(X: ArrayLike, vector: list[tuple]) -> np.ndarray:
+        """
+        Format the 2D vector to match with the input columns'
+        names.
+        """
+        res = []
+        for x in X:
+            for y in vector:
+                if quote_ident(y[0]).lower() == x.lower():
+                    res += [y[1]]
+        return np.array(res)
+
+    @staticmethod
+    def get_match_index(x: str, col_list: list, str_check: bool = True) -> None:
+        """
+        Returns the matching index.
+        """
+        for idx, col in enumerate(col_list):
+            if (str_check and quote_ident(x.lower()) == quote_ident(col.lower())) or (
+                x == col
+            ):
+                return idx
 
-    from verticapy.learn.cluster import KMeans
+    # System & Special Methods.
 
-    if isinstance(n_cluster, tuple):
-        L = range(n_cluster[0], n_cluster[1])
-    else:
-        L = n_cluster
-        L.sort()
-    schema, relation = schema_relation(input_relation)
-    if not (schema):
-        schema = verticapy.options["temp_schema"]
-    schema = quote_ident(schema)
-    if verticapy.options["tqdm"] and (
-        "tqdm" not in kwargs or ("tqdm" in kwargs and kwargs["tqdm"])
-    ):
-        from tqdm.auto import tqdm
-
-        loop = tqdm(L)
-    else:
-        loop = L
-    for i in loop:
-        model_name = gen_tmp_name(schema=schema, name="kmeans")
-        drop(model_name, method="model")
-        model = KMeans(model_name, i, init, max_iter, tol)
-        model.fit(input_relation, X)
-        score = model.metrics_.values["value"][3]
-        if score > elbow_score_stop:
-            return i
-        score_prev = score
-    print(
-        "\u26A0 The K was not found. The last K (= {}) is returned with an elbow score of {}".format(
-            i, score
-        )
-    )
-    return i
-
-
-# ---#
-def cross_validate(
-    estimator,
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    metric: Union[str, list] = "all",
-    cv: int = 3,
-    pos_label: Union[int, float, str] = None,
-    cutoff: float = -1,
-    show_time: bool = True,
-    training_score: bool = False,
-    **kwargs,
-):
-    """
----------------------------------------------------------------------------
-Computes the K-Fold cross validation of an estimator.
+    @abstractmethod
+    def __init__(self) -> None:
+        """Must be overridden in the child class"""
+        return None
+        # self.X = None
+        # self.parameters = {}
+        # for att in self._attributes:
+        #    setattr(self, att, None)
+
+    def __repr__(self) -> str:
+        """
+        Returns the model Representation.
+        """
+        return f"<{self._model_type}>"
+
+    def drop(self) -> bool:
+        """
+        Drops the model from the Vertica database.
+        """
+        return drop(self.model_name, method="model")
+
+    def _is_already_stored(
+        self,
+        raise_error: bool = False,
+        return_model_type: bool = False,
+    ) -> Union[bool, str]:
+        """
+        Checks whether the model is stored in the
+        Vertica database.
+
+        Parameters
+        ----------
+        raise_error: bool, optional
+            If set to True and an error occurs,
+            raises the error.
+        return_model_type: bool, optional
+            If set to True, returns the model type.
+
+        Returns
+        -------
+        bool
+            True if the model is stored in the
+            Vertica database.
+        """
+        return self.does_model_exists(
+            name=self.model_name,
+            raise_error=raise_error,
+            return_model_type=return_model_type,
+        )
+
+    @staticmethod
+    def does_model_exists(
+        name: str,
+        raise_error: bool = False,
+        return_model_type: bool = False,
+    ) -> Union[bool, str]:
+        """
+        Checks whether the model is stored in the Vertica
+        database.
+
+        Parameters
+        ----------
+        name: str, optional
+            Model's name.
+        raise_error: bool, optional
+            If set to True and an error occurs,
+            raises the error.
+        return_model_type: bool, optional
+            If set to True, returns the model type.
+
+        Returns
+        -------
+        bool
+            True if the model is stored in the
+            Vertica database.
+        """
+        model_type = None
+        schema, model_name = schema_relation(name)
+        schema, model_name = schema[1:-1], model_name[1:-1]
+        res = _executeSQL(
+            query=f"""
+                SELECT 
+                    /*+LABEL('learn.tools._is_already_stored')*/ 
+                    model_type 
+                FROM MODELS 
+                WHERE LOWER(model_name) = LOWER('{model_name}') 
+                  AND LOWER(schema_name) = LOWER('{schema}') 
+                LIMIT 1""",
+            method="fetchrow",
+            print_time_sql=False,
+        )
+        if res:
+            model_type = res[0]
+            res = True
+        else:
+            res = False
+        if raise_error and res:
+            raise NameError(f"The model '{model_name}' already exists !")
+        if return_model_type:
+            return model_type
+        return res
+
+    # Attributes Methods.
+
+    def get_attributes(self, attr_name: Optional[str] = None) -> Any:
+        """
+        Returns the model attributes.
+
+        Parameters
+        ----------
+        attr_name: str, optional
+            Attribute name.
+
+        Returns
+        -------
+        Any
+            model attribute.
+        """
+        if not attr_name:
+            return self._attributes
+        elif attr_name in self._attributes:
+            if hasattr(self, attr_name):
+                return copy.deepcopy(getattr(self, attr_name))
+            else:
+                return AttributeError("The attribute is not yet computed.")
+        elif attr_name + "_" in self._attributes:
+            return self.get_attributes(attr_name + "_")
+        else:
+            raise AttributeError(
+                "Method 'get_vertica_attributes' is not available for "
+                "non-native models.\nUse 'get_attributes' method instead."
+            )
 
-Parameters
-----------
-estimator: object
-	Vertica estimator with a fit method.
-input_relation: str/vDataFrame
-	Relation to use to train the model.
-X: list
-	List of the predictor columns.
-y: str
-	Response Column.
-metric: str/list, optional
-    Metric used to do the model evaluation. It can also be a list of metrics.
-        all: The model will compute all the possible metrics.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        best_cutoff : Cutoff which optimised the ROC Curve prediction.
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        aic    : Akaikes information criterion
-        bic    : Bayesian information criterion
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-	Number of folds.
-pos_label: int/float/str, optional
-	The main class to be considered as positive (classification only).
-cutoff: float, optional
-	The model cutoff (classification only).
-show_time: bool, optional
-    If set to True, the time and the average time will be added to the report.
-training_score: bool, optional
-    If set to True, the training score will be computed with the validation score.
-
-Returns
--------
-tablesample
- 	An object containing the result. For more information, see
- 	utilities.tablesample.
-	"""
-    if isinstance(X, str):
-        X = [X]
-    check_types(
-        [
-            ("X", X, [list]),
-            ("input_relation", input_relation, [str, vDataFrame]),
-            ("y", y, [str]),
-            ("metric", metric, [str, list]),
-            ("cv", cv, [int, float]),
-            ("cutoff", cutoff, [int, float]),
-        ]
-    )
-    if isinstance(input_relation, str):
-        input_relation = vDataFrameSQL(input_relation)
-    if cv < 2:
-        raise ParameterError("Cross Validation is only possible with at least 2 folds")
-    if get_model_category(estimator.type)[0] == "regressor":
-        all_metrics = [
-            "explained_variance",
-            "max_error",
-            "median_absolute_error",
-            "mean_absolute_error",
-            "mean_squared_error",
-            "root_mean_squared_error",
-            "r2",
-            "r2_adj",
-            "aic",
-            "bic",
-        ]
-    elif get_model_category(estimator.type)[0] == "classifier":
-        all_metrics = [
-            "auc",
-            "prc_auc",
-            "accuracy",
-            "log_loss",
-            "precision",
-            "recall",
-            "f1_score",
-            "mcc",
-            "informedness",
-            "markedness",
-            "csi",
-        ]
-    else:
-        raise Exception(
-            "Cross Validation is only possible for Regressors and Classifiers"
-        )
-    if metric == "all":
-        final_metrics = all_metrics
-    elif isinstance(metric, str):
-        final_metrics = [metric]
-    else:
-        final_metrics = metric
-    result = {"index": final_metrics}
-    if training_score:
-        result_train = {"index": final_metrics}
-    total_time = []
-    if verticapy.options["tqdm"] and (
-        "tqdm" not in kwargs or ("tqdm" in kwargs and kwargs["tqdm"])
-    ):
-        from tqdm.auto import tqdm
-
-        loop = tqdm(range(cv))
-    else:
-        loop = range(cv)
-    for i in loop:
-        try:
-            estimator.drop()
-        except:
-            pass
-        random_state = verticapy.options["random_state"]
-        random_state = (
-            random.randint(-10e6, 10e6) if not (random_state) else random_state + i
-        )
-        train, test = input_relation.train_test_split(
-            test_size=float(1 / cv), order_by=[X[0]], random_state=random_state
-        )
-        start_time = time.time()
-        estimator.fit(
-            train, X, y, test,
-        )
-        total_time += [time.time() - start_time]
-        if get_model_category(estimator.type)[0] == "regressor":
-            if metric == "all":
-                result["{}-fold".format(i + 1)] = estimator.regression_report().values[
-                    "value"
-                ]
-                if training_score:
-                    estimator.test_relation = estimator.input_relation
-                    result_train[
-                        "{}-fold".format(i + 1)
-                    ] = estimator.regression_report().values["value"]
-            elif isinstance(metric, str):
-                result["{}-fold".format(i + 1)] = [estimator.score(metric)]
-                if training_score:
-                    estimator.test_relation = estimator.input_relation
-                    result_train["{}-fold".format(i + 1)] = [estimator.score(metric)]
+    def get_vertica_attributes(self, attr_name: Optional[str] = None) -> TableSample:
+        """
+        Returns the model Vertica attributes. These are stored
+        in Vertica.
+
+        Parameters
+        ----------
+        attr_name: str, optional
+            Attribute name.
+
+        Returns
+        -------
+        TableSample
+            model attributes.
+        """
+        if self._is_native or self._is_using_native:
+            vertica_version(condition=[8, 1, 1])
+            if attr_name:
+                attr_name_str = f", attr_name = '{attr_name}'"
             else:
-                result["{}-fold".format(i + 1)] = [estimator.score(m) for m in metric]
-                if training_score:
-                    estimator.test_relation = estimator.input_relation
-                    result_train["{}-fold".format(i + 1)] = [
-                        estimator.score(m) for m in metric
-                    ]
+                attr_name_str = ""
+            return TableSample.read_sql(
+                query=f"""
+                    SELECT 
+                        GET_MODEL_ATTRIBUTE(
+                            USING PARAMETERS 
+                            model_name = '{self.model_name}'{attr_name_str})""",
+                title="Getting Model Attributes.",
+            )
         else:
-            if (len(estimator.classes_) > 2) and (pos_label not in estimator.classes_):
-                raise ParameterError(
-                    "'pos_label' must be in the estimator classes, it must be the main class to study for the Cross Validation"
+            raise AttributeError(
+                "Method 'get_vertica_attributes' is not available for "
+                "non-native models.\nUse 'get_attributes' method instead."
+            )
+
+    def _is_binary_classifier(self) -> Literal[False]:
+        """
+        Returns True if the model is a Binary Classifier.
+        """
+        return False
+
+    # Parameters Methods.
+
+    @staticmethod
+    def _map_to_vertica_param_dict() -> dict[str, str]:
+        """
+        Returns a dictionary used to map VerticaPy parameter
+        names to Vertica parameter names.
+        """
+        return {
+            "class_weights": "class_weight",
+            "solver": "optimizer",
+            "tol": "epsilon",
+            "max_iter": "max_iterations",
+            "penalty": "regularization",
+            "c": "lambda",
+            "l1_ratio": "alpha",
+            "n_estimators": "ntree",
+            "max_features": "mtry",
+            "sample": "sampling_size",
+            "max_leaf_nodes": "max_breadth",
+            "min_samples_leaf": "min_leaf_size",
+            "n_components": "num_components",
+            "init": "init_method",
+        }
+
+    def _map_to_vertica_param_name(self, param: str) -> str:
+        """
+        Maps the input VerticaPy parameter name to the
+        Vertica parameter name.
+        """
+        options = self._map_to_vertica_param_dict()
+        param = param.lower()
+        if param in options:
+            return options[param]
+        return param
+
+    def _get_vertica_param_dict(self) -> dict[str, str]:
+        """
+        Returns the Vertica parameters dict to use when fitting
+        the model. As some model's parameters names are not the
+        same in Vertica. It is important to map them.
+
+        Returns
+        -------
+        dict
+            Vertica parameters.
+        """
+        parameters = {}
+
+        for param in self.parameters:
+            if param == "class_weight":
+                if isinstance(self.parameters[param], (list, np.ndarray)):
+                    parameters[
+                        "class_weights"
+                    ] = f"'{', '.join([str(p) for p in self.parameters[param]])}'"
+                else:
+                    parameters["class_weights"] = f"'{self.parameters[param]}'"
+
+            elif isinstance(self.parameters[param], (str, dict)):
+                parameters[
+                    self._map_to_vertica_param_name(param)
+                ] = f"'{self.parameters[param]}'"
+
+            else:
+                parameters[self._map_to_vertica_param_name(param)] = self.parameters[
+                    param
+                ]
+
+        return parameters
+
+    def _map_to_verticapy_param_name(self, param: str) -> str:
+        """
+        Maps the Vertica parameter name to the VerticaPy one.
+        """
+        options = self._map_to_vertica_param_dict()
+        for key in options:
+            if options[key] == param:
+                return key
+        return param
+
+    def _get_verticapy_param_dict(
+        self, options: Optional[dict] = None, **kwargs
+    ) -> dict:
+        """
+        Takes as input a dictionary of Vertica options and
+        returns  the  associated  dictionary of  VerticaPy
+        options.
+        """
+        options = format_type(options, dtype=dict)
+        parameters = {}
+        map_dict = {**options, **kwargs}
+        for param in map_dict:
+            parameters[self._map_to_verticapy_param_name(param)] = map_dict[param]
+        return parameters
+
+    def get_params(self) -> dict:
+        """
+        Returns the parameters of the model.
+
+        Returns
+        -------
+        dict
+            model parameters.
+        """
+        all_init_params = list(get_type_hints(self.__init__).keys())
+        parameters = copy.deepcopy(self.parameters)
+        parameters_keys = list(parameters.keys())
+        for p in parameters_keys:
+            if p not in all_init_params:
+                del parameters[p]
+        return parameters
+
+    def set_params(self, parameters: Optional[dict] = None, **kwargs) -> None:
+        """
+        Sets the parameters of the model.
+
+        Parameters
+        ----------
+        parameters: dict, optional
+            New parameters.
+        **kwargs
+            New  parameters can  also be passed as arguments,
+            example: set_params(param1 = val1, param2 = val2)
+        """
+        parameters = format_type(parameters, dtype=dict)
+        all_init_params = list(get_type_hints(self.__init__).keys())
+        new_parameters = copy.deepcopy({**self.parameters, **kwargs})
+        new_parameters_keys = list(new_parameters.keys())
+        for p in new_parameters_keys:
+            if p not in all_init_params:
+                del new_parameters[p]
+        for p in parameters:
+            if p not in all_init_params:
+                warning_message = (
+                    f"parameter 'parameters' got an unexpected keyword argument '{p}'"
                 )
-            elif (len(estimator.classes_) == 2) and (
-                pos_label not in estimator.classes_
-            ):
-                pos_label = estimator.classes_[1]
+                warnings.warn(warning_message, Warning)
+            new_parameters[p] = parameters[p]
+        self.__init__(name=self.model_name, **new_parameters)
+
+    # Model's Summary.
+
+    def summarize(self) -> str:
+        """
+        Summarizes the model.
+        """
+        if self._is_native:
             try:
-                if metric == "all":
-                    result["{}-fold".format(i + 1)] = estimator.classification_report(
-                        labels=[pos_label], cutoff=cutoff
-                    ).values["value"][0:-1]
-                    if training_score:
-                        estimator.test_relation = estimator.input_relation
-                        result_train[
-                            "{}-fold".format(i + 1)
-                        ] = estimator.classification_report(
-                            labels=[pos_label], cutoff=cutoff
-                        ).values[
-                            "value"
-                        ][
-                            0:-1
-                        ]
+                vertica_version(condition=[9, 0, 0])
+                func = f"""
+                    GET_MODEL_SUMMARY(USING PARAMETERS 
+                    model_name = '{self.model_name}')"""
+            except VersionError:
+                func = f"SUMMARIZE_MODEL('{self.model_name}')"
+            return _executeSQL(
+                f"SELECT /*+LABEL('learn.VerticaModel.__repr__')*/ {func}",
+                title="Summarizing the model.",
+                method="fetchfirstelem",
+            )
+        else:
+            raise AttributeError(
+                "Method 'summarize' is not available for non-native models."
+            )
 
-                elif isinstance(metric, str):
-                    result["{}-fold".format(i + 1)] = [
-                        estimator.score(metric, pos_label=pos_label, cutoff=cutoff)
-                    ]
-                    if training_score:
-                        estimator.test_relation = estimator.input_relation
-                        result_train["{}-fold".format(i + 1)] = [
-                            estimator.score(metric, pos_label=pos_label, cutoff=cutoff)
-                        ]
-                else:
-                    result["{}-fold".format(i + 1)] = [
-                        estimator.score(m, pos_label=pos_label, cutoff=cutoff)
-                        for m in metric
-                    ]
-                    if training_score:
-                        estimator.test_relation = estimator.input_relation
-                        result_train["{}-fold".format(i + 1)] = [
-                            estimator.score(m, pos_label=pos_label, cutoff=cutoff)
-                            for m in metric
-                        ]
-            except:
-                if metric == "all":
-                    result["{}-fold".format(i + 1)] = estimator.classification_report(
-                        cutoff=cutoff
-                    ).values["value"][0:-1]
-                    if training_score:
-                        estimator.test_relation = estimator.input_relation
-                        result_train[
-                            "{}-fold".format(i + 1)
-                        ] = estimator.classification_report(cutoff=cutoff).values[
-                            "value"
-                        ][
-                            0:-1
-                        ]
-                elif isinstance(metric, str):
-                    result["{}-fold".format(i + 1)] = [
-                        estimator.score(metric, cutoff=cutoff)
-                    ]
-                    if training_score:
-                        estimator.test_relation = estimator.input_relation
-                        result_train["{}-fold".format(i + 1)] = [
-                            estimator.score(metric, cutoff=cutoff)
-                        ]
-                else:
-                    result["{}-fold".format(i + 1)] = [
-                        estimator.score(m, cutoff=cutoff) for m in metric
-                    ]
-                    if training_score:
-                        estimator.test_relation = estimator.input_relation
-                        result_train["{}-fold".format(i + 1)] = [
-                            estimator.score(m, cutoff=cutoff) for m in metric
-                        ]
-        try:
-            estimator.drop()
-        except:
-            pass
-    n = len(final_metrics)
-    total = [[] for item in range(n)]
-    for i in range(cv):
-        for k in range(n):
-            total[k] += [result["{}-fold".format(i + 1)][k]]
-    if training_score:
-        total_train = [[] for item in range(n)]
-        for i in range(cv):
-            for k in range(n):
-                total_train[k] += [result_train["{}-fold".format(i + 1)][k]]
-    result["avg"], result["std"] = [], []
-    if training_score:
-        result_train["avg"], result_train["std"] = [], []
-    for item in total:
-        result["avg"] += [statistics.mean([float(elem) for elem in item])]
-        result["std"] += [statistics.stdev([float(elem) for elem in item])]
-    if training_score:
-        for item in total_train:
-            result_train["avg"] += [statistics.mean([float(elem) for elem in item])]
-            result_train["std"] += [statistics.stdev([float(elem) for elem in item])]
-    total_time += [
-        statistics.mean([float(elem) for elem in total_time]),
-        statistics.stdev([float(elem) for elem in total_time]),
-    ]
-    result = tablesample(values=result).transpose()
-    if show_time:
-        result.values["time"] = total_time
-    if training_score:
-        result_train = tablesample(values=result_train).transpose()
-        if show_time:
-            result_train.values["time"] = total_time
-    if training_score:
-        return result, result_train
-    else:
-        return result
-
-
-# ---#
-def elbow(
-    input_relation: Union[str, vDataFrame],
-    X: list = [],
-    n_cluster: Union[tuple, list] = (1, 15),
-    init: Union[str, list] = "kmeanspp",
-    max_iter: int = 50,
-    tol: float = 1e-4,
-    ax=None,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Draws an elbow curve.
+    # I/O Methods.
 
-Parameters
-----------
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list, optional
-    List of the predictor columns. If empty all the numerical vcolumns will
-    be used.
-n_cluster: tuple/list, optional
-    Tuple representing the number of cluster to start with and to end with.
-    It can also be customized list with the different K to test.
-init: str/list, optional
-    The method to use to find the initial cluster centers.
-        kmeanspp : Use the k-means++ method to initialize the centers.
-        random   : Randomly subsamples the data to find initial centers.
-    Alternatively, you can specify a list with the initial custer centers.
-max_iter: int, optional
-    The maximum number of iterations for the algorithm.
-tol: float, optional
-    Determines whether the algorithm has converged. The algorithm is considered 
-    converged after no center has moved more than a distance of 'tol' from the 
-    previous iteration.
-ax: Matplotlib axes object, optional
-    The axes to plot on.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    if isinstance(X, str):
-        X = [X]
-    check_types(
-        [
-            ("X", X, [list]),
-            ("input_relation", input_relation, [str, vDataFrame]),
-            ("n_cluster", n_cluster, [list]),
-            ("init", init, ["kmeanspp", "random"]),
-            ("max_iter", max_iter, [int, float]),
-            ("tol", tol, [int, float]),
-        ]
-    )
-    version(condition=[8, 0, 0])
-    if isinstance(n_cluster, tuple):
-        L = range(n_cluster[0], n_cluster[1])
-    else:
-        L = n_cluster
-        L.sort()
-    schema, relation = schema_relation(input_relation)
-    all_within_cluster_SS, model_name = [], gen_tmp_name(schema=schema, name="kmeans")
-    if isinstance(n_cluster, tuple):
-        L = [i for i in range(n_cluster[0], n_cluster[1])]
-    else:
-        L = n_cluster
-        L.sort()
-    if verticapy.options["tqdm"]:
-        from tqdm.auto import tqdm
-
-        loop = tqdm(L)
-    else:
-        loop = L
-    for i in loop:
-        drop(model_name, method="model")
-        from verticapy.learn.cluster import KMeans
-
-        model = KMeans(model_name, i, init, max_iter, tol)
-        model.fit(input_relation, X)
-        all_within_cluster_SS += [float(model.metrics_.values["value"][3])]
-        model.drop()
-    if not (ax):
-        fig, ax = plt.subplots()
-        if isnotebook():
-            fig.set_size_inches(8, 6)
-        ax.grid(axis="y")
-    param = {
-        "color": gen_colors()[0],
-        "marker": "o",
-        "markerfacecolor": "white",
-        "markersize": 7,
-        "markeredgecolor": "black",
-    }
-    ax.plot(L, all_within_cluster_SS, **updated_dict(param, style_kwds))
-    ax.set_title("Elbow Curve")
-    ax.set_xlabel("Number of Clusters")
-    ax.set_ylabel("Between-Cluster SS / Total SS")
-    values = {"index": L, "Within-Cluster SS": all_within_cluster_SS}
-    return tablesample(values=values)
-
-
-# ---#
-def enet_search_cv(
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    metric: str = "auto",
-    cv: int = 3,
-    estimator_type: str = "auto",
-    cutoff: float = -1,
-    print_info: bool = True,
-    **kwargs,
-):
-    """
----------------------------------------------------------------------------
-Computes the k-fold grid search using multiple ENet models.
+    def deploySQL(self, X: Optional[SQLColumns] = None) -> str:
+        """
+        Returns the SQL code needed to deploy the model.
+
+        Parameters
+        ----------
+        X: SQLColumns, optional
+            List of the columns used to deploy the model.
+            If empty,  the model predictors are used.
+
+        Returns
+        -------
+        str
+            the SQL code needed to deploy the model.
+        """
+        if self._vertica_predict_sql:
+            X = format_type(X, dtype=list, na_out=self.X)
+            X = quote_ident(X)
+            sql = f"""
+                {self._vertica_predict_sql}({', '.join(X)} 
+                                            USING PARAMETERS 
+                                            model_name = '{self.model_name}',
+                                            match_by_pos = 'true')"""
+            return clean_query(sql)
+        else:
+            raise AttributeError(
+                f"Method 'deploySQL' does not exist for {self._model_type} models."
+            )
 
-Parameters
-----------
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-estimator_type: str, optional
-    Estimator Type.
-        auto : detects if it is a Logit Model or ENet.
-        logit: Logistic Regression
-        enet : ElasticNet
-cutoff: float, optional
-    The model cutoff (logit only).
-print_info: bool, optional
-    If set to True, prints the model information at each step.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    check_types([("estimator_type", estimator_type, ["logit", "enet", "auto"])])
-    param_grid = parameter_grid(
-        {
-            "solver": ["cgd"],
-            "penalty": ["enet"],
-            "C": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 5.0, 10.0, 50.0, 100.0]
-            if "small" not in kwargs
-            else [1e-1, 1.0, 10.0],
-            "l1_ratio": [0.1 * i for i in range(0, 10)]
-            if "small" not in kwargs
-            else [0.1, 0.5, 0.9],
-        }
-    )
+    def to_python(
+        self,
+        return_proba: bool = False,
+        return_distance_clusters: bool = False,
+    ) -> Callable:
+        """
+        Returns the Python function needed for in-memory
+        scoring  without using built-in Vertica functions.
+
+        Parameters
+        ----------
+        return_proba: bool, optional
+            If  set to True  and  the  model is a  classifier,
+            the  function  returns  the  model  probabilities.
+        return_distance_clusters: bool, optional
+            If  set to  True and the  model is  cluster-based,
+            the function returns the model clusters distances.
+            If the model is KPrototypes, the  function returns
+            the dissimilarity function.
+
+        Returns
+        -------
+        Callable
+            Python function.
+        """
+        model = self.to_memmodel()
+        if return_proba:
+            return model.predict_proba
+        elif hasattr(model, "predict") and not return_distance_clusters:
+            return model.predict
+        else:
+            return model.transform
 
-    from verticapy.learn.linear_model import LogisticRegression, ElasticNet
+    def to_sql(
+        self,
+        X: Optional[SQLColumns] = None,
+        return_proba: bool = False,
+        return_distance_clusters: bool = False,
+    ) -> SQLExpression:
+        """
+        Returns  the SQL  code  needed  to deploy the  model
+        without using built-in Vertica functions.
+
+        Parameters
+        ----------
+        X: SQLColumns, optional
+            Input predictor's name.
+        return_proba: bool, optional
+            If  set to  True and  the  model is a  classifier,
+            the function returns the class probabilities.
+        return_distance_clusters: bool, optional
+            If  set to  True and the  model is  cluster-based,
+            the function returns the model clusters distances.
+            If the model is  KPrototypes, the function returns
+            the dissimilarity function.
+
+        Returns
+        -------
+        SQLExpression
+            SQL code.
+        """
+        X = format_type(X, dtype=list)
+        if len(X) == 0:
+            X = self.X
+        model = self.to_memmodel()
+        if return_proba:
+            return model.predict_proba_sql(X)
+        elif hasattr(model, "predict") and not return_distance_clusters:
+            return model.predict_sql(X)
+        else:
+            return model.transform_sql(X)
 
-    if estimator_type == "auto":
-        if not (isinstance(input_relation, vDataFrame)):
-            vdf = vDataFrameSQL(input_relation)
-        else:
-            vdf = input_relation
-        if sorted(vdf[y].distinct()) == [0, 1]:
-            estimator_type = "logit"
-        else:
-            estimator_type = "enet"
-    if estimator_type == "logit":
-        estimator = LogisticRegression(
-            gen_tmp_name(schema=verticapy.options["temp_schema"], name="logit")
-        )
-    else:
-        estimator = ElasticNet(
-            gen_tmp_name(schema=verticapy.options["temp_schema"], name="enet")
-        )
-    result = bayesian_search_cv(
-        estimator,
-        input_relation,
-        X,
-        y,
-        metric,
-        cv,
-        None,
-        cutoff,
-        param_grid,
-        random_grid=False,
-        bayesian_nbins=1000,
-        print_info=print_info,
-        enet=True,
-    )
-    return result
-
-
-# ---#
-def gen_params_grid(
-    estimator,
-    nbins: int = 10,
-    max_nfeatures: int = 3,
-    lmax: int = -1,
-    optimized_grid: int = 0,
-):
-    """
----------------------------------------------------------------------------
-Generates the estimator grid.
+    # Plotting Methods.
 
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-nbins: int, optional
-    Number of bins used to discretize numberical features.
-max_nfeatures: int, optional
-    Maximum number of features used to compute Random Forest, PCA...
-lmax: int, optional
-    Maximum length of the parameter grid.
-optimized_grid: int, optional
-    If set to 0, the randomness is based on the input parameters.
-    If set to 1, the randomness is limited to some parameters while others
-    are picked based on a default grid.
-    If set to 2, there is no randomness and a default grid is returned.
-    
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    from verticapy.learn.cluster import KMeans, BisectingKMeans, DBSCAN
-    from verticapy.learn.decomposition import PCA, SVD
-    from verticapy.learn.ensemble import (
-        RandomForestRegressor,
-        RandomForestClassifier,
-        XGBoostRegressor,
-        XGBoostClassifier,
-    )
-    from verticapy.learn.linear_model import (
-        LinearRegression,
-        ElasticNet,
-        Lasso,
-        Ridge,
-        LogisticRegression,
-    )
-    from verticapy.learn.naive_bayes import NaiveBayes
-    from verticapy.learn.neighbors import (
-        KNeighborsRegressor,
-        KNeighborsClassifier,
-        LocalOutlierFactor,
-        NearestCentroid,
-    )
-    from verticapy.learn.preprocessing import Normalizer, OneHotEncoder
-    from verticapy.learn.svm import LinearSVC, LinearSVR
-    from verticapy.learn.tree import (
-        DummyTreeRegressor,
-        DummyTreeClassifier,
-        DecisionTreeRegressor,
-        DecisionTreeClassifier,
-    )
-
-    params_grid = {}
-    if isinstance(estimator, (DummyTreeRegressor, DummyTreeClassifier, OneHotEncoder)):
-        return params_grid
-    elif isinstance(
-        estimator,
-        (
-            RandomForestRegressor,
-            RandomForestClassifier,
-            DecisionTreeRegressor,
-            DecisionTreeClassifier,
-        ),
-    ):
-        if optimized_grid == 0:
-            params_grid = {
-                "max_features": ["auto", "max"]
-                + list(range(1, max_nfeatures, math.ceil(max_nfeatures / nbins))),
-                "max_leaf_nodes": list(range(1, int(1e9), math.ceil(int(1e9) / nbins))),
-                "max_depth": list(range(1, 100, math.ceil(100 / nbins))),
-                "min_samples_leaf": list(
-                    range(1, int(1e6), math.ceil(int(1e6) / nbins))
-                ),
-                "min_info_gain": [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ],
-                "nbins": list(range(2, 100, math.ceil(100 / nbins))),
-            }
-            if isinstance(RandomForestRegressor, RandomForestClassifier):
-                params_grid["sample"] = [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ]
-                params_grid["n_estimators"] = list(
-                    range(1, 100, math.ceil(100 / nbins))
-                )
-        elif optimized_grid == 1:
-            params_grid = {
-                "max_features": ["auto", "max"],
-                "max_leaf_nodes": [32, 64, 128, 1000, 1e4, 1e6, 1e9],
-                "max_depth": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50],
-                "min_samples_leaf": [1, 2, 3, 4, 5],
-                "min_info_gain": [0.0, 0.1, 0.2],
-                "nbins": [10, 15, 20, 25, 30, 35, 40],
-            }
-            if isinstance(RandomForestRegressor, RandomForestClassifier):
-                params_grid["sample"] = [
-                    0.1,
-                    0.2,
-                    0.3,
-                    0.4,
-                    0.5,
-                    0.6,
-                    0.7,
-                    0.8,
-                    0.9,
-                    1.0,
-                ]
-                params_grid["n_estimators"] = [1, 5, 10, 15, 20, 30, 40, 50, 100]
-        elif optimized_grid == 2:
-            params_grid = {
-                "max_features": ["auto", "max"],
-                "max_leaf_nodes": [32, 64, 128, 1000],
-                "max_depth": [4, 5, 6],
-                "min_samples_leaf": [1, 2],
-                "min_info_gain": [0.0],
-                "nbins": [32],
-            }
-            if isinstance(RandomForestRegressor, RandomForestClassifier):
-                params_grid["sample"] = [0.7]
-                params_grid["n_estimators"] = [20]
-        elif optimized_grid == -666:
-            result = {
-                "max_features": {
-                    "type": int,
-                    "range": [1, max_nfeatures],
-                    "nbins": nbins,
-                },
-                "max_leaf_nodes": {"type": int, "range": [32, 1e9], "nbins": nbins},
-                "max_depth": {"type": int, "range": [2, 30], "nbins": nbins},
-                "min_samples_leaf": {"type": int, "range": [1, 15], "nbins": nbins},
-                "min_info_gain": {"type": float, "range": [0.0, 0.1], "nbins": nbins},
-                "nbins": {"type": int, "range": [10, 1000], "nbins": nbins},
-            }
-            if isinstance(RandomForestRegressor, RandomForestClassifier):
-                result["sample"] = {"type": float, "range": [0.1, 1.0], "nbins": nbins}
-                result["n_estimators"] = {
-                    "type": int,
-                    "range": [1, 100],
-                    "nbins": nbins,
-                }
-            return result
-    elif isinstance(estimator, (LinearSVC, LinearSVR)):
-        if optimized_grid == 0:
-            params_grid = {
-                "tol": [1e-4, 1e-6, 1e-8],
-                "C": [elem / 1000 for elem in range(1, 5000, math.ceil(5000 / nbins))],
-                "fit_intercept": [False, True],
-                "intercept_mode": ["regularized", "unregularized"],
-                "max_iter": [100, 500, 1000],
-            }
-        elif optimized_grid == 1:
-            params_grid = {
-                "tol": [1e-6],
-                "C": [1e-1, 0.0, 1.0, 10.0],
-                "fit_intercept": [True],
-                "intercept_mode": ["regularized", "unregularized"],
-                "max_iter": [100],
-            }
-        elif optimized_grid == 2:
-            params_grid = {
-                "tol": [1e-6],
-                "C": [0.0, 1.0],
-                "fit_intercept": [True],
-                "intercept_mode": ["regularized", "unregularized"],
-                "max_iter": [100],
-            }
-        elif optimized_grid == -666:
-            return {
-                "tol": {"type": float, "range": [1e-8, 1e-2], "nbins": nbins},
-                "C": {"type": float, "range": [0.0, 1000.0], "nbins": nbins},
-                "fit_intercept": {"type": bool},
-                "intercept_mode": {
-                    "type": str,
-                    "values": ["regularized", "unregularized"],
-                },
-                "max_iter": {"type": int, "range": [10, 1000], "nbins": nbins},
-            }
-    elif isinstance(estimator, (XGBoostClassifier, XGBoostRegressor)):
-        if optimized_grid == 0:
-            params_grid = {
-                "nbins": list(range(2, 100, math.ceil(100 / nbins))),
-                "max_depth": list(range(1, 20, math.ceil(100 / nbins))),
-                "weight_reg": [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ],
-                "min_split_loss": [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ],
-                "learning_rate": [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ],
-                # "sample": [elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))],
-                "tol": [1e-4, 1e-6, 1e-8],
-                "max_ntree": list(range(1, 100, math.ceil(100 / nbins))),
-            }
-        elif optimized_grid == 1:
-            params_grid = {
-                "nbins": [10, 15, 20, 25, 30, 35, 40],
-                "max_depth": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
-                "weight_reg": [0.0, 0.5, 1.0, 2.0],
-                "min_split_loss": [0.0, 0.1, 0.25],
-                "learning_rate": [0.01, 0.05, 0.1, 1.0],
-                # "sample": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
-                "tol": [1e-8],
-                "max_ntree": [1, 10, 20, 30, 40, 50, 100],
-            }
-        elif optimized_grid == 2:
-            params_grid = {
-                "nbins": [32],
-                "max_depth": [3, 4, 5],
-                "weight_reg": [0.0, 0.25],
-                "min_split_loss": [0.0],
-                "learning_rate": [0.05, 0.1, 1.0],
-                # "sample": [0.5, 0.6, 0.7],
-                "tol": [1e-8],
-                "max_ntree": [20],
-            }
-        elif optimized_grid == -666:
-            return {
-                "nbins": {"type": int, "range": [2, 100], "nbins": nbins},
-                "max_depth": {"type": int, "range": [1, 20], "nbins": nbins},
-                "weight_reg": {"type": float, "range": [0.0, 1.0], "nbins": nbins},
-                "min_split_loss": {
-                    "type": float,
-                    "values": [0.0, 0.25],
-                    "nbins": nbins,
-                },
-                "learning_rate": {"type": float, "range": [0.0, 1.0], "nbins": nbins},
-                "sample": {"type": float, "range": [0.0, 1.0], "nbins": nbins},
-                "tol": {"type": float, "range": [1e-8, 1e-2], "nbins": nbins},
-                "max_ntree": {"type": int, "range": [1, 20], "nbins": nbins},
-            }
-    elif isinstance(estimator, NaiveBayes):
-        if optimized_grid == 0:
-            params_grid = {
-                "alpha": [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ]
-            }
-        elif optimized_grid == 1:
-            params_grid = {"alpha": [0.01, 0.1, 1.0, 5.0, 10.0]}
-        elif optimized_grid == 2:
-            params_grid = {"alpha": [0.01, 1.0, 10.0]}
-        elif optimized_grid == -666:
-            return {
-                "alpha": {"type": float, "range": [0.00001, 1000.0], "nbins": nbins}
-            }
-    elif isinstance(estimator, (PCA, SVD)):
-        if optimized_grid == 0:
-            params_grid = {
-                "max_features": list(
-                    range(1, max_nfeatures, math.ceil(max_nfeatures / nbins))
-                )
-            }
-        if isinstance(estimator, (PCA)):
-            params_grid["scale"] = [False, True]
-        if optimized_grid == -666:
-            return {
-                "scale": {"type": bool},
-                "max_features": {
-                    "type": int,
-                    "range": [1, max_nfeatures],
-                    "nbins": nbins,
-                },
-            }
-    elif isinstance(estimator, (Normalizer)):
-        params_grid = {"method": ["minmax", "robust_zscore", "zscore"]}
-        if optimized_grid == -666:
-            return {
-                "method": {"type": str, "values": ["minmax", "robust_zscore", "zscore"]}
-            }
-    elif isinstance(
-        estimator,
-        (
-            KNeighborsRegressor,
-            KNeighborsClassifier,
-            LocalOutlierFactor,
-            NearestCentroid,
-        ),
-    ):
-        if optimized_grid == 0:
-            params_grid = {
-                "p": [1, 2] + list(range(3, 100, math.ceil(100 / (nbins - 2))))
+    def _get_plot_args(self, method: Optional[str] = None) -> list:
+        """
+        Returns the args used by plotting methods.
+        """
+        if method == "contour":
+            args = [self.X, self.deploySQL(X=self.X)]
+        else:
+            raise NotImplementedError
+        return args
+
+    def _get_plot_kwargs(
+        self,
+        nbins: int = 30,
+        chart: Optional[PlottingObject] = None,
+        method: Optional[str] = None,
+    ) -> dict:
+        """
+        Returns the kwargs used by plotting methods.
+        """
+        res = {"nbins": nbins, "chart": chart}
+        if method == "contour":
+            if self._model_subcategory == "CLASSIFIER":
+                res["func_name"] = f"p({self.y} = 1)"
+            else:
+                res["func_name"] = self.y
+        else:
+            raise NotImplementedError
+        return res
+
+    def contour(
+        self,
+        nbins: int = 100,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the model's contour plot.
+
+        Parameters
+        ----------
+        nbins: int, optional
+            Number of bins used to discretize the
+            two predictors.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        return vDataFrame(self.input_relation).contour(
+            *self._get_plot_args(method="contour"),
+            **self._get_plot_kwargs(nbins=nbins, chart=chart, method="contour"),
+            **style_kwargs,
+        )
+
+
+class Supervised(VerticaModel):
+    # Properties
+
+    @property
+    @abstractmethod
+    def _vertica_predict_sql(self) -> str:
+        """Must be overridden in child class"""
+        raise NotImplementedError
+
+    # System & Special Methods.
+
+    @abstractmethod
+    def __init__(self) -> None:
+        """Must be overridden in the child class"""
+        super().__init__()
+        # self.test_relation = None
+        # self.y = None
+
+    # Model Fitting Method.
+
+    def fit(
+        self,
+        input_relation: SQLRelation,
+        X: SQLColumns,
+        y: str,
+        test_relation: SQLRelation = "",
+    ) -> Optional[str]:
+        """
+        Trains the model.
+
+        Parameters
+        ----------
+        input_relation: SQLRelation
+                Training relation.
+        X: SQLColumns
+                List of the predictors.
+        y: str
+                Response column.
+        test_relation: SQLRelation, optional
+                Relation used to test the model.
+
+        Returns
+        -------
+        str
+            model's summary.
+        """
+        if conf.get_option("overwrite_model"):
+            self.drop()
+        else:
+            self._is_already_stored(raise_error=True)
+        X = format_type(X, dtype=list)
+        self.X = quote_ident(X)
+        self.y = quote_ident(y)
+        id_column, id_column_name = "", gen_tmp_name(name="id_column")
+        if self._is_native:
+            nb_lookup_table = {
+                "bernoulli": "bool",
+                "categorical": "varchar",
+                "multinomial": "int",
+                "gaussian": "float",
             }
-            if isinstance(
-                estimator,
-                (KNeighborsRegressor, KNeighborsClassifier, LocalOutlierFactor),
+            if (self._model_type == "NaiveBayes") and (
+                self.parameters["nbtype"] in nb_lookup_table
             ):
-                params_grid["n_neighbors"] = list(
-                    range(1, 100, math.ceil(100 / (nbins)))
+                new_types = {}
+                for x in self.X:
+                    new_types[x] = nb_lookup_table[self.parameters["nbtype"]]
+                if not isinstance(input_relation, vDataFrame):
+                    input_relation = vDataFrame(input_relation)
+                else:
+                    input_relation.copy()
+                input_relation.astype(new_types)
+            if self._model_type in (
+                "RandomForestClassifier",
+                "RandomForestRegressor",
+                "XGBClassifier",
+                "XGBRegressor",
+            ) and isinstance(conf.get_option("random_state"), int):
+                id_column = f""", 
+                    ROW_NUMBER() OVER 
+                    (ORDER BY {', '.join(X)}) 
+                    AS {id_column_name}"""
+            tmp_view = False
+        if isinstance(input_relation, vDataFrame) or (id_column):
+            tmp_view = True
+            if isinstance(input_relation, vDataFrame):
+                self.input_relation = input_relation.current_relation()
+            else:
+                self.input_relation = input_relation
+            if self._is_native:
+                relation = gen_tmp_name(
+                    schema=schema_relation(self.model_name)[0], name="view"
                 )
-        elif optimized_grid == 1:
-            params_grid = {"p": [1, 2, 3, 4]}
-            if isinstance(
-                estimator,
-                (KNeighborsRegressor, KNeighborsClassifier, LocalOutlierFactor),
-            ):
-                params_grid["n_neighbors"] = [1, 2, 3, 4, 5, 10, 20, 100]
-        elif optimized_grid == 2:
-            params_grid = {"p": [1, 2]}
-            if isinstance(
-                estimator,
-                (KNeighborsRegressor, KNeighborsClassifier, LocalOutlierFactor),
-            ):
-                params_grid["n_neighbors"] = [5, 10]
-        elif optimized_grid == -666:
-            return {
-                "p": {"type": int, "range": [1, 10], "nbins": nbins},
-                "n_neighbors": {"type": int, "range": [1, 100], "nbins": nbins},
-            }
-    elif isinstance(estimator, (DBSCAN)):
-        if optimized_grid == 0:
-            params_grid = {
-                "p": [1, 2] + list(range(3, 100, math.ceil(100 / (nbins - 2)))),
-                "eps": [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ],
-                "min_samples": list(range(1, 1000, math.ceil(1000 / nbins))),
-            }
-        elif optimized_grid == 1:
-            params_grid = {"p": [1, 2, 3, 4], "min_samples": [1, 2, 3, 4, 5, 10, 100]}
-        elif optimized_grid == 2:
-            params_grid = {"p": [1, 2], "min_samples": [5, 10]}
-        elif optimized_grid == -666:
-            return {
-                "p": {"type": int, "range": [1, 10], "nbins": nbins},
-                "min_samples": {"type": int, "range": [1, 100], "nbins": nbins},
-            }
-    elif isinstance(
-        estimator, (LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge)
-    ):
-        if optimized_grid == 0:
-            params_grid = {"tol": [1e-4, 1e-6, 1e-8], "max_iter": [100, 500, 1000]}
-            if isinstance(estimator, LogisticRegression):
-                params_grid["penalty"] = ["none", "l1", "l2", "enet"]
-            if isinstance(estimator, LinearRegression):
-                params_grid["solver"] = ["newton", "bfgs"]
-            elif isinstance(estimator, (Lasso, LogisticRegression, ElasticNet)):
-                params_grid["solver"] = ["newton", "bfgs", "cgd"]
-            if isinstance(estimator, (Lasso, Ridge, ElasticNet, LogisticRegression)):
-                params_grid["C"] = [
-                    elem / 1000 for elem in range(1, 5000, math.ceil(5000 / nbins))
-                ]
-            if isinstance(estimator, (LogisticRegression, ElasticNet)):
-                params_grid["l1_ratio"] = [
-                    elem / 1000 for elem in range(1, 1000, math.ceil(1000 / nbins))
-                ]
-        elif optimized_grid == 1:
-            params_grid = {"tol": [1e-6], "max_iter": [100]}
-            if isinstance(estimator, LogisticRegression):
-                params_grid["penalty"] = ["none", "l1", "l2", "enet"]
-            if isinstance(estimator, LinearRegression):
-                params_grid["solver"] = ["newton", "bfgs"]
-            elif isinstance(estimator, (Lasso, LogisticRegression, ElasticNet)):
-                params_grid["solver"] = ["newton", "bfgs", "cgd"]
-            if isinstance(estimator, (Lasso, Ridge, ElasticNet, LogisticRegression)):
-                params_grid["C"] = [1e-1, 0.0, 1.0, 10.0]
-            if isinstance(estimator, (LogisticRegression)):
-                params_grid["penalty"] = ["none", "l1", "l2", "enet"]
-            if isinstance(estimator, (LogisticRegression, ElasticNet)):
-                params_grid["l1_ratio"] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
-        elif optimized_grid == 2:
-            params_grid = {"tol": [1e-6], "max_iter": [100]}
-            if isinstance(estimator, LogisticRegression):
-                params_grid["penalty"] = ["none", "l1", "l2", "enet"]
-            if isinstance(estimator, LinearRegression):
-                params_grid["solver"] = ["newton", "bfgs"]
-            elif isinstance(estimator, (Lasso, LogisticRegression, ElasticNet)):
-                params_grid["solver"] = ["bfgs", "cgd"]
-            if isinstance(estimator, (Lasso, Ridge, ElasticNet, LogisticRegression)):
-                params_grid["C"] = [1.0]
-            if isinstance(estimator, (LogisticRegression)):
-                params_grid["penalty"] = ["none", "l1", "l2", "enet"]
-            if isinstance(estimator, (LogisticRegression, ElasticNet)):
-                params_grid["l1_ratio"] = [0.5]
-        elif optimized_grid == -666:
-            result = {
-                "tol": {"type": float, "range": [1e-8, 1e-2], "nbins": nbins},
-                "max_iter": {"type": int, "range": [1, 1000], "nbins": nbins},
-            }
-            if isinstance(estimator, LogisticRegression):
-                result["penalty"] = {
-                    "type": str,
-                    "values": ["none", "l1", "l2", "enet"],
-                }
-            if isinstance(estimator, LinearRegression):
-                result["solver"] = {"type": str, "values": ["newton", "bfgs"]}
-            elif isinstance(estimator, (Lasso, LogisticRegression, ElasticNet)):
-                result["solver"] = {"type": str, "values": ["bfgs", "cgd"]}
-            if isinstance(estimator, (Lasso, Ridge, ElasticNet, LogisticRegression)):
-                result["C"] = {"type": float, "range": [0.0, 1000.0], "nbins": nbins}
-            if isinstance(estimator, (LogisticRegression)):
-                result["penalty"] = {
-                    "type": str,
-                    "values": ["none", "l1", "l2", "enet"],
-                }
-            if isinstance(estimator, (LogisticRegression, ElasticNet)):
-                result["l1_ratio"] = {
-                    "type": float,
-                    "range": [0.0, 1.0],
-                    "nbins": nbins,
-                }
-            return result
-    elif isinstance(estimator, KMeans):
-        if optimized_grid == 0:
-            params_grid = {
-                "n_cluster": list(range(2, 100, math.ceil(100 / nbins))),
-                "init": ["kmeanspp", "random"],
-                "max_iter": [100, 500, 1000],
-                "tol": [1e-4, 1e-6, 1e-8],
-            }
-        elif optimized_grid == 1:
-            params_grid = {
-                "n_cluster": [
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9,
-                    10,
-                    15,
-                    20,
-                    50,
-                    100,
-                    200,
-                    300,
-                    1000,
-                ],
-                "init": ["kmeanspp", "random"],
-                "max_iter": [1000],
-                "tol": [1e-8],
-            }
-        elif optimized_grid == 2:
-            params_grid = {
-                "n_cluster": [2, 3, 4, 5, 10, 20, 100],
-                "init": ["kmeanspp"],
-                "max_iter": [1000],
-                "tol": [1e-8],
-            }
-        elif optimized_grid == -666:
-            return {
-                "tol": {"type": float, "range": [1e-2, 1e-8], "nbins": nbins},
-                "max_iter": {"type": int, "range": [1, 1000], "nbins": nbins},
-                "n_cluster": {"type": int, "range": [1, 10000], "nbins": nbins},
-                "init": {"type": str, "values": ["kmeanspp", "random"]},
-            }
-    elif isinstance(estimator, BisectingKMeans):
-        if optimized_grid == 0:
-            params_grid = {
-                "n_cluster": list(range(2, 100, math.ceil(100 / nbins))),
-                "bisection_iterations": list(range(10, 1000, math.ceil(1000 / nbins))),
-                "split_method": ["size", "sum_squares"],
-                "min_divisible_cluster_size": list(
-                    range(2, 100, math.ceil(100 / nbins))
-                ),
-                "init": ["kmeanspp", "pseudo"],
-                "max_iter": [100, 500, 1000],
-                "tol": [1e-4, 1e-6, 1e-8],
-            }
-        elif optimized_grid == 1:
-            params_grid = {
-                "n_cluster": [
-                    2,
-                    3,
-                    4,
-                    5,
-                    6,
-                    7,
-                    8,
-                    9,
-                    10,
-                    15,
-                    20,
-                    50,
-                    100,
-                    200,
-                    300,
-                    1000,
-                ],
-                "bisection_iterations": list(range(10, 1000, math.ceil(1000 / nbins))),
-                "split_method": ["size", "sum_squares"],
-                "min_divisible_cluster_size": list(
-                    range(2, 100, math.ceil(100 / nbins))
-                ),
-                "init": ["kmeanspp", "pseudo"],
-                "max_iter": [1000],
-                "tol": [1e-8],
-            }
-        elif optimized_grid == 2:
-            params_grid = {
-                "n_cluster": [2, 3, 4, 5, 10, 20, 100],
-                "bisection_iterations": [1, 2, 3],
-                "split_method": ["sum_squares"],
-                "min_divisible_cluster_size": [2, 3, 4],
-                "init": ["kmeanspp"],
-                "max_iter": [1000],
-                "tol": [1e-8],
-            }
-        elif optimized_grid == -666:
-            return {
-                "tol": {"type": float, "range": [1e-8, 1e-2], "nbins": nbins},
-                "max_iter": {"type": int, "range": [1, 1000], "nbins": nbins},
-                "bisection_iterations": {
-                    "type": int,
-                    "range": [1, 1000],
-                    "nbins": nbins,
-                },
-                "split_method": {"type": str, "values": ["sum_squares"]},
-                "n_cluster": {"type": int, "range": [1, 10000], "nbins": nbins},
-                "init": {"type": str, "values": ["kmeanspp", "pseudo"]},
-            }
-    params_grid = parameter_grid(params_grid)
-    final_param_grid = []
-    for param in params_grid:
-        if "C" in param and param["C"] == 0:
-            del param["C"]
-            if "l1_ratio" in param:
-                del param["l1_ratio"]
-            if "penalty" in param:
-                param["penalty"] = "none"
-        if "penalty" in param:
+                drop(relation, method="view")
+                _executeSQL(
+                    query=f"""
+                        CREATE VIEW {relation} AS 
+                            SELECT 
+                                /*+LABEL('learn.VerticaModel.fit')*/ 
+                                *{id_column} 
+                            FROM {self.input_relation}""",
+                    title="Creating a temporary view to fit the model.",
+                )
+        else:
+            self.input_relation = input_relation
+            relation = input_relation
+        if isinstance(test_relation, vDataFrame):
+            self.test_relation = test_relation.current_relation()
+        elif test_relation:
+            self.test_relation = test_relation
+        else:
+            self.test_relation = self.input_relation
+        if self._is_native:
+            parameters = self._get_vertica_param_dict()
             if (
-                param["penalty"] in ("none", "l2")
-                and "solver" in param
-                and param["solver"] == "cgd"
+                "regularization" in parameters
+                and parameters["regularization"].lower() == "'enet'"
             ):
-                param["solver"] = "bfgs"
-            if param["penalty"] in ("none", "l1", "l2") and "l1_ratio" in param:
-                del param["l1_ratio"]
-            if param["penalty"] == "none" and "C" in param:
-                del param["C"]
-            if param["penalty"] in ("l1", "enet") and "solver" in param:
-                param["solver"] = "cgd"
-        if param not in final_param_grid:
-            final_param_grid += [param]
-    if len(final_param_grid) > lmax and lmax > 0:
-        final_param_grid = random.sample(final_param_grid, lmax)
-    return final_param_grid
-
-
-# ---#
-def grid_search_cv(
-    estimator,
-    param_grid: Union[dict, list],
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    metric: str = "auto",
-    cv: int = 3,
-    pos_label: Union[int, float, str] = None,
-    cutoff: float = -1,
-    training_score: bool = True,
-    skip_error: bool = True,
-    print_info: bool = True,
-    **kwargs,
-):
-    """
----------------------------------------------------------------------------
-Computes the k-fold grid search of an estimator.
+                alpha = parameters["alpha"]
+                del parameters["alpha"]
+            else:
+                alpha = None
+            if "mtry" in parameters:
+                if parameters["mtry"] == "'auto'":
+                    parameters["mtry"] = int(len(self.X) / 3 + 1)
+                elif parameters["mtry"] == "'max'":
+                    parameters["mtry"] = len(self.X)
+            for param in ("nbtype",):
+                if param in parameters:
+                    del parameters[param]
+            query = f"""
+                SELECT 
+                    /*+LABEL('learn.VerticaModel.fit')*/ 
+                    {self._vertica_fit_sql}
+                    ('{self.model_name}', 
+                     '{relation}',
+                     '{self.y}',
+                     '{', '.join(self.X)}' 
+                     USING PARAMETERS 
+                     {', '.join([f"{p} = {parameters[p]}" for p in parameters])}"""
+            if not isinstance(alpha, NoneType):
+                query += f", alpha = {alpha}"
+            if self._model_type in (
+                "RandomForestClassifier",
+                "RandomForestRegressor",
+                "XGBClassifier",
+                "XGBRegressor",
+            ) and isinstance(conf.get_option("random_state"), int):
+                query += f""", 
+                    seed={conf.get_option('random_state')}, 
+                    id_column='{id_column_name}'"""
+            query += ")"
+            try:
+                _executeSQL(query, title="Fitting the model.")
+            finally:
+                if tmp_view:
+                    drop(relation, method="view")
+        self._compute_attributes()
+        if self._is_native:
+            return self.summarize()
+        return None
+
+
+class Tree:
+    # Properties.
+
+    @property
+    def _attributes(self) -> list:
+        """Must be overridden in the final class"""
+        return []
+
+    # System & Special Methods.
+
+    @abstractmethod
+    def __init__(self) -> None:
+        """Must be overridden in the child class"""
+        return None
+        # self.input_relation = None
+        # self.test_relation = None
+        # self.X = None
+        # self.y = None
+        # self.parameters = {}
+        # self.classes_ = None
+        # for att in self._attributes:
+        #    setattr(self, att, None)
+
+    def _compute_trees_arrays(
+        self, tree: TableSample, X: list, return_probability: bool = False
+    ) -> list[list]:
+        """
+        Takes as input a tree which is represented by a
+        TableSample.  It returns a list of arrays. Each
+        index of the arrays represents a node value.
+        """
+        for i in range(len(tree["tree_id"])):
+            tree.values["left_child_id"] = [
+                i if node_id == tree.values["node_id"][i] else node_id
+                for node_id in tree.values["left_child_id"]
+            ]
+            tree.values["right_child_id"] = [
+                i if node_id == tree.values["node_id"][i] else node_id
+                for node_id in tree.values["right_child_id"]
+            ]
+            tree.values["node_id"][i] = i
 
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-param_grid: dict/list
-    Dictionary of the parameters to test. It can also be a list of the
-    different combinations.
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-pos_label: int/float/str, optional
-    The main class to be considered as positive (classification only).
-cutoff: float, optional
-    The model cutoff (classification only).
-training_score: bool, optional
-    If set to True, the training score will be computed with the validation score.
-skip_error: bool, optional
-    If set to True and an error occurs, it will be displayed and not raised.
-print_info: bool, optional
-    If set to True, prints the model information at each step.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    if isinstance(X, str):
-        X = [X]
-    check_types(
-        [
-            ("metric", metric, [str]),
-            ("param_grid", param_grid, [dict, list]),
-            ("training_score", training_score, [bool]),
-            ("skip_error", skip_error, [bool, str]),
-            ("print_info", print_info, [bool]),
+            for j, xj in enumerate(X):
+                if (
+                    quote_ident(tree["split_predictor"][i]).lower()
+                    == quote_ident(xj).lower()
+                ):
+                    tree["split_predictor"][i] = j
+
+            if self._model_type == "XGBClassifier" and isinstance(
+                tree["log_odds"][i], str
+            ):
+                val, all_val = tree["log_odds"][i].split(","), {}
+                for v in val:
+                    all_val[v.split(":")[0]] = float(v.split(":")[1])
+                tree.values["log_odds"][i] = all_val
+        if self._model_type == "IsolationForest":
+            tree.values["prediction"], n = [], len(tree.values["leaf_path_length"])
+            for i in range(n):
+                if not isinstance(tree.values["leaf_path_length"][i], NoneType):
+                    tree.values["prediction"] += [
+                        [
+                            int(float(tree.values["leaf_path_length"][i])),
+                            int(float(tree.values["training_row_count"][i])),
+                        ]
+                    ]
+                else:
+                    tree.values["prediction"] += [None]
+        trees_arrays = [
+            tree["left_child_id"],
+            tree["right_child_id"],
+            tree["split_predictor"],
+            tree["split_value"],
+            tree["prediction"],
+            tree["is_categorical_split"],
         ]
-    )
-    if get_model_category(estimator.type)[0] == "regressor" and metric == "auto":
-        metric = "rmse"
-    elif metric == "auto":
-        metric = "logloss"
-    if isinstance(param_grid, dict):
-        for param in param_grid:
-            assert isinstance(param_grid[param], Iterable) and not (
-                isinstance(param_grid[param], str)
-            ), ParameterError(
-                f"When of type dictionary, the parameter 'param_grid' must be a dictionary where each value is a list of parameters, found {type(param_grid[param])} for parameter '{param}'."
-            )
-        all_configuration = parameter_grid(param_grid)
-    else:
-        for idx, param in enumerate(param_grid):
-            assert isinstance(param, dict), ParameterError(
-                f"When of type List, the parameter 'param_grid' must be a list of dictionaries, found {type(param)} for elem '{idx}'."
-            )
-        all_configuration = param_grid
-    # testing all the config
-    for config in all_configuration:
-        estimator.set_params(config)
-    # applying all the config
-    data = []
-    if all_configuration == []:
-        all_configuration = [{}]
-    if (
-        verticapy.options["tqdm"]
-        and ("tqdm" not in kwargs or ("tqdm" in kwargs and kwargs["tqdm"]))
-        and print_info
-    ):
-        from tqdm.auto import tqdm
-
-        loop = tqdm(all_configuration)
-    else:
-        loop = all_configuration
-    for config in loop:
-        try:
-            estimator.set_params(config)
-            current_cv = cross_validate(
-                estimator,
-                input_relation,
-                X,
-                y,
-                metric,
-                cv,
-                pos_label,
-                cutoff,
-                True,
-                training_score,
-                tqdm=False,
-            )
-            if training_score:
-                keys = [elem for elem in current_cv[0].values]
-                data += [
-                    (
-                        estimator.get_params(),
-                        current_cv[0][keys[1]][cv],
-                        current_cv[1][keys[1]][cv],
-                        current_cv[0][keys[2]][cv],
-                        current_cv[0][keys[1]][cv + 1],
-                        current_cv[1][keys[1]][cv + 1],
-                    )
-                ]
-                if print_info:
-                    print(
-                        f"Model: {str(estimator.__class__).split('.')[-1][:-2]}; Parameters: {config}; \033[91mTest_score: {current_cv[0][keys[1]][cv]}\033[0m; \033[92mTrain_score: {current_cv[1][keys[1]][cv]}\033[0m; \033[94mTime: {current_cv[0][keys[2]][cv]}\033[0m;"
-                    )
+        if self._model_type == "XGBClassifier":
+            trees_arrays += [tree["log_odds"]]
+        if return_probability:
+            trees_arrays += [tree["probability/variance"]]
+        return trees_arrays
+
+    # Features Importance Methods.
+
+    def _compute_features_importance(self, tree_id: Optional[int] = None) -> None:
+        """
+        Computes the model's features importance.
+        """
+        vertica_version(condition=[9, 1, 1])
+        tree_id_str = "" if tree_id is None else f", tree_id={tree_id}"
+        query = f"""
+        SELECT /*+LABEL('learn.VerticaModel.features_importance')*/
+            predictor_name AS predictor, 
+            SIGN({self._model_importance_feature})::int * 
+            ROUND(100 * ABS({self._model_importance_feature}) / 
+            SUM(ABS({self._model_importance_feature}))
+            OVER (), 2)::float AS importance
+        FROM 
+            (SELECT {self._model_importance_function} ( 
+                    USING PARAMETERS model_name = '{self.model_name}'{tree_id_str})) 
+                    VERTICAPY_SUBTABLE 
+        ORDER BY 2 DESC;"""
+        importance = _executeSQL(
+            query=query, title="Computing Features Importance.", method="fetchall"
+        )
+        importance = self._format_vector(self.X, importance)
+        if isinstance(tree_id, int) and (0 <= tree_id < self.n_estimators_):
+            if hasattr(self, "features_importance_trees_"):
+                self.features_importance_trees_[tree_id] = importance
             else:
-                keys = [elem for elem in current_cv.values]
-                data += [
-                    (
-                        config,
-                        current_cv[keys[1]][cv],
-                        current_cv[keys[2]][cv],
-                        current_cv[keys[1]][cv + 1],
-                    )
-                ]
-                if print_info:
-                    print(
-                        f"Model: {str(estimator.__class__).split('.')[-1][:-2]}; Parameters: {config}; \033[91mTest_score: {current_cv[keys[1]][cv]}\033[0m; \033[94mTime:{current_cv[keys[2]][cv]}\033[0m;"
-                    )
-        except Exception as e:
-            if skip_error and skip_error != "no_print":
-                print(e)
-            elif not (skip_error):
-                raise (e)
-    if not (data):
-        if training_score:
-            return tablesample(
-                {
-                    "parameters": [],
-                    "avg_score": [],
-                    "avg_train_score": [],
-                    "avg_time": [],
-                    "score_std": [],
-                    "score_train_std": [],
-                }
-            )
+                self.features_importance_trees_ = {tree_id: importance}
+        elif isinstance(tree_id, NoneType):
+            self.features_importance_ = importance
+
+    def _get_features_importance(self, tree_id: Optional[int] = None) -> np.ndarray:
+        """
+        Returns model's features importances.
+        """
+        if isinstance(tree_id, NoneType) and hasattr(self, "features_importance_"):
+            return copy.deepcopy(self.features_importance_)
+        elif (
+            isinstance(tree_id, int)
+            and (0 <= tree_id < self.n_estimators_)
+            and hasattr(self, "features_importance_trees_")
+            and (tree_id in self.features_importance_trees_)
+        ):
+            return copy.deepcopy(self.features_importance_trees_[tree_id])
         else:
-            return tablesample(
-                {"parameters": [], "avg_score": [], "avg_time": [], "score_std": [],}
+            self._compute_features_importance(tree_id=tree_id)
+            return self._get_features_importance(tree_id=tree_id)
+
+    def features_importance(
+        self,
+        tree_id: Optional[int] = None,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Computes the model's features importance.
+
+        Parameters
+        ----------
+        tree_id: int
+            Tree ID.
+        show: bool
+            If  set to True,  draw the features  importance.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the Plotting
+            functions.
+
+        Returns
+        -------
+        TableSample
+            features importance.
+        """
+        fi = self._get_features_importance(tree_id=tree_id)
+        if show:
+            data = {
+                "importance": fi,
+            }
+            layout = {"columns": copy.deepcopy(self.X)}
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="ImportanceBarChart",
+                chart=chart,
+                style_kwargs=style_kwargs,
             )
-    reverse = reverse_score(metric)
-    data.sort(key=lambda tup: tup[1], reverse=reverse)
-    if training_score:
-        result = tablesample(
-            {
-                "parameters": [elem[0] for elem in data],
-                "avg_score": [elem[1] for elem in data],
-                "avg_train_score": [elem[2] for elem in data],
-                "avg_time": [elem[3] for elem in data],
-                "score_std": [elem[4] for elem in data],
-                "score_train_std": [elem[5] for elem in data],
-            }
-        )
-        if print_info and (
-            "final_print" not in kwargs or kwargs["final_print"] != "no_print"
+            return vpy_plt.ImportanceBarChart(data=data, layout=layout).draw(**kwargs)
+        importances = {
+            "index": [quote_ident(x)[1:-1].lower() for x in self.X],
+            "importance": list(abs(fi)),
+            "sign": list(np.sign(fi)),
+        }
+        return TableSample(values=importances).sort(column="importance", desc=True)
+
+    def get_score(
+        self,
+        tree_id: Optional[int] = None,
+    ) -> TableSample:
+        """
+        Returns the feature importance metrics for the input
+        tree.
+
+        Parameters
+        ----------
+        tree_id: int, optional
+            Unique  tree identifier, an integer in the range
+            [0, n_estimators - 1]. If tree_id is  undefined,
+            all  the trees in the model are used to  compute
+            the metrics.
+
+        Returns
+        -------
+        TableSample
+            model's score.
+        """
+        tree_id = "" if isinstance(tree_id, NoneType) else f", tree_id={tree_id}"
+        query = f"""
+            SELECT {self._model_importance_function} 
+            (USING PARAMETERS model_name = '{self.model_name}'{tree_id})"""
+        return TableSample.read_sql(query=query, title="Reading Tree.")
+
+    # Plotting Methods.
+
+    def plot(
+        self,
+        max_nb_points: int = 100,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the model.
+
+        Parameters
+        ----------
+        max_nb_points: int
+            Maximum  number of points to display.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        if self._model_subcategory == "REGRESSOR":
+            vdf = vDataFrame(self.input_relation)
+            vdf["_prediction"] = self.deploySQL()
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="RegressionTreePlot",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            return vpy_plt.RegressionTreePlot(
+                vdf=vdf,
+                columns=self.X + [self.y] + ["_prediction"],
+                max_nb_points=max_nb_points,
+            ).draw(**kwargs)
+        else:
+            raise NotImplementedError
+
+    # Trees Representation Methods.
+
+    @check_minimum_version
+    def get_tree(self, tree_id: int = 0) -> TableSample:
+        """
+        Returns a table with all the input tree information.
+
+        Parameters
+        ----------
+        tree_id: int, optional
+            Unique tree  identifier, an integer in the range
+            [0, n_estimators - 1].
+
+        Returns
+        -------
+        TableSample
+            tree.
+        """
+        query = f"""
+            SELECT * FROM (SELECT READ_TREE (
+                             USING PARAMETERS 
+                             model_name = '{self.model_name}', 
+                             tree_id = {tree_id}, 
+                             format = 'tabular')) x ORDER BY node_id;"""
+        return TableSample.read_sql(query=query, title="Reading Tree.")
+
+    def to_graphviz(
+        self,
+        tree_id: int = 0,
+        classes_color: Optional[list] = None,
+        round_pred: int = 2,
+        percent: bool = False,
+        vertical: bool = True,
+        node_style: dict = {"shape": "box", "style": "filled"},
+        arrow_style: Optional[dict] = None,
+        leaf_style: Optional[dict] = None,
+    ) -> str:
+        """
+        Returns the code for a Graphviz tree.
+
+        Parameters
+        ----------
+        tree_id: int, optional
+            Unique  tree identifier,  an integer in the  range
+            [0, n_estimators - 1].
+        classes_color: ArrayLike, optional
+            Colors that represent the different classes.
+        round_pred: int, optional
+            The number of decimals to round the prediction to.
+            Zero rounds to an integer.
+        percent: bool, optional
+            If set to True, the probabilities are returned as
+            percents.
+        vertical: bool, optional
+            If set to True, the function generates a vertical
+            tree.
+        node_style: dict, optional
+            Dictionary  of options to customize each node  of
+            the tree. For a list of options, see the Graphviz
+            API: https://graphviz.org/doc/info/attrs.html
+        arrow_style: dict, optional
+            Dictionary of options to customize each arrow  of
+            the tree. For a list of options, see the Graphviz
+            API: https://graphviz.org/doc/info/attrs.html
+        leaf_style: dict, optional
+            Dictionary  of options to customize each leaf  of
+            the tree. For a list of options, see the Graphviz
+            API: https://graphviz.org/doc/info/attrs.html
+
+        Returns
+        -------
+        str
+            Graphviz code.
+        """
+        return self.trees_[tree_id].to_graphviz(
+            feature_names=self.X,
+            classes_color=classes_color,
+            round_pred=round_pred,
+            percent=percent,
+            vertical=vertical,
+            node_style=node_style,
+            arrow_style=arrow_style,
+            leaf_style=leaf_style,
+        )
+
+    def plot_tree(
+        self,
+        tree_id: int = 0,
+        pic_path: Optional[str] = None,
+        *args,
+        **kwargs,
+    ) -> "Source":
+        """
+        Draws the input tree. Requires the graphviz module.
+
+        Parameters
+        ----------
+        tree_id: int, optional
+            Unique tree identifier, an integer in the range
+            [0, n_estimators - 1].
+        pic_path: str, optional
+            Absolute  path to save  the image of the  tree.
+        *args, **kwargs: Any, optional
+            Arguments to pass to the 'to_graphviz'  method.
+
+        Returns
+        -------
+        graphviz.Source
+            graphviz object.
+        """
+        return self.trees_[tree_id].plot_tree(
+            pic_path=pic_path,
+            feature_names=self.X,
+            *args,
+            **kwargs,
+        )
+
+
+class BinaryClassifier(Supervised):
+    # Properties.
+
+    @property
+    def classes_(self) -> np.ndarray:
+        return np.array([0, 1])
+
+    # System & Special Methods.
+
+    @abstractmethod
+    def __init__(self) -> None:
+        """Must be overridden in the child class"""
+        super().__init__()
+
+    # Attributes Methods.
+
+    def _is_binary_classifier(self) -> Literal[True]:
+        """
+        Returns True if the model is a Binary Classifier.
+        """
+        return True
+
+    # I/O Methods.
+
+    def deploySQL(
+        self, X: Optional[SQLColumns] = None, cutoff: Optional[PythonNumber] = None
+    ) -> str:
+        """
+        Returns  the  SQL code  needed to deploy  the  model.
+
+        Parameters
+        ----------
+        X: SQLColumns, optional
+            List of the  columns used to deploy the model. If
+            empty, the model predictors are used.
+        cutoff: PythonNumber, optional
+                Probability cutoff. If this number is not between
+            0 and 1,  the method retruns the  probability
+            of class 1.
+
+        Returns
+        -------
+        str
+                the SQL code needed to deploy the model.
+        """
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        sql = f"""
+        {self._vertica_predict_sql}({', '.join(X)} 
+            USING PARAMETERS
+            model_name = '{self.model_name}',
+            type = 'probability',
+            match_by_pos = 'true')"""
+        if not isinstance(cutoff, NoneType) and 0 <= cutoff <= 1:
+            sql = f"""
+                (CASE 
+                    WHEN {sql} >= {cutoff} 
+                        THEN 1 
+                    WHEN {sql} IS NULL 
+                        THEN NULL 
+                    ELSE 0 
+                END)"""
+        return clean_query(sql)
+
+    # Model Evaluation Methods.
+
+    def classification_report(
+        self,
+        metrics: Union[
+            None, str, list[Literal[tuple(mt.FUNCTIONS_CLASSIFICATION_DICTIONNARY)]]
+        ] = None,
+        cutoff: PythonNumber = 0.5,
+        nbins: int = 10000,
+    ) -> Union[float, TableSample]:
+        """
+        Computes a classification report using multiple model
+        evaluation metrics (AUC, accuracy, PRC AUC, F1...).
+
+        Parameters
+        ----------
+        metrics: list, optional
+            List  of the  metrics  used to compute the  final
+            report.
+                accuracy    : Accuracy
+                aic         : Akaikes  Information  Criterion
+                auc         : Area Under the Curve (ROC)
+                ba          : Balanced Accuracy
+                              = (tpr + tnr) / 2
+                best_cutoff : Cutoff  which optimised the  ROC
+                              Curve prediction.
+                bic         : Bayesian  Information  Criterion
+                bm          : Informedness = tpr + tnr - 1
+                csi         : Critical Success Index
+                              = tp / (tp + fn + fp)
+                f1          : F1 Score
+                fdr         : False Discovery Rate = 1 - ppv
+                fm          : FowlkesMallows index
+                              = sqrt(ppv * tpr)
+                fnr         : False Negative Rate
+                              = fn / (fn + tp)
+                for         : False Omission Rate = 1 - npv
+                fpr         : False Positive Rate
+                              = fp / (fp + tn)
+                logloss     : Log Loss
+                lr+         : Positive Likelihood Ratio
+                              = tpr / fpr
+                lr-         : Negative Likelihood Ratio
+                              = fnr / tnr
+                dor         : Diagnostic Odds Ratio
+                mcc         : Matthews Correlation Coefficient
+                mk          : Markedness = ppv + npv - 1
+                npv         : Negative Predictive Value
+                              = tn / (tn + fn)
+                prc_auc     : Area Under the Curve (PRC)
+                precision   : Precision = tp / (tp + fp)
+                pt          : Prevalence Threshold
+                              = sqrt(fpr) / (sqrt(tpr) + sqrt(fpr))
+                recall      : Recall = tp / (tp + fn)
+                specificity : Specificity = tn / (tn + fp)
+        cutoff: PythonNumber, optional
+            Probability cutoff.
+        nbins: int, optional
+            [Used to compute ROC AUC, PRC AUC and the best cutoff]
+            An  integer  value  that   determines  the  number  of
+            decision  boundaries. Decision  boundaries are set  at
+            equally  spaced intervals between 0 and 1,  inclusive.
+            Greater values for nbins give more precise estimations
+            of   the   metrics,  but   can  potentially   decrease
+            performance. The maximum value is 999,999. If negative,
+            the maximum value is used.
+
+        Returns
+        -------
+        TableSample
+            report.
+        """
+        return mt.classification_report(
+            self.y,
+            [self.deploySQL(), self.deploySQL(cutoff=cutoff)],
+            self.test_relation,
+            metrics=metrics,
+            cutoff=cutoff,
+            nbins=nbins,
+        )
+
+    report = classification_report
+
+    def confusion_matrix(self, cutoff: PythonNumber = 0.5) -> TableSample:
+        """
+        Computes the model confusion matrix.
+
+        Parameters
+        ----------
+        cutoff: PythonNumber, optional
+            Probability cutoff.
+
+        Returns
+        -------
+        TableSample
+            confusion matrix.
+        """
+        return mt.confusion_matrix(
+            self.y,
+            self.deploySQL(cutoff=cutoff),
+            self.test_relation,
+        )
+
+    def score(
+        self,
+        metric: Literal[tuple(mt.FUNCTIONS_CLASSIFICATION_DICTIONNARY)] = "accuracy",
+        cutoff: PythonNumber = 0.5,
+        nbins: int = 10000,
+    ) -> float:
+        """
+        Computes the model score.
+
+        Parameters
+        ----------
+        metric: str, optional
+            The metric used to compute the score.
+                accuracy    : Accuracy
+                aic         : Akaikes  Information  Criterion
+                auc         : Area Under the Curve (ROC)
+                ba          : Balanced Accuracy
+                              = (tpr + tnr) / 2
+                best_cutoff : Cutoff  which optimised the  ROC
+                              Curve prediction.
+                bic         : Bayesian  Information  Criterion
+                bm          : Informedness = tpr + tnr - 1
+                csi         : Critical Success Index
+                              = tp / (tp + fn + fp)
+                f1          : F1 Score
+                fdr         : False Discovery Rate = 1 - ppv
+                fm          : FowlkesMallows index
+                              = sqrt(ppv * tpr)
+                fnr         : False Negative Rate
+                              = fn / (fn + tp)
+                for         : False Omission Rate = 1 - npv
+                fpr         : False Positive Rate
+                              = fp / (fp + tn)
+                logloss     : Log Loss
+                lr+         : Positive Likelihood Ratio
+                              = tpr / fpr
+                lr-         : Negative Likelihood Ratio
+                              = fnr / tnr
+                dor         : Diagnostic Odds Ratio
+                mcc         : Matthews Correlation Coefficient
+                mk          : Markedness = ppv + npv - 1
+                npv         : Negative Predictive Value
+                              = tn / (tn + fn)
+                prc_auc     : Area Under the Curve (PRC)
+                precision   : Precision = tp / (tp + fp)
+                pt          : Prevalence Threshold
+                              = sqrt(fpr) / (sqrt(tpr) + sqrt(fpr))
+                recall      : Recall = tp / (tp + fn)
+                specificity : Specificity = tn / (tn + fp)
+        cutoff: PythonNumber, optional
+            Cutoff for which the tested category will be
+            accepted as a prediction.
+        nbins: int, optional
+            [Only  when  method  is set  to  auc|prc_auc|best_cutoff]
+            An  integer value that determines the number of  decision
+            boundaries. Decision boundaries are set at equally spaced
+            intervals between 0 and 1,  inclusive. Greater values for
+            nbins give more precise  estimations of the AUC,  but can
+            potentially  decrease performance.  The maximum value  is
+            999,999. If negative, the maximum value is used.
+
+        Returns
+        -------
+        float
+            score
+        """
+        fun = mt.FUNCTIONS_CLASSIFICATION_DICTIONNARY[metric]
+        if metric in (
+            "log_loss",
+            "logloss",
+            "aic",
+            "bic",
+            "auc",
+            "roc_auc",
+            "prc_auc",
+            "best_cutoff",
+            "best_threshold",
         ):
-            print("\033[1mGrid Search Selected Model\033[0m")
-            print(
-                f"{str(estimator.__class__).split('.')[-1][:-2]}; Parameters: {result['parameters'][0]}; \033[91mTest_score: {result['avg_score'][0]}\033[0m; \033[92mTrain_score: {result['avg_train_score'][0]}\033[0m; \033[94mTime: {result['avg_time'][0]}\033[0m;"
-            )
-    else:
-        result = tablesample(
-            {
-                "parameters": [elem[0] for elem in data],
-                "avg_score": [elem[1] for elem in data],
-                "avg_time": [elem[2] for elem in data],
-                "score_std": [elem[3] for elem in data],
-            }
-        )
-        if print_info and (
-            "final_print" not in kwargs or kwargs["final_print"] != "no_print"
+            args2 = self.deploySQL()
+        else:
+            args2 = self.deploySQL(cutoff=cutoff)
+        args = [self.y, args2, self.test_relation]
+        kwargs = {}
+        if metric in mt.FUNCTIONS_CLASSIFICATION_DICTIONNARY and (
+            metric not in ("aic", "bic")
         ):
-            print("\033[1mGrid Search Selected Model\033[0m")
-            print(
-                f"{str(estimator.__class__).split('.')[-1][:-2]}; Parameters: {result['parameters'][0]}; \033[91mTest_score: {result['avg_score'][0]}\033[0m; \033[94mTime: {result['avg_time'][0]}\033[0m;"
-            )
-    return result
-
-
-# ---#
-def learning_curve(
-    estimator,
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    sizes: list = [0.1, 0.33, 0.55, 0.78, 1.0],
-    method="efficiency",
-    metric: str = "auto",
-    cv: int = 3,
-    pos_label: Union[int, float, str] = None,
-    cutoff: float = -1,
-    std_coeff: float = 1,
-    ax=None,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Draws the learning curve.
+            kwargs["pos_label"] = 1
+        if metric in ("aic", "bic"):
+            args += [len(self.X)]
+        elif metric in ("auc", "roc_auc", "prc_auc", "best_cutoff", "best_threshold"):
+            kwargs["nbins"] = nbins
+        return fun(*args, **kwargs)
+
+    # Prediction / Transformation Methods.
+
+    def predict(
+        self,
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        cutoff: PythonNumber = 0.5,
+        inplace: bool = True,
+    ) -> vDataFrame:
+        """
+        Makes predictions on the input relation.
+
+        Parameters
+        ----------
+        vdf: SQLRelation
+            Object used to run  the prediction.  You can
+            also  specify a  customized  relation,  but you
+            must  enclose  it with an alias.  For  example,
+            "(SELECT 1) x" is valid, whereas "(SELECT 1)"
+            and "SELECT 1" are invalid.
+        X: SQLColumns, optional
+            List of the columns  used to deploy the models.
+            If empty, the model predictors are used.
+        name: str, optional
+            Name of the added vDataColumn. If empty, a name
+            is generated.
+        cutoff: float, optional
+            Probability cutoff.
+        inplace: bool, optional
+            If set to True, the prediction is added to
+            the vDataFrame.
+
+        Returns
+        -------
+        vDataFrame
+            the input object.
+        """
+        # Inititalization
+        X = format_type(X, dtype=list, na_out=self.X)
+        if not 0 <= cutoff <= 1:
+            raise ValueError(
+                "Incorrect parameter 'cutoff'.\nThe cutoff "
+                "must be between 0 and 1, inclusive."
+            )
+        if isinstance(vdf, str):
+            vdf = vDataFrame(vdf)
+        X = quote_ident(X)
+        if not name:
+            name = gen_name([self._model_type, self.model_name])
+
+        # In Place
+        vdf_return = vdf if inplace else vdf.copy()
+
+        # Result
+        return vdf_return.eval(name, self.deploySQL(X=X, cutoff=cutoff))
+
+    def predict_proba(
+        self,
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        pos_label: Optional[PythonScalar] = None,
+        inplace: bool = True,
+    ) -> vDataFrame:
+        """
+        Returns the model's  probabilities  using the input
+        relation.
+
+        Parameters
+        ----------
+        vdf: SQLRelation
+            Object  used to run  the prediction.  You can
+            also  specify a  customized  relation,  but you
+            must  enclose  it with an alias.  For  example,
+            "(SELECT 1) x" is valid, whereas "(SELECT 1)"
+            and "SELECT 1" are invalid.
+        X: SQLColumns, optional
+            List of the columns  used to deploy the models.
+            If empty, the model predictors are used.
+        name: str, optional
+            Name of the added vDataColumn. If empty, a name
+            is generated.
+        pos_label: PythonScalar, optional
+            Class  label.  For binary classification,  this
+            can be either 1 or 0.
+        inplace: bool, optional
+            If set to True, the prediction is added to
+            the vDataFrame.
+
+        Returns
+        -------
+        vDataFrame
+            the input object.
+        """
+        # Inititalization
+        X = format_type(X, dtype=list, na_out=self.X)
+        if pos_label not in [1, 0, "0", "1", None]:
+            raise ValueError(
+                "Incorrect parameter 'pos_label'.\nThe class label "
+                "can only be 1 or 0 in case of Binary Classification."
+            )
+        if isinstance(vdf, str):
+            vdf = vDataFrame(vdf)
+        X = quote_ident(X)
+        if not name:
+            name = gen_name([self._model_type, self.model_name])
+
+        # In Place
+        vdf_return = vdf if inplace else vdf.copy()
+
+        # Result
+        name_tmp = name
+        if pos_label in [0, "0", None]:
+            if isinstance(pos_label, NoneType):
+                name_tmp = f"{name}_0"
+            vdf_return.eval(name_tmp, f"1 - {self.deploySQL(X=X)}")
+        if pos_label in [1, "1", None]:
+            if isinstance(pos_label, NoneType):
+                name_tmp = f"{name}_1"
+            vdf_return.eval(name_tmp, self.deploySQL(X=X))
+
+        return vdf_return
+
+    # Plotting Methods.
+
+    def cutoff_curve(
+        self,
+        nbins: int = 30,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model Cutoff curve.
+
+        Parameters
+        ----------
+        nbins: int, optional
+            The number of bins.
+        show: bool, optional
+            If set to True,  the  Plotting
+            object is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass
+            to the Plotting functions.
+
+        Returns
+        -------
+        TableSample
+            cutoff curve data points.
+        """
+        return mt.roc_curve(
+            self.y,
+            self.deploySQL(),
+            self.test_relation,
+            nbins=nbins,
+            cutoff_curve=True,
+            show=show,
+            chart=chart,
+            **style_kwargs,
+        )
+
+    def lift_chart(
+        self,
+        nbins: int = 1000,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model Lift Chart.
+
+        Parameters
+        ----------
+        nbins: int, optional
+            The number of bins.
+        show: bool, optional
+            If set to True,  the  Plotting
+            object  is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass
+            to the Plotting functions.
+
+        Returns
+        -------
+        TableSample
+                lift chart data points.
+        """
+        return mt.lift_chart(
+            self.y,
+            self.deploySQL(),
+            self.test_relation,
+            nbins=nbins,
+            show=show,
+            chart=chart,
+            **style_kwargs,
+        )
+
+    def prc_curve(
+        self,
+        nbins: int = 30,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model PRC curve.
+
+        Parameters
+        ----------
+        nbins: int, optional
+            The number of bins.
+        show: bool, optional
+            If set to True,  the  Plotting
+            object  is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass
+            to the Plotting functions.
+
+        Returns
+        -------
+        TableSample
+                PRC curve data points.
+        """
+        return mt.prc_curve(
+            self.y,
+            self.deploySQL(),
+            self.test_relation,
+            nbins=nbins,
+            show=show,
+            chart=chart,
+            **style_kwargs,
+        )
+
+    def roc_curve(
+        self,
+        nbins: int = 30,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model ROC curve.
+
+        Parameters
+        ----------
+        nbins: int, optional
+            The number of bins.
+        show: bool, optional
+            If set to True,  the  Plotting
+            object  is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass
+            to the Plotting functions.
+
+        Returns
+        -------
+        TableSample
+            ROC curve data points.
+        """
+        return mt.roc_curve(
+            self.y,
+            self.deploySQL(),
+            self.test_relation,
+            nbins=nbins,
+            show=show,
+            chart=chart,
+            **style_kwargs,
+        )
+
+
+class MulticlassClassifier(Supervised):
+    # System & Special Methods.
+
+    @abstractmethod
+    def __init__(self) -> None:
+        """Must be overridden in the child class"""
+        super().__init__()
+        # self.classes_ = None
+
+    def _check_pos_label(self, pos_label: PythonScalar) -> PythonScalar:
+        """
+        Checks if the pos_label is correct.
+        """
+        if isinstance(pos_label, NoneType) and self._is_binary_classifier():
+            return 1
+        if isinstance(pos_label, NoneType):
+            return None
+        if str(pos_label) not in [str(c) for c in self.classes_]:
+            raise ValueError(
+                "Parameter 'pos_label' must be one of the response column classes."
+            )
+        return pos_label
 
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-sizes: list, optional
-    Different sizes of the dataset used to train the model. Multiple models
-    will be trained using the different sizes.
-method: str, optional
-    Method used to plot the curve.
-        efficiency  : draws train/test score vs sample size.
-        performance : draws score vs time.
-        scalability : draws time vs sample size.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-pos_label: int/float/str, optional
-    The main class to be considered as positive (classification only).
-cutoff: float, optional
-    The model cutoff (classification only).
-std_coeff: float, optional
-    Value of the standard deviation coefficient used to compute the area plot 
-    around each score.
-ax: Matplotlib axes object, optional
-    The axes to plot on.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    check_types([("method", method, ["efficiency", "performance", "scalability"])])
-    from verticapy.plot import range_curve
+    # Attributes Methods.
+
+    def _get_classes(self) -> np.ndarray:
+        """
+        Returns the model's classes.
+        """
+        classes = _executeSQL(
+            query=f"""
+                SELECT 
+                    /*+LABEL('learn.VerticaModel.fit')*/ 
+                    DISTINCT {self.y} 
+                FROM {self.input_relation} 
+                WHERE {self.y} IS NOT NULL 
+                ORDER BY 1""",
+            method="fetchall",
+            print_time_sql=False,
+        )
+        classes = np.array([c[0] for c in classes])
+        return self._array_to_int(classes)
+
+    def _is_binary_classifier(self) -> bool:
+        """
+        Returns True if the model is a Binary Classifier.
+        """
+        if len(self.classes_) == 2 and self.classes_[0] == 0 and self.classes_[1] == 1:
+            return True
+        return False
+
+    # I/O Methods.
+
+    def deploySQL(
+        self,
+        X: Optional[SQLColumns] = None,
+        pos_label: Optional[PythonScalar] = None,
+        cutoff: Optional[PythonNumber] = None,
+        allSQL: bool = False,
+    ) -> SQLExpression:
+        """
+        Returns the SQL code needed to deploy the model.
+
+        Parameters
+        ----------
+        X: SQLColumns, optional
+            List of the columns used to deploy the model.
+            If empty, the model predictors are used.
+        pos_label: PythonScalar, optional
+            Label to consider as positive. All the other
+            classes  are  merged and  considered  as
+            negative for multiclass classification.
+        cutoff: PythonNumber, optional
+            Cutoff for which the tested category will be
+            accepted  as a prediction. If the cutoff  is
+            not  between 0 and 1,  a probability is
+            returned.
+        allSQL: bool, optional
+            If set to True, the output is a list of
+            the different SQL codes needed to deploy the
+            different categories score.
+
+        Returns
+        -------
+        SQLExpression
+            the SQL code needed to deploy the model.
+        """
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        if not self._is_native:
+            sql = self.to_memmodel().predict_proba_sql(X)
+        else:
+            sql = [
+                f"""
+                {self._vertica_predict_sql}({', '.join(X)} 
+                    USING PARAMETERS 
+                    model_name = '{self.model_name}',
+                    class = '{{}}',
+                    type = 'probability',
+                    match_by_pos = 'true')""",
+                f"""
+                {self._vertica_predict_sql}({', '.join(X)} 
+                    USING PARAMETERS 
+                    model_name = '{self.model_name}',
+                    match_by_pos = 'true')""",
+            ]
+        if not allSQL:
+            if pos_label in list(self.classes_):
+                if not self._is_native:
+                    sql = sql[self.get_match_index(pos_label, self.classes_, False)]
+                else:
+                    sql = sql[0].format(pos_label)
+                if isinstance(cutoff, (int, float)) and 0.0 <= cutoff <= 1.0:
+                    sql = f"""
+                        (CASE 
+                            WHEN {sql} >= {cutoff} 
+                                THEN '{pos_label}' 
+                            WHEN {sql} IS NULL 
+                                THEN NULL 
+                            ELSE '{{}}' 
+                        END)"""
+                    if len(self.classes_) > 2:
+                        sql = sql.format(f"Non-{pos_label}")
+                    else:
+                        if self.classes_[0] != pos_label:
+                            non_pos_label = self.classes_[0]
+                        else:
+                            non_pos_label = self.classes_[1]
+                        sql = sql.format(non_pos_label)
+            else:
+                if not self._is_native:
+                    sql = self.to_memmodel().predict_sql(X)
+                else:
+                    sql = sql[1]
+        return clean_query(sql)
 
-    for s in sizes:
-        assert 0 < s <= 1, ParameterError("Each size must be in ]0,1].")
-    if get_model_category(estimator.type)[0] == "regressor" and metric == "auto":
-        metric = "rmse"
-    elif metric == "auto":
-        metric = "logloss"
-    if isinstance(input_relation, str):
-        input_relation = vDataFrameSQL(input_relation)
-    lc_result_final = []
-    sizes = sorted(set(sizes))
-    if verticapy.options["tqdm"]:
-        from tqdm.auto import tqdm
-
-        loop = tqdm(sizes)
-    else:
-        loop = sizes
-    for s in loop:
-        relation = input_relation.sample(x=s)
-        lc_result = cross_validate(
-            estimator,
-            relation,
-            X,
-            y,
-            metric,
-            cv,
-            pos_label,
-            cutoff,
-            True,
-            True,
-            tqdm=False,
-        )
-        lc_result_final += [
-            (
-                relation.shape()[0],
-                lc_result[0][metric][cv],
-                lc_result[0][metric][cv + 1],
-                lc_result[1][metric][cv],
-                lc_result[1][metric][cv + 1],
-                lc_result[0]["time"][cv],
-                lc_result[0]["time"][cv + 1],
+    # Model Evaluation Methods.
+
+    def _get_final_relation(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+    ) -> str:
+        """
+        Returns  the  final  relation  used to do  the
+        predictions.
+        """
+        return self.test_relation
+
+    def _get_y_proba(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+    ) -> str:
+        """
+        Returns the input which represents the  model's
+        probabilities.
+        """
+        return self.deploySQL(allSQL=True)[0].format(pos_label)
+
+    def _get_y_score(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        cutoff: Optional[PythonNumber] = None,
+        allSQL: bool = False,
+    ) -> str:
+        """
+        Returns  the input which represents the model's
+        scoring.
+        """
+        return self.deploySQL(pos_label=pos_label, cutoff=cutoff, allSQL=allSQL)
+
+    def classification_report(
+        self,
+        metrics: Union[
+            None, str, list[Literal[tuple(mt.FUNCTIONS_CLASSIFICATION_DICTIONNARY)]]
+        ] = None,
+        cutoff: PythonNumber = None,
+        labels: Union[None, str, list[str]] = None,
+        nbins: int = 10000,
+    ) -> Union[float, TableSample]:
+        """
+        Computes a classification report using multiple model
+        evaluation metrics (AUC, accuracy, PRC AUC, F1...).
+        For  multiclass classification,  it considers  each
+        category as positive and switches to the next one during
+        the computation.
+
+        Parameters
+        ----------
+        metrics: list, optional
+            List of  the metrics  used to compute the  final
+            report.
+                accuracy    : Accuracy
+                aic         : Akaikes  Information  Criterion
+                auc         : Area Under the Curve (ROC)
+                ba          : Balanced Accuracy
+                              = (tpr + tnr) / 2
+                best_cutoff : Cutoff  which optimised the  ROC
+                              Curve prediction.
+                bic         : Bayesian  Information  Criterion
+                bm          : Informedness = tpr + tnr - 1
+                csi         : Critical Success Index
+                              = tp / (tp + fn + fp)
+                f1          : F1 Score
+                fdr         : False Discovery Rate = 1 - ppv
+                fm          : FowlkesMallows index
+                              = sqrt(ppv * tpr)
+                fnr         : False Negative Rate
+                              = fn / (fn + tp)
+                for         : False Omission Rate = 1 - npv
+                fpr         : False Positive Rate
+                              = fp / (fp + tn)
+                logloss     : Log Loss
+                lr+         : Positive Likelihood Ratio
+                              = tpr / fpr
+                lr-         : Negative Likelihood Ratio
+                              = fnr / tnr
+                dor         : Diagnostic Odds Ratio
+                mcc         : Matthews Correlation Coefficient
+                mk          : Markedness = ppv + npv - 1
+                npv         : Negative Predictive Value
+                              = tn / (tn + fn)
+                prc_auc     : Area Under the Curve (PRC)
+                precision   : Precision = tp / (tp + fp)
+                pt          : Prevalence Threshold
+                              = sqrt(fpr) / (sqrt(tpr) + sqrt(fpr))
+                recall      : Recall = tp / (tp + fn)
+                specificity : Specificity = tn / (tn + fp)
+        cutoff: PythonNumber, optional
+            Cutoff for which the tested category is accepted
+            as a prediction.  For multiclass  classification, each
+            tested category becomes  the positives  and the others
+            are  merged  into  the   negatives.  The  cutoff
+            represents the classes  threshold.  If  it  is  empty,
+            the  regular  cutoff (1 / number of classes) is used.
+        labels: str / list, optional
+            List  of the  different  labels to be used during  the
+            computation.
+        nbins: int, optional
+            [Used to compute ROC AUC, PRC AUC and the best cutoff]
+            An  integer  value  that   determines  the  number  of
+            decision  boundaries.  Decision boundaries are set  at
+            equally spaced intervals  between 0 and 1,  inclusive.
+            Greater values for nbins give more precise estimations
+            of   the  metrics,   but   can  potentially   decrease
+            performance.
+            The maximum value is 999,999. If negative, the maximum
+            value is used.
+
+        Returns
+        -------
+        TableSample
+            report.
+        """
+        if isinstance(labels, NoneType):
+            labels = self.classes_
+        elif isinstance(labels, str):
+            labels = [labels]
+        return mt.classification_report(
+            estimator=self,
+            metrics=metrics,
+            labels=labels,
+            cutoff=cutoff,
+            nbins=nbins,
+        )
+
+    report = classification_report
+
+    def confusion_matrix(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        cutoff: Optional[PythonNumber] = None,
+    ) -> TableSample:
+        """
+        Computes the model confusion matrix.
+
+        Parameters
+        ----------
+        pos_label: PythonScalar, optional
+            Label  to consider  as positive.  All the other  classes
+            are merged and considered as negative for multiclass
+            classification.  If  the 'pos_label' is not defined, the
+            entire confusion matrix is drawn.
+        cutoff: PythonNumber, optional
+            Cutoff for which the tested category is accepted as
+            a prediction. It is only used if 'pos_label' is defined.
+
+        Returns
+        -------
+        TableSample
+            confusion matrix.
+        """
+        if hasattr(self, "_confusion_matrix"):
+            return self._confusion_matrix(
+                pos_label=pos_label,
+                cutoff=cutoff,
+            )
+        elif isinstance(pos_label, NoneType):
+            return mt.confusion_matrix(
+                self.y, self.deploySQL(), self.test_relation, labels=self.classes_
+            )
+        else:
+            pos_label = self._check_pos_label(pos_label=pos_label)
+            if isinstance(cutoff, NoneType):
+                cutoff = 1.0 / len(self.classes_)
+            return mt.confusion_matrix(
+                self.y,
+                self.deploySQL(pos_label=pos_label, cutoff=cutoff),
+                self.test_relation,
+                pos_label=pos_label,
             )
-        ]
-    if method in ("efficiency", "scalability"):
-        lc_result_final.sort(key=lambda tup: tup[0])
-    else:
-        lc_result_final.sort(key=lambda tup: tup[5])
-    result = tablesample(
-        {
-            "n": [elem[0] for elem in lc_result_final],
-            metric: [elem[1] for elem in lc_result_final],
-            metric + "_std": [elem[2] for elem in lc_result_final],
-            metric + "_train": [elem[3] for elem in lc_result_final],
-            metric + "_train_std": [elem[4] for elem in lc_result_final],
-            "time": [elem[5] for elem in lc_result_final],
-            "time_std": [elem[6] for elem in lc_result_final],
-        }
-    )
-    if method == "efficiency":
-        X = result["n"]
-        Y = [
-            [
-                [
-                    result[metric][i] - std_coeff * result[metric + "_std"][i]
-                    for i in range(len(sizes))
-                ],
-                result[metric],
-                [
-                    result[metric][i] + std_coeff * result[metric + "_std"][i]
-                    for i in range(len(sizes))
-                ],
-            ],
-            [
-                [
-                    result[metric + "_train"][i]
-                    - std_coeff * result[metric + "_train_std"][i]
-                    for i in range(len(sizes))
-                ],
-                result[metric + "_train"],
-                [
-                    result[metric + "_train"][i]
-                    + std_coeff * result[metric + "_train_std"][i]
-                    for i in range(len(sizes))
-                ],
-            ],
-        ]
-        x_label = "n"
-        y_label = metric
-        labels = [
-            "test",
-            "train",
-        ]
-    elif method == "performance":
-        X = result["time"]
-        Y = [
-            [
-                [
-                    result[metric][i] - std_coeff * result[metric + "_std"][i]
-                    for i in range(len(sizes))
-                ],
-                result[metric],
-                [
-                    result[metric][i] + std_coeff * result[metric + "_std"][i]
-                    for i in range(len(sizes))
-                ],
-            ],
-        ]
-        x_label = "time"
-        y_label = metric
-        labels = []
-    else:
-        X = result["n"]
-        Y = [
-            [
-                [
-                    result["time"][i] - std_coeff * result["time_std"][i]
-                    for i in range(len(sizes))
-                ],
-                result["time"],
-                [
-                    result["time"][i] + std_coeff * result["time_std"][i]
-                    for i in range(len(sizes))
-                ],
-            ],
-        ]
-        x_label = "n"
-        y_label = "time"
-        labels = []
-    range_curve(X, Y, x_label, y_label, ax, labels, **style_kwds)
-    return result
-
-
-# ---#
-def lift_chart(
-    y_true: str,
-    y_score: str,
-    input_relation: Union[str, vDataFrame],
-    pos_label: Union[int, float, str] = 1,
-    nbins: int = 30,
-    ax=None,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Draws the Lift Chart.
 
-Parameters
-----------
-y_true: str
-    Response column.
-y_score: str
-    Prediction Probability.
-input_relation: str/vDataFrame
-    Relation to use for scoring. This relation can be a view, table, or a 
-    customized relation (if an alias is used at the end of the relation). 
-    For example: (SELECT ... FROM ...) x
-pos_label: int/float/str, optional
-    To compute the Lift Chart, one of the response column classes must be the
-    positive one. The parameter 'pos_label' represents this class.
-nbins: int, optional
-    An integer value that determines the number of decision boundaries. Decision 
-    boundaries are set at equally-spaced intervals between 0 and 1, inclusive.
-ax: Matplotlib axes object, optional
-    The axes to plot on.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    check_types(
-        [
-            ("y_true", y_true, [str]),
-            ("y_score", y_score, [str]),
-            ("input_relation", input_relation, [str, vDataFrame]),
-            ("nbins", nbins, [int, float]),
-        ]
-    )
-    version(condition=[8, 0, 0])
-    query = "SELECT LIFT_TABLE(obs, prob USING PARAMETERS num_bins = {}) OVER() FROM (SELECT (CASE WHEN {} = '{}' THEN 1 ELSE 0 END) AS obs, {}::float AS prob FROM {}) AS prediction_output"
-    query = query.format(
-        nbins,
-        y_true,
-        pos_label,
-        y_score,
-        input_relation
-        if isinstance(input_relation, str)
-        else input_relation.__genSQL__(),
-    )
-    query_result = executeSQL(
-        query, title="Computing the Lift Table.", method="fetchall"
-    )
-    decision_boundary, positive_prediction_ratio, lift = (
-        [item[0] for item in query_result],
-        [item[1] for item in query_result],
-        [item[2] for item in query_result],
-    )
-    decision_boundary.reverse()
-    if not (ax):
-        fig, ax = plt.subplots()
-        if isnotebook():
-            fig.set_size_inches(8, 6)
-    ax.set_xlabel("Cumulative Data Fraction")
-    max_value = max([0 if elem != elem else elem for elem in lift])
-    lift = [max_value if elem != elem else elem for elem in lift]
-    param1 = {"color": gen_colors()[0]}
-    ax.plot(decision_boundary, lift, **updated_dict(param1, style_kwds, 0))
-    param2 = {"color": gen_colors()[1]}
-    ax.plot(
-        decision_boundary,
-        positive_prediction_ratio,
-        **updated_dict(param2, style_kwds, 1),
-    )
-    color1, color2 = color_dict(style_kwds, 0), color_dict(style_kwds, 1)
-    if color1 == color2:
-        color2 = gen_colors()[1]
-    ax.fill_between(
-        decision_boundary, positive_prediction_ratio, lift, facecolor=color1, alpha=0.2
-    )
-    ax.fill_between(
-        decision_boundary,
-        [0 for elem in decision_boundary],
-        positive_prediction_ratio,
-        facecolor=color2,
-        alpha=0.2,
-    )
-    ax.set_title("Lift Table")
-    ax.set_axisbelow(True)
-    ax.grid()
-    color1 = mpatches.Patch(color=color1, label="Cumulative Lift")
-    color2 = mpatches.Patch(color=color2, label="Cumulative Capture Rate")
-    ax.legend(handles=[color1, color2], loc="center left", bbox_to_anchor=[1, 0.5])
-    ax.set_xlim(0, 1)
-    ax.set_ylim(0)
-    return tablesample(
-        values={
-            "decision_boundary": decision_boundary,
-            "positive_prediction_ratio": positive_prediction_ratio,
-            "lift": lift,
-        }
-    )
+    def score(
+        self,
+        metric: Literal[tuple(mt.FUNCTIONS_CLASSIFICATION_DICTIONNARY)] = "accuracy",
+        average: Literal[None, "binary", "micro", "macro", "scores", "weighted"] = None,
+        pos_label: Optional[PythonScalar] = None,
+        cutoff: PythonNumber = 0.5,
+        nbins: int = 10000,
+    ) -> Union[float, list[float]]:
+        """
+        Computes the model score.
+
+        Parameters
+        ----------
+        metric: str, optional
+            The metric used to compute the score.
+                accuracy    : Accuracy
+                aic         : Akaikes  Information  Criterion
+                auc         : Area Under the Curve (ROC)
+                ba          : Balanced Accuracy
+                              = (tpr + tnr) / 2
+                best_cutoff : Cutoff  which optimised the  ROC
+                              Curve prediction.
+                bic         : Bayesian  Information  Criterion
+                bm          : Informedness = tpr + tnr - 1
+                csi         : Critical Success Index
+                              = tp / (tp + fn + fp)
+                f1          : F1 Score
+                fdr         : False Discovery Rate = 1 - ppv
+                fm          : FowlkesMallows index
+                              = sqrt(ppv * tpr)
+                fnr         : False Negative Rate
+                              = fn / (fn + tp)
+                for         : False Omission Rate = 1 - npv
+                fpr         : False Positive Rate
+                              = fp / (fp + tn)
+                logloss     : Log Loss
+                lr+         : Positive Likelihood Ratio
+                              = tpr / fpr
+                lr-         : Negative Likelihood Ratio
+                              = fnr / tnr
+                dor         : Diagnostic Odds Ratio
+                mcc         : Matthews Correlation Coefficient
+                mk          : Markedness = ppv + npv - 1
+                npv         : Negative Predictive Value
+                              = tn / (tn + fn)
+                prc_auc     : Area Under the Curve (PRC)
+                precision   : Precision = tp / (tp + fp)
+                pt          : Prevalence Threshold
+                              = sqrt(fpr) / (sqrt(tpr) + sqrt(fpr))
+                recall      : Recall = tp / (tp + fn)
+                specificity : Specificity = tn / (tn + fp)
+        average: str, optional
+            The method used to  compute the final score for
+            multiclass-classification.
+                binary   : considers one of the classes  as
+                           positive  and  use  the   binary
+                           confusion  matrix to compute the
+                           score.
+                micro    : positive  and   negative  values
+                           globally.
+                macro    : average  of  the  score of  each
+                           class.
+                scores   : scores  for   all  the  classes.
+                weighted : weighted average of the score of
+                           each class.
+            If empty,  the result will depend on the  input
+            metric.  Whenever  it  is  possible, the  exact
+            score is computed.  Otherwise, the behaviour is
+            similar to the 'scores' option.
+        pos_label: PythonScalar, optional
+            Label  to  consider   as  positive.  All the
+            other classes will be  merged and considered
+            as negative  for multiclass  classification.
+        cutoff: PythonNumber, optional
+            Cutoff for which the tested category is
+            accepted as a prediction.
+        nbins: int, optional
+            [Only  when  method  is set  to  auc|prc_auc|best_cutoff]
+            An  integer value that determines the number of  decision
+            boundaries. Decision boundaries are set at equally spaced
+            intervals between 0 and 1,  inclusive. Greater values for
+            nbins give more precise  estimations of the AUC,  but can
+            potentially  decrease performance.  The maximum value  is
+            999,999. If negative, the maximum value is used.
+
+        Returns
+        -------
+        float
+            score.
+        """
+        fun = mt.FUNCTIONS_CLASSIFICATION_DICTIONNARY[metric]
+        pos_label = self._check_pos_label(pos_label=pos_label)
+        if metric in (
+            "auc",
+            "roc_auc",
+            "prc_auc",
+            "best_cutoff",
+            "best_threshold",
+            "logloss",
+            "log_loss",
+        ):
+            y_score = self._get_y_score(allSQL=True)
+        else:
+            y_score = self._get_y_score(pos_label=pos_label, cutoff=cutoff)
+        final_relation = self._get_final_relation(pos_label=pos_label)
+        args = [self.y, y_score, final_relation]
+        kwargs = {}
+        if metric not in ("aic", "bic"):
+            labels = None
+            if isinstance(pos_label, NoneType):
+                labels = self.classes_
+            kwargs = {
+                "average": average,
+                "labels": labels,
+                "pos_label": pos_label,
+            }
+        if metric in ("aic", "bic"):
+            args += [len(self.X)]
+        elif metric in ("auc", "roc_auc", "prc_auc", "best_cutoff", "best_threshold"):
+            kwargs["nbins"] = nbins
+        return fun(*args, **kwargs)
+
+    # Prediction / Transformation Methods.
+
+    def predict(
+        self,
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        cutoff: Optional[PythonNumber] = None,
+        inplace: bool = True,
+    ) -> vDataFrame:
+        """
+        Predicts using the input relation.
+
+        Parameters
+        ----------
+        vdf: SQLRelation
+            Object  used to run  the prediction.  You can
+            also  specify a  customized  relation,  but you
+            must  enclose  it with an alias.  For  example,
+            "(SELECT 1) x" is valid, whereas "(SELECT 1)"
+            and "SELECT 1" are invalid.
+        X: SQLColumns, optional
+            List of the columns  used to deploy the models.
+            If empty, the model predictors are used.
+        name: str, optional
+            Name of the added vDataColumn. If empty, a name
+            is generated.
+        cutoff: PythonNumber, optional
+            Cutoff  for which  the tested category is
+            accepted  as a  prediction.  This parameter  is
+            only used for binary classification.
+        inplace: bool, optional
+            If set to True, the prediction is added to
+            the vDataFrame.
+
+        Returns
+        -------
+        vDataFrame
+            the input object.
+        """
+        # Using special method in case of non-native models
+        if hasattr(self, "_predict"):
+            return self._predict(
+                vdf=vdf, X=X, name=name, cutoff=cutoff, inplace=inplace
+            )
 
+        # Inititalization
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        if not name:
+            name = gen_name([self._model_type, self.model_name])
+        if isinstance(cutoff, NoneType):
+            cutoff = 1.0 / len(self.classes_)
+        elif not 0 <= cutoff <= 1:
+            raise ValueError(
+                "Incorrect parameter 'cutoff'.\nThe cutoff "
+                "must be between 0 and 1, inclusive."
+            )
+        if isinstance(vdf, str):
+            vdf = vDataFrame(vdf)
 
-# ---#
-def parameter_grid(param_grid: dict):
-    """
----------------------------------------------------------------------------
-Generates the list of the different combinations of input parameters.
+        # In Place
+        vdf_return = vdf if inplace else vdf.copy()
 
-Parameters
-----------
-param_grid: dict
-    Dictionary of parameters.
-
-Returns
--------
-list of dict
-    List of the different combinations.
-    """
-    check_types([("param_grid", param_grid, [dict])])
-    return [
-        dict(zip(param_grid.keys(), values)) for values in product(*param_grid.values())
-    ]
+        # Check if it is a Binary Classifier
+        pos_label = None
+        if (
+            len(self.classes_) == 2
+            and self.classes_[0] in [0, "0"]
+            and self.classes_[1] in [1, "1"]
+        ):
+            pos_label = 1
 
+        # Result
+        return vdf_return.eval(
+            name, self.deploySQL(X=X, pos_label=pos_label, cutoff=cutoff)
+        )
+
+    def predict_proba(
+        self,
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        pos_label: Optional[PythonScalar] = None,
+        inplace: bool = True,
+    ) -> vDataFrame:
+        """
+        Returns the model's probabilities using the input
+        relation.
+
+        Parameters
+        ----------
+        vdf: SQLRelation
+            Object  used to run  the prediction.  You can
+            also  specify a  customized  relation,  but you
+            must  enclose  it with an alias.  For  example,
+            "(SELECT 1) x" is valid, whereas "(SELECT 1)"
+            and "SELECT 1" are invalid.
+        X: SQLColumns, optional
+            List of the columns  used to deploy the models.
+            If empty, the model predictors are used.
+        name: str, optional
+            Name of the added vDataColumn. If empty, a name
+            is generated.
+        pos_label: PythonScalar, optional
+            Class  label.  For binary classification,  this
+            can be either 1 or 0.
+        inplace: bool, optional
+            If set to True, the prediction is added to
+            the vDataFrame.
+
+        Returns
+        -------
+        vDataFrame
+            the input object.
+        """
+        if hasattr(self, "_predict_proba"):
+            return self._predict_proba(
+                vdf=vdf,
+                X=X,
+                name=name,
+                pos_label=pos_label,
+                inplace=inplace,
+            )
+        # Inititalization
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        assert pos_label is None or pos_label in self.classes_, ValueError(
+            "Incorrect parameter 'pos_label'.\nThe class label "
+            f"must be in [{'|'.join([str(c) for c in self.classes_])}]. "
+            f"Found '{pos_label}'."
+        )
+        if isinstance(vdf, str):
+            vdf = vDataFrame(vdf)
+        if not name:
+            name = gen_name([self._model_type, self.model_name])
+
+        # In Place
+        vdf_return = vdf if inplace else vdf.copy()
+
+        # Result
+        if isinstance(pos_label, NoneType):
+            for c in self.classes_:
+                name_tmp = gen_name([name, c])
+                vdf_return.eval(name_tmp, self.deploySQL(pos_label=c, cutoff=None, X=X))
+        else:
+            vdf_return.eval(name, self.deploySQL(pos_label=pos_label, cutoff=None, X=X))
 
-# ---#
-def plot_acf_pacf(
-    vdf: vDataFrame,
-    column: str,
-    ts: str,
-    by: list = [],
-    p: Union[int, list] = 15,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Draws the ACF and PACF Charts.
+        return vdf_return
 
-Parameters
-----------
-vdf: vDataFrame
-    Input vDataFrame.
-column: str
-    Response column.
-ts: str
-    vcolumn used as timeline. It will be to use to order the data. 
-    It can be a numerical or type date like (date, datetime, timestamp...) 
-    vcolumn.
-by: list, optional
-    vcolumns used in the partition.
-p: int/list, optional
-    Int equals to the maximum number of lag to consider during the computation
-    or List of the different lags to include during the computation.
-    p must be positive or a list of positive integers.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    if isinstance(by, str):
-        by = [by]
-    check_types(
-        [
-            ("column", column, [str]),
-            ("ts", ts, [str]),
-            ("by", by, [list]),
-            ("p", p, [int, float]),
-            ("vdf", vdf, [vDataFrame]),
-        ]
-    )
-    tmp_style = {}
-    for elem in style_kwds:
-        if elem not in ("color", "colors"):
-            tmp_style[elem] = style_kwds[elem]
-    if "color" in style_kwds:
-        color = style_kwds["color"]
-    else:
-        color = gen_colors()[0]
-    vdf.are_namecols_in([column, ts] + by)
-    by = vdf.format_colnames(by)
-    column, ts = vdf.format_colnames([column, ts])
-    acf = vdf.acf(ts=ts, column=column, by=by, p=p, show=False)
-    pacf = vdf.pacf(ts=ts, column=column, by=by, p=p, show=False)
-    result = tablesample(
-        {
-            "index": [i for i in range(0, len(acf.values["value"]))],
-            "acf": acf.values["value"],
-            "pacf": pacf.values["value"],
-            "confidence": pacf.values["confidence"],
-        }
-    )
-    fig = plt.figure(figsize=(10, 6)) if isnotebook() else plt.figure(figsize=(10, 6))
-    plt.rcParams["axes.facecolor"] = "#FCFCFC"
-    ax1 = fig.add_subplot(211)
-    x, y, confidence = (
-        result.values["index"],
-        result.values["acf"],
-        result.values["confidence"],
-    )
-    plt.xlim(-1, x[-1] + 1)
-    ax1.bar(x, y, width=0.007 * len(x), color="#444444", zorder=1, linewidth=0)
-    param = {
-        "s": 90,
-        "marker": "o",
-        "facecolors": color,
-        "edgecolors": "black",
-        "zorder": 2,
-    }
-    ax1.scatter(x, y, **updated_dict(param, tmp_style))
-    ax1.plot(
-        [-1] + x + [x[-1] + 1], [0 for elem in range(len(x) + 2)], color=color, zorder=0
-    )
-    ax1.fill_between(x, confidence, color="#FE5016", alpha=0.1)
-    ax1.fill_between(x, [-elem for elem in confidence], color="#FE5016", alpha=0.1)
-    ax1.set_title("Autocorrelation")
-    y = result.values["pacf"]
-    ax2 = fig.add_subplot(212)
-    ax2.bar(x, y, width=0.007 * len(x), color="#444444", zorder=1, linewidth=0)
-    ax2.scatter(x, y, **updated_dict(param, tmp_style))
-    ax2.plot(
-        [-1] + x + [x[-1] + 1], [0 for elem in range(len(x) + 2)], color=color, zorder=0
-    )
-    ax2.fill_between(x, confidence, color="#FE5016", alpha=0.1)
-    ax2.fill_between(x, [-elem for elem in confidence], color="#FE5016", alpha=0.1)
-    ax2.set_title("Partial Autocorrelation")
-    plt.show()
-    return result
-
-
-# ---#
-def prc_curve(
-    y_true: str,
-    y_score: str,
-    input_relation: Union[str, vDataFrame],
-    pos_label: Union[int, float, str] = 1,
-    nbins: int = 30,
-    auc_prc: bool = False,
-    ax=None,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Draws the PRC Curve.
+    # Plotting Methods.
 
-Parameters
-----------
-y_true: str
-    Response column.
-y_score: str
-    Prediction Probability.
-input_relation: str/vDataFrame
-    Relation to use for scoring. This relation can be a view, table, or a 
-    customized relation (if an alias is used at the end of the relation). 
-    For example: (SELECT ... FROM ...) x
-pos_label: int/float/str, optional
-    To compute the PRC Curve, one of the response column classes must be the
-    positive one. The parameter 'pos_label' represents this class.
-nbins: int, optional
-    An integer value that determines the number of decision boundaries. Decision 
-    boundaries are set at equally-spaced intervals between 0 and 1, inclusive.
-auc_prc: bool, optional
-    If set to True, the function will return the PRC AUC without drawing the 
-    curve.
-ax: Matplotlib axes object, optional
-    The axes to plot on.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    check_types(
-        [
-            ("y_true", y_true, [str]),
-            ("y_score", y_score, [str]),
-            ("input_relation", input_relation, [str, vDataFrame]),
-            ("nbins", nbins, [int, float]),
-            ("auc_prc", auc_prc, [bool]),
-        ]
-    )
-    if nbins < 0:
-        nbins = 999999
-    version(condition=[9, 1, 0])
-    query = "SELECT PRC(obs, prob USING PARAMETERS num_bins = {}) OVER() FROM (SELECT (CASE WHEN {} = '{}' THEN 1 ELSE 0 END) AS obs, {}::float AS prob FROM {}) AS prediction_output"
-    query = query.format(
-        nbins,
-        y_true,
-        pos_label,
-        y_score,
-        input_relation
-        if isinstance(input_relation, str)
-        else input_relation.__genSQL__(),
-    )
-    query_result = executeSQL(
-        query, title="Computing the PRC table.", method="fetchall"
-    )
-    threshold, recall, precision = (
-        [0] + [item[0] for item in query_result] + [1],
-        [1] + [item[1] for item in query_result] + [0],
-        [0] + [item[2] for item in query_result] + [1],
-    )
-    auc = 0
-    for i in range(len(recall) - 1):
-        if recall[i + 1] - recall[i] != 0.0:
-            a = (precision[i + 1] - precision[i]) / (recall[i + 1] - recall[i])
-            b = precision[i + 1] - a * recall[i + 1]
-            auc = (
-                auc
-                + a * (recall[i + 1] * recall[i + 1] - recall[i] * recall[i]) / 2
-                + b * (recall[i + 1] - recall[i])
-            )
-    auc = -auc
-    if auc_prc:
-        return auc
-    if not (ax):
-        fig, ax = plt.subplots()
-        if isnotebook():
-            fig.set_size_inches(8, 6)
-    ax.set_xlabel("Recall")
-    ax.set_ylabel("Precision")
-    param = {"color": color_dict(style_kwds, 0)}
-    ax.plot(recall, precision, **updated_dict(param, style_kwds))
-    ax.fill_between(
-        recall,
-        [0 for item in recall],
-        precision,
-        facecolor=color_dict(style_kwds, 0),
-        alpha=0.1,
-    )
-    ax.set_ylim(0, 1)
-    ax.set_xlim(0, 1)
-    ax.set_title("PRC Curve")
-    ax.text(
-        0.995,
-        0,
-        "AUC = " + str(round(auc, 4) * 100) + "%",
-        verticalalignment="bottom",
-        horizontalalignment="right",
-        fontsize=11.5,
-    )
-    ax.set_axisbelow(True)
-    ax.grid()
-    return tablesample(
-        values={"threshold": threshold, "recall": recall, "precision": precision}
-    )
-
-
-# ---#
-def randomized_features_search_cv(
-    estimator,
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    metric: str = "auto",
-    cv: int = 3,
-    pos_label: Union[int, float, str] = None,
-    cutoff: float = -1,
-    training_score: bool = True,
-    comb_limit: int = 100,
-    skip_error: bool = True,
-    print_info: bool = True,
-    **kwargs,
-):
-    """
----------------------------------------------------------------------------
-Computes the k-fold grid search of an estimator using different features
-combinations. It can be used to find the parameters which will optimize
-the model.
-
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-pos_label: int/float/str, optional
-    The main class to be considered as positive (classification only).
-cutoff: float, optional
-    The model cutoff (classification only).
-training_score: bool, optional
-    If set to True, the training score will be computed with the validation score.
-comb_limit: int, optional
-    Maximum number of features combinations used to train the model.
-skip_error: bool, optional
-    If set to True and an error occurs, it will be displayed and not raised.
-print_info: bool, optional
-    If set to True, prints the model information at each step.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    if isinstance(X, str):
-        X = [X]
-    check_types(
-        [
-            ("metric", metric, [str]),
-            ("training_score", training_score, [bool]),
-            ("skip_error", skip_error, [bool, str]),
-            ("print_info", print_info, [bool]),
-            ("comb_limit", comb_limit, [int]),
-        ]
-    )
-    if get_model_category(estimator.type)[0] == "regressor" and metric == "auto":
-        metric = "rmse"
-    elif metric == "auto":
-        metric = "logloss"
-    if len(X) < 20:
-        all_configuration = all_comb(X)
-        if len(all_configuration) > comb_limit and comb_limit > 0:
-            all_configuration = random.sample(all_configuration, comb_limit)
-    else:
-        all_configuration = []
-        for k in range(max(comb_limit, 1)):
-            config = sorted(random.sample(X, random.randint(1, len(X))))
-            if config not in all_configuration:
-                all_configuration += [config]
-    if (
-        verticapy.options["tqdm"]
-        and ("tqdm" not in kwargs or ("tqdm" in kwargs and kwargs["tqdm"]))
-        and print_info
-    ):
-        from tqdm.auto import tqdm
-
-        loop = tqdm(all_configuration)
-    else:
-        loop = all_configuration
-    data = []
-    for config in loop:
-        if config:
-            config = list(config)
-            try:
-                current_cv = cross_validate(
-                    estimator,
-                    input_relation,
-                    config,
-                    y,
-                    metric,
-                    cv,
-                    pos_label,
-                    cutoff,
-                    True,
-                    training_score,
-                    tqdm=False,
-                )
-                if training_score:
-                    keys = [elem for elem in current_cv[0].values]
-                    data += [
-                        (
-                            config,
-                            current_cv[0][keys[1]][cv],
-                            current_cv[1][keys[1]][cv],
-                            current_cv[0][keys[2]][cv],
-                            current_cv[0][keys[1]][cv + 1],
-                            current_cv[1][keys[1]][cv + 1],
-                        )
-                    ]
-                    if print_info:
-                        print(
-                            f"Model: {str(estimator.__class__).split('.')[-1][:-2]}; Features: {config}; \033[91mTest_score: {current_cv[0][keys[1]][cv]}\033[0m; \033[92mTrain_score: {current_cv[1][keys[1]][cv]}\033[0m; \033[94mTime: {current_cv[0][keys[2]][cv]}\033[0m;"
-                        )
-                else:
-                    keys = [elem for elem in current_cv.values]
-                    data += [
-                        (
-                            config,
-                            current_cv[keys[1]][cv],
-                            current_cv[keys[2]][cv],
-                            current_cv[keys[1]][cv + 1],
-                        )
-                    ]
-                    if print_info:
-                        print(
-                            f"Model: {str(estimator.__class__).split('.')[-1][:-2]}; Features: {config}; \033[91mTest_score: {current_cv[keys[1]][cv]}\033[0m; \033[94mTime:{current_cv[keys[2]][cv]}\033[0m;"
-                        )
-            except Exception as e:
-                if skip_error and skip_error != "no_print":
-                    print(e)
-                elif not (skip_error):
-                    raise (e)
-    if not (data):
-        if training_score:
-            return tablesample(
+    def _get_sql_plot(self, pos_label: PythonScalar) -> str:
+        """
+        Returns the SQL needed to draw the plot.
+        """
+        pos_label = self._check_pos_label(pos_label)
+        if not self._is_native:
+            return self.deploySQL(allSQL=True)[
+                self.get_match_index(pos_label, self.classes_, False)
+            ]
+        else:
+            return self.deploySQL(allSQL=True)[0].format(pos_label)
+
+    def _get_plot_args(
+        self, pos_label: Optional[PythonScalar] = None, method: Optional[str] = None
+    ) -> list:
+        """
+        Returns the args used by plotting methods.
+        """
+        pos_label = self._check_pos_label(pos_label)
+        if method == "contour":
+            args = [
+                self.X,
+                self.deploySQL(X=self.X, pos_label=pos_label),
+            ]
+        else:
+            args = [
+                self.y,
+                self._get_sql_plot(pos_label),
+                self.test_relation,
+                pos_label,
+            ]
+        return args
+
+    def _get_plot_kwargs(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        nbins: int = 30,
+        chart: Optional[PlottingObject] = None,
+        method: Optional[str] = None,
+    ) -> dict:
+        """
+        Returns the kwargs used by plotting methods.
+        """
+        pos_label = self._check_pos_label(pos_label)
+        res = {"nbins": nbins, "chart": chart}
+        if method == "contour":
+            res["func_name"] = f"p({self.y} = '{pos_label}')"
+        elif method == "cutoff":
+            res["cutoff_curve"] = True
+        return res
+
+    def contour(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        nbins: int = 100,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the model's contour plot.
+
+        Parameters
+        ----------
+        pos_label: PythonScalar, optional
+            Label  to  consider  as positive. All other
+            classes  are  merged   and   considered  as
+            negative for multiclass classification.
+        nbins: int, optional
+             Number  of  bins  used to  discretize  the  two
+             predictors.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the Plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        pos_label = self._check_pos_label(pos_label=pos_label)
+        return vDataFrame(self.input_relation).contour(
+            *self._get_plot_args(pos_label=pos_label, method="contour"),
+            **self._get_plot_kwargs(
+                pos_label=pos_label, nbins=nbins, chart=chart, method="contour"
+            ),
+            **style_kwargs,
+        )
+
+    def cutoff_curve(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        nbins: int = 30,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model Cutoff curve.
+
+        Parameters
+        ----------
+        pos_label: PythonScalar, optional
+            To  draw the Cutoff curve, one of the response  column
+            classes  must  be  the  positive  class. The  parameter
+            'pos_label' represents this class.
+        nbins: int, optional
+            An integer value that determines the number of decision
+            boundaries.  Decision  boundaries  are   set at equally
+            -spaced intervals between 0 and 1, inclusive.
+        show: bool, optional
+            If set to True,  the  Plotting object is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the  Plotting
+            functions.
+
+        Returns
+        -------
+        TableSample
+            cutoff curve data points.
+        """
+        return mt.roc_curve(
+            *self._get_plot_args(pos_label=pos_label, method="cutoff"),
+            show=show,
+            **self._get_plot_kwargs(nbins=nbins, chart=chart, method="cutoff"),
+            **style_kwargs,
+        )
+
+    def lift_chart(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        nbins: int = 1000,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model Lift Chart.
+
+        Parameters
+        ----------
+        pos_label: PythonScalar, optional
+            To  draw  the Lift Chart, one of the  response  column
+            classes  must  be  the  positive  class. The parameter
+            'pos_label' represents this class.
+        nbins: int, optional
+            An integer value that determines the number of decision
+            boundaries.  Decision  boundaries  are   set at equally
+            -spaced intervals between 0 and 1, inclusive.
+        show: bool, optional
+            If set to True,  the  Plotting object is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the  Plotting
+            functions.
+
+        Returns
+        -------
+        TableSample
+                lift chart data points.
+        """
+        return mt.lift_chart(
+            *self._get_plot_args(pos_label=pos_label),
+            show=show,
+            **self._get_plot_kwargs(nbins=nbins, chart=chart),
+            **style_kwargs,
+        )
+
+    def prc_curve(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        nbins: int = 30,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model PRC curve.
+
+        Parameters
+        ----------
+        pos_label: PythonScalar, optional
+            To  draw  the PRC curve,  one of the  response  column
+            classes  must  be  the  positive  class. The parameter
+            'pos_label' represents this class.
+        nbins: int, optional
+            An integer value that determines the number of decision
+            boundaries.  Decision  boundaries  are   set at equally
+            -spaced intervals between 0 and 1, inclusive.
+        show: bool, optional
+            If set to True,  the  Plotting object is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the  Plotting
+            functions.
+
+        Returns
+        -------
+        TableSample
+                PRC curve data points.
+        """
+        return mt.prc_curve(
+            *self._get_plot_args(pos_label=pos_label),
+            show=show,
+            **self._get_plot_kwargs(nbins=nbins, chart=chart),
+            **style_kwargs,
+        )
+
+    def roc_curve(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        nbins: int = 30,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> TableSample:
+        """
+        Draws the model ROC curve.
+
+        Parameters
+        ----------
+        pos_label: PythonScalar, optional
+            To  draw  the ROC curve,  one of the  response  column
+            classes  must  be  the  positive  class. The parameter
+            'pos_label' represents this class.
+        nbins: int, optional
+            An integer value that determines the number of decision
+            boundaries.  Decision  boundaries  are   set at equally
+            -spaced intervals between 0 and 1, inclusive.
+        show: bool, optional
+            If set to True,  the  Plotting object is returned.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the  Plotting
+            functions.
+
+        Returns
+        -------
+        TableSample
+                roc curve data points.
+        """
+        return mt.roc_curve(
+            *self._get_plot_args(pos_label=pos_label),
+            show=show,
+            **self._get_plot_kwargs(nbins=nbins, chart=chart),
+            **style_kwargs,
+        )
+
+
+class Regressor(Supervised):
+    # System & Special Methods.
+
+    @abstractmethod
+    def __init__(self) -> None:
+        """Must be overridden in the child class"""
+        super().__init__()
+
+    # Model Evaluation Methods.
+
+    def regression_report(
+        self,
+        metrics: Union[
+            str,
+            Literal[None, "anova", "details"],
+            list[Literal[tuple(mt.FUNCTIONS_REGRESSION_DICTIONNARY)]],
+        ] = None,
+    ) -> Union[float, TableSample]:
+        """
+        Computes a regression report using multiple metrics to
+        evaluate the model (r2, mse, max error...).
+
+        Parameters
+        ----------
+        metrics: str, optional
+            The metrics used to compute the regression report.
+                None    : Computes the model different metrics.
+                anova   : Computes the model ANOVA table.
+                details : Computes the model details.
+            You can also provide a list of different metrics,
+            including the following:
+                aic    : Akaikes Information Criterion
+                bic    : Bayesian Information Criterion
+                max    : Max Error
+                mae    : Mean Absolute Error
+                median : Median Absolute Error
+                mse    : Mean Squared Error
+                msle   : Mean Squared Log Error
+                qe     : quantile  error,  the quantile must be
+                         included in the name. Example:
+                         qe50.1% will return the quantile error
+                         using q=0.501.
+                r2     : R squared coefficient
+                r2a    : R2 adjusted
+                rmse   : Root Mean Squared Error
+                var    : Explained Variance
+
+        Returns
+        -------
+        TableSample
+            report.
+        """
+        prediction = self.deploySQL()
+        if self._model_type == "KNeighborsRegressor":
+            test_relation = self.deploySQL()
+            prediction = "predict_neighbors"
+        elif self._model_type == "KernelDensity":
+            test_relation = self.map
+        else:
+            test_relation = self.test_relation
+        if metrics == "anova":
+            return mt.anova_table(self.y, prediction, test_relation, len(self.X))
+        elif metrics == "details":
+            vdf = vDataFrame(f"SELECT {self.y} FROM {self.input_relation}")
+            n = vdf[self.y].count()
+            kurt = vdf[self.y].kurt()
+            skew = vdf[self.y].skew()
+            jb = vdf[self.y].agg(["jb"])[self.y][0]
+            R2 = self.score(metric="r2")
+            R2_adj = 1 - ((1 - R2) * (n - 1) / (n - len(self.X) - 1))
+            anova_T = mt.anova_table(self.y, prediction, test_relation, len(self.X))
+            F = anova_T["F"][0]
+            p_F = anova_T["p_value"][0]
+            return TableSample(
                 {
-                    "parameters": [],
-                    "avg_score": [],
-                    "avg_train_score": [],
-                    "avg_time": [],
-                    "score_std": [],
-                    "score_train_std": [],
+                    "index": [
+                        "Dep. Variable",
+                        "Model",
+                        "No. Observations",
+                        "No. Predictors",
+                        "R-squared",
+                        "Adj. R-squared",
+                        "F-statistic",
+                        "Prob (F-statistic)",
+                        "Kurtosis",
+                        "Skewness",
+                        "Jarque-Bera (JB)",
+                    ],
+                    "value": [
+                        self.y,
+                        self._model_type,
+                        n,
+                        len(self.X),
+                        R2,
+                        R2_adj,
+                        F,
+                        p_F,
+                        kurt,
+                        skew,
+                        jb,
+                    ],
                 }
             )
-        else:
-            return tablesample(
-                {"parameters": [], "avg_score": [], "avg_time": [], "score_std": [],}
-            )
-    reverse = reverse_score(metric)
-    data.sort(key=lambda tup: tup[1], reverse=reverse)
-    if training_score:
-        result = tablesample(
-            {
-                "features": [elem[0] for elem in data],
-                "avg_score": [elem[1] for elem in data],
-                "avg_train_score": [elem[2] for elem in data],
-                "avg_time": [elem[3] for elem in data],
-                "score_std": [elem[4] for elem in data],
-                "score_train_std": [elem[5] for elem in data],
-            }
-        )
-        if print_info and (
-            "final_print" not in kwargs or kwargs["final_print"] != "no_print"
+        elif isinstance(metrics, NoneType) or isinstance(
+            metrics, (str, list, np.ndarray)
         ):
-            print("\033[1mRandomized Features Search Selected Model\033[0m")
-            print(
-                f"{str(estimator.__class__).split('.')[-1][:-2]}; Features: {result['features'][0]}; \033[91mTest_score: {result['avg_score'][0]}\033[0m; \033[92mTrain_score: {result['avg_train_score'][0]}\033[0m; \033[94mTime: {result['avg_time'][0]}\033[0m;"
-            )
-    else:
-        result = tablesample(
-            {
-                "features": [elem[0] for elem in data],
-                "avg_score": [elem[1] for elem in data],
-                "avg_time": [elem[2] for elem in data],
-                "score_std": [elem[3] for elem in data],
-            }
-        )
-        if print_info and (
-            "final_print" not in kwargs or kwargs["final_print"] != "no_print"
-        ):
-            print("\033[1mRandomized Features Search Selected Model\033[0m")
-            print(
-                f"{str(estimator.__class__).split('.')[-1][:-2]}; Features: {result['features'][0]}; \033[91mTest_score: {result['avg_score'][0]}\033[0m; \033[94mTime: {result['avg_time'][0]}\033[0m;"
-            )
-    return result
-
-
-# ---#
-def randomized_search_cv(
-    estimator,
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    metric: str = "auto",
-    cv: int = 3,
-    pos_label: Union[int, float, str] = None,
-    cutoff: float = -1,
-    nbins: int = 1000,
-    lmax: int = 4,
-    optimized_grid: int = 1,
-    print_info: bool = True,
-):
-    """
----------------------------------------------------------------------------
-Computes the K-Fold randomized search of an estimator.
+            return mt.regression_report(
+                self.y, prediction, test_relation, metrics=metrics, k=len(self.X)
+            )
 
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-pos_label: int/float/str, optional
-    The main class to be considered as positive (classification only).
-cutoff: float, optional
-    The model cutoff (classification only).
-nbins: int, optional
-    Number of bins used to compute the different parameters categories.
-lmax: int, optional
-    Maximum length of each parameter list.
-optimized_grid: int, optional
-    If set to 0, the randomness is based on the input parameters.
-    If set to 1, the randomness is limited to some parameters while others
-    are picked based on a default grid.
-    If set to 2, there is no randomness and a default grid is returned.
-print_info: bool, optional
-    If set to True, prints the model information at each step.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    param_grid = gen_params_grid(estimator, nbins, len(X), lmax, optimized_grid)
-    return grid_search_cv(
-        estimator,
-        param_grid,
-        input_relation,
-        X,
-        y,
-        metric,
-        cv,
-        pos_label,
-        cutoff,
-        True,
-        "no_print",
-        print_info,
-    )
-
-
-# ---#
-def roc_curve(
-    y_true: str,
-    y_score: str,
-    input_relation: Union[str, vDataFrame],
-    pos_label: Union[int, float, str] = 1,
-    nbins: int = 30,
-    auc_roc: bool = False,
-    best_threshold: bool = False,
-    cutoff_curve: bool = False,
-    ax=None,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Draws the ROC Curve.
+    report = regression_report
 
-Parameters
-----------
-y_true: str
-    Response column.
-y_score: str
-    Prediction Probability.
-input_relation: str/vDataFrame
-    Relation to use for scoring. This relation can be a view, table, or a 
-    customized relation (if an alias is used at the end of the relation). 
-    For example: (SELECT ... FROM ...) x
-pos_label: int/float/str, optional
-    To compute the PRC Curve, one of the response column classes must be the
-    positive one. The parameter 'pos_label' represents this class.
-nbins: int, optional
-    An integer value that determines the number of decision boundaries. Decision 
-    boundaries are set at equally-spaced intervals between 0 and 1, inclusive.
-auc_roc: bool, optional
-    If set to true, the function will return the ROC AUC without drawing the 
-    curve.
-best_threshold: bool, optional
-    If set to True, the function will return the best threshold without drawing 
-    the curve. The best threshold is the threshold of the point which is the 
-    farest from the random line.
-cutoff_curve: bool, optional
-    If set to True, the Cutoff curve will be drawn.
-ax: Matplotlib axes object, optional
-    The axes to plot on.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    check_types(
-        [
-            ("y_true", y_true, [str]),
-            ("y_score", y_score, [str]),
-            ("input_relation", input_relation, [str, vDataFrame]),
-            ("nbins", nbins, [int, float]),
-            ("auc_roc", auc_roc, [bool]),
-            ("best_threshold", best_threshold, [bool]),
-            ("cutoff_curve", cutoff_curve, [bool]),
-        ]
-    )
-    if nbins < 0:
-        nbins = 999999
-    version(condition=[8, 0, 0])
-    query = "SELECT decision_boundary, false_positive_rate, true_positive_rate FROM (SELECT ROC(obs, prob USING PARAMETERS num_bins = {}) OVER() FROM (SELECT (CASE WHEN {} = '{}' THEN 1 ELSE 0 END) AS obs, {}::float AS prob FROM {}) AS prediction_output) x"
-    query = query.format(
-        nbins,
-        y_true,
-        pos_label,
-        y_score,
-        input_relation
-        if isinstance(input_relation, str)
-        else input_relation.__genSQL__(),
-    )
-    query_result = executeSQL(
-        query, title="Computing the ROC Table.", method="fetchall"
-    )
-    threshold, false_positive, true_positive = (
-        [item[0] for item in query_result],
-        [item[1] for item in query_result],
-        [item[2] for item in query_result],
-    )
-    auc = 0
-    for i in range(len(false_positive) - 1):
-        if false_positive[i + 1] - false_positive[i] != 0.0:
-            a = (true_positive[i + 1] - true_positive[i]) / (
-                false_positive[i + 1] - false_positive[i]
-            )
-            b = true_positive[i + 1] - a * false_positive[i + 1]
-            auc = (
-                auc
-                + a
-                * (
-                    false_positive[i + 1] * false_positive[i + 1]
-                    - false_positive[i] * false_positive[i]
-                )
-                / 2
-                + b * (false_positive[i + 1] - false_positive[i])
+    def score(
+        self,
+        metric: Literal[
+            tuple(mt.FUNCTIONS_REGRESSION_DICTIONNARY)
+            + ("r2a", "r2_adj", "rsquared_adj", "r2adj", "r2adjusted", "rmse")
+        ] = "r2",
+    ) -> float:
+        """
+        Computes the model score.
+
+        Parameters
+        ----------
+        metric: str, optional
+            The metric used to compute the score.
+                aic    : Akaikes Information Criterion
+                bic    : Bayesian Information Criterion
+                max    : Max Error
+                mae    : Mean Absolute Error
+                median : Median Absolute Error
+                mse    : Mean Squared Error
+                msle   : Mean Squared Log Error
+                r2     : R squared coefficient
+                r2a    : R2 adjusted
+                rmse   : Root Mean Squared Error
+                var    : Explained Variance
+
+        Returns
+        -------
+        float
+            score.
+        """
+        # Initialization
+        metric = str(metric).lower()
+        if metric in ["r2adj", "r2adjusted"]:
+            metric = "r2a"
+        adj, root = False, False
+        if metric in ("r2a", "r2adj", "r2adjusted", "r2_adj", "rsquared_adj"):
+            metric, adj = "r2", True
+        elif metric == "rmse":
+            metric, root = "mse", True
+        fun = mt.FUNCTIONS_REGRESSION_DICTIONNARY[metric]
+
+        # Scoring
+        if self._model_type == "KNeighborsRegressor":
+            test_relation, prediction = self.deploySQL(), "predict_neighbors"
+        elif self._model_type == "KernelDensity":
+            test_relation, prediction = self.map, self.deploySQL()
+        else:
+            test_relation, prediction = self.test_relation, self.deploySQL()
+        arg = [self.y, prediction, test_relation]
+        if metric in ("aic", "bic") or adj:
+            arg += [len(self.X)]
+        if root or adj:
+            arg += [True]
+        return fun(*arg)
+
+    # Prediction / Transformation Methods.
+
+    def predict(
+        self,
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        inplace: bool = True,
+    ) -> vDataFrame:
+        """
+        Predicts using the input relation.
+
+        Parameters
+        ----------
+        vdf: SQLRelation
+            Object  used to run  the prediction.  You can
+            also  specify a  customized  relation,  but you
+            must  enclose  it with an alias.  For  example,
+            "(SELECT 1) x" is valid, whereas "(SELECT 1)"
+            and "SELECT 1" are invalid.
+        X: SQLColumns, optional
+            List of the columns  used to deploy the models.
+            If empty, the model predictors are used.
+        name: str, optional
+            Name of the added vDataColumn. If empty, a name
+            is generated.
+        inplace: bool, optional
+                If set to True, the prediction is added to
+            the vDataFrame.
+
+        Returns
+        -------
+        vDataFrame
+                the input object.
+        """
+        if hasattr(self, "_predict"):
+            return self._predict(vdf=vdf, X=X, name=name, inplace=inplace)
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        if isinstance(vdf, str):
+            vdf = vDataFrame(vdf)
+        if not name:
+            name = f"{self._model_type}_" + "".join(
+                ch for ch in self.model_name if ch.isalnum()
             )
-    auc = -auc
-    auc = min(auc, 1.0)
-    if auc_roc:
-        return auc
-    if best_threshold:
-        l = [abs(y - x) for x, y in zip(false_positive, true_positive)]
-        best_threshold_arg = max(zip(l, range(len(l))))[1]
-        best = max(threshold[best_threshold_arg], 0.001)
-        best = min(best, 0.999)
-        return best
-    if not (ax):
-        fig, ax = plt.subplots()
-        if isnotebook():
-            fig.set_size_inches(8, 6)
-    color1, color2 = color_dict(style_kwds, 0), color_dict(style_kwds, 1)
-    if color1 == color2:
-        color2 = gen_colors()[1]
-    if cutoff_curve:
-        ax.plot(
-            threshold,
-            [1 - item for item in false_positive],
-            label="Specificity",
-            **updated_dict({"color": gen_colors()[0]}, style_kwds),
-        )
-        ax.plot(
-            threshold,
-            true_positive,
-            label="Sensitivity",
-            **updated_dict({"color": gen_colors()[1]}, style_kwds),
-        )
-        ax.fill_between(
-            threshold,
-            [1 - item for item in false_positive],
-            true_positive,
-            facecolor="black",
-            alpha=0.02,
-        )
-        ax.set_xlabel("Decision Boundary")
-        ax.set_title("Cutoff Curve")
-        ax.legend(loc="center left", bbox_to_anchor=[1, 0.5])
-    else:
-        ax.set_xlabel("False Positive Rate (1-Specificity)")
-        ax.set_ylabel("True Positive Rate (Sensitivity)")
-        ax.plot(
-            false_positive,
-            true_positive,
-            **updated_dict({"color": gen_colors()[0]}, style_kwds),
-        )
-        ax.fill_between(
-            false_positive, false_positive, true_positive, facecolor=color1, alpha=0.1
-        )
-        ax.fill_between([0, 1], [0, 0], [0, 1], facecolor=color2, alpha=0.1)
-        ax.plot([0, 1], [0, 1], color=color2)
-        ax.set_title("ROC Curve")
-        ax.text(
-            0.995,
-            0,
-            "AUC = " + str(round(auc, 4) * 100) + "%",
-            verticalalignment="bottom",
-            horizontalalignment="right",
-            fontsize=11.5,
-        )
-    ax.set_ylim(0, 1)
-    ax.set_xlim(0, 1)
-    ax.set_axisbelow(True)
-    ax.grid()
-    return tablesample(
-        values={
-            "threshold": threshold,
-            "false_positive": false_positive,
-            "true_positive": true_positive,
-        }
-    )
+        if inplace:
+            return vdf.eval(name, self.deploySQL(X=X))
+        else:
+            return vdf.copy().eval(name, self.deploySQL(X=X))
 
 
-# ---#
-def stepwise(
-    estimator,
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    criterion: str = "bic",
-    direction: str = "backward",
-    max_steps: int = 100,
-    criterion_threshold: int = 3,
-    drop_final_estimator: bool = True,
-    x_order: str = "pearson",
-    print_info: bool = True,
-    show: bool = True,
-    ax=None,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Uses the Stepwise algorithm to find the most suitable number of features
-when fitting the estimator.
-
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-criterion: str, optional
-    Criterion used to evaluate the model.
-        aic : Akaikes Information Criterion
-        bic : Bayesian Information Criterion
-direction: str, optional
-    How to start the stepwise search. Can be done 'backward' or 'forward'.
-max_steps: int, optional
-    The maximum number of steps to be considered.
-criterion_threshold: int, optional
-    Threshold used when comparing the models criterions. If the difference
-    is lesser than the threshold then the current 'best' model is changed.
-drop_final_estimator: bool, optional
-    If set to True, the final estimator will be dropped.
-x_order: str, optional
-    How to preprocess X before using the stepwise algorithm.
-        pearson  : X is ordered based on the Pearson's correlation coefficient.
-        spearman : X is ordered based on the Spearman's correlation coefficient.
-        random   : Shuffles the vector X before applying the stepwise algorithm.
-        none     : Does not change the order of X.
-print_info: bool, optional
-    If set to True, prints the model information at each step.
-show: bool, optional
-    If set to True, the stepwise graphic will be drawn.
-ax: Matplotlib axes object, optional
-    The axes to plot on.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    from verticapy.learn.metrics import aic_bic
+class Unsupervised(VerticaModel):
+    # System & Special Methods.
 
-    if isinstance(X, str):
-        X = [X]
-    if isinstance(x_order, str):
-        x_order = x_order.lower()
-    assert len(X) >= 1, ParameterError("Vector X must have at least one element.")
-    check_types(
-        [
-            ("criterion", criterion, ["aic", "bic"]),
-            ("direction", direction, ["forward", "backward"]),
-            ("max_steps", max_steps, [int, float]),
-            ("print_info", print_info, [bool]),
-            ("x_order", x_order, ["pearson", "spearman", "random", "none"]),
-        ]
-    )
-    does_model_exist(name=estimator.name, raise_error=True)
-    result, current_step = [], 0
-    table = (
-        input_relation
-        if isinstance(input_relation, str)
-        else input_relation.__genSQL__()
-    )
-    avg = executeSQL(
-        f"SELECT AVG({y}) FROM {table}", method="fetchfirstelem", print_time_sql=False
-    )
-    k = 0 if criterion == "aic" else 1
-    if x_order == "random":
-        random.shuffle(X)
-    elif x_order in ("spearman", "pearson"):
-        if isinstance(input_relation, str):
-            vdf = vDataFrameSQL(input_relation)
-        else:
-            vdf = input_relation
-        X = [
-            elem
-            for elem in vdf.corr(method=x_order, focus=y, columns=X, show=False)[
-                "index"
-            ]
-        ]
-        if direction == "backward":
-            X.reverse()
-    if print_info:
-        print("\033[1m\033[4mStarting Stepwise\033[0m\033[0m")
-    if verticapy.options["tqdm"] and print_info:
-        from tqdm.auto import tqdm
-
-        loop = tqdm(range(len(X)))
-    else:
-        loop = range(len(X))
-    model_id = 0
-    if direction == "backward":
-        X_current = [elem for elem in X]
-        estimator.drop()
-        estimator.fit(input_relation, X, y)
-        current_score = estimator.score(criterion)
-        result += [(X_current, current_score, None, None, 0, None)]
-        for idx in loop:
-            if print_info and idx == 0:
-                print(
-                    f"\033[1m[Model 0]\033[0m \033[92m{criterion}: {current_score}\033[0m; Variables: {X_current}"
-                )
-            if current_step >= max_steps:
-                break
-            X_test = [elem for elem in X_current]
-            X_test.remove(X[idx])
-            if len(X_test) != 0:
-                estimator.drop()
-                estimator.fit(input_relation, X_test, y)
-                test_score = estimator.score(criterion)
-            else:
-                test_score = aic_bic(y, str(avg), input_relation, 0)[k]
-            score_diff = test_score - current_score
-            if test_score - current_score < criterion_threshold:
-                sign = "-"
-                model_id += 1
-                current_score = test_score
-                X_current = [elem for elem in X_test]
-                if print_info:
-                    print(
-                        f"\033[1m[Model {model_id}]\033[0m \033[92m{criterion}: {test_score}\033[0m; \033[91m(-) Variable: {X[idx]}\033[0m"
-                    )
+    @abstractmethod
+    def __init__(self) -> None:
+        """Must be overridden in the child class"""
+        super().__init__()
+
+    # Model Fitting Method.
+
+    def fit(
+        self, input_relation: SQLRelation, X: Optional[SQLColumns] = None
+    ) -> Optional[str]:
+        """
+        Trains the model.
+
+        Parameters
+        ----------
+        input_relation: SQLRelation
+                Training relation.
+        X: SQLColumns, optional
+                List of the predictors. If empty, all the
+            numerical columns are used.
+
+        Returns
+        -------
+        str
+                model's summary.
+        """
+        if conf.get_option("overwrite_model"):
+            self.drop()
+        else:
+            self._is_already_stored(raise_error=True)
+        id_column, id_column_name = "", gen_tmp_name(name="id_column")
+        if self._model_type in ("BisectingKMeans", "IsolationForest") and isinstance(
+            conf.get_option("random_state"), int
+        ):
+            X_str = ", ".join(quote_ident(X))
+            id_column = f", ROW_NUMBER() OVER (ORDER BY {X_str}) AS {id_column_name}"
+        if isinstance(input_relation, str) and self._model_type == "MCA":
+            input_relation = vDataFrame(input_relation)
+        tmp_view = False
+        if isinstance(input_relation, vDataFrame) or (id_column):
+            tmp_view = True
+            if isinstance(input_relation, vDataFrame):
+                self.input_relation = input_relation.current_relation()
             else:
-                sign = "+"
-            result += [(X_test, test_score, sign, X[idx], idx + 1, score_diff)]
-            current_step += 1
-    else:
-        X_current = []
-        current_score = aic_bic(y, str(avg), input_relation, 0)[k]
-        result += [(X_current, current_score, None, None, 0, None)]
-        for idx in loop:
-            if print_info and idx == 0:
-                print(
-                    f"\033[1m[Model 0]\033[0m \033[92m{criterion}: {current_score}\033[0m; Variables: {X_current}"
+                self.input_relation = input_relation
+            if self._model_type == "MCA":
+                result = input_relation.sum(columns=X)
+                if isinstance(result, (int, float)):
+                    result = [result]
+                else:
+                    result = result["sum"]
+                result = sum(result) + (input_relation.shape()[0] - 1) * len(result)
+                assert abs(result) < 0.01, ConversionError(
+                    "MCA can only work on a transformed complete disjunctive table. "
+                    "You should transform your relation first.\nTips: Use the "
+                    "vDataFrame.cdt method to transform the relation."
+                )
+            relation = gen_tmp_name(
+                schema=schema_relation(self.model_name)[0], name="view"
+            )
+            drop(relation, method="view")
+            _executeSQL(
+                query=f"""
+                    CREATE VIEW {relation} AS 
+                        SELECT 
+                            /*+LABEL('learn.VerticaModel.fit')*/ *
+                            {id_column} 
+                        FROM {self.input_relation}""",
+                title="Creating a temporary view to fit the model.",
+            )
+            if isinstance(X, NoneType) and (self._model_type == "KPrototypes"):
+                X = input_relation.get_columns()
+            elif isinstance(X, NoneType):
+                X = input_relation.numcol()
+        else:
+            self.input_relation = input_relation
+            relation = input_relation
+            if isinstance(X, NoneType):
+                X = vDataFrame(input_relation).numcol()
+        X = format_type(X, dtype=list)
+        self.X = quote_ident(X)
+        parameters = self._get_vertica_param_dict()
+        if "num_components" in parameters and not parameters["num_components"]:
+            del parameters["num_components"]
+        fun = self._vertica_fit_sql if self._model_type != "MCA" else "PCA"
+        query = f"""
+            SELECT 
+                /*+LABEL('learn.VerticaModel.fit')*/ 
+                {fun}('{self.model_name}', '{relation}', '{', '.join(self.X)}'"""
+        if self._model_type in (
+            "BisectingKMeans",
+            "KMeans",
+            "KPrototypes",
+        ):
+            query += f", {parameters['n_cluster']}"
+        elif self._model_type == "Scaler":
+            query += f", {parameters['method']}"
+            del parameters["method"]
+        if self._model_type not in ("Scaler", "MCA"):
+            query += " USING PARAMETERS "
+        for param in (
+            "n_cluster",
+            "separator",
+            "null_column_name",
+            "column_naming",
+            "ignore_null",
+            "drop_first",
+        ):
+            if param in parameters:
+                del parameters[param]
+        if (
+            "init_method" in parameters
+            and not isinstance(parameters["init_method"], str)
+            and self._model_type
+            in (
+                "KMeans",
+                "BisectingKMeans",
+                "KPrototypes",
+            )
+        ):
+            name_init = gen_tmp_name(
+                schema=schema_relation(self.model_name)[0],
+                name=f"{self._model_type.lower()}_init",
+            )
+            del parameters["init_method"]
+            drop(name_init, method="table")
+            if len(self.parameters["init"]) != self.parameters["n_cluster"]:
+                raise ValueError(
+                    f"'init' must be a list of 'n_cluster' = {self.parameters['n_cluster']} points"
                 )
-            if current_step >= max_steps:
-                break
-            X_test = [elem for elem in X_current] + [X[idx]]
-            estimator.drop()
-            estimator.fit(input_relation, X_test, y)
-            test_score = estimator.score(criterion)
-            score_diff = current_score - test_score
-            if current_score - test_score > criterion_threshold:
-                sign = "+"
-                model_id += 1
-                current_score = test_score
-                X_current = [elem for elem in X_test]
-                if print_info:
-                    print(
-                        f"\033[1m[Model {model_id}]\033[0m \033[92m{criterion}: {test_score}\033[0m; \033[91m(+) Variable: {X[idx]}\033[0m"
-                    )
             else:
-                sign = "-"
-            result += [(X_test, test_score, sign, X[idx], idx + 1, score_diff)]
-            current_step += 1
-    if print_info:
-        print(f"\033[1m\033[4mSelected Model\033[0m\033[0m\n")
-        print(
-            f"\033[1m[Model {model_id}]\033[0m \033[92m{criterion}: {current_score}\033[0m; Variables: {X_current}"
-        )
-    features = [elem[0] for elem in result]
-    for idx, elem in enumerate(features):
-        features[idx] = [item.replace('"', "") for item in elem]
-    importance = [elem[5] if (elem[5]) and elem[5] > 0 else 0 for elem in result]
-    importance = [100 * elem / sum(importance) for elem in importance]
-    result = tablesample(
-        {
-            "index": [elem[4] for elem in result],
-            "features": features,
-            criterion: [elem[1] for elem in result],
-            "change": [elem[2] for elem in result],
-            "variable": [elem[3] for elem in result],
-            "importance": importance,
-        }
-    )
-    estimator.drop()
-    if not (drop_final_estimator):
-        estimator.fit(input_relation, X_current, y)
-    result.best_list_ = X_current
-    if show:
-        plot_stepwise_ml(
-            [len(elem) for elem in result["features"]],
-            result[criterion],
-            result["variable"],
-            result["change"],
-            [result["features"][0], X_current],
-            x_label="n_features",
-            y_label=criterion,
-            direction=direction,
-            ax=ax,
-            **style_kwds,
-        )
-        coeff_importances = {}
-        for idx in range(len(importance)):
-            if result["variable"][idx] != None:
-                coeff_importances[result["variable"][idx]] = importance[idx]
-        plot_importance(coeff_importances, print_legend=False, ax=ax, **style_kwds)
-    return result
-
-
-# ---#
-def validation_curve(
-    estimator,
-    param_name: str,
-    param_range: list,
-    input_relation: Union[str, vDataFrame],
-    X: list,
-    y: str,
-    metric: str = "auto",
-    cv: int = 3,
-    pos_label: Union[int, float, str] = None,
-    cutoff: float = -1,
-    std_coeff: float = 1,
-    ax=None,
-    **style_kwds,
-):
-    """
----------------------------------------------------------------------------
-Draws the validation curve.
-
-Parameters
-----------
-estimator: object
-    Vertica estimator with a fit method.
-param_name: str
-    Parameter name.
-param_range: list
-    Parameter Range.
-input_relation: str/vDataFrame
-    Relation to use to train the model.
-X: list
-    List of the predictor columns.
-y: str
-    Response Column.
-metric: str, optional
-    Metric used to do the model evaluation.
-        auto: logloss for classification & rmse for regression.
-    For Classification:
-        accuracy    : Accuracy
-        auc         : Area Under the Curve (ROC)
-        bm          : Informedness = tpr + tnr - 1
-        csi         : Critical Success Index = tp / (tp + fn + fp)
-        f1          : F1 Score 
-        logloss     : Log Loss
-        mcc         : Matthews Correlation Coefficient 
-        mk          : Markedness = ppv + npv - 1
-        npv         : Negative Predictive Value = tn / (tn + fn)
-        prc_auc     : Area Under the Curve (PRC)
-        precision   : Precision = tp / (tp + fp)
-        recall      : Recall = tp / (tp + fn)
-        specificity : Specificity = tn / (tn + fp)
-    For Regression:
-        max    : Max error
-        mae    : Mean absolute error
-        median : Median absolute error
-        mse    : Mean squared error
-        msle   : Mean squared log error
-        r2     : R-squared coefficient
-        r2a    : R2 adjusted
-        rmse   : Root-mean-squared error
-        var    : Explained variance
-cv: int, optional
-    Number of folds.
-pos_label: int/float/str, optional
-    The main class to be considered as positive (classification only).
-cutoff: float, optional
-    The model cutoff (classification only).
-std_coeff: float, optional
-    Value of the standard deviation coefficient used to compute the area plot 
-    around each score.
-ax: Matplotlib axes object, optional
-    The axes to plot on.
-**style_kwds
-    Any optional parameter to pass to the Matplotlib functions.
-
-Returns
--------
-tablesample
-    An object containing the result. For more information, see
-    utilities.tablesample.
-    """
-    if not (isinstance(param_range, Iterable)) or isinstance(param_range, str):
-        param_range = [param_range]
-    from verticapy.plot import range_curve
-
-    gs_result = grid_search_cv(
-        estimator,
-        {param_name: param_range},
-        input_relation,
-        X,
-        y,
-        metric,
-        cv,
-        pos_label,
-        cutoff,
-        True,
-        False,
-        False,
-    )
-    gs_result_final = [
-        (
-            gs_result["parameters"][i][param_name],
-            gs_result["avg_score"][i],
-            gs_result["avg_train_score"][i],
-            gs_result["score_std"][i],
-            gs_result["score_train_std"][i],
-        )
-        for i in range(len(param_range))
-    ]
-    gs_result_final.sort(key=lambda tup: tup[0])
-    X = [elem[0] for elem in gs_result_final]
-    Y = [
-        [
-            [elem[2] - std_coeff * elem[4] for elem in gs_result_final],
-            [elem[2] for elem in gs_result_final],
-            [elem[2] + std_coeff * elem[4] for elem in gs_result_final],
-        ],
-        [
-            [elem[1] - std_coeff * elem[3] for elem in gs_result_final],
-            [elem[1] for elem in gs_result_final],
-            [elem[1] + std_coeff * elem[3] for elem in gs_result_final],
-        ],
-    ]
-    result = tablesample(
-        {
-            param_name: X,
-            "training_score_lower": Y[0][0],
-            "training_score": Y[0][1],
-            "training_score_upper": Y[0][2],
-            "test_score_lower": Y[1][0],
-            "test_score": Y[1][1],
-            "test_score_upper": Y[1][2],
-        }
-    )
-    range_curve(X, Y, param_name, metric, ax, ["train", "test"], **style_kwds)
-    return result
+                for item in self.parameters["init"]:
+                    if len(X) != len(item):
+                        raise ValueError(
+                            f"Each points of 'init' must be of size len(X) = {len(self.X)}"
+                        )
+                query0 = []
+                for i in range(len(self.parameters["init"])):
+                    line = []
+                    for j in range(len(self.parameters["init"][0])):
+                        val = self.parameters["init"][i][j]
+                        if isinstance(val, str):
+                            val = "'" + val.replace("'", "''") + "'"
+                        line += [str(val) + " AS " + X[j]]
+                    line = ",".join(line)
+                    if i == 0:
+                        query0 += [
+                            "SELECT /*+LABEL('learn.VerticaModel.fit')*/ " + line
+                        ]
+                    else:
+                        query0 += ["SELECT " + line]
+                query0 = " UNION ".join(query0)
+                query0 = f"CREATE TABLE {name_init} AS {query0}"
+                _executeSQL(query0, print_time_sql=False)
+                query += f"initial_centers_table = '{name_init}', "
+        elif "init_method" in parameters:
+            del parameters["init_method"]
+            query += f"init_method = '{self.parameters['init']}', "
+        query += ", ".join([f"{p} = {parameters[p]}" for p in parameters])
+        if self._model_type == "BisectingKMeans" and isinstance(
+            conf.get_option("random_state"), int
+        ):
+            query += f", kmeans_seed={conf.get_option('random_state')}"
+            query += f", id_column='{id_column_name}'"
+        elif self._model_type == "IsolationForest" and isinstance(
+            conf.get_option("random_state"), int
+        ):
+            query += f", seed={conf.get_option('random_state')}"
+            query += f", id_column='{id_column_name}'"
+        query += ")"
+        try:
+            _executeSQL(query, "Fitting the model.")
+        except QueryError:
+            if (
+                "init_method" in parameters
+                and not isinstance(parameters["init_method"], str)
+                and self._model_type
+                in (
+                    "KMeans",
+                    "BisectingKMeans",
+                    "KPrototypes",
+                )
+            ):
+                drop(name_init, method="table")
+            raise
+        finally:
+            if tmp_view:
+                drop(relation, method="view")
+        if self._model_type in (
+            "KMeans",
+            "BisectingKMeans",
+            "KPrototypes",
+        ):
+            if "init_method" in parameters and not (
+                isinstance(parameters["init_method"], str)
+            ):
+                drop(name_init, method="table")
+        self._compute_attributes()
+        if self._is_native:
+            return self.summarize()
+        return None
```

### Comparing `verticapy-0.9.0/verticapy/learn/naive_bayes.py` & `verticapy-1.0.0b1/verticapy/machine_learning/vertica/naive_bayes.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,148 +1,175 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# VerticaPy Modules
-from verticapy.learn.vmodel import *
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+from typing import Literal
+import numpy as np
+
+from verticapy._typing import PythonNumber
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import quote_ident
+from verticapy._utils._sql._vertica_version import check_minimum_version
+
+from verticapy.core.vdataframe.base import vDataFrame
+
+import verticapy.machine_learning.memmodel as mm
+
+from verticapy.machine_learning.vertica.base import MulticlassClassifier
+
+"""
+Algorithms used for classification.
+"""
+
 
-# ---#
 class NaiveBayes(MulticlassClassifier):
     """
----------------------------------------------------------------------------
-Creates a NaiveBayes object using the Vertica Naive Bayes algorithm on 
-the data. It is a "probabilistic classifier" based on applying Bayes' 
-theorem with strong (nave) independence assumptions between the features.
-
-Parameters
-----------
-name: str
-	Name of the the model. The model will be stored in the DB.
-alpha: float, optional
-	A float that specifies use of Laplace smoothing if the event model is 
-	categorical, multinomial, or Bernoulli.
-nbtype: str, optional
-    Naive Bayes Type.
-    - auto        : Vertica NB will treat columns according to data type:
-        * FLOAT        : Values are assumed to follow some Gaussian 
-                         distribution.
-        * INTEGER      : Values are assumed to belong to one multinomial 
-                         distribution.
-        * CHAR/VARCHAR : Values are assumed to follow some categorical 
-                         distribution. The string values stored in these 
-                         columns must not be greater than 128 characters.
-        * BOOLEAN      : Values are treated as categorical with two values.
-     - bernoulli   : Casts the variables to boolean.
-     - categorical : Casts the variables to categorical.
-     - multinomial : Casts the variables to integer.
-     - gaussian    : Casts the variables to float.
-	"""
-
-    def __init__(self, name: str, alpha: float = 1.0, nbtype: str = "auto"):
-
-        nbtype_vals = ["auto", "bernoulli", "categorical", "multinomial", "gaussian"]
-        check_types(
-            [
-                ("name", name, [str]),
-                ("alpha", alpha, [int, float]),
-                ("nbtype", nbtype, nbtype_vals),
-            ]
+    Creates  a  NaiveBayes object using the Vertica
+    Naive  Bayes  algorithm.  It is a "probabilistic
+    classifier"  based  on  applying Bayes' theorem
+    with strong (nave) independence assumptions
+    between the features.
+
+    Parameters
+    ----------
+    name: str
+        Name  of  the  model.  The  model is stored
+        in the database.
+    alpha: float, optional
+        A  float  that  specifies  use  of  Laplace
+        smoothing if the event model is categorical,
+        multinomial, or Bernoulli.
+    nbtype: str, optional
+        Naive Bayes type.
+        - auto        : Vertica NaiveBayes objects
+                treat columns according to data type:
+              * FLOAT values are assumed to follow some
+                Gaussian distribution.
+              * INTEGER values are assumed to belong to
+                one multinomial distribution.
+              * CHAR/VARCHAR   values  are  assumed  to
+                follow  some categorical distribution.
+                The  string  values  stored  in  these
+                columns  must be no greater than  128
+                characters.
+              * BOOLEAN    values    are   treated   as
+                categorical with two values.
+        - bernoulli   : Casts the variables to boolean.
+        - categorical : Casts the variables to categorical.
+        - multinomial : Casts the variables to integer.
+        - gaussian    : Casts the variables to float.
+    """
+
+    # Properties.
+
+    @property
+    def _vertica_fit_sql(self) -> Literal["NAIVE_BAYES"]:
+        return "NAIVE_BAYES"
+
+    @property
+    def _vertica_predict_sql(self) -> Literal["PREDICT_NAIVE_BAYES"]:
+        return "PREDICT_NAIVE_BAYES"
+
+    @property
+    def _model_category(self) -> Literal["SUPERVISED"]:
+        return "SUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["CLASSIFIER"]:
+        return "CLASSIFIER"
+
+    @property
+    def _model_type(self) -> Literal["NaiveBayes"]:
+        return "NaiveBayes"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["attributes_", "prior_", "classes_"]
+
+    # System & Special Methods.
+
+    @check_minimum_version
+    @save_verticapy_logs
+    def __init__(
+        self,
+        name: str,
+        alpha: PythonNumber = 1.0,
+        nbtype: Literal[
+            "auto", "bernoulli", "categorical", "multinomial", "gaussian"
+        ] = "auto",
+    ) -> None:
+        super().__init__()
+        self.model_name = name
+        self.parameters = {"alpha": alpha, "nbtype": str(nbtype).lower()}
+
+    # Attributes Methods.
+
+    def _compute_attributes(self) -> None:
+        """
+        Computes the model's attributes.
+        """
+        self.classes_ = self._array_to_int(
+            np.array(self.get_vertica_attributes("prior")["class"])
         )
-        self.type, self.name = "NaiveBayes", name
-        self.set_params({"alpha": alpha, "nbtype": nbtype})
-        version(condition=[8, 0, 0])
-
-    # ---#
-    def get_var_info(self):
-        # Returns a list of dictionary for each of the NB variables.
-        # It is used to translate NB to Python
-        from verticapy.utilities import vDataFrameSQL
+        self.prior_ = np.array(self.get_vertica_attributes("prior")["probability"])
+        self.attributes_ = self._get_nb_attributes()
 
-        vdf = vDataFrameSQL(self.input_relation)
+    def _get_nb_attributes(self) -> list[dict]:
+        """
+        Returns a list of dictionary for each of the NB
+        variables. It is used to translate NB to Python.
+        """
+        vdf = vDataFrame(self.input_relation)
         var_info = {}
         gaussian_incr, bernoulli_incr, multinomial_incr = 0, 0, 0
         for idx, elem in enumerate(self.X):
             var_info[elem] = {"rank": idx}
             if vdf[elem].isbool():
                 var_info[elem]["type"] = "bernoulli"
                 for c in self.classes_:
-                    var_info[elem][c] = self.get_attr("bernoulli.{}".format(c))[
+                    var_info[elem][c] = self.get_vertica_attributes(f"bernoulli.{c}")[
                         "probability"
                     ][bernoulli_incr]
                 bernoulli_incr += 1
             elif vdf[elem].category() == "int":
                 var_info[elem]["type"] = "multinomial"
                 for c in self.classes_:
-                    multinomial = self.get_attr("multinomial.{}".format(c))
+                    multinomial = self.get_vertica_attributes(f"multinomial.{c}")
                     var_info[elem][c] = multinomial["probability"][multinomial_incr]
                 multinomial_incr += 1
             elif vdf[elem].isnum():
                 var_info[elem]["type"] = "gaussian"
                 for c in self.classes_:
-                    gaussian = self.get_attr("gaussian.{}".format(c))
+                    gaussian = self.get_vertica_attributes(f"gaussian.{c}")
                     var_info[elem][c] = {
                         "mu": gaussian["mu"][gaussian_incr],
                         "sigma_sq": gaussian["sigma_sq"][gaussian_incr],
                     }
                 gaussian_incr += 1
             else:
                 var_info[elem]["type"] = "categorical"
                 my_cat = "categorical." + quote_ident(elem)[1:-1]
-                attr = self.get_attr()["attr_name"]
+                attr = self.get_vertica_attributes()["attr_name"]
                 for item in attr:
                     if item.lower() == my_cat.lower():
                         my_cat = item
                         break
-                val = self.get_attr(my_cat).values
+                val = self.get_vertica_attributes(my_cat).values
                 for item in val:
                     if item != "category":
                         if item not in var_info[elem]:
                             var_info[elem][item] = {}
                         for i, p in enumerate(val[item]):
                             var_info[elem][item][val["category"][i]] = p
         var_info_simplified = []
@@ -151,42 +178,53 @@
                 if var_info[elem]["rank"] == i:
                     var_info_simplified += [var_info[elem]]
                     break
         for elem in var_info_simplified:
             del elem["rank"]
         return var_info_simplified
 
+    # Parameters Methods.
 
-# ---#
-class BernoulliNB(NaiveBayes):
-    """i.e. NaiveBayes with param nbtype = 'bernoulli'"""
+    @staticmethod
+    def _map_to_vertica_param_dict() -> dict:
+        return {}
+
+    # I/O Methods.
+
+    def to_memmodel(self) -> mm.NaiveBayes:
+        """
+        Converts  the model to an InMemory object  that
+        can be used for different types of predictions.
+        """
+        return mm.NaiveBayes(
+            self.attributes_,
+            self.prior_,
+            self.classes_,
+        )
 
-    def __init__(self, name: str, alpha: float = 1.0):
 
-        super().__init__(name, alpha, "bernoulli")
+class BernoulliNB(NaiveBayes):
+    """NaiveBayes with parameter nbtype = 'bernoulli'"""
 
+    def __init__(self, name: str, alpha: float = 1.0) -> None:
+        super().__init__(name, alpha, nbtype="bernoulli")
 
-# ---#
-class CategoricalNB(NaiveBayes):
-    """i.e. NaiveBayes with param nbtype = 'categorical'"""
 
-    def __init__(self, name: str, alpha: float = 1.0):
+class CategoricalNB(NaiveBayes):
+    """NaiveBayes with parameter nbtype = 'categorical'"""
 
-        super().__init__(name, alpha, "categorical")
+    def __init__(self, name: str, alpha: float = 1.0) -> None:
+        super().__init__(name, alpha, nbtype="categorical")
 
 
-# ---#
 class GaussianNB(NaiveBayes):
-    """i.e. NaiveBayes with param nbtype = 'gaussian'"""
-
-    def __init__(self, name: str):
+    """NaiveBayes with parameter nbtype = 'gaussian'"""
 
+    def __init__(self, name: str) -> None:
         super().__init__(name, nbtype="gaussian")
 
 
-# ---#
 class MultinomialNB(NaiveBayes):
-    """i.e. NaiveBayes with param nbtype = 'multinomial'"""
-
-    def __init__(self, name: str, alpha: float = 1.0):
+    """NaiveBayes with parameter nbtype = 'multinomial'"""
 
-        super().__init__(name, alpha, "multinomial")
+    def __init__(self, name: str, alpha: float = 1.0) -> None:
+        super().__init__(name, alpha, nbtype="multinomial")
```

### Comparing `verticapy-0.9.0/verticapy/learn/neighbors.py` & `verticapy-1.0.0b1/verticapy/machine_learning/vertica/neighbors.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1767 +1,1468 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# VerticaPy Modules
-from verticapy.learn.metrics import *
-from verticapy.learn.mlplot import *
-from verticapy.utilities import *
-from verticapy.toolbox import *
-from verticapy import vDataFrame
-from verticapy.learn.mlplot import *
-from verticapy.learn.model_selection import *
-from verticapy.errors import *
-from verticapy.learn.vmodel import *
-from verticapy.learn.tools import *
-
-# Standard Python Modules
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import itertools
 import warnings
-from typing import Union
+from typing import Literal, Optional
+import numpy as np
 
-# ---#
-class NearestCentroid(MulticlassClassifier):
-    """
----------------------------------------------------------------------------
-[Beta Version]
-Creates a NearestCentroid object using the k-nearest centroid algorithm. 
-This object uses pure SQL to compute the distances and final score. 
-
-\u26A0 Warning : Because this algorithm uses p-distances, it is highly 
-                 sensitive to unnormalized data.
-
-Parameters
-----------
-p: int, optional
-	The p corresponding to the one of the p-distances (distance metric used
-	to compute the model).
-	"""
-
-    def __init__(self, name: str, p: int = 2):
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "NearestCentroid", name
-        self.set_params({"p": p})
+from vertica_python.errors import QueryError
 
-    # ---#
-    def fit(
-        self,
-        input_relation: Union[str, vDataFrame],
-        X: list,
-        y: str,
-        test_relation: Union[str, vDataFrame] = "",
-    ):
-        """
-	---------------------------------------------------------------------------
-	Trains the model.
-
-	Parameters
-	----------
-	input_relation: str/vDataFrame
-		Training relation.
-	X: list
-		List of the predictors.
-	y: str
-		Response column.
-	test_relation: str/vDataFrame, optional
-		Relation used to test the model.
-
-	Returns
-	-------
-	object
- 		self
-		"""
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [
-                ("input_relation", input_relation, [str, vDataFrame], False),
-                ("X", X, [list], False),
-                ("y", y, [str], False),
-                ("test_relation", test_relation, [str, vDataFrame], False),
-            ]
-        )
-        if verticapy.options["overwrite_model"]:
-            self.drop()
-        else:
-            does_model_exist(name=self.name, raise_error=True)
-        func = "APPROXIMATE_MEDIAN" if (self.parameters["p"] == 1) else "AVG"
-        if isinstance(input_relation, vDataFrame):
-            self.input_relation = input_relation.__genSQL__()
-        else:
-            self.input_relation = input_relation
-        if isinstance(test_relation, vDataFrame):
-            self.test_relation = test_relation.__genSQL__()
-        elif test_relation:
-            self.test_relation = test_relation
-        else:
-            self.test_relation = self.input_relation
-        self.X = [quote_ident(column) for column in X]
-        self.y = quote_ident(y)
-        query = "SELECT {}, {} FROM {} WHERE {} IS NOT NULL GROUP BY {} ORDER BY {} ASC".format(
-            ", ".join(
-                ["{}({}) AS {}".format(func, column, column) for column in self.X]
-            ),
-            self.y,
-            self.input_relation,
-            self.y,
-            self.y,
-            self.y,
-        )
-        self.centroids_ = to_tablesample(query=query, title="Getting Model Centroids.",)
-        self.classes_ = self.centroids_.values[y]
-        model_save = {
-            "type": "NearestCentroid",
-            "input_relation": self.input_relation,
-            "test_relation": self.test_relation,
-            "X": self.X,
-            "y": self.y,
-            "p": self.parameters["p"],
-            "centroids": self.centroids_.values,
-            "classes": self.classes_,
-        }
-        insert_verticapy_schema(
-            model_name=self.name, model_type="NearestCentroid", model_save=model_save,
-        )
-        return self
+import verticapy._config.config as conf
+from verticapy._typing import (
+    NoneType,
+    PlottingObject,
+    PythonNumber,
+    PythonScalar,
+    SQLColumns,
+    SQLRelation,
+)
+from verticapy._utils._gen import gen_name, gen_tmp_name
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import (
+    clean_query,
+    format_type,
+    quote_ident,
+    schema_relation,
+)
+from verticapy._utils._sql._sys import _executeSQL
+
+
+from verticapy.core.tablesample.base import TableSample
+from verticapy.core.vdataframe.base import vDataFrame
+
+from verticapy.plotting._utils import PlottingUtils
+
+import verticapy.machine_learning.metrics as mt
+from verticapy.machine_learning.vertica.base import (
+    MulticlassClassifier,
+    Regressor,
+    Tree,
+    VerticaModel,
+)
+from verticapy.machine_learning.vertica.tree import DecisionTreeRegressor
+
+from verticapy.sql.drop import drop
+
+
+"""
+Algorithms used for regression.
+"""
 
 
-# ---#
-class KNeighborsClassifier(vModel):
+class KNeighborsRegressor(Regressor):
     """
----------------------------------------------------------------------------
-[Beta Version]
-Creates a KNeighborsClassifier object using the k-nearest neighbors algorithm. 
-This object uses pure SQL to compute the distances and final score.
-
-\u26A0 Warning : This algorithm uses a CROSS JOIN during computation and
-                 is therefore computationally expensive at O(n * n), where
-                 n is the total number of elements. Because this algorithm  
-                 uses the p-distance, it is highly sensitive to unnormalized 
-                 data.
-
-Parameters
-----------
-n_neighbors: int, optional
-	Number of neighbors to consider when computing the score.
-p: int, optional
-	The p corresponding to the one of the p-distances (distance metric used  
-	to compute the model).
-	"""
-
-    def __init__(self, name: str, n_neighbors: int = 5, p: int = 2):
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "KNeighborsClassifier", name
-        self.set_params({"n_neighbors": n_neighbors, "p": p})
-
-    # ---#
-    def deploySQL(
-        self,
-        X: list = [],
-        test_relation: str = "",
-        predict: bool = False,
-        key_columns: list = [],
-    ):
-        """
-	---------------------------------------------------------------------------
-	Returns the SQL code needed to deploy the model. 
+    [Beta Version]
+    Creates a  KNeighborsRegressor object using the
+    k-nearest neighbors algorithm. This object uses
+    pure SQL to compute all the distances and final
+    score.
+
+    \u26A0 Warning : This   algorithm   uses  a   CROSS  JOIN
+                     during   computation  and  is  therefore
+                     computationally  expensive at  O(n * n),
+                     where n is the total number of elements.
+                     Since  KNeighborsRegressor  uses  the p-
+                     distance,  it  is  highly  sensitive  to
+                     unnormalized data.
 
     Parameters
     ----------
-    X: list
-        List of the predictors.
-    test_relation: str, optional
-        Relation to use to do the predictions.
-    predict: bool, optional
-        If set to True, returns the prediction instead of the probability.
-    key_columns: list, optional
-        A list of columns to include in the results, but to exclude from 
-        computation of the prediction.
-
-    Returns
-    -------
-    str/list
-        the SQL code needed to deploy the model.
-		"""
-        if isinstance(X, str):
-            X = [X]
-        if isinstance(key_columns, str):
-            key_columns = [key_columns]
-        check_types(
-            [
-                ("test_relation", test_relation, [str], False),
-                ("predict", predict, [bool], False),
-                ("X", X, [list], False),
-                ("key_columns", key_columns, [list], False),
-            ],
-        )
-        X = [quote_ident(elem) for elem in X] if (X) else self.X
-        if not (test_relation):
+    n_neighbors: int, optional
+        Number of neighbors to consider when computing
+        the score.
+    p: int, optional
+        The p of the p-distances (distance metric used
+        during the model computation).
+    """
+
+    # Properties.
+
+    @property
+    def _is_native(self) -> Literal[False]:
+        return False
+
+    @property
+    def _vertica_fit_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _vertica_predict_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _model_category(self) -> Literal["SUPERVISED"]:
+        return "SUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["REGRESSOR"]:
+        return "REGRESSOR"
+
+    @property
+    def _model_type(self) -> Literal["KNeighborsRegressor"]:
+        return "KNeighborsRegressor"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["n_neighbors_", "p_"]
+
+    # System & Special Methods.
+
+    @save_verticapy_logs
+    def __init__(self, name: str, n_neighbors: int = 5, p: int = 2) -> None:
+        super().__init__()
+        self.model_name = name
+        self.parameters = {"n_neighbors": n_neighbors, "p": p}
+
+    def drop(self) -> bool:
+        """
+        KNN models are not stored in the Vertica DB.
+        """
+        return False
+
+    # Attributes Methods.
+
+    def _compute_attributes(self) -> None:
+        self.p_ = self.parameters["p"]
+        self.n_neighbors_ = self.parameters["n_neighbors"]
+
+    # I/O Methods.
+
+    def deploySQL(
+        self,
+        X: Optional[SQLColumns] = None,
+        test_relation: Optional[str] = None,
+        key_columns: Optional[SQLColumns] = None,
+    ) -> str:
+        """
+        Returns the SQL code needed to deploy the model.
+
+        Parameters
+        ----------
+        X: SQLColumns
+            List of the predictors.
+        test_relation: str, optional
+            Relation used to do the predictions.
+        key_columns: SQLColumns, optional
+            A  list  of columns  to  include in  the  results,
+            but to exclude from computation of the prediction.
+
+        Returns
+        -------
+        str
+            the SQL code needed to deploy the model.
+        """
+        key_columns = format_type(key_columns, dtype=list)
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        if not test_relation:
             test_relation = self.test_relation
-        if not (key_columns) and key_columns != None:
-            key_columns = [self.y]
-        sql = [
-            "POWER(ABS(x.{} - y.{}), {})".format(X[i], self.X[i], self.parameters["p"])
-            for i in range(len(self.X))
-        ]
-        sql = "POWER({}, 1 / {})".format(" + ".join(sql), self.parameters["p"])
-        sql = "ROW_NUMBER() OVER(PARTITION BY {}, row_id ORDER BY {})".format(
-            ", ".join(["x.{}".format(item) for item in X]), sql
-        )
-        sql = "SELECT {}{}, {} AS ordered_distance, y.{} AS predict_neighbors, row_id FROM (SELECT *, ROW_NUMBER() OVER() AS row_id FROM {} WHERE {}) x CROSS JOIN (SELECT * FROM {} WHERE {}) y".format(
-            ", ".join(["x.{}".format(item) for item in X]),
-            ", " + ", ".join(["x." + quote_ident(elem) for elem in key_columns])
-            if (key_columns)
-            else "",
-            sql,
-            self.y,
-            test_relation,
-            " AND ".join(["{} IS NOT NULL".format(item) for item in X]),
-            self.input_relation,
-            " AND ".join(["{} IS NOT NULL".format(item) for item in self.X]),
-        )
-        sql = "(SELECT row_id, {}{}, predict_neighbors, COUNT(*) / {} AS proba_predict FROM ({}) z WHERE ordered_distance <= {} GROUP BY {}{}, row_id, predict_neighbors) kneighbors_table".format(
-            ", ".join(X),
-            ", " + ", ".join([quote_ident(elem) for elem in key_columns])
-            if (key_columns)
-            else "",
-            self.parameters["n_neighbors"],
-            sql,
-            self.parameters["n_neighbors"],
-            ", ".join(X),
-            ", " + ", ".join([quote_ident(elem) for elem in key_columns])
-            if (key_columns)
-            else "",
-        )
-        if predict:
-            sql = "(SELECT {}{}, predict_neighbors FROM (SELECT {}{}, predict_neighbors, ROW_NUMBER() OVER (PARTITION BY {} ORDER BY proba_predict DESC) AS order_prediction FROM {}) VERTICAPY_SUBTABLE WHERE order_prediction = 1) predict_neighbors_table".format(
-                ", ".join(X),
-                ", " + ", ".join([quote_ident(elem) for elem in key_columns])
-                if (key_columns)
-                else "",
-                ", ".join(X),
-                ", " + ", ".join([quote_ident(elem) for elem in key_columns])
-                if (key_columns)
-                else "",
-                ", ".join(X),
-                sql,
-            )
-        return sql
+            if not key_columns:
+                key_columns = [self.y]
+        p = self.parameters["p"]
+        X_str = ", ".join([f"x.{x}" for x in X])
+        if key_columns:
+            key_columns_str = ", " + ", ".join(
+                ["x." + quote_ident(x) for x in key_columns]
+            )
+        else:
+            key_columns_str = ""
+        sql = [f"POWER(ABS(x.{X[i]} - y.{self.X[i]}), {p})" for i in range(len(self.X))]
+        sql = f"""
+            SELECT 
+                {X_str}{key_columns_str}, 
+                ROW_NUMBER() OVER(PARTITION BY {X_str}, row_id 
+                                  ORDER BY POWER({' + '.join(sql)}, 1 / {p})) 
+                                  AS ordered_distance, 
+                y.{self.y} AS predict_neighbors, 
+                row_id 
+            FROM
+                (SELECT 
+                    *, 
+                    ROW_NUMBER() OVER() AS row_id 
+                 FROM {test_relation} 
+                 WHERE {" AND ".join([f"{x} IS NOT NULL" for x in X])}) x 
+                 CROSS JOIN 
+                 (SELECT 
+                    * 
+                 FROM {self.input_relation} 
+                 WHERE {" AND ".join([f"{x} IS NOT NULL" for x in self.X])}) y"""
+        if key_columns:
+            key_columns_str = ", " + ", ".join(quote_ident(key_columns))
+        n_neighbors = self.parameters["n_neighbors"]
+        sql = f"""
+            (SELECT 
+                {", ".join(X)}{key_columns_str}, 
+                AVG(predict_neighbors) AS predict_neighbors 
+             FROM ({sql}) z 
+             WHERE ordered_distance <= {n_neighbors} 
+             GROUP BY {", ".join(X)}{key_columns_str}, row_id) knr_table"""
+        return clean_query(sql)
 
-    # ---#
-    def fit(
+    # Prediction / Transformation Methods.
+
+    def _predict(
         self,
-        input_relation: Union[str, vDataFrame],
-        X: list,
-        y: str,
-        test_relation: Union[str, vDataFrame] = "",
-    ):
-        """
-	---------------------------------------------------------------------------
-	Trains the model.
-
-	Parameters
-	----------
-	input_relation: str/vDataFrame
-		Training relation.
-	X: list
-		List of the predictors.
-	y: str
-		Response column.
-	test_relation: str/vDataFrame, optional
-		Relation used to test the model.
-
-	Returns
-	-------
-	object
- 		self
-		"""
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [
-                ("input_relation", input_relation, [str, vDataFrame], False),
-                ("X", X, [list], False),
-                ("y", y, [str], False),
-                ("test_relation", test_relation, [str, vDataFrame], False),
-            ]
-        )
-        if verticapy.options["overwrite_model"]:
-            self.drop()
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        inplace: bool = True,
+        **kwargs,
+    ) -> vDataFrame:
+        """
+        Predicts using the input relation.
+        """
+        X = format_type(X, dtype=list)
+        if isinstance(vdf, str):
+            vdf = vDataFrame(vdf)
+        X = quote_ident(X) if (X) else self.X
+        key_columns = vdf.get_columns(exclude_columns=X)
+        if "key_columns" in kwargs:
+            key_columns_arg = None
         else:
-            does_model_exist(name=self.name, raise_error=True)
-        if isinstance(input_relation, vDataFrame):
-            self.input_relation = input_relation.__genSQL__()
+            key_columns_arg = key_columns
+        if not name:
+            name = f"{self._model_type}_" + "".join(
+                ch for ch in self.model_name if ch.isalnum()
+            )
+        if key_columns:
+            key_columns_str = ", " + ", ".join(key_columns)
+        else:
+            key_columns_str = ""
+        table = self.deploySQL(
+            X=X, test_relation=vdf.current_relation(), key_columns=key_columns_arg
+        )
+        sql = f"""
+            SELECT 
+                {", ".join(X)}{key_columns_str}, 
+                predict_neighbors AS {name} 
+             FROM {table}"""
+        if inplace:
+            vdf.__init__(sql)
+            return vdf
         else:
-            self.input_relation = input_relation
-        if isinstance(test_relation, vDataFrame):
-            self.test_relation = test_relation.__genSQL__()
-        elif test_relation:
-            self.test_relation = test_relation
-        else:
-            self.test_relation = self.input_relation
-        self.X = [quote_ident(column) for column in X]
-        self.y = quote_ident(y)
-        classes = executeSQL(
-            "SELECT DISTINCT {} FROM {} WHERE {} IS NOT NULL ORDER BY {} ASC".format(
-                self.y, self.input_relation, self.y, self.y
-            ),
-            method="fetchall",
-            print_time_sql=False,
-        )
-        self.classes_ = [item[0] for item in classes]
-        model_save = {
-            "type": "KNeighborsClassifier",
-            "input_relation": self.input_relation,
-            "test_relation": self.test_relation,
-            "X": self.X,
-            "y": self.y,
-            "p": self.parameters["p"],
-            "n_neighbors": self.parameters["n_neighbors"],
-            "classes": self.classes_,
-        }
-        insert_verticapy_schema(
-            model_name=self.name,
-            model_type="KNeighborsClassifier",
-            model_save=model_save,
-        )
-        return self
+            return vDataFrame(sql)
+
+    # Plotting Methods.
 
-    # ---#
-    def classification_report(self, cutoff: Union[float, list] = [], labels: list = []):
+    def _get_plot_args(self, method: Optional[str] = None) -> list:
         """
-    ---------------------------------------------------------------------------
-    Computes a classification report using multiple metrics to evaluate the model
-    (AUC, accuracy, PRC AUC, F1, etc.). For multiclass classification, this 
-    function tests the model by considering one class as the sole positive case, 
-    repeating the process until it tests all classes.
+        Returns the args used by plotting methods.
+        """
+        if method == "contour":
+            args = [
+                self.X,
+                self.deploySQL(X=self.X, test_relation="{1}").replace(
+                    "predict_neighbors", "{0}"
+                ),
+            ]
+        else:
+            raise NotImplementedError
+        return args
+
+
+"""
+Algorithms used for classification.
+"""
+
+
+class KNeighborsClassifier(MulticlassClassifier):
+    """
+    [Beta Version]
+    Creates a KNeighborsClassifier object using the
+    k-nearest neighbors algorithm. This object uses
+    pure SQL to compute all the distances and final
+    score.
+
+    \u26A0 Warning : This   algorithm   uses  a   CROSS  JOIN
+                     during   computation  and  is  therefore
+                     computationally  expensive at  O(n * n),
+                     where n is the total number of elements.
+                     Since  KNeighborsClassifier uses  the p-
+                     distance,  it  is  highly  sensitive  to
+                     unnormalized data.
 
     Parameters
     ----------
-    cutoff: float/list, optional
-        Cutoff for which the tested category is accepted as a prediction. 
-        For multiclass classification, each tested category becomes positive case
-        and untested categories are merged into the negative cases. This list 
-        represents the threshold for each class. If empty, the best cutoff is be used.
-    labels: list, optional
-        List of the different labels to be used during the computation.
-
-    Returns
-    -------
-    tablesample
-        An object containing the result. For more information, see
-        utilities.tablesample.
-        """
-        if not (isinstance(labels, Iterable)) or isinstance(labels, str):
-            labels = [labels]
-        check_types(
-            [("cutoff", cutoff, [int, float, list]), ("labels", labels, [list])]
-        )
-        if not (labels):
-            labels = self.classes_
-        return classification_report(cutoff=cutoff, estimator=self, labels=labels)
-
-    report = classification_report
-
-    # ---#
-    def cutoff_curve(
-        self, pos_label: Union[int, float, str] = None, ax=None, **style_kwds
-    ):
+    n_neighbors: int, optional
+        Number  of neighbors to consider when computing  the
+        score.
+    p: int, optional
+        The p of the p-distances (distance metric used
+        during the model computation).
+    """
+
+    # Properties.
+
+    @property
+    def _is_native(self) -> Literal[False]:
+        return False
+
+    @property
+    def _vertica_fit_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _vertica_predict_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _model_category(self) -> Literal["SUPERVISED"]:
+        return "SUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["CLASSIFIER"]:
+        return "CLASSIFIER"
+
+    @property
+    def _model_type(self) -> Literal["KNeighborsClassifier"]:
+        return "KNeighborsClassifier"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["classes_", "n_neighbors_", "p_"]
+
+    # System & Special Methods.
+
+    @save_verticapy_logs
+    def __init__(self, name: str, n_neighbors: int = 5, p: int = 2) -> None:
+        super().__init__()
+        self.model_name = name
+        self.parameters = {"n_neighbors": n_neighbors, "p": p}
+
+    def drop(self) -> bool:
+        """
+        KNN models are not stored in the Vertica DB.
         """
-    ---------------------------------------------------------------------------
-    Draws the ROC curve of a classification model.
+        return False
 
-    Parameters
-    ----------
-    pos_label: int/float/str
-        The response column class to be considered positive.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    tablesample
-        An object containing the result. For more information, see
-        utilities.tablesample.
-        """
-        pos_label = (
-            self.classes_[1]
-            if (pos_label == None and len(self.classes_) == 2)
-            else pos_label
-        )
-        if pos_label not in self.classes_:
-            raise ParameterError(
-                "'pos_label' must be one of the response column classes"
+    def _check_cutoff(
+        self, cutoff: Optional[PythonNumber] = None
+    ) -> Optional[PythonNumber]:
+        if isinstance(cutoff, NoneType):
+            return 1.0 / len(self.classes_)
+        elif not 0 <= cutoff <= 1:
+            ValueError(
+                "Incorrect parameter 'cutoff'.\nThe cutoff "
+                "must be between 0 and 1, inclusive."
             )
-        input_relation = self.deploySQL() + " WHERE predict_neighbors = '{}'".format(
-            pos_label
-        )
-        return roc_curve(
-            self.y,
-            "proba_predict",
-            input_relation,
-            pos_label,
-            ax=ax,
-            cutoff_curve=True,
-            **style_kwds,
-        )
+        else:
+            return cutoff
+
+    # Attributes Methods.
 
-    # ---#
-    def confusion_matrix(
-        self, pos_label: Union[int, float, str] = None, cutoff: float = -1
-    ):
+    def _compute_attributes(self) -> None:
+        """
+        Computes the model's attributes.
         """
-    ---------------------------------------------------------------------------
-    Computes the model confusion matrix.
+        self.classes_ = self._get_classes()
+        self.p_ = self.parameters["p"]
+        self.n_neighbors_ = self.parameters["n_neighbors"]
 
-    Parameters
-    ----------
-    pos_label: int/float/str, optional
-        Label to consider as positive. All the other classes will be merged and
-        considered as negative for multiclass classification.
-    cutoff: float, optional
-        Cutoff for which the tested category will be accepted as a prediction. If the 
-        cutoff is not between 0 and 1, the entire confusion matrix will be drawn.
-
-    Returns
-    -------
-    tablesample
-        An object containing the result. For more information, see
-        utilities.tablesample.
-        """
-        check_types([("cutoff", cutoff, [int, float])])
-        pos_label = (
-            self.classes_[1]
-            if (pos_label == None and len(self.classes_) == 2)
-            else pos_label
-        )
-        if pos_label in self.classes_ and cutoff <= 1 and cutoff >= 0:
-            input_relation = self.deploySQL() + " WHERE predict_neighbors = '{}'".format(
-                pos_label
-            )
-            y_score = "(CASE WHEN proba_predict > {} THEN 1 ELSE 0 END)".format(cutoff)
-            y_true = "DECODE({}, '{}', 1, 0)".format(self.y, pos_label)
-            result = confusion_matrix(y_true, y_score, input_relation)
-            if pos_label == 1:
-                return result
-            else:
-                return tablesample(
-                    values={
-                        "index": ["Non-{}".format(pos_label), "{}".format(pos_label)],
-                        "Non-{}".format(pos_label): result.values[0],
-                        "{}".format(pos_label): result.values[1],
-                    },
-                )
-        else:
-            input_relation = "(SELECT *, ROW_NUMBER() OVER(PARTITION BY {}, row_id ORDER BY proba_predict DESC) AS pos FROM {}) neighbors_table WHERE pos = 1".format(
-                ", ".join(self.X), self.deploySQL()
-            )
-            return multilabel_confusion_matrix(
-                self.y, "predict_neighbors", input_relation, self.classes_
-            )
+    # I/O Methods.
 
-    # ---#
-    def lift_chart(
-        self, pos_label: Union[int, float, str] = None, ax=None, **style_kwds
-    ):
+    def deploySQL(
+        self,
+        X: Optional[SQLColumns] = None,
+        test_relation: Optional[str] = None,
+        predict: bool = False,
+        key_columns: Optional[SQLColumns] = None,
+    ) -> str:
         """
-    ---------------------------------------------------------------------------
-    Draws the model Lift Chart.
+        Returns the SQL code needed to deploy the model.
 
-    Parameters
-    ----------
-    pos_label: int/float/str
-        To draw a lift chart, one of the response column classes must be the 
-        positive one. The parameter 'pos_label' represents this class.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    tablesample
-        An object containing the result. For more information, see
-        utilities.tablesample.
-        """
-        pos_label = (
-            self.classes_[1]
-            if (pos_label == None and len(self.classes_) == 2)
-            else pos_label
-        )
-        if pos_label not in self.classes_:
-            raise ParameterError(
-                "'pos_label' must be one of the response column classes"
-            )
-        input_relation = self.deploySQL() + " WHERE predict_neighbors = '{}'".format(
-            pos_label
-        )
-        return lift_chart(
-            self.y, "proba_predict", input_relation, pos_label, ax=ax, **style_kwds,
-        )
+        Parameters
+        ----------
+        X: SQLColumns
+            List of the predictors.
+        test_relation: str, optional
+            Relation used to do the predictions.
+        predict: bool, optional
+            If set to True, returns the prediction instead
+            of the probability.
+        key_columns: SQLColumns, optional
+            A  list of columns to include in the  results,
+            but  to   exclude  from   computation  of  the
+            prediction.
+
+        Returns
+        -------
+        SQLExpression
+            the SQL code needed to deploy the model.
+        """
+        key_columns = format_type(key_columns, dtype=list)
+        X = format_type(X, dtype=list, na_out=self.X)
+        X = quote_ident(X)
+        if not test_relation:
+            test_relation = self.test_relation
+            if not key_columns:
+                key_columns = [self.y]
+        p = self.parameters["p"]
+        n_neighbors = self.parameters["n_neighbors"]
+        X_str = ", ".join([f"x.{x}" for x in X])
+        if key_columns:
+            key_columns_str = ", " + ", ".join(
+                ["x." + quote_ident(x) for x in key_columns]
+            )
+        else:
+            key_columns_str = ""
+        sql = [f"POWER(ABS(x.{X[i]} - y.{self.X[i]}), {p})" for i in range(len(self.X))]
+        sql = f"""
+            SELECT 
+                {X_str}{key_columns_str}, 
+                ROW_NUMBER() OVER(PARTITION BY 
+                                  {X_str}, row_id 
+                                  ORDER BY POWER({' + '.join(sql)}, 1 / {p})) 
+                                  AS ordered_distance, 
+                y.{self.y} AS predict_neighbors, 
+                row_id 
+            FROM 
+                (SELECT 
+                    *, 
+                    ROW_NUMBER() OVER() AS row_id 
+                 FROM {test_relation} 
+                 WHERE {" AND ".join([f"{x} IS NOT NULL" for x in X])}) x 
+                 CROSS JOIN 
+                (SELECT * FROM {self.input_relation} 
+                 WHERE {" AND ".join([f"{x} IS NOT NULL" for x in self.X])}) y"""
+
+        if key_columns:
+            key_columns_str = ", " + ", ".join(quote_ident(key_columns))
+
+        sql = f"""
+            (SELECT 
+                row_id, 
+                {", ".join(X)}{key_columns_str}, 
+                predict_neighbors, 
+                COUNT(*) / {n_neighbors} AS proba_predict 
+             FROM ({sql}) z 
+             WHERE ordered_distance <= {n_neighbors} 
+             GROUP BY {", ".join(X)}{key_columns_str}, 
+                      row_id, 
+                      predict_neighbors) kneighbors_table"""
+        if predict:
+            sql = f"""
+                (SELECT 
+                    {", ".join(X)}{key_columns_str}, 
+                    predict_neighbors 
+                 FROM 
+                    (SELECT 
+                        {", ".join(X)}{key_columns_str}, 
+                        predict_neighbors, 
+                        ROW_NUMBER() OVER (PARTITION BY {", ".join(X)} 
+                                           ORDER BY proba_predict DESC) 
+                                           AS order_prediction 
+                     FROM {sql}) VERTICAPY_SUBTABLE 
+                     WHERE order_prediction = 1) predict_neighbors_table"""
+        return clean_query(sql)
+
+    # Prediction / Transformation Methods.
 
-    # ---#
-    def prc_curve(
-        self, pos_label: Union[int, float, str] = None, ax=None, **style_kwds
-    ):
+    def _get_final_relation(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+    ) -> str:
+        """
+        Returns the final relation used to do the predictions.
         """
-    ---------------------------------------------------------------------------
-    Draws the model PRC curve.
+        return f"""
+            (SELECT 
+                * 
+            FROM {self.deploySQL()} 
+            WHERE predict_neighbors = '{pos_label}') 
+            final_centroids_relation"""
 
-    Parameters
-    ----------
-    pos_label: int/float/str
-        To draw the PRC curve, one of the response column classes must be the 
-        positive one. The parameter 'pos_label' represents this class.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    tablesample
-        An object containing the result. For more information, see
-        utilities.tablesample.
-        """
-        pos_label = (
-            self.classes_[1]
-            if (pos_label == None and len(self.classes_) == 2)
-            else pos_label
-        )
-        if pos_label not in self.classes_:
-            raise ParameterError(
-                "'pos_label' must be one of the response column classes"
-            )
-        input_relation = self.deploySQL() + " WHERE predict_neighbors = '{}'".format(
-            pos_label
-        )
-        return prc_curve(
-            self.y, "proba_predict", input_relation, pos_label, ax=ax, **style_kwds,
+    def _get_y_proba(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+    ) -> str:
+        """
+        Returns the input which represents the model's probabilities.
+        """
+        return "proba_predict"
+
+    def _get_y_score(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        cutoff: Optional[PythonNumber] = None,
+        allSQL: bool = False,
+    ) -> str:
+        """
+        Returns the input that represents the model's scoring.
+        """
+        cutoff = self._check_cutoff(cutoff=cutoff)
+        return f"(CASE WHEN proba_predict > {cutoff} THEN 1 ELSE 0 END)"
+
+    def _compute_accuracy(self) -> float:
+        """
+        Computes the model accuracy.
+        """
+        return mt.accuracy_score(
+            self.y, "predict_neighbors", self.deploySQL(predict=True)
         )
 
-    # ---#
-    def predict(
+    def _confusion_matrix(
+        self,
+        pos_label: Optional[PythonScalar] = None,
+        cutoff: Optional[PythonNumber] = None,
+    ) -> TableSample:
+        """
+        Computes the model confusion matrix.
+        """
+        if isinstance(pos_label, NoneType):
+            input_relation = f"""
+                (SELECT 
+                    *, 
+                    ROW_NUMBER() OVER(PARTITION BY {", ".join(self.X)}, row_id 
+                                      ORDER BY proba_predict DESC) AS pos 
+                 FROM {self.deploySQL()}) neighbors_table WHERE pos = 1"""
+            return mt.confusion_matrix(
+                self.y, "predict_neighbors", input_relation, classes=self.classes_
+            )
+        else:
+            cutoff = self._check_cutoff(cutoff=cutoff)
+            pos_label = self._check_pos_label(pos_label=pos_label)
+            input_relation = (
+                self.deploySQL() + f" WHERE predict_neighbors = '{pos_label}'"
+            )
+            y_score = f"(CASE WHEN proba_predict > {cutoff} THEN 1 ELSE 0 END)"
+            y_true = f"DECODE({self.y}, '{pos_label}', 1, 0)"
+            return mt.confusion_matrix(y_true, y_score, input_relation)
+
+    # Model Evaluation Methods.
+
+    def _predict(
         self,
-        vdf: Union[str, vDataFrame],
-        X: list = [],
-        name: str = "",
-        cutoff: float = 0.5,
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        cutoff: Optional[PythonNumber] = None,
         inplace: bool = True,
         **kwargs,
-    ):
+    ) -> vDataFrame:
         """
-    ---------------------------------------------------------------------------
-    Predicts using the input relation.
-
-    Parameters
-    ----------
-    vdf: str/vDataFrame
-        Object to use to run the prediction. You can also specify a customized 
-        relation, but you must enclose it with an alias. For example,  
-        "(SELECT 1) x" is correct, whereas "(SELECT 1)" and "SELECT 1" are 
-        incorrect.
-    X: list, optional
-        List of the columns used to deploy the models. If empty, the model
-        predictors will be used.
-    name: str, optional
-        Name of the added vcolumn. If empty, a name will be generated.
-    cutoff: float, optional
-        Cutoff for which the tested category will be accepted as a prediction.
-        This parameter is only used for binary classification.
-    inplace: bool, optional
-        If set to True, the prediction will be added to the vDataFrame.
-
-    Returns
-    -------
-    vDataFrame
-        the vDataFrame of the prediction
-        """
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [
-                ("name", name, [str]),
-                ("cutoff", cutoff, [int, float]),
-                ("X", X, [list]),
-                ("inplace", inplace, [bool]),
-                ("vdf", vdf, [str, vDataFrame]),
-            ],
-        )
-        assert 0 <= cutoff <= 1, ParameterError(
-            "Incorrect parameter 'cutoff'.\nThe cutoff "
-            "must be between 0 and 1, inclusive."
-        )
+        Predicts using the input relation.
+        """
+        X = format_type(X, dtype=list)
+        cutoff = self._check_cutoff(cutoff=cutoff)
         if isinstance(vdf, str):
-            vdf = vDataFrameSQL(relation=vdf)
-        X = [quote_ident(elem) for elem in X] if (X) else self.X
+            vdf = vDataFrame(vdf)
+        X = quote_ident(X) if (X) else self.X
         key_columns = vdf.get_columns(exclude_columns=X)
         if "key_columns" in kwargs:
             key_columns_arg = None
         else:
             key_columns_arg = key_columns
-        if not (name):
-            name = gen_name([self.type, self.name])
-
-        if (
-            len(self.classes_) == 2
-            and self.classes_[0] in [0, "0"]
-            and self.classes_[1] in [1, "1"]
-        ):
-            sql = (
-                "(SELECT {0}{1}, (CASE WHEN proba_predict > {2} THEN '{3}' ELSE '{4}' END)"
-                " AS {5} FROM {6} WHERE predict_neighbors = '{3}') VERTICAPY_SUBTABLE"
-            ).format(
-                ", ".join(X),
-                ", " + ", ".join(key_columns) if key_columns else "",
-                cutoff,
-                self.classes_[1],
-                self.classes_[0],
-                name,
-                self.deploySQL(
-                    X=X, test_relation=vdf.__genSQL__(), key_columns=key_columns_arg
-                ),
-            )
+        if key_columns:
+            key_columns_str = ", " + ", ".join(key_columns)
         else:
-            sql = "(SELECT {0}{1}, predict_neighbors AS {2} FROM {3}) VERTICAPY_SUBTABLE".format(
-                ", ".join(X),
-                ", " + ", ".join(key_columns) if key_columns else "",
-                name,
-                self.deploySQL(
-                    X=X,
-                    test_relation=vdf.__genSQL__(),
-                    key_columns=key_columns_arg,
-                    predict=True,
-                ),
-            )
+            key_columns_str = ""
+        if not name:
+            name = gen_name([self._model_type, self.model_name])
+
+        if self._is_binary_classifier():
+            table = self.deploySQL(
+                X=X, test_relation=vdf.current_relation(), key_columns=key_columns_arg
+            )
+            sql = f"""
+                (SELECT 
+                    {", ".join(X)}{key_columns_str}, 
+                    (CASE 
+                        WHEN proba_predict > {cutoff} 
+                            THEN '{self.classes_[1]}' 
+                        ELSE '{self.classes_[0]}' 
+                     END) AS {name} 
+                 FROM {table} 
+                 WHERE predict_neighbors = '{self.classes_[1]}') VERTICAPY_SUBTABLE"""
+        else:
+            table = self.deploySQL(
+                X=X,
+                test_relation=vdf.current_relation(),
+                key_columns=key_columns_arg,
+                predict=True,
+            )
+            sql = f"""
+                SELECT 
+                    {", ".join(X)}{key_columns_str}, 
+                    predict_neighbors AS {name} 
+                 FROM {table}"""
         if inplace:
-            return vDataFrameSQL(name="Neighbors", relation=sql, vdf=vdf)
+            vdf.__init__(sql)
+            return vdf
         else:
-            return vDataFrameSQL(name="Neighbors", relation=sql)
+            return vDataFrame(sql)
 
-    # ---#
-    def predict_proba(
+    def _predict_proba(
         self,
-        vdf: Union[str, vDataFrame],
-        X: list = [],
-        name: str = "",
-        pos_label: Union[int, str, float] = None,
+        vdf: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        name: Optional[str] = None,
+        pos_label: Optional[PythonScalar] = None,
         inplace: bool = True,
         **kwargs,
-    ):
+    ) -> vDataFrame:
         """
-    ---------------------------------------------------------------------------
-    Returns the model's probabilities using the input relation.
-
-    Parameters
-    ----------
-    vdf: str/vDataFrame
-        Object to use to run the prediction. You can also specify a customized 
-        relation, but you must enclose it with an alias. For example, "(SELECT 1) x" 
-        is correct, whereas "(SELECT 1)" and "SELECT 1" are incorrect.
-    X: list, optional
-        List of the columns used to deploy the models. If empty, the model
-        predictors will be used.
-    name: str, optional
-        Name of the additional prediction vColumn. If unspecified, a name is 
-	generated based on the model and class names.
-    pos_label: int/float/str, optional
-        Class label, the class for which the probability is calculated. 
-	If name is specified and pos_label is unspecified, the probability column 
-	names use the following format: name_class1, name_class2, etc.
-    inplace: bool, optional
-        If set to True, the prediction will be added to the vDataFrame.
-
-    Returns
-    -------
-    vDataFrame
-        the vDataFrame of the prediction
+        Returns the model's probabilities using the
+        input relation.
         """
         # Inititalization
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [
-                ("name", name, [str]),
-                ("X", X, [list]),
-                ("inplace", inplace, [bool]),
-                ("vdf", vdf, [str, vDataFrame]),
-                ("pos_label", pos_label, [int, float, str]),
-            ],
-        )
-        assert pos_label is None or pos_label in self.classes_, ParameterError(
+        X = format_type(X, dtype=list)
+        assert pos_label is None or pos_label in self.classes_, ValueError(
             (
                 "Incorrect parameter 'pos_label'.\nThe class label "
-                "must be in [{0}]. Found '{1}'."
-            ).format("|".join(["{}".format(c) for c in self.classes_]), pos_label)
+                f"must be in [{'|'.join([str(c) for c in self.classes_])}]. "
+                f"Found '{pos_label}'."
+            )
         )
         if isinstance(vdf, str):
-            vdf = vDataFrameSQL(relation=vdf)
-        X = [quote_ident(elem) for elem in X] if (X) else self.X
+            vdf = vDataFrame(vdf)
+        X = quote_ident(X) if (X) else self.X
         key_columns = vdf.get_columns(exclude_columns=X)
-        if not (name):
-            name = gen_name([self.type, self.name])
+        if not name:
+            name = gen_name([self._model_type, self.model_name])
         if "key_columns" in kwargs:
             key_columns_arg = None
         else:
             key_columns_arg = key_columns
 
         # Generating the probabilities
-        if pos_label == None:
+        if isinstance(pos_label, NoneType):
             predict = [
-                (
-                    "ZEROIFNULL(AVG(DECODE(predict_neighbors, '{0}', "
-                    "proba_predict, NULL))) AS {1}"
-                ).format(elem, gen_name([name, elem]))
-                for elem in self.classes_
+                f"""ZEROIFNULL(AVG(DECODE(predict_neighbors, 
+                                          '{c}', 
+                                          proba_predict, 
+                                          NULL))) AS {gen_name([name, c])}"""
+                for c in self.classes_
             ]
         else:
             predict = [
-                (
-                    "ZEROIFNULL(AVG(DECODE(predict_neighbors, '{0}', "
-                    "proba_predict, NULL))) AS {1}"
-                ).format(pos_label, name)
+                f"""ZEROIFNULL(AVG(DECODE(predict_neighbors, 
+                                          '{pos_label}', 
+                                          proba_predict, 
+                                          NULL))) AS {name}"""
             ]
-        sql = "(SELECT {0}{1}, {2} FROM {3} GROUP BY {4}) VERTICAPY_SUBTABLE".format(
-            ", ".join(X),
-            ", " + ", ".join(key_columns) if key_columns else "",
-            ", ".join(predict),
-            self.deploySQL(
-                X=X, test_relation=vdf.__genSQL__(), key_columns=key_columns_arg
-            ),
-            ", ".join(X + key_columns),
-        )
+        if key_columns:
+            key_columns_str = ", " + ", ".join(key_columns)
+        else:
+            key_columns_str = ""
+        table = self.deploySQL(
+            X=X, test_relation=vdf.current_relation(), key_columns=key_columns_arg
+        )
+        sql = f"""
+            SELECT 
+                {", ".join(X)}{key_columns_str}, 
+                {", ".join(predict)} 
+             FROM {table} 
+             GROUP BY {", ".join(X + key_columns)}"""
 
         # Result
         if inplace:
-            return vDataFrameSQL(name="Neighbors", relation=sql, vdf=vdf)
+            vdf.__init__(sql)
+            return vdf
         else:
-            return vDataFrameSQL(name="Neighbors", relation=sql)
+            return vDataFrame(sql)
 
-    # ---#
-    def roc_curve(
-        self, pos_label: Union[int, float, str] = None, ax=None, **style_kwds
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Draws the model ROC curve.
+    # Plotting Methods.
 
-    Parameters
-    ----------
-    pos_label: int/float/str
-        To draw the ROC curve, one of the response column classes must be the 
-        positive one. The parameter 'pos_label' represents this class.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    tablesample
-        An object containing the result. For more information, see
-        utilities.tablesample.
-        """
-        pos_label = (
-            self.classes_[1]
-            if (pos_label == None and len(self.classes_) == 2)
-            else pos_label
-        )
-        if pos_label not in self.classes_:
-            raise ParameterError(
-                "'pos_label' must be one of the response column classes"
+    def _get_plot_args(
+        self, pos_label: Optional[PythonScalar] = None, method: Optional[str] = None
+    ) -> list:
+        """
+        Returns the args used by plotting methods.
+        """
+        pos_label = self._check_pos_label(pos_label)
+        if method == "contour":
+            sql = (
+                f"""
+                SELECT
+                    {', '.join(self.X)},
+                    ZEROIFNULL(AVG(DECODE(predict_neighbors, 
+                                          '{pos_label}', 
+                                          proba_predict, 
+                                          NULL))) AS {{0}}
+                FROM """
+                + self.deploySQL(X=self.X, test_relation="{1}")
+                + f" GROUP BY {', '.join(self.X)}"
+            )
+            args = [self.X, sql]
+        else:
+            input_relation = (
+                self.deploySQL() + f" WHERE predict_neighbors = '{pos_label}'"
             )
-        input_relation = self.deploySQL() + " WHERE predict_neighbors = '{}'".format(
-            pos_label
-        )
-        return roc_curve(
-            self.y, "proba_predict", input_relation, pos_label, ax=ax, **style_kwds,
-        )
+            args = [self.y, "proba_predict", input_relation, pos_label]
+        return args
 
-    # ---#
-    def score(
+    def _get_plot_kwargs(
         self,
-        method: str = "accuracy",
-        pos_label: Union[int, float, str] = None,
-        cutoff: float = -1,
-        nbins: int = 10000,
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Computes the model score.
+        pos_label: Optional[PythonScalar] = None,
+        nbins: int = 30,
+        chart: Optional[PlottingObject] = None,
+        method: Optional[str] = None,
+    ) -> dict:
+        """
+        Returns the kwargs used by plotting methods.
+        """
+        pos_label = self._check_pos_label(pos_label)
+        res = {"nbins": nbins, "chart": chart}
+        if method == "contour":
+            res["func_name"] = f"p({self.y} = '{pos_label}')"
+        elif method == "cutoff":
+            res["cutoff_curve"] = True
+        return res
+
+
+"""
+Algorithms used for density analysis.
+"""
+
+
+class KernelDensity(Regressor, Tree):
+    """
+    [Beta Version]
+    Creates a KernelDensity object.
+    This object uses pure SQL to compute the final score.
 
     Parameters
     ----------
-    pos_label: int/float/str, optional
-        Label to consider as positive. All the other classes will be merged and
-        considered as negative for multiclass classification.
-    cutoff: float, optional
-        Cutoff for which the tested category will be accepted as a prediction. 
-    method: str, optional
-        The method to use to compute the score.
-            accuracy    : Accuracy
-            auc         : Area Under the Curve (ROC)
-            best_cutoff : Cutoff which optimised the ROC Curve prediction.
-            bm          : Informedness = tpr + tnr - 1
-            csi         : Critical Success Index = tp / (tp + fn + fp)
-            f1          : F1 Score 
-            logloss     : Log Loss
-            mcc         : Matthews Correlation Coefficient 
-            mk          : Markedness = ppv + npv - 1
-            npv         : Negative Predictive Value = tn / (tn + fn)
-            prc_auc     : Area Under the Curve (PRC)
-            precision   : Precision = tp / (tp + fp)
-            recall      : Recall = tp / (tp + fn)
-            specificity : Specificity = tn / (tn + fp)
+    name: str
+        Name of the model. This is not a built-in model, so
+        this name is used  to build the final table.
+    bandwidth: PythonNumber, optional
+        The bandwidth of the kernel.
+    kernel: str, optional
+        The kernel used during the learning phase.
+            gaussian  : Gaussian Kernel.
+            logistic  : Logistic Kernel.
+            sigmoid   : Sigmoid Kernel.
+            silverman : Silverman Kernel.
+    p: int, optional
+        The p of the p-distances (distance metric used
+        during the model computation).
+    max_leaf_nodes: PythonNumber, optional
+        The maximum number of leaf nodes,  an integer between
+        1 and 1e9, inclusive.
+    max_depth: int, optional
+        The maximum tree depth,  an integer between 1 and 100,
+        inclusive.
+    min_samples_leaf: int, optional
+        The  minimum number of  samples each branch must  have
+        after splitting a node,  an integer between 1 and 1e6,
+        inclusive. A split that results in fewer remaining
+        samples is discarded.
     nbins: int, optional
-        [Only when method is set to auc|prc_auc|best_cutoff] 
-        An integer value that determines the number of decision boundaries. 
-        Decision boundaries are set at equally spaced intervals between 0 and 1, 
-        inclusive. Greater values for nbins give more precise estimations of the AUC, 
-        but can potentially decrease performance. The maximum value is 999,999. 
-        If negative, the maximum value is used.
-
-    Returns
-    -------
-    float
-        score
-        """
-        check_types(
-            [
-                ("cutoff", cutoff, [int, float]),
-                ("method", method, [str]),
-                ("nbins", nbins, [int]),
-            ]
-        )
-        if pos_label == None and len(self.classes_) == 2:
-            pos_label = self.classes_[1]
-        input_relation = "(SELECT * FROM {0} WHERE predict_neighbors = '{1}') final_centroids_relation".format(
-            self.deploySQL(), pos_label
-        )
-        y_score = "(CASE WHEN proba_predict > {} THEN 1 ELSE 0 END)".format(cutoff)
-        y_proba = "proba_predict"
-        y_true = "DECODE({}, '{}', 1, 0)".format(self.y, pos_label)
-        if (pos_label not in self.classes_) and (method != "accuracy"):
-            raise ParameterError(
-                "'pos_label' must be one of the response column classes"
-            )
-        elif (cutoff >= 1 or cutoff <= 0) and (method != "accuracy"):
-            cutoff = self.score(pos_label=pos_label, cutoff=0.5, method="best_cutoff")
-        if method in ("accuracy", "acc"):
-            if pos_label not in self.classes_:
-                return accuracy_score(
-                    self.y,
-                    "predict_neighbors",
-                    self.deploySQL(predict=True),
-                    pos_label=None,
-                )
-            else:
-                return accuracy_score(
-                    y_true, y_score, input_relation, pos_label=pos_label
-                )
-        elif method == "auc":
-            return auc(y_true, y_proba, input_relation, nbins=nbins)
-        elif method == "prc_auc":
-            return prc_auc(y_true, y_proba, input_relation, nbins=nbins)
-        elif method in ("best_cutoff", "best_threshold"):
-            return roc_curve(
-                y_true, y_proba, input_relation, best_threshold=True, nbins=nbins
-            )
-        elif method in ("recall", "tpr"):
-            return recall_score(y_true, y_score, input_relation)
-        elif method in ("precision", "ppv"):
-            return precision_score(y_true, y_score, input_relation)
-        elif method in ("specificity", "tnr"):
-            return specificity_score(y_true, y_score, input_relation)
-        elif method in ("negative_predictive_value", "npv"):
-            return precision_score(y_true, y_score, input_relation)
-        elif method in ("log_loss", "logloss"):
-            return log_loss(y_true, y_proba, input_relation)
-        elif method == "f1":
-            return f1_score(y_true, y_score, input_relation)
-        elif method == "mcc":
-            return matthews_corrcoef(y_true, y_score, input_relation)
-        elif method in ("bm", "informedness"):
-            return informedness(y_true, y_score, input_relation)
-        elif method in ("mk", "markedness"):
-            return markedness(y_true, y_score, input_relation)
-        elif method in ("csi", "critical_success_index"):
-            return critical_success_index(y_true, y_score, input_relation)
-        else:
-            raise ParameterError(
-                "The parameter 'method' must be in accuracy|auc|prc_auc|best_cutoff|"
-                "recall|precision|log_loss|negative_predictive_value|specificity|"
-                "mcc|informedness|markedness|critical_success_index"
-            )
+        The  number  of  bins used to discretize  the  input
+        features.
+    xlim: list, optional
+        List of tuples used to compute the kernel window.
+    """
 
+    # Properties.
 
-# ---#
-class KernelDensity(Regressor, Tree):
-    """
----------------------------------------------------------------------------
-[Beta Version]
-Creates a KernelDensity object. 
-This object uses pure SQL to compute the final score.
-
-Parameters
-----------
-bandwidth: float, optional
-    The bandwidth of the kernel.
-kernel: str, optional
-    The kernel used during the learning phase.
-        gaussian  : Gaussian Kernel.
-        logistic  : Logistic Kernel.
-        sigmoid   : Sigmoid Kernel.
-        silverman : Silverman Kernel.
-p: int, optional
-    The p corresponding to the one of the p-distances (distance metric used during 
-    the model computation).
-max_leaf_nodes: int, optional
-    The maximum number of leaf nodes, an integer between 1 and 1e9, inclusive.
-max_depth: int, optional
-    The maximum tree depth, an integer between 1 and 100, inclusive.
-min_samples_leaf: int, optional
-    The minimum number of samples each branch must have after splitting a node, an 
-    integer between 1 and 1e6, inclusive. A split that causes fewer remaining samples 
-    is discarded.
-nbins: int, optional 
-    The number of bins to use to discretize the input features.
-xlim: list, optional
-    List of tuples use to compute the kernel window.
-    """
+    @property
+    def _is_native(self) -> Literal[False]:
+        return False
+
+    @property
+    def _is_using_native(self) -> Literal[True]:
+        return True
+
+    @property
+    def _vertica_fit_sql(self) -> Literal["RF_REGRESSOR"]:
+        return "RF_REGRESSOR"
+
+    @property
+    def _vertica_predict_sql(self) -> Literal["PREDICT_RF_REGRESSOR"]:
+        return "PREDICT_RF_REGRESSOR"
+
+    @property
+    def _model_category(self) -> Literal["UNSUPERVISED"]:
+        return "UNSUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["PREPROCESSING"]:
+        return "PREPROCESSING"
+
+    @property
+    def _model_type(self) -> Literal["KernelDensity"]:
+        return "KernelDensity"
 
+    # System & Special Methods.
+
+    @save_verticapy_logs
     def __init__(
         self,
         name: str,
-        bandwidth: float = 1,
-        kernel: str = "gaussian",
+        bandwidth: PythonNumber = 1.0,
+        kernel: Literal["gaussian", "logistic", "sigmoid", "silverman"] = "gaussian",
         p: int = 2,
-        max_leaf_nodes: int = 1e9,
+        max_leaf_nodes: PythonNumber = 1e9,
         max_depth: int = 5,
         min_samples_leaf: int = 1,
         nbins: int = 5,
-        xlim: list = [],
+        xlim: Optional[list] = None,
         **kwargs,
-    ):
-        check_types(
-            [
-                ("name", name, [str], False),
-                ("bandwidth", bandwidth, [int, float], False),
-                ("kernel", kernel, ["gaussian", "logistic", "sigmoid", "silverman"]),
-                ("max_leaf_nodes", max_leaf_nodes, [int, float], False),
-                ("max_depth", max_depth, [int, float], False),
-                ("min_samples_leaf", min_samples_leaf, [int, float], False),
-                ("nbins", nbins, [int, float], False),
-                ("xlim", xlim, [list], False),
-            ]
-        )
-        self.type, self.name = "KernelDensity", name
-        self.set_params(
-            {
-                "nbins": nbins,
-                "p": p,
-                "bandwidth": bandwidth,
-                "kernel": kernel,
-                "max_leaf_nodes": int(max_leaf_nodes),
-                "max_depth": int(max_depth),
-                "min_samples_leaf": int(min_samples_leaf),
-                "xlim": xlim,
-            }
-        )
-        if "store" not in kwargs or kwargs["store"]:
-            self.verticapy_store = True
-        else:
-            self.verticapy_store = False
+    ) -> None:
+        super().__init__()
+        self.model_name = name
+        self.parameters = {
+            "nbins": nbins,
+            "p": p,
+            "bandwidth": bandwidth,
+            "kernel": str(kernel).lower(),
+            "max_leaf_nodes": int(max_leaf_nodes),
+            "max_depth": int(max_depth),
+            "min_samples_leaf": int(min_samples_leaf),
+            "xlim": format_type(xlim, dtype=list),
+        }
+        self._verticapy_store = "store" not in kwargs or kwargs["store"]
+        self.verticapy_x = None
+        self.verticapy_y = None
 
-    # ---#
-    def fit(self, input_relation: Union[str, vDataFrame], X: list = []):
+    def drop(self) -> bool:
         """
-    ---------------------------------------------------------------------------
-    Trains the model.
+        Drops the model from the Vertica database.
+        """
+        try:
+            table_name = self.model_name.replace('"', "") + "_KernelDensity_Map"
+            _executeSQL(
+                query=f"SELECT KDE FROM {table_name} LIMIT 0;",
+                title="Looking if the KDE table exists.",
+            )
+            drop(table_name, method="table")
+        except QueryError:
+            return False
+        return drop(self.model_name, method="model")
+
+    # Attributes Methods.
+
+    def _density_kde(
+        self, vdf: vDataFrame, columns: SQLColumns, kernel: str, x, p: int, h=None
+    ) -> str:
+        """
+        Returns the result of the KDE.
+        """
+        for col in columns:
+            if not vdf[col].isnum():
+                raise TypeError(
+                    f"Cannot compute KDE for non-numerical columns. {col} is not numerical."
+                )
+        if kernel == "gaussian":
+            fkernel = "EXP(-1 / 2 * POWER({0}, 2)) / SQRT(2 * PI())"
 
-    Parameters
-    ----------
-    input_relation: str/vDataFrame
-        Training relation.
-    X: list, optional
-        List of the predictors.
-
-    Returns
-    -------
-    object
-        self
-        """
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [("input_relation", input_relation, [str, vDataFrame]), ("X", X, [list])]
-        )
-        if verticapy.options["overwrite_model"]:
+        elif kernel == "logistic":
+            fkernel = "1 / (2 + EXP({0}) + EXP(-{0}))"
+
+        elif kernel == "sigmoid":
+            fkernel = "2 / (PI() * (EXP({0}) + EXP(-{0})))"
+
+        elif kernel == "silverman":
+            fkernel = (
+                "EXP(-1 / SQRT(2) * ABS({0})) / 2 * SIN(ABS({0}) / SQRT(2) + PI() / 4)"
+            )
+
+        else:
+            raise ValueError(
+                "The parameter 'kernel' must be in [gaussian|logistic|sigmoid|silverman]."
+            )
+        if isinstance(x, (tuple)):
+            return self._density_kde(vdf, columns, kernel, [x], p, h)[0]
+        elif isinstance(x, (list)):
+            N = vdf.shape()[0]
+            L = []
+            for xj in x:
+                distance = []
+                for i in range(len(columns)):
+                    distance += [f"POWER({columns[i]} - {xj[i]}, {p})"]
+                distance = " + ".join(distance)
+                distance = f"POWER({distance}, {1.0 / p})"
+                fkernel_tmp = fkernel.format(f"{distance} / {h}")
+                L += [f"SUM({fkernel_tmp}) / ({h} * {N})"]
+            query = f"""
+                SELECT 
+                    /*+LABEL('learn.neighbors.KernelDensity.fit')*/ 
+                    {", ".join(L)} 
+                FROM {vdf}"""
+            result = _executeSQL(
+                query=query, title="Computing the KDE", method="fetchrow"
+            )
+            return list(result)
+        else:
+            return 0
+
+    def _density_compute(
+        self,
+        vdf: vDataFrame,
+        columns: SQLColumns,
+        h=None,
+        kernel: str = "gaussian",
+        nbins: int = 5,
+        p: int = 2,
+    ) -> list:
+        """
+        Returns the result of the KDE for all the data points.
+        """
+        columns = vdf.format_colnames(columns)
+        x_vars = []
+        y = []
+        for idx, column in enumerate(columns):
+            if self.parameters["xlim"]:
+                try:
+                    x_min, x_max = self.parameters["xlim"][idx]
+                except:
+                    warning_message = (
+                        f"Wrong xlim for the vDataColumn {column}.\n"
+                        "The max and the min will be used instead."
+                    )
+                    warnings.warn(warning_message, Warning)
+                    x_min, x_max = vdf.agg(
+                        func=["min", "max"], columns=[column]
+                    ).transpose()[column]
+            else:
+                x_min, x_max = vdf.agg(
+                    func=["min", "max"], columns=[column]
+                ).transpose()[column]
+            x_vars += [
+                [(x_max - x_min) * i / nbins + x_min for i in range(0, nbins + 1)]
+            ]
+        x = list(itertools.product(*x_vars))
+        try:
+            y = self._density_kde(vdf, columns, kernel, x, p, h)
+        except:
+            for xi in x:
+                K = self._density_kde(vdf, columns, kernel, xi, p, h)
+                y += [K]
+        return [x, y]
+
+    # Model Fitting Method.
+
+    def fit(self, input_relation: SQLRelation, X: Optional[SQLColumns] = None) -> None:
+        """
+        Trains the model.
+
+        Parameters
+        ----------
+        input_relation: SQLRelation
+            Training relation.
+        X: list, optional
+            List of the predictors.
+        """
+        X = format_type(X, dtype=list)
+        X = quote_ident(X)
+        if conf.get_option("overwrite_model"):
             self.drop()
         else:
-            does_model_exist(name=self.name, raise_error=True)
+            self._is_already_stored(raise_error=True)
         if isinstance(input_relation, vDataFrame):
-            if not (X):
+            if not X:
                 X = input_relation.numcol()
             vdf = input_relation
-            input_relation = input_relation.__genSQL__()
+            input_relation = input_relation.current_relation()
         else:
-            try:
-                vdf = vDataFrame(input_relation)
-            except:
-                vdf = vDataFrameSQL(input_relation)
-            if not (X):
+            vdf = vDataFrame(input_relation)
+            if not X:
                 X = vdf.numcol()
-        vdf.are_namecols_in(X)
         X = vdf.format_colnames(X)
-
-        # ---#
-        def density_compute(
-            vdf: vDataFrame,
-            columns: list,
-            h=None,
-            kernel: str = "gaussian",
-            nbins: int = 5,
-            p: int = 2,
-        ):
-            # ---#
-            def density_kde(vdf, columns: list, kernel: str, x, p: int, h=None):
-                for elem in columns:
-                    if not (vdf[elem].isnum()):
-                        raise TypeError(
-                            f"Cannot compute KDE for non-numerical columns. {elem} is not numerical."
-                        )
-                if kernel == "gaussian":
-                    fkernel = "EXP(-1 / 2 * POWER({0}, 2)) / SQRT(2 * PI())"
-
-                elif kernel == "logistic":
-                    fkernel = "1 / (2 + EXP({0}) + EXP(-{0}))"
-
-                elif kernel == "sigmoid":
-                    fkernel = "2 / (PI() * (EXP({0}) + EXP(-{0})))"
-
-                elif kernel == "silverman":
-                    fkernel = "EXP(-1 / SQRT(2) * ABS({0})) / 2 * SIN(ABS({0}) / SQRT(2) + PI() / 4)"
-
-                else:
-                    raise ParameterError(
-                        "The parameter 'kernel' must be in [gaussian|logistic|sigmoid|silverman]."
-                    )
-                if isinstance(x, (tuple)):
-                    return density_kde(vdf, columns, kernel, [x], p, h)[0]
-                elif isinstance(x, (list)):
-                    N = vdf.shape()[0]
-                    L = []
-                    for elem in x:
-                        distance = []
-                        for i in range(len(columns)):
-                            distance += [
-                                "POWER({0} - {1}, {2})".format(columns[i], elem[i], p)
-                            ]
-                        distance = " + ".join(distance)
-                        distance = "POWER({0}, {1})".format(distance, 1.0 / p)
-                        fkernel_tmp = fkernel.format(f"{distance} / {h}")
-                        L += [f"SUM({fkernel_tmp}) / ({h} * {N})"]
-                    query = "SELECT {0} FROM {1}".format(", ".join(L), vdf.__genSQL__())
-                    result = executeSQL(
-                        query, title="Computing the KDE", method="fetchrow"
-                    )
-                    return [elem for elem in result]
-                else:
-                    return 0
-
-            vdf.are_namecols_in(columns)
-            columns = vdf.format_colnames(columns)
-            x_vars = []
-            y = []
-            for idx, column in enumerate(columns):
-                if self.parameters["xlim"]:
-                    try:
-                        x_min, x_max = self.parameters["xlim"][idx]
-                        N = vdf[column].count()
-                    except:
-                        warning_message = (
-                            f"Wrong xlim for the vcolumn {column}.\n"
-                            "The max and the min will be used instead."
-                        )
-                        warnings.warn(warning_message, Warning)
-                        x_min, x_max, N = vdf.agg(
-                            func=["min", "max", "count"], columns=[column]
-                        ).transpose()[column]
-                else:
-                    x_min, x_max, N = vdf.agg(
-                        func=["min", "max", "count"], columns=[column]
-                    ).transpose()[column]
-                x_vars += [
-                    [(x_max - x_min) * i / nbins + x_min for i in range(0, nbins + 1)]
-                ]
-            import itertools
-
-            x = list(itertools.product(*x_vars))
-            try:
-                y = density_kde(vdf, columns, kernel, x, p, h)
-            except:
-                for xi in x:
-                    K = density_kde(vdf, columns, kernel, xi, p, h)
-                    y += [K]
-            return [x, y]
-
-        x, y = density_compute(
+        x, y = self._density_compute(
             vdf,
             X,
             self.parameters["bandwidth"],
             self.parameters["kernel"],
             self.parameters["nbins"],
             self.parameters["p"],
         )
-        if self.verticapy_store:
-            query = """CREATE TABLE {0}_KernelDensity_Map AS    
-                            SELECT 
-                                {1}, 0.0::float AS KDE 
-                            FROM {2} 
-                            LIMIT 0""".format(
-                self.name.replace('"', ""), ", ".join(X), vdf.__genSQL__()
+        table_name = self.model_name.replace('"', "") + "_KernelDensity_Map"
+        if self._verticapy_store:
+            _executeSQL(
+                query=f"""
+                    CREATE TABLE {table_name} AS    
+                        SELECT 
+                            /*+LABEL('learn.neighbors.KernelDensity.fit')*/
+                            {", ".join(X)}, 0.0::float AS KDE 
+                        FROM {vdf} 
+                        LIMIT 0""",
+                print_time_sql=False,
             )
-            executeSQL(query, print_time_sql=False)
             r, idx = 0, 0
             while r < len(y):
                 values = []
                 m = min(r + 100, len(y))
                 for i in range(r, m):
                     values += ["SELECT " + str(x[i] + (y[i],))[1:-1]]
-                query = "INSERT INTO {0}_KernelDensity_Map ({1}, KDE) {2}".format(
-                    self.name.replace('"', ""), ", ".join(X), " UNION ".join(values)
+                _executeSQL(
+                    query=f"""
+                    INSERT /*+LABEL('learn.neighbors.KernelDensity.fit')*/ 
+                    INTO {table_name}
+                    ({", ".join(X)}, KDE) {" UNION ".join(values)}""",
+                    title=f"Computing the KDE [Step {idx}].",
                 )
-                executeSQL(query, f"Computing the KDE [Step {idx}].")
-                executeSQL("COMMIT;", print_time_sql=False)
+                _executeSQL("COMMIT;", print_time_sql=False)
                 r += 100
                 idx += 1
             self.X, self.input_relation = X, input_relation
-            self.map = "{0}_KernelDensity_Map".format(self.name.replace('"', ""))
-            self.tree_name = "{0}_KernelDensity_Tree".format(self.name.replace('"', ""))
+            self.map = table_name
             self.y = "KDE"
-
-            from verticapy.learn.tree import DecisionTreeRegressor
-
             model = DecisionTreeRegressor(
-                name=self.tree_name,
+                name=self.model_name,
                 max_features=len(self.X),
                 max_leaf_nodes=self.parameters["max_leaf_nodes"],
                 max_depth=self.parameters["max_depth"],
                 min_samples_leaf=self.parameters["min_samples_leaf"],
                 nbins=1000,
             )
             model.fit(self.map, self.X, "KDE")
-            model_save = {
-                "type": "KernelDensity",
-                "input_relation": self.input_relation,
-                "X": self.X,
-                "map": self.map,
-                "tree_name": self.tree_name,
-                "bandwidth": self.parameters["bandwidth"],
-                "kernel": self.parameters["kernel"],
-                "p": self.parameters["p"],
-                "max_leaf_nodes": self.parameters["max_leaf_nodes"],
-                "max_depth": self.parameters["max_depth"],
-                "min_samples_leaf": self.parameters["min_samples_leaf"],
-                "nbins": self.parameters["nbins"],
-                "xlim": self.parameters["xlim"],
-            }
-            insert_verticapy_schema(
-                model_name=self.name, model_type="KernelDensity", model_save=model_save
-            )
         else:
             self.X, self.input_relation = X, input_relation
             self.verticapy_x = x
             self.verticapy_y = y
-        return self
 
-    # ---#
-    def plot(self, ax=None, **style_kwds):
-        """
-    ---------------------------------------------------------------------------
-    Draws the Model.
+    # Plotting Methods.
 
-    Parameters
-    ----------
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    ax
-        Matplotlib axes object
-        """
+    def _compute_plot_params(self) -> tuple[dict, dict]:
         if len(self.X) == 1:
-            if self.verticapy_store:
-                query = "SELECT {}, KDE FROM {} ORDER BY 1".format(self.X[0], self.map)
-                result = executeSQL(query, method="fetchall", print_time_sql=False)
-                x, y = [elem[0] for elem in result], [elem[1] for elem in result]
+            if self._verticapy_store:
+                query = f"""
+                    SELECT 
+                        /*+LABEL('learn.neighbors.KernelDensity.plot')*/ 
+                        {self.X[0]}, KDE 
+                    FROM {self.map} ORDER BY 1"""
+                result = _executeSQL(query, method="fetchall", print_time_sql=False)
+                x, y = [v[0] for v in result], [v[1] for v in result]
             else:
-                x, y = [elem[0] for elem in self.verticapy_x], self.verticapy_y
-            if not (ax):
-                fig, ax = plt.subplots()
-                if isnotebook():
-                    fig.set_size_inches(7, 5)
-                ax.grid()
-                ax.set_axisbelow(True)
-            from verticapy.plot import gen_colors
-
-            param = {
-                "color": gen_colors()[0],
+                x, y = [v[0] for v in self.verticapy_x], self.verticapy_y
+            data = {
+                "x": np.array(x).astype(float),
+                "y": np.array(y).astype(float),
+            }
+            layout = {
+                "x_label": self.X[0],
+                "y_label": "density",
             }
-            ax.plot(x, y, **updated_dict(param, style_kwds))
-            ax.fill_between(
-                x, y, facecolor=updated_dict(param, style_kwds)["color"], alpha=0.7
-            )
-            ax.set_xlim(min(x), max(x))
-            ax.set_ylim(bottom=0)
-            ax.set_ylabel("density")
-            return ax
         elif len(self.X) == 2:
             n = self.parameters["nbins"]
-            if self.verticapy_store:
-                query = "SELECT {}, {}, KDE FROM {} ORDER BY 1, 2".format(
-                    self.X[0], self.X[1], self.map,
-                )
-                result = executeSQL(query, method="fetchall", print_time_sql=False)
+            if self._verticapy_store:
+                query = f"""
+                    SELECT 
+                        /*+LABEL('learn.neighbors.KernelDensity.plot')*/ 
+                        {self.X[0]}, 
+                        {self.X[1]}, 
+                        KDE 
+                    FROM {self.map} 
+                    ORDER BY 1, 2"""
+                result = _executeSQL(query, method="fetchall", print_time_sql=False)
                 x, y, z = (
-                    [elem[0] for elem in result],
-                    [elem[1] for elem in result],
-                    [elem[2] for elem in result],
+                    [v[0] for v in result],
+                    [v[1] for v in result],
+                    [v[2] for v in result],
                 )
             else:
                 x, y, z = (
-                    [elem[0] for elem in self.verticapy_x],
-                    [elem[1] for elem in self.verticapy_x],
+                    [v[0] for v in self.verticapy_x],
+                    [v[1] for v in self.verticapy_x],
                     self.verticapy_y,
                 )
-            result, idx = [], 0
+            X, idx = [], 0
             while idx < (n + 1) * (n + 1):
-                result += [[z[idx + i] for i in range(n + 1)]]
+                X += [[z[idx + i] for i in range(n + 1)]]
                 idx += n + 1
-            if not (ax):
-                fig, ax = plt.subplots()
-                if isnotebook():
-                    fig.set_size_inches(8, 6)
-            else:
-                fig = plt
-            param = {
-                "cmap": "Reds",
-                "origin": "lower",
-                "interpolation": "bilinear",
+            extent = [
+                float(np.nanmin(x)),
+                float(np.nanmax(x)),
+                float(np.nanmin(y)),
+                float(np.nanmax(y)),
+            ]
+            data = {
+                "X": np.array(X).astype(float),
+            }
+            layout = {
+                "x_label": self.X[0],
+                "y_label": self.X[1],
+                "extent": extent,
             }
-            extent = [min(x), max(x), min(y), max(y)]
-            extent = [float(elem) for elem in extent]
-            im = ax.imshow(result, extent=extent, **updated_dict(param, style_kwds))
-            fig.colorbar(im, ax=ax)
-            ax.set_ylabel(self.X[1])
-            ax.set_xlabel(self.X[0])
-            return ax
         else:
-            raise Exception("KDE Plots are only available in 1D or 2D.")
+            raise AttributeError("KDE Plots are only available in 1D or 2D.")
+        return data, layout
 
+    def plot(
+        self, chart: Optional[PlottingObject] = None, **style_kwargs
+    ) -> PlottingObject:
+        """
+        Draws the Model.
+
+        Parameters
+        ----------
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        data, layout = self._compute_plot_params()
+        if len(self.X) == 1:
+            vpy_plt, kwargs = PlottingUtils().get_plotting_lib(
+                class_name="DensityPlot",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            fun = vpy_plt.DensityPlot
+        elif len(self.X) == 2:
+            vpy_plt, kwargs = PlottingUtils().get_plotting_lib(
+                class_name="DensityPlot2D",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            fun = vpy_plt.DensityPlot2D
+        else:
+            raise AttributeError("KDE Plots are only available in 1D or 2D.")
+        return fun(data=data, layout=layout).draw(**kwargs)
 
-# ---#
-class KNeighborsRegressor(Regressor):
-    """
----------------------------------------------------------------------------
-[Beta Version]
-Creates a KNeighborsRegressor object using the k-nearest neighbors 
-algorithm. This object uses pure SQL to compute all the distances and 
-final score.
-
-\u26A0 Warning : This algorithm uses a CROSS JOIN during computation and
-                 is therefore computationally expensive at O(n * n), where
-                 n is the total number of elements. Since KNeighborsRegressor 
-                 uses the p-distance, it is highly sensitive to unnormalized 
-                 data.
-
-Parameters
-----------
-n_neighbors: int, optional
-	Number of neighbors to consider when computing the score.
-p: int, optional
-	The p corresponding to the one of the p-distances (distance metric used during 
-	the model computation).
-	"""
-
-    def __init__(self, name: str, n_neighbors: int = 5, p: int = 2):
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "KNeighborsRegressor", name
-        self.set_params({"n_neighbors": n_neighbors, "p": p})
 
-    # ---#
-    def deploySQL(self, X: list = [], test_relation: str = "", key_columns: list = []):
-        """
-    ---------------------------------------------------------------------------
-    Returns the SQL code needed to deploy the model. 
+"""
+Algorithms used for anomaly detection.
+"""
+
+
+class LocalOutlierFactor(VerticaModel):
+    """
+    [Beta Version]
+    Creates a LocalOutlierFactor object by using the
+    Local Outlier Factor algorithm as defined by Markus
+    M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jrg
+    Sander. This object is using pure SQL to compute all
+    the distances and final score.
+
+    \u26A0 Warning : This   algorithm   uses  a   CROSS  JOIN
+                     during   computation  and  is  therefore
+                     computationally  expensive at  O(n * n),
+                     where n is the total number of elements.
+                     Since  LocalOutlierFactor   uses  the p-
+                     distance,  it  is  highly  sensitive  to
+                     unnormalized data.
+                     A  table  is created at the  end of
+                     the learning phase.
 
     Parameters
     ----------
-    X: list
-        List of the predictors.
-    test_relation: str, optional
-        Relation to use to do the predictions.
-    key_columns: list, optional
-        A list of columns to include in the results, but to exclude from 
-        computation of the prediction.
-
-    Returns
-    -------
-    str/list
-        the SQL code needed to deploy the model.
-        """
-        if isinstance(X, str):
-            X = [X]
-        if isinstance(key_columns, str):
-            key_columns = [key_columns]
-        check_types(
-            [
-                ("test_relation", test_relation, [str], False),
-                ("X", X, [list], False),
-                ("key_columns", key_columns, [list], False),
-            ],
-        )
-        X = [quote_ident(elem) for elem in X] if (X) else self.X
-        if not (test_relation):
-            test_relation = self.test_relation
-        if not (key_columns) and key_columns != None:
-            key_columns = [self.y]
-        sql = [
-            "POWER(ABS(x.{} - y.{}), {})".format(X[i], self.X[i], self.parameters["p"])
-            for i in range(len(self.X))
-        ]
-        sql = "POWER({}, 1 / {})".format(" + ".join(sql), self.parameters["p"])
-        sql = "ROW_NUMBER() OVER(PARTITION BY {}, row_id ORDER BY {})".format(
-            ", ".join(["x.{}".format(item) for item in X]), sql
-        )
-        sql = "SELECT {}{}, {} AS ordered_distance, y.{} AS predict_neighbors, row_id FROM (SELECT *, ROW_NUMBER() OVER() AS row_id FROM {} WHERE {}) x CROSS JOIN (SELECT * FROM {} WHERE {}) y".format(
-            ", ".join(["x.{}".format(item) for item in X]),
-            ", " + ", ".join(["x." + quote_ident(elem) for elem in key_columns])
-            if (key_columns)
-            else "",
-            sql,
-            self.y,
-            test_relation,
-            " AND ".join(["{} IS NOT NULL".format(item) for item in X]),
-            self.input_relation,
-            " AND ".join(["{} IS NOT NULL".format(item) for item in self.X]),
-        )
-        sql = "(SELECT {}{}, AVG(predict_neighbors) AS predict_neighbors FROM ({}) z WHERE ordered_distance <= {} GROUP BY {}{}, row_id) knr_table".format(
-            ", ".join(X),
-            ", " + ", ".join([quote_ident(elem) for elem in key_columns])
-            if (key_columns)
-            else "",
-            sql,
-            self.parameters["n_neighbors"],
-            ", ".join(X),
-            ", " + ", ".join([quote_ident(elem) for elem in key_columns])
-            if (key_columns)
-            else "",
-        )
-        return sql
+    name: str
+        Name  of the  model.  This is not a  built-in
+        model, so this name is used to build the
+        final table.
+    n_neighbors: int, optional
+        Number of neighbors to consider when computing
+        the score.
+    p: int, optional
+        The p of the p-distances (distance metric used
+        during the model computation).
+    """
 
-    # ---#
-    def fit(
-        self,
-        input_relation: Union[str, vDataFrame],
-        X: list,
-        y: str,
-        test_relation: Union[str, vDataFrame] = "",
-    ):
-        """
-	---------------------------------------------------------------------------
-	Trains the model.
-
-	Parameters
-	----------
-	input_relation: str/vDataFrame
-		Training relation.
-	X: list
-		List of the predictors.
-	y: str
-		Response column.
-	test_relation: str/vDataFrame, optional
-		Relation used to test the model.
-
-	Returns
-	-------
-	object
- 		self
-		"""
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [
-                ("input_relation", input_relation, [str, vDataFrame], False),
-                ("X", X, [list], False),
-                ("y", y, [str], False),
-                ("test_relation", test_relation, [str, vDataFrame], False),
-            ]
-        )
-        if verticapy.options["overwrite_model"]:
-            self.drop()
-        else:
-            does_model_exist(name=self.name, raise_error=True)
-        if isinstance(input_relation, vDataFrame):
-            self.input_relation = input_relation.__genSQL__()
-        else:
-            self.input_relation = input_relation
-        if isinstance(test_relation, vDataFrame):
-            self.test_relation = test_relation.__genSQL__()
-        elif test_relation:
-            self.test_relation = test_relation
-        else:
-            self.test_relation = self.input_relation
-        self.X = [quote_ident(column) for column in X]
-        self.y = quote_ident(y)
-        model_save = {
-            "type": "KNeighborsRegressor",
-            "input_relation": self.input_relation,
-            "test_relation": self.test_relation,
-            "X": self.X,
-            "y": self.y,
-            "p": self.parameters["p"],
-            "n_neighbors": self.parameters["n_neighbors"],
-        }
-        insert_verticapy_schema(
-            model_name=self.name,
-            model_type="KNeighborsRegressor",
-            model_save=model_save,
-        )
-        return self
+    # Properties.
 
-    # ---#
-    def predict(
-        self,
-        vdf: Union[str, vDataFrame],
-        X: list = [],
-        name: str = "",
-        inplace: bool = True,
-        **kwargs,
-    ):
+    @property
+    def _is_native(self) -> Literal[False]:
+        return False
+
+    @property
+    def _vertica_fit_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _vertica_predict_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _model_category(self) -> Literal["UNSUPERVISED"]:
+        return "UNSUPERVISED"
+
+    @property
+    def _model_subcategory(self) -> Literal["ANOMALY_DETECTION"]:
+        return "ANOMALY_DETECTION"
+
+    @property
+    def _model_type(self) -> Literal["LocalOutlierFactor"]:
+        return "LocalOutlierFactor"
+
+    @property
+    def _attributes(self) -> list[str]:
+        return ["n_neighbors_", "p_", "n_errors_", "cnt_"]
+
+    # System & Special Methods.
+
+    @save_verticapy_logs
+    def __init__(self, name: str, n_neighbors: int = 20, p: int = 2) -> None:
+        super().__init__()
+        self.model_name = name
+        self.parameters = {"n_neighbors": n_neighbors, "p": p}
+
+    def drop(self) -> bool:
         """
-    ---------------------------------------------------------------------------
-    Predicts using the input relation.
+        Drops the model from the Vertica database.
+        """
+        try:
+            _executeSQL(
+                query=f"SELECT lof_score FROM {self.model_name} LIMIT 0;",
+                title="Looking if the LOF table exists.",
+            )
+            return drop(self.model_name, method="table")
+        except QueryError:
+            return False
 
-    Parameters
-    ----------
-    vdf: str/vDataFrame
-        Object to use to run the prediction. You can also specify a customized 
-        relation, but you must enclose it with an alias. For example "(SELECT 1) x" 
-        is correct whereas "(SELECT 1)" and "SELECT 1" are incorrect.
-    X: list, optional
-        List of the columns used to deploy the models. If empty, the model
-        predictors will be used.
-    name: str, optional
-        Name of the added vcolumn. If empty, a name will be generated.
-    inplace: bool, optional
-        If set to True, the prediction will be added to the vDataFrame.
-
-    Returns
-    -------
-    vDataFrame
-        the vDataFrame of the prediction
-        """
-        if isinstance(X, str):
-            X = [X]
-        check_types(
-            [
-                ("name", name, [str]),
-                ("X", X, [list]),
-                ("vdf", vdf, [str, vDataFrame]),
-                ("inplace", inplace, [bool]),
-            ],
-        )
-        if isinstance(vdf, str):
-            vdf = vDataFrameSQL(vdf)
-        X = [quote_ident(elem) for elem in X] if (X) else self.X
-        key_columns = vdf.get_columns(exclude_columns=X)
-        if "key_columns" in kwargs:
-            key_columns_arg = None
-        else:
-            key_columns_arg = key_columns
-        name = (
-            "{}_".format(self.type) + "".join(ch for ch in self.name if ch.isalnum())
-            if not (name)
-            else name
-        )
-        sql = "(SELECT {}{}, {} AS {} FROM {}) VERTICAPY_SUBTABLE".format(
-            ", ".join(X),
-            ", " + ", ".join(key_columns) if key_columns else "",
-            "predict_neighbors",
-            name,
-            self.deploySQL(
-                X=X, test_relation=vdf.__genSQL__(), key_columns=key_columns_arg
-            ),
-        )
-        if inplace:
-            return vDataFrameSQL(name="Neighbors", relation=sql, vdf=vdf)
-        else:
-            return vDataFrameSQL(name="Neighbors", relation=sql)
+    # Attributes Methods.
 
+    def _compute_attributes(self) -> None:
+        """
+        Computes the model's attributes.
+        """
+        self.p_ = self.parameters["p"]
+        self.n_neighbors_ = self.parameters["n_neighbors"]
+        self.cnt_ = _executeSQL(
+            query=f"SELECT /*+LABEL('learn.VerticaModel.plot')*/ COUNT(*) FROM {self.model_name}",
+            method="fetchfirstelem",
+            print_time_sql=False,
+        )
 
-# ---#
-class LocalOutlierFactor(vModel):
-    """
----------------------------------------------------------------------------
-[Beta Version]
-Creates a LocalOutlierFactor object by using the Local Outlier Factor algorithm 
-as defined by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jrg 
-Sander. This object is using pure SQL to compute all the distances and final 
-score.
-
-\u26A0 Warning : This algorithm uses a CROSS JOIN during computation and
-                 is therefore computationally expensive at O(n * n), where
-                 n is the total number of elementss. Since LocalOutlierFactor 
-                 is uses the p-distance, it is highly sensitive to unnormalized 
-                 data. A table will be created at the end of the learning phase.
-
-Parameters
-----------
-name: str
-	Name of the the model. This is not a built-in model, so this name will be used
-	to build the final table.
-n_neighbors: int, optional
-	Number of neighbors to consider when computing the score.
-p: int, optional
-	The p of the p-distances (distance metric used during the model computation).
-	"""
-
-    def __init__(self, name: str, n_neighbors: int = 20, p: int = 2):
-        check_types([("name", name, [str], False)])
-        self.type, self.name = "LocalOutlierFactor", name
-        self.set_params({"n_neighbors": n_neighbors, "p": p})
+    # Model Fitting Method.
 
-    # ---#
     def fit(
         self,
-        input_relation: Union[str, vDataFrame],
-        X: list = [],
-        key_columns: list = [],
-        index: str = "",
-    ):
-        """
-	---------------------------------------------------------------------------
-	Trains the model.
-
-	Parameters
-	----------
-	input_relation: str/vDataFrame
-		Training relation.
-	X: list, optional
-		List of the predictors.
-	key_columns: list, optional
-		Columns not used during the algorithm computation but which will be used
-		to create the final relation.
-	index: str, optional
-		Index used to identify each row separately. It is highly recommanded to
-        have one already in the main table to avoid creating temporary tables.
-
-	Returns
-	-------
-	object
- 		self
-		"""
-        if isinstance(X, str):
-            X = [X]
-        if isinstance(key_columns, str):
-            key_columns = [key_columns]
-        check_types(
-            [
-                ("input_relation", input_relation, [str, vDataFrame], False),
-                ("X", X, [list], False),
-                ("key_columns", key_columns, [list], False),
-                ("index", index, [str], False),
-            ]
-        )
-        if verticapy.options["overwrite_model"]:
+        input_relation: SQLRelation,
+        X: Optional[SQLColumns] = None,
+        key_columns: Optional[SQLColumns] = None,
+        index: Optional[str] = None,
+    ) -> None:
+        """
+        Trains the model.
+
+        Parameters
+        ----------
+        input_relation: SQLRelation
+                Training relation.
+        X: SQLColumns, optional
+                List of the predictors.
+        key_columns: SQLColumns, optional
+                Columns  not  used   during  the   algorithm
+            computation  but   which  are  used  to
+            create the final relation.
+        index: str, optional
+                Index  used to seperately identify each row.
+            To avoid the creation of temporary tables,
+            it is recommended that you already have an
+            index in the main table.
+        """
+        X, key_columns = format_type(X, key_columns, dtype=list)
+        X = quote_ident(X)
+        if conf.get_option("overwrite_model"):
             self.drop()
         else:
-            does_model_exist(name=self.name, raise_error=True)
-        self.key_columns = [quote_ident(column) for column in key_columns]
+            self._is_already_stored(raise_error=True)
+        self.key_columns = quote_ident(key_columns)
         if isinstance(input_relation, vDataFrame):
-            self.input_relation = input_relation.__genSQL__()
-            if not (X):
+            self.input_relation = input_relation.current_relation()
+            if not X:
                 X = input_relation.numcol()
         else:
             self.input_relation = input_relation
-            if not (X):
+            if not X:
                 X = vDataFrame(input_relation).numcol()
-        X = [quote_ident(column) for column in X]
         self.X = X
         n_neighbors = self.parameters["n_neighbors"]
         p = self.parameters["p"]
-        schema, relation = schema_relation(input_relation)
-        name_list = [
-            gen_tmp_name(name="main"),
-            gen_tmp_name(name="distance"),
-            gen_tmp_name(name="lrd"),
-            gen_tmp_name(name="lof"),
-        ]
+        schema = schema_relation(input_relation)[0]
         tmp_main_table_name = gen_tmp_name(name="main")
         tmp_distance_table_name = gen_tmp_name(name="distance")
         tmp_lrd_table_name = gen_tmp_name(name="lrd")
         tmp_lof_table_name = gen_tmp_name(name="lof")
         try:
-            if not (index):
+            if not index:
                 index = "id"
                 main_table = tmp_main_table_name
                 schema = "v_temp_schema"
-                sql = "CREATE LOCAL TEMPORARY TABLE {} ON COMMIT PRESERVE ROWS AS SELECT ROW_NUMBER() OVER() AS id, {} FROM {} WHERE {}".format(
-                    main_table,
-                    ", ".join(X + key_columns),
-                    self.input_relation,
-                    " AND ".join(["{} IS NOT NULL".format(item) for item in X]),
+                drop(f"v_temp_schema.{tmp_main_table_name}", method="table")
+                _executeSQL(
+                    query=f"""
+                        CREATE LOCAL TEMPORARY TABLE {main_table} 
+                        ON COMMIT PRESERVE ROWS AS 
+                            SELECT 
+                                /*+LABEL('learn.neighbors.LocalOutlierFactor.fit')*/ 
+                                ROW_NUMBER() OVER() AS id, 
+                                {', '.join(X + key_columns)} 
+                            FROM {self.input_relation} 
+                            WHERE {' AND '.join([f"{x} IS NOT NULL" for x in X])}""",
+                    print_time_sql=False,
                 )
-                drop("v_temp_schema.{}".format(tmp_main_table_name), method="table")
-                executeSQL(sql, print_time_sql=False)
             else:
                 main_table = self.input_relation
-            sql = [
-                "POWER(ABS(x.{} - y.{}), {})".format(X[i], X[i], p)
-                for i in range(len(X))
-            ]
-            distance = "POWER({}, 1 / {})".format(" + ".join(sql), p)
-            sql = "SELECT x.{} AS node_id, y.{} AS nn_id, {} AS distance, ROW_NUMBER() OVER(PARTITION BY x.{} ORDER BY {}) AS knn FROM {}.{} AS x CROSS JOIN {}.{} AS y".format(
-                index,
-                index,
-                distance,
-                index,
-                distance,
-                schema,
-                main_table,
-                schema,
-                main_table,
-            )
-            sql = "SELECT node_id, nn_id, distance, knn FROM ({}) distance_table WHERE knn <= {}".format(
-                sql, n_neighbors + 1
-            )
-            sql = "CREATE LOCAL TEMPORARY TABLE {} ON COMMIT PRESERVE ROWS AS {}".format(
-                tmp_distance_table_name, sql
-            )
-            drop("v_temp_schema.{}".format(tmp_distance_table_name), method="table")
-            executeSQL(sql, "Computing the LOF [Step 0].")
-            kdistance = "(SELECT node_id, nn_id, distance AS distance FROM v_temp_schema.{} WHERE knn = {}) AS kdistance_table".format(
-                tmp_distance_table_name, n_neighbors + 1
-            )
-            lrd = "SELECT distance_table.node_id, {} / SUM(CASE WHEN distance_table.distance > kdistance_table.distance THEN distance_table.distance ELSE kdistance_table.distance END) AS lrd FROM (v_temp_schema.{} AS distance_table LEFT JOIN {} ON distance_table.nn_id = kdistance_table.node_id) x GROUP BY 1".format(
-                n_neighbors, tmp_distance_table_name, kdistance
-            )
-            sql = "CREATE LOCAL TEMPORARY TABLE {} ON COMMIT PRESERVE ROWS AS {}".format(
-                tmp_lrd_table_name, lrd
-            )
-            drop("v_temp_schema.{}".format(tmp_lrd_table_name), method="table")
-            executeSQL(sql, "Computing the LOF [Step 1].")
-            sql = "SELECT x.node_id, SUM(y.lrd) / (MAX(x.node_lrd) * {}) AS LOF FROM (SELECT n_table.node_id, n_table.nn_id, lrd_table.lrd AS node_lrd FROM v_temp_schema.{} AS n_table LEFT JOIN v_temp_schema.{} AS lrd_table ON n_table.node_id = lrd_table.node_id) x LEFT JOIN v_temp_schema.{} AS y ON x.nn_id = y.node_id GROUP BY 1".format(
-                n_neighbors,
-                tmp_distance_table_name,
-                tmp_lrd_table_name,
-                tmp_lrd_table_name,
-            )
-            sql = "CREATE LOCAL TEMPORARY TABLE {} ON COMMIT PRESERVE ROWS AS {}".format(
-                tmp_lof_table_name, sql
-            )
-            drop("v_temp_schema.{}".format(tmp_lof_table_name), method="table")
-            executeSQL(sql, "Computing the LOF [Step 2].")
-            sql = "SELECT {}, (CASE WHEN lof > 1e100 OR lof != lof THEN 0 ELSE lof END) AS lof_score FROM {} AS x LEFT JOIN v_temp_schema.{} AS y ON x.{} = y.node_id".format(
-                ", ".join(X + self.key_columns), main_table, tmp_lof_table_name, index
-            )
-            executeSQL(
-                "CREATE TABLE {} AS {}".format(self.name, sql),
+            sql = [f"POWER(ABS(x.{X[i]} - y.{X[i]}), {p})" for i in range(len(X))]
+            distance = f"POWER({' + '.join(sql)}, 1 / {p})"
+            drop(f"v_temp_schema.{tmp_distance_table_name}", method="table")
+            _executeSQL(
+                query=f"""
+                    CREATE LOCAL TEMPORARY TABLE {tmp_distance_table_name} 
+                    ON COMMIT PRESERVE ROWS AS 
+                        SELECT 
+                            /*+LABEL('learn.neighbors.LocalOutlierFactor.fit')*/ 
+                            node_id, 
+                            nn_id, 
+                            distance, 
+                            knn 
+                        FROM 
+                            (SELECT 
+                                x.{index} AS node_id, 
+                                y.{index} AS nn_id, 
+                                {distance} AS distance, 
+                                ROW_NUMBER() OVER(PARTITION BY x.{index} 
+                                                  ORDER BY {distance}) AS knn 
+                             FROM {schema}.{main_table} AS x 
+                             CROSS JOIN 
+                             {schema}.{main_table} AS y) distance_table 
+                        WHERE knn <= {n_neighbors + 1}""",
+                title="Computing the LOF [Step 0].",
+            )
+            drop(f"v_temp_schema.{tmp_lrd_table_name}", method="table")
+            _executeSQL(
+                query=f"""
+                    CREATE LOCAL TEMPORARY TABLE {tmp_lrd_table_name} 
+                    ON COMMIT PRESERVE ROWS AS 
+                        SELECT 
+                            /*+LABEL('learn.neighbors.LocalOutlierFactor.fit')*/ 
+                            distance_table.node_id, 
+                            {n_neighbors} / SUM(
+                                    CASE 
+                                        WHEN distance_table.distance 
+                                             > kdistance_table.distance 
+                                        THEN distance_table.distance 
+                                        ELSE kdistance_table.distance 
+                                     END) AS lrd 
+                        FROM 
+                            (v_temp_schema.{tmp_distance_table_name} AS distance_table 
+                             LEFT JOIN 
+                             (SELECT 
+                                 node_id, 
+                                 nn_id, 
+                                 distance AS distance 
+                              FROM v_temp_schema.{tmp_distance_table_name} 
+                              WHERE knn = {n_neighbors + 1}) AS kdistance_table
+                             ON distance_table.nn_id = kdistance_table.node_id) x 
+                        GROUP BY 1""",
+                title="Computing the LOF [Step 1].",
+            )
+            drop(f"v_temp_schema.{tmp_lof_table_name}", method="table")
+            _executeSQL(
+                query=f"""
+                    CREATE LOCAL TEMPORARY TABLE {tmp_lof_table_name} 
+                    ON COMMIT PRESERVE ROWS AS 
+                    SELECT 
+                        /*+LABEL('learn.neighbors.LocalOutlierFactor.fit')*/ 
+                        x.node_id, 
+                        SUM(y.lrd) / (MAX(x.node_lrd) * {n_neighbors}) AS LOF 
+                    FROM 
+                        (SELECT 
+                            n_table.node_id, 
+                            n_table.nn_id, 
+                            lrd_table.lrd AS node_lrd 
+                         FROM 
+                            v_temp_schema.{tmp_distance_table_name} AS n_table 
+                         LEFT JOIN 
+                            v_temp_schema.{tmp_lrd_table_name} AS lrd_table 
+                        ON n_table.node_id = lrd_table.node_id) x 
+                    LEFT JOIN 
+                        v_temp_schema.{tmp_lrd_table_name} AS y 
+                    ON x.nn_id = y.node_id GROUP BY 1""",
+                title="Computing the LOF [Step 2].",
+            )
+            _executeSQL(
+                query=f"""
+                    CREATE TABLE {self.model_name} AS 
+                        SELECT 
+                            /*+LABEL('learn.neighbors.LocalOutlierFactor.fit')*/ 
+                            {', '.join(X + self.key_columns)}, 
+                            (CASE WHEN lof > 1e100 OR lof != lof THEN 0 ELSE lof END) AS lof_score
+                        FROM 
+                            {main_table} AS x 
+                        LEFT JOIN 
+                            v_temp_schema.{tmp_lof_table_name} AS y 
+                        ON x.{index} = y.node_id""",
                 title="Computing the LOF [Step 3].",
             )
-            self.n_errors_ = executeSQL(
-                "SELECT COUNT(*) FROM {}.{} z WHERE lof > 1e100 OR lof != lof".format(
-                    schema, tmp_lof_table_name
-                ),
+            self.n_errors_ = _executeSQL(
+                query=f"""
+                    SELECT 
+                        /*+LABEL('learn.neighbors.LocalOutlierFactor.fit')*/ 
+                        COUNT(*) 
+                    FROM {schema}.{tmp_lof_table_name} z 
+                    WHERE lof > 1e100 OR lof != lof""",
                 method="fetchfirstelem",
                 print_time_sql=False,
             )
-        except:
-            drop("v_temp_schema.{}".format(tmp_main_table_name), method="table")
-            drop("v_temp_schema.{}".format(tmp_distance_table_name), method="table")
-            drop("v_temp_schema.{}".format(tmp_lrd_table_name), method="table")
-            drop("v_temp_schema.{}".format(tmp_lof_table_name), method="table")
-            raise
-        drop("v_temp_schema.{}".format(tmp_main_table_name), method="table")
-        drop("v_temp_schema.{}".format(tmp_distance_table_name), method="table")
-        drop("v_temp_schema.{}".format(tmp_lrd_table_name), method="table")
-        drop("v_temp_schema.{}".format(tmp_lof_table_name), method="table")
-        model_save = {
-            "type": "LocalOutlierFactor",
-            "input_relation": self.input_relation,
-            "key_columns": self.key_columns,
-            "X": self.X,
-            "p": self.parameters["p"],
-            "n_neighbors": self.parameters["n_neighbors"],
-            "n_errors": self.n_errors_,
-        }
-        insert_verticapy_schema(
-            model_name=self.name,
-            model_type="LocalOutlierFactor",
-            model_save=model_save,
-        )
-        return self
+            self._compute_attributes()
+        finally:
+            drop(f"v_temp_schema.{tmp_main_table_name}", method="table")
+            drop(f"v_temp_schema.{tmp_distance_table_name}", method="table")
+            drop(f"v_temp_schema.{tmp_lrd_table_name}", method="table")
+            drop(f"v_temp_schema.{tmp_lof_table_name}", method="table")
 
-    # ---#
-    def predict(self):
+    # Prediction / Transformation Methods.
+
+    def predict(self) -> vDataFrame:
+        """
+        Creates a vDataFrame of the model.
+
+        Returns
+        -------
+        vDataFrame
+            the vDataFrame including the prediction.
         """
-	---------------------------------------------------------------------------
-	Creates a vDataFrame of the model.
+        return vDataFrame(self.model_name)
 
-	Returns
-	-------
-	vDataFrame
- 		the vDataFrame including the prediction.
-		"""
-        return vDataFrame(self.name)
+    # Plotting Methods.
+
+    def plot(
+        self,
+        max_nb_points: int = 100,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the model.
+
+        Parameters
+        ----------
+        max_nb_points: int
+            Maximum  number of points to display.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the
+            Plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="LOFPlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.LOFPlot(
+            vdf=vDataFrame(self.model_name),
+            columns=self.X + ["lof_score"],
+            max_nb_points=max_nb_points,
+        ).draw(**kwargs)
```

### Comparing `verticapy-0.9.0/verticapy/learn/pipeline.py` & `verticapy-1.0.0b1/verticapy/machine_learning/vertica/pipeline.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,456 +1,391 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# VerticaPy Modules
-from verticapy import vDataFrame
-from verticapy.utilities import *
-from verticapy.toolbox import *
-from verticapy.errors import *
-from verticapy.learn.vmodel import *
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import copy
+
+from typing import Literal, Optional
+
+import verticapy._config.config as conf
+from verticapy._typing import NoneType, SQLColumns, SQLRelation
+from verticapy._utils._sql._format import format_type
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy.errors import ModelError
+
+from verticapy.core.tablesample.base import TableSample
+from verticapy.core.vdataframe.base import vDataFrame
+
+from verticapy.machine_learning.vertica.base import Regressor, VerticaModel
+
+"""
+General Class.
+"""
 
-# Standard Python Modules
-from typing import Union
 
-# ---#
 class Pipeline:
     """
----------------------------------------------------------------------------
-Creates a Pipeline object. Sequentially apply a list of transforms and a 
-final estimator. The intermediate steps must implement a transform method.
-
-Parameters
-----------
-steps: list
-    List of (name, transform) tuples (implementing fit/transform) that are chained, 
-    in the order in which they are chained, with the last object an estimator.
-	"""
-
-    def __init__(self, steps: list):
-        check_types([("steps", steps, [list])])
-        self.type = "Pipeline"
+    Creates a Pipeline object, which sequentially
+    applies a list of transforms and a final estimator.
+    The intermediate steps must  implement a transform
+    method.
+
+    Parameters
+    ----------
+    steps: list
+        List of (name, transform)  tuples (implementing
+        fit/transform) that  are chained, in  the order
+        in which they are chained, where the last object
+        is an estimator.
+    """
+
+    # Properties.
+
+    @property
+    def _is_native(self) -> Literal[False]:
+        return False
+
+    @property
+    def _vertica_fit_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _vertica_predict_sql(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _model_category(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _model_subcategory(self) -> Literal[""]:
+        return ""
+
+    @property
+    def _model_type(self) -> Literal["Pipeline"]:
+        return "Pipeline"
+
+    @property
+    def _attributes(self) -> None:
+        raise NotImplementedError
+
+    # System & Special Methods.
+
+    @save_verticapy_logs
+    def __init__(self, steps: list) -> None:
         self.steps = []
-        for idx, elem in enumerate(steps):
-            if len(elem) != 2:
-                raise ParameterError(
+        for idx, s in enumerate(steps):
+            if len(s) != 2:
+                raise ValueError(
                     "The steps of the Pipeline must be composed of 2 elements "
-                    "(name, transform). Found {}.".format(
-                        len(elem)
-                    )
+                    f"(name, transform). Found {len(s)}."
                 )
-            elif not (isinstance(elem[0], str)):
-                raise ParameterError(
+            elif not isinstance(s[0], str):
+                raise ValueError(
                     "The steps 'name' of the Pipeline must be of "
-                    "type str. Found {}.".format(
-                        type(elem[0])
-                    )
+                    f"type str. Found {type(s[0])}."
                 )
-            else:
-                try:
-                    if idx < len(steps) - 1:
-                        elem[1].transform
-                    elem[1].fit
-                except:
-                    if idx < len(steps) - 1:
-                        raise ParameterError(
-                            "The estimators of the Pipeline must have a "
-                            "'transform' and a 'fit' method."
-                        )
-                    else:
-                        raise ParameterError(
-                            "The last estimator of the Pipeline must have a "
-                            "'fit' method."
-                        )
-            self.steps += [elem]
+            elif idx < len(steps) - 1 and (
+                not hasattr(s[1], "transform") or not hasattr(s[1], "fit")
+            ):
+                raise AttributeError(
+                    "The estimators of the Pipeline must have a "
+                    "'transform' and a 'fit' method."
+                )
+            elif not hasattr(s[1], "fit"):
+                raise ValueError(
+                    "The last estimator of the Pipeline must have a " "'fit' method."
+                )
+            self.steps += [s]
 
-    # ---#
-    def __getitem__(self, index):
+    def __getitem__(self, index) -> VerticaModel:
         if isinstance(index, slice):
             return self.steps[index]
         elif isinstance(index, int):
             return self.steps[index][1]
         else:
             return getattr(self, index)
 
-    # ---#
-    def drop(self):
+    def drop(self) -> None:
         """
-    ---------------------------------------------------------------------------
-    Drops the model from the Vertica database.
+        Drops the model from the Vertica database.
         """
         for step in self.steps:
             step[1].drop()
 
-    # ---#
+    # Parameters Methods.
+
+    def get_params(self) -> dict[dict]:
+        """
+        Returns the model's Parameters.
+
+        Returns
+        -------
+        dict
+            model's parameters.
+        """
+        params = {}
+        for step in self.steps:
+            params[step[0]] = step[1].get_params()
+        return params
+
+    def set_params(self, parameters: Optional[dict[dict]] = None, **kwargs) -> None:
+        """
+        Sets the parameters of the model.
+
+        Parameters
+        ----------
+        parameters: dict, optional
+            New parameters.  It must be a  dictionary with
+            the  Pipeline names as keys and the parameter
+            dictionary as values.
+        **kwargs
+            New parameters can also be passed as arguments.
+            Example: set_params(pipeline1 = dict1,
+                                pipeline2 = dict2)
+        """
+        parameters = format_type(parameters, dtype=dict)
+        for param in {**parameters, **kwargs}:
+            for step in self.steps:
+                if param.lower() == step[0].lower():
+                    step[1].set_params(parameters[param])
+
+    # Model Fitting Method.
+
     def fit(
         self,
-        input_relation: Union[str, vDataFrame],
+        input_relation: SQLRelation,
         X: list,
-        y: str = "",
-        test_relation: Union[str, vDataFrame] = "",
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Trains the model.
-
-    Parameters
-    ----------
-    input_relation: str/vDataFrame
-        Training relation.
-    X: list
-        List of the predictors.
-    y: str, optional
-        Response column.
-    test_relation: str/vDataFrame, optional
-        Relation used to test the model.
-
-    Returns
-    -------
-    object
-        model
+        y: Optional[str] = None,
+        test_relation: SQLRelation = "",
+    ) -> None:
+        """
+        Trains the model.
+
+        Parameters
+        ----------
+        input_relation: SQLRelation
+            Training relation.
+        X: list
+            List of the predictors.
+        y: str, optional
+            Response column.
+        test_relation: SQLRelation, optional
+            Relation used to test the model.
+
+        Returns
+        -------
+        object
+            model.
         """
-        if isinstance(X, str):
-            X = [X]
+        X = format_type(X, dtype=list)
         if isinstance(input_relation, str):
-            vdf = vDataFrameSQL(relation=input_relation)
+            vdf = vDataFrame(input_relation)
         else:
             vdf = input_relation
-        if verticapy.options["overwrite_model"]:
+        if conf.get_option("overwrite_model"):
             self.drop()
-        else:
-            does_model_exist(name=self.name, raise_error=True)
-        X_new = [elem for elem in X]
+        X_new = copy.deepcopy(X)
         current_vdf = vdf
         for idx, step in enumerate(self.steps):
             if (idx == len(self.steps) - 1) and (y):
                 step[1].fit(current_vdf, X_new, y, test_relation)
             else:
                 step[1].fit(current_vdf, X_new)
             if idx < len(self.steps) - 1:
                 current_vdf = step[1].transform(current_vdf, X_new)
-                X_new = step[1].get_names(X=X)
+                X_new = step[1]._get_names(X=X)
         self.input_relation = self.steps[0][1].input_relation
         self.X = [column for column in self.steps[0][1].X]
-        try:
+        if hasattr(self.steps[-1][1], "y"):
             self.y = self.steps[-1][1].y
+        if hasattr(self.steps[-1][1], "test_relation"):
             self.test_relation = self.steps[-1][1].test_relation
-        except:
-            pass
-        return self
-
-    # ---#
-    def get_params(self):
-        """
-    ---------------------------------------------------------------------------
-    Returns the models Parameters.
-
-    Returns
-    -------
-    dict
-        models parameters
+
+    # Model Evaluation Methods.
+
+    def report(self) -> TableSample:
         """
-        params = {}
-        for step in self.steps:
-            params[step[0]] = step[1].get_params()
-        return params
+        Computes a regression/classification report using
+        multiple metrics to  evaluate the model depending
+        on its type.
+
+        Returns
+        -------
+        TableSample
+            report.
+        """
+        if isinstance(self.steps[-1][1], Regressor):
+            return self.steps[-1][1].regression_report()
+        else:
+            return self.steps[-1][1].classification_report()
 
-    # ---#
-    def predict(
-        self, vdf: Union[str, vDataFrame] = None, X: list = [], name: str = "estimator"
-    ):
+    def score(self, metric: Optional[str] = None) -> float:
         """
-    ---------------------------------------------------------------------------
-    Applies the model on a vDataFrame.
+        Computes the model score.
 
-    Parameters
-    ----------
-    vdf: str/vDataFrame, optional
-        Input vDataFrame. You can also specify a customized relation, 
-        but you must enclose it with an alias. For example "(SELECT 1) x" is 
-        correct whereas "(SELECT 1)" and "SELECT 1" are incorrect.
-    X: list, optional
-        List of the input vcolumns.
-    name: str, optional
-        Name of the added vcolumn.
-
-    Returns
-    -------
-    vDataFrame
-        object result of the model transformation.
-        """
-        if isinstance(X, str):
-            X = [X]
-        try:
-            self.steps[-1][1].predict
-        except:
+        Parameters
+        ----------
+        metric: str, optional
+            The metric used to compute the score.
+            Depends  on  the  final estimator type
+            (classification or regression).
+
+        Returns
+        -------
+        float
+            score.
+        """
+        if isinstance(metric, NoneType):
+            if isinstance(self.steps[-1][1], Regressor):
+                metric = "r2"
+            else:
+                metric = "accuracy"
+        return self.steps[-1][1].score(metric=metric)
+
+    # Prediction / Transformation Methods.
+
+    def predict(
+        self,
+        vdf: SQLRelation = None,
+        X: Optional[SQLColumns] = None,
+        name: str = "estimator",
+    ) -> vDataFrame:
+        """
+        Applies the model on a vDataFrame.
+
+        Parameters
+        ----------
+        vdf: SQLRelation, optional
+            Input  vDataFrame.  You  can  also  specify  a
+            customized  relation,  but  you  must  enclose
+            it with an alias.  For example: "(SELECT 1) x"
+            is valid whereas "(SELECT 1)" and "SELECT 1"
+            are invalid.
+        X: SQLColumns, optional
+            List of the input vDataColumns.
+        name: str, optional
+            Name of the added vDataColumn.
+
+        Returns
+        -------
+        vDataFrame
+            object result of the model transformation.
+        """
+        X = format_type(X, dtype=list)
+        if not hasattr(self.steps[-1][1], "predict"):
             raise ModelError(
                 "The last estimator of the Pipeline has no 'predict' method."
             )
-        if not (vdf):
+        if not vdf:
             vdf = self.input_relation
         if isinstance(vdf, str):
-            vdf = vDataFrameSQL(relation=vdf)
-        X_new, X_all = [elem for elem in X], []
+            vdf = vDataFrame(vdf)
+        X_new, X_all = copy.deepcopy(X), []
         current_vdf = vdf
         for idx, step in enumerate(self.steps):
             if idx == len(self.steps) - 1:
                 try:
                     current_vdf = step[1].predict(
                         current_vdf, X_new, name=name, inplace=False
                     )
                 except:
                     current_vdf = step[1].predict(current_vdf, X_new, name=name)
             else:
                 current_vdf = step[1].transform(current_vdf, X_new)
-                X_new = step[1].get_names(X=X)
+                X_new = step[1]._get_names(X=X)
                 X_all += X_new
         return current_vdf[vdf.get_columns() + [name]]
 
-    # ---#
-    def report(self):
-        """
-    ---------------------------------------------------------------------------
-    Computes a regression/classification report using multiple metrics to evaluate 
-    the model depending on its type. 
-
-    Returns
-    -------
-    tablesample
-        An object containing the result. For more information, see
-        utilities.tablesample.
-        """
-        if isinstance(self.steps[-1][1], Regressor):
-            return self.steps[-1][1].regression_report()
-        else:
-            return self.steps[-1][1].classification_report()
-
-    # ---#
-    def score(self, method: str = ""):
-        """
-    ---------------------------------------------------------------------------
-    Computes the model score.
-
-    Parameters
-    ----------
-    method: str, optional
-        The method to use to compute the score.
-        Depends on the final estimator type (classification or regression).
-
-    Returns
-    -------
-    float
-        score
-        """
-        if not (method):
-            if isinstance(self.steps[-1][1], Regressor):
-                method = "r2"
-            else:
-                method = "accuracy"
-        return self.steps[-1][1].score(method)
-
-    # ---#
-    def transform(self, vdf: Union[str, vDataFrame] = None, X: list = []):
+    def transform(
+        self, vdf: SQLRelation = None, X: Optional[SQLColumns] = None
+    ) -> vDataFrame:
+        """
+        Applies the model on a vDataFrame.
+
+        Parameters
+        ----------
+        vdf: SQLRelation, optional
+            Input  vDataFrame.  You  can  also  specify  a
+            customized  relation,  but  you  must  enclose
+            it with an alias. For  example: "(SELECT 1) x"
+            is valid whereas "(SELECT 1)" and "SELECT 1"
+            are invalid.
+        X: SQLColumns, optional
+            List of the input vDataColumns.
+
+        Returns
+        -------
+        vDataFrame
+            object result of the model transformation.
         """
-    ---------------------------------------------------------------------------
-    Applies the model on a vDataFrame.
-
-    Parameters
-    ----------
-    vdf: str/vDataFrame, optional
-        Input vDataFrame. You can also specify a customized relation, 
-        but you must enclose it with an alias. For example "(SELECT 1) x" is 
-        correct whereas "(SELECT 1)" and "SELECT 1" are incorrect.
-    X: list, optional
-        List of the input vcolumns.
-
-    Returns
-    -------
-    vDataFrame
-        object result of the model transformation.
-        """
-        if isinstance(X, str):
-            X = [X]
-        try:
-            self.steps[-1][1].transform
-        except:
+        X = format_type(X, dtype=list)
+        if not hasattr(self.steps[-1][1], "transform"):
             raise ModelError(
                 "The last estimator of the Pipeline has no 'transform' method."
             )
-        if not (vdf):
+        if not vdf:
             vdf = self.input_relation
         if isinstance(vdf, str):
-            vdf = vDataFrameSQL(relation=vdf)
-        X_new, X_all = [elem for elem in X], []
+            vdf = vDataFrame(vdf)
+        X_new, X_all = copy.deepcopy(X), []
         current_vdf = vdf
-        for idx, step in enumerate(self.steps):
+        for step in self.steps:
             current_vdf = step[1].transform(current_vdf, X_new)
-            X_new = step[1].get_names(X=X)
+            X_new = step[1]._get_names(X=X)
             X_all += X_new
         return current_vdf
 
-    # ---#
-    def inverse_transform(self, vdf: Union[str, vDataFrame] = None, X: list = []):
-        """
-    ---------------------------------------------------------------------------
-    Applies the inverse model transformation on a vDataFrame.
-
-    Parameters
-    ----------
-    vdf: str/vDataFrame, optional
-        Input vDataFrame. You can also specify a customized relation, 
-        but you must enclose it with an alias. For example "(SELECT 1) x" is 
-        correct whereas "(SELECT 1)" and "SELECT 1" are incorrect.
-    X: list, optional
-        List of the input vcolumns.
-
-    Returns
-    -------
-    vDataFrame
-        object result of the model inverse transformation.
-        """
-        if isinstance(X, str):
-            X = [X]
-        try:
-            for idx in range(len(self.steps)):
-                self.steps[idx][1].inverse_transform
-        except:
-            raise ModelError(
-                f"The estimator [{idx}] of the Pipeline has "
-                "no 'inverse_transform' method."
-            )
-        if not (vdf):
+    def inverse_transform(
+        self, vdf: SQLRelation = None, X: Optional[SQLColumns] = None
+    ) -> vDataFrame:
+        """
+        Applies  the  inverse model transformation  on  a
+        vDataFrame.
+
+        Parameters
+        ----------
+        vdf: SQLRelation, optional
+            Input  vDataFrame.  You  can  also  specify  a
+            customized  relation,  but  you  must  enclose
+            it with an alias. For  example: "(SELECT 1) x"
+            is valid whereas "(SELECT 1)" and "SELECT 1"
+            are invalid.
+        X: SQLColumns, optional
+            List of the input vDataColumns.
+
+        Returns
+        -------
+        vDataFrame
+            object result of the model inverse transformation.
+        """
+        X = format_type(X, dtype=list)
+        for idx in range(len(self.steps)):
+            if not hasattr(self.steps[idx][1], "inverse_transform"):
+                raise ModelError(
+                    f"The estimator [{idx}] of the Pipeline has "
+                    "no 'inverse_transform' method."
+                )
+        if not vdf:
             vdf = self.input_relation
         if isinstance(vdf, str):
-            vdf = vDataFrameSQL(relation=vdf)
-        X_new, X_all = [elem for elem in X], []
+            vdf = vDataFrame(vdf)
+        X_new, X_all = copy.deepcopy(X), []
         current_vdf = vdf
         for idx in range(1, len(self.steps) + 1):
             step = self.steps[-idx]
             current_vdf = step[1].inverse_transform(current_vdf, X_new)
-            X_new = step[1].get_names(inverse=True, X=X)
+            X_new = step[1]._get_names(inverse=True, X=X)
             X_all += X_new
         return current_vdf
-
-    # ---#
-    def set_params(self, parameters: dict = {}):
-        """
-    ---------------------------------------------------------------------------
-    Sets the parameters of the model.
-
-    Parameters
-    ----------
-    parameters: dict, optional
-        New parameters. It must be a dictionary with as keys the Pipeline 
-        names and as value the parameters dictionary.
-        """
-        for param in parameters:
-            for step in self.steps:
-                if param.lower() == step[0].lower():
-                    step[1].set_params(parameters[param])
-
-    # ---#
-    def to_python(
-        self,
-        name: str = "predict",
-        return_proba: bool = False,
-        return_distance_clusters: bool = False,
-        return_str: bool = False,
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Returns the Python code needed to deploy the pipeline without using 
-    built-in Vertica functions.
-
-    Parameters
-    ----------
-    name: str, optional
-        Function Name.
-    return_proba: bool, optional
-        If set to True and the model is a classifier, the function will return 
-        the model probabilities.
-    return_distance_clusters: bool, optional
-        If set to True and the model type is KMeans or NearestCentroids, the 
-        function will return the model clusters distances.
-    return_str: bool, optional
-        If set to True, the function str will be returned.
-
-
-    Returns
-    -------
-    str / func
-        Python function
-        """
-        if not (return_str):
-            func = self.to_python(
-                name=name,
-                return_proba=return_proba,
-                return_distance_clusters=return_distance_clusters,
-                return_str=True,
-            )
-            _locals = locals()
-            exec(func, globals(), _locals)
-            return _locals[name]
-        str_representation = "def {}(X):\n".format(name)
-        final_function = "X"
-        for idx, step in enumerate(self.steps):
-            str_representation += (
-                "\t"
-                + step[1]
-                .to_python(
-                    name=step[0],
-                    return_proba=return_proba,
-                    return_distance_clusters=return_distance_clusters,
-                    return_str=True,
-                )
-                .replace("\n", "\n\t")
-                + "\n"
-            )
-            final_function = step[0] + "({})".format(final_function)
-        str_representation += "\treturn {}".format(final_function)
-        return str_representation
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/learn/tree.py` & `verticapy-1.0.0b1/verticapy/core/vdataframe/_pivot.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,293 +1,248 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# VerticaPy Modules
-from verticapy.learn.vmodel import *
-
-# Standard Python Modules
-from typing import Union
-
-#
-# Functions used to simplify the code
-#
-# ---#
-def get_tree_list_of_arrays(
-    tree, X: list, model_type: str, return_probability: bool = False
-):
-    """
-    Takes as input a tree which is represented by a tablesample
-    It returns a list of arrays. Each index of the arrays represents
-    a node value.
-    """
-
-    def map_idx(x):
-        for idx, elem in enumerate(X):
-            if quote_ident(x).lower() == quote_ident(elem).lower():
-                return idx
-
-    tree_list = []
-    for idx in range(len(tree["tree_id"])):
-        tree.values["left_child_id"] = [
-            idx if elem == tree.values["node_id"][idx] else elem
-            for elem in tree.values["left_child_id"]
-        ]
-        tree.values["right_child_id"] = [
-            idx if elem == tree.values["node_id"][idx] else elem
-            for elem in tree.values["right_child_id"]
-        ]
-        tree.values["node_id"][idx] = idx
-        tree.values["split_predictor"][idx] = map_idx(tree["split_predictor"][idx])
-        if model_type == "XGBoostClassifier" and isinstance(tree["log_odds"][idx], str):
-            val, all_val = tree["log_odds"][idx].split(","), {}
-            for elem in val:
-                all_val[elem.split(":")[0]] = float(elem.split(":")[1])
-            tree.values["log_odds"][idx] = all_val
-    tree_list = [
-        tree["left_child_id"],
-        tree["right_child_id"],
-        tree["split_predictor"],
-        tree["split_value"],
-        tree["prediction"],
-        tree["is_categorical_split"],
-    ]
-    if model_type == "XGBoostClassifier":
-        tree_list += [tree["log_odds"]]
-    if return_probability:
-        tree_list += [tree["probability/variance"]]
-    return tree_list
-
-
-#
-# Tree Algorithms
-#
-# ---#
-class DecisionTreeClassifier(MulticlassClassifier, Tree):
-    """
-    ---------------------------------------------------------------------------
-    A DecisionTreeClassifier made of a single tree.
-
-    Parameters
-    ----------
-    name: str
-            Name of the the model. The model will be stored in the DB.
-    max_features: str/int, optional
-            The number of randomly chosen features from which to pick the best
-        feature to split on a given tree node. It can be an integer or one
-        of the two following methods.
-                    auto : square root of the total number of predictors.
-                    max  : number of predictors.
-    max_leaf_nodes: int, optional
-            The maximum number of leaf nodes a tree in the forest can have, an
-        integer between 1 and 1e9, inclusive.
-    max_depth: int, optional
-            The maximum depth for growing each tree, an integer between 1 and 100,
-        inclusive.
-    min_samples_leaf: int, optional
-            The minimum number of samples each branch must have after splitting a
-        node, an integer between 1 and 1e6, inclusive. A split that causes
-        fewer remaining samples is discarded.
-    min_info_gain: float, optional
-            The minimum threshold for including a split, a float between 0.0 and
-        1.0, inclusive. A split with information gain less than this threshold
-        is discarded.
-    nbins: int, optional
-            The number of bins to use for continuous features, an integer between 2
-        and 1000, inclusive.
-    """
-
-    def __init__(
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+from typing import Optional, Union, TYPE_CHECKING
+
+from verticapy._typing import NoneType, SQLColumns, SQLExpression
+from verticapy._utils._object import create_new_vdf
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import format_type, quote_ident
+from verticapy._utils._sql._merge import gen_coalesce, group_similar_names
+from verticapy.errors import EmptyParameter
+
+from verticapy.core.vdataframe._join_union_sort import vDFJoinUnionSort
+
+from verticapy.sql.flex import compute_vmap_keys
+
+if TYPE_CHECKING:
+    from verticapy.core.vdataframe.base import vDataFrame
+
+
+class vDFPivot(vDFJoinUnionSort):
+    @save_verticapy_logs
+    def flat_vmap(
         self,
-        name: str,
-        max_features: Union[int, str] = "auto",
-        max_leaf_nodes: int = 1e9,
-        max_depth: int = 100,
-        min_samples_leaf: int = 1,
-        min_info_gain: float = 0.0,
-        nbins: int = 32,
-    ):
-        version(condition=[8, 1, 1])
-        check_types([("name", name, [str])])
-        self.type, self.name = "RandomForestClassifier", name
-        self.set_params(
-            {
-                "n_estimators": 1,
-                "max_features": max_features,
-                "max_leaf_nodes": max_leaf_nodes,
-                "sample": 1.0,
-                "max_depth": max_depth,
-                "min_samples_leaf": min_samples_leaf,
-                "min_info_gain": min_info_gain,
-                "nbins": nbins,
-            }
-        )
-
+        vmap_col: Optional[SQLExpression] = None,
+        limit: int = 100,
+        exclude_columns: Optional[SQLColumns] = None,
+    ) -> "vDataFrame":
+        """
+        Flatten the selected VMap. A new vDataFrame is returned.
+
+        \u26A0 Warning : This  function  might  have  a  long  runtime
+                         and can make your  vDataFrame less performant.
+                         It makes many calls to the MAPLOOKUP function,
+                         which can be slow if your VMap is large.
+
+        Parameters
+        ----------
+        vmap_col: SQLColumns, optional
+            List of VMap columns to flatten.
+        limit: int, optional
+            Maximum number of keys to consider for each VMap. Only the
+            most occurent keys are used.
+        exclude_columns: SQLColumns, optional
+            List of VMap columns to exclude.
+
+        Returns
+        -------
+        vDataFrame
+            object with the flattened VMaps.
+        """
+        vmap_col = format_type(vmap_col, dtype=list)
+        if not vmap_col:
+            vmap_col = []
+            all_cols = self.get_columns()
+            for col in all_cols:
+                if self[col].isvmap():
+                    vmap_col += [col]
+        exclude_columns = format_type(exclude_columns, dtype=list)
+        exclude_columns_final = quote_ident(exclude_columns, lower=True)
+        vmap_col_final = []
+        for col in vmap_col:
+            if quote_ident(col).lower() not in exclude_columns_final:
+                vmap_col_final += [col]
+        if not vmap_col:
+            raise EmptyParameter("No VMAP was detected.")
+        maplookup = []
+        for vmap in vmap_col_final:
+            keys = compute_vmap_keys(expr=self, vmap_col=vmap, limit=limit)
+            keys = [k[0] for k in keys]
+            for k in keys:
+                column = quote_ident(vmap)
+                alias = quote_ident(vmap.replace('"', "") + "." + k.replace('"', ""))
+                maplookup += [f"MAPLOOKUP({column}, '{k}') AS {alias}"]
+        return self.select(self.get_columns() + maplookup)
+
+    @save_verticapy_logs
+    def merge_similar_names(self, skip_word: Union[str, list[str]]) -> "vDataFrame":
+        """
+        Merges  columns with  similar names.  The function  generates
+        a COALESCE  statement that  merges the columns into a  single
+        column that excludes  the input words. Note that the order of
+        the variables in the COALESCE statement is based on the order
+        of the 'get_columns' method.
+
+        Parameters
+        ----------
+        skip_word: str / list, optional
+            List  of words to  exclude  from  the provided column  names.
+            For example,     if      two      columns      are     named
+            'age.information.phone'  and  'age.phone' AND  skip_word  is
+            set  to  ['.information'],  then  the  two  columns are
+            merged  together  with  the   following  COALESCE  statement:
+            COALESCE("age.phone", "age.information.phone") AS "age.phone"
+
+        Returns
+        -------
+        vDataFrame
+            An object containing the merged element.
+        """
+        columns = self.get_columns()
+        skip_word = format_type(skip_word, dtype=list)
+        group_dict = group_similar_names(columns, skip_word=skip_word)
+        sql = f"SELECT {gen_coalesce(group_dict)} FROM {self}"
+        return create_new_vdf(sql)
 
-# ---#
-class DecisionTreeRegressor(Regressor, Tree):
-    """
-    ---------------------------------------------------------------------------
-    A DecisionTreeRegressor made of a single tree.
-
-    Parameters
-    ----------
-    name: str
-            Name of the the model. The model will be stored in the DB.
-    max_features: str/int, optional
-            The number of randomly chosen features from which to pick the best
-        feature to split on a given tree node. It can be an integer or one
-        of the two following methods.
-                    auto : square root of the total number of predictors.
-                    max  : number of predictors.
-    max_leaf_nodes: int, optional
-            The maximum number of leaf nodes a tree in the forest can have, an
-        integer between 1 and 1e9, inclusive.
-    max_depth: int, optional
-            The maximum depth for growing each tree, an integer between 1 and 100,
-        inclusive.
-    min_samples_leaf: int, optional
-            The minimum number of samples each branch must have after splitting
-        a node, an integer between 1 and 1e6, inclusive. A split that causes
-        fewer remaining samples is discarded.
-    min_info_gain: float, optional
-            The minimum threshold for including a split, a float between 0.0 and
-        1.0, inclusive. A split with information gain less than this threshold
-        is discarded.
-    nbins: int, optional
-            The number of bins to use for continuous features, an integer between 2
-        and 1000, inclusive.
-    """
-
-    def __init__(
+    @save_verticapy_logs
+    def narrow(
         self,
-        name: str,
-        max_features: Union[int, str] = "auto",
-        max_leaf_nodes: int = 1e9,
-        max_depth: int = 100,
-        min_samples_leaf: int = 1,
-        min_info_gain: float = 0.0,
-        nbins: int = 32,
-    ):
-        version(condition=[9, 0, 1])
-        check_types([("name", name, [str])])
-        self.type, self.name = "RandomForestRegressor", name
-        self.set_params(
-            {
-                "n_estimators": 1,
-                "max_features": max_features,
-                "max_leaf_nodes": max_leaf_nodes,
-                "sample": 1.0,
-                "max_depth": max_depth,
-                "min_samples_leaf": min_samples_leaf,
-                "min_info_gain": min_info_gain,
-                "nbins": nbins,
-            }
-        )
+        index: SQLColumns,
+        columns: Optional[SQLColumns] = None,
+        col_name: str = "column",
+        val_name: str = "value",
+    ) -> "vDataFrame":
+        """
+        Returns the Narrow Table of the vDataFrame using the input
+        vDataColumns.
+
+        Parameters
+        ----------
+        index: SQLColumns
+            Index(es) used to identify the Row.
+        columns: SQLColumns, optional
+            List of the vDataColumns names. If empty, all vDataColumns
+            except the index(es) are used.
+        col_name: str, optional
+            Alias of the vDataColumn  representing the different input
+            vDataColumns names as categories.
+        val_name: str, optional
+            Alias of the vDataColumn  representing the different input
+            vDataColumns values.
+
+        Returns
+        -------
+        vDataFrame
+            the narrow table object.
+        """
+        index, columns = format_type(index, columns, dtype=list, na_out=self.numcol())
+        index, columns = self.format_colnames(index, columns)
+        for idx in index:
+            if idx in columns:
+                columns.remove(idx)
+        query = []
+        all_are_num, all_are_date = True, True
+        for column in columns:
+            if not self[column].isnum():
+                all_are_num = False
+            if not self[column].isdate():
+                all_are_date = False
+        for column in columns:
+            conv = ""
+            if not all_are_num and not all_are_date:
+                conv = "::varchar"
+            elif self[column].category() == "int":
+                conv = "::int"
+            column_str = column.replace("'", "''")[1:-1]
+            query += [
+                f"""
+                (SELECT 
+                    {', '.join(index)}, 
+                    '{column_str}' AS {col_name}, 
+                    {column}{conv} AS {val_name} 
+                FROM {self})"""
+            ]
+        query = " UNION ALL ".join(query)
+        return create_new_vdf(query)
 
+    melt = narrow
 
-# ---#
-class DummyTreeClassifier(MulticlassClassifier, Tree):
-    """
-    ---------------------------------------------------------------------------
-    A classifier that overfits the training data. These models are typically
-    used as a control to compare with your other models.
-
-    Parameters
-    ----------
-    name: str
-            Name of the the model. The model will be stored in the DB.
-    """
-
-    def __init__(self, name: str):
-        version(condition=[8, 1, 1])
-        check_types([("name", name, [str])])
-        self.type, self.name = "RandomForestClassifier", name
-        self.set_params(
-            {
-                "n_estimators": 1,
-                "max_features": "max",
-                "max_leaf_nodes": 1e9,
-                "sample": 1.0,
-                "max_depth": 100,
-                "min_samples_leaf": 1,
-                "min_info_gain": 0.0,
-                "nbins": 1000,
-            }
-        )
-
-
-# ---#
-class DummyTreeRegressor(Regressor, Tree):
-    """
-    ---------------------------------------------------------------------------
-    A regressor that overfits the training data. These models are typically
-    used as a control to compare with your other models.
-
-    Parameters
-    ----------
-    name: str
-            Name of the the model. The model will be stored in the DB.
-    """
-
-    def __init__(self, name: str):
-        version(condition=[9, 0, 1])
-        check_types([("name", name, [str])])
-        self.type, self.name = "RandomForestRegressor", name
-        self.set_params(
-            {
-                "n_estimators": 1,
-                "max_features": "max",
-                "max_leaf_nodes": 1e9,
-                "sample": 1.0,
-                "max_depth": 100,
-                "min_samples_leaf": 1,
-                "min_info_gain": 0.0,
-                "nbins": 1000,
-            }
+    @save_verticapy_logs
+    def pivot(
+        self,
+        index: str,
+        columns: str,
+        values: str,
+        aggr: str = "sum",
+        prefix: Optional[str] = None,
+    ) -> "vDataFrame":
+        """
+        Returns the Pivot of the vDataFrame using the input aggregation.
+
+        Parameters
+        ----------
+        index: str
+            vDataColumn used to group the elements.
+        columns: str
+            The vDataColumn used to compute the different categories,
+            which then act as the columns in the pivot table.
+        values: str
+            The vDataColumn whose values populate the new vDataFrame.
+        aggr: str, optional
+            Aggregation to use on 'values'.  To use complex aggregations,
+            you must use braces: {}. For example, to aggregate using the
+            aggregation: x -> MAX(x) - MIN(x), write "MAX({}) - MIN({})".
+        prefix: str, optional
+            The prefix for the pivot table's column names.
+
+        Returns
+        -------
+        vDataFrame
+            the pivot table object.
+        """
+        if isinstance(prefix, NoneType):
+            prefix = ""
+        index, columns, values = self.format_colnames(index, columns, values)
+        aggr = aggr.upper()
+        if "{}" not in aggr:
+            aggr += "({})"
+        new_cols = self[columns].distinct()
+        new_cols_trans = []
+        for col in new_cols:
+            if isinstance(col, NoneType):
+                new_cols_trans += [
+                    aggr.replace(
+                        "{}",
+                        f"(CASE WHEN {columns} IS NULL THEN {values} ELSE NULL END)",
+                    )
+                    + f"AS '{prefix}NULL'"
+                ]
+            else:
+                new_cols_trans += [
+                    aggr.replace(
+                        "{}",
+                        f"(CASE WHEN {columns} = '{col}' THEN {values} ELSE NULL END)",
+                    )
+                    + f"AS '{prefix}{col}'"
+                ]
+        return create_new_vdf(
+            f"""
+            SELECT 
+                {index},
+                {", ".join(new_cols_trans)}
+            FROM {self}
+            GROUP BY 1""",
         )
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/learn/tsa.py` & `verticapy-1.0.0b1/verticapy/core/vdataframe/_plotting.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,1794 +1,2157 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# Standard Python Modules
-import math, warnings
-from typing import Union
-
-# VerticaPy Modules
-from verticapy.learn.vmodel import *
-from verticapy.learn.linear_model import LinearRegression
-from verticapy import vDataFrame
-from verticapy.plot import gen_colors
-from verticapy.learn.tools import *
-
-# Other Python Modules
-from dateutil.parser import parse
-import matplotlib.pyplot as plt
-
-# ---#
-class SARIMAX(Regressor):
-    """
----------------------------------------------------------------------------
-[Beta Version]
-Creates an SARIMAX object using the Vertica Linear Regression algorithm on 
-the data.
-
-Parameters
-----------
-name: str
-    Name of the the model. The model will be stored in the DB.
-p: int, optional
-    Order of the AR (Auto-Regressive) part.
-d: int, optional
-    Order of the I (Integrated) part.
-q: int, optional
-    Order of the MA (Moving-Average) part.
-P: int, optional
-    Order of the seasonal AR (Auto-Regressive) part.
-D: int, optional
-    Order of the seasonal I (Integrated) part.
-Q: int, optional
-    Order of the seasonal MA (Moving-Average) part.
-s: int, optional
-    Span of the seasonality.
-tol: float, optional
-    Determines whether the algorithm has reached the specified accuracy result.
-max_iter: int, optional
-    Determines the maximum number of iterations the algorithm performs before 
-    achieving the specified accuracy result.
-solver: str, optional
-    The optimizer method to use to train the model. 
-        Newton : Newton Method
-        BFGS   : Broyden Fletcher Goldfarb Shanno
-max_pik: int, optional
-    Number of inverse MA coefficient used to approximate the MA.
-papprox_ma: int, optional
-    the p of the AR(p) used to approximate the MA coefficients.
-    """
-
-    def __init__(
-        self,
-        name: str,
-        p: int = 0,
-        d: int = 0,
-        q: int = 0,
-        P: int = 0,
-        D: int = 0,
-        Q: int = 0,
-        s: int = 0,
-        tol: float = 1e-4,
-        max_iter: int = 1000,
-        solver: str = "Newton",
-        max_pik: int = 100,
-        papprox_ma: int = 200,
-    ):
-        check_types([("name", name, [str])])
-        self.type, self.name = "SARIMAX", name
-        self.set_params(
-            {
-                "p": p,
-                "d": d,
-                "q": q,
-                "P": P,
-                "D": D,
-                "Q": Q,
-                "s": s,
-                "tol": tol,
-                "max_iter": max_iter,
-                "solver": solver,
-                "max_pik": max_pik,
-                "papprox_ma": papprox_ma,
-            }
-        )
-        if self.parameters["s"] == 0:
-            assert (
-                self.parameters["D"] == 0
-                and self.parameters["P"] == 0
-                and self.parameters["Q"] == 0
-            ), ParameterError(
-                "In case of non-seasonality (s = 0), all the parameters P, D or Q must be equal to 0."
-            )
-        else:
-            assert (
-                self.parameters["D"] > 0
-                or self.parameters["P"] > 0
-                or self.parameters["Q"] > 0
-            ), ParameterError(
-                "In case of seasonality (s > 0), at least one of the parameters P, D or Q must be strictly greater than 0."
-            )
-        version(condition=[8, 0, 0])
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import math
+from typing import Callable, Literal, Optional, Union
+from collections.abc import Iterable
+import numpy as np
+
+import verticapy._config.config as conf
+from verticapy._utils._object import get_vertica_mllib
+from verticapy._typing import (
+    ArrayLike,
+    ColorType,
+    NoneType,
+    PlottingMethod,
+    PlottingObject,
+    PythonNumber,
+    PythonScalar,
+    SQLColumns,
+)
+from verticapy._utils._gen import gen_tmp_name
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import format_type
+from verticapy._utils._sql._sys import _executeSQL
+
+from verticapy.core.tablesample.base import TableSample
+
+from verticapy.core.vdataframe._machine_learning import vDFMachineLearning
+from verticapy.core.vdataframe._normalize import vDCNorm
 
-    # ---#
-    def deploySQL(self):
-        """
-    ---------------------------------------------------------------------------
-    Returns the SQL code needed to deploy the model.
+from verticapy.plotting.base import PlottingBase
 
-    Returns
-    -------
-    str
-        the SQL code needed to deploy the model.
-        """
-        sql = self.deploy_predict_
-        if (self.parameters["d"] > 0) or (
-            self.parameters["D"] > 0 and self.parameters["s"] > 0
-        ):
-            for i in range(0, self.parameters["d"] + 1):
-                for k in range(
-                    0, max((self.parameters["D"] + 1) * min(1, self.parameters["s"]), 1)
-                ):
-                    if (k, i) != (0, 0):
-                        comb_i_d = (
-                            math.factorial(self.parameters["d"])
-                            / math.factorial(self.parameters["d"] - i)
-                            / math.factorial(i)
-                        )
-                        comb_k_D = (
-                            math.factorial(self.parameters["D"])
-                            / math.factorial(self.parameters["D"] - k)
-                            / math.factorial(k)
-                        )
-                        sql += " + {} * LAG(VerticaPy_y_copy, {}) OVER (ORDER BY [VerticaPy_ts])".format(
-                            (-1) ** (i + k + 1) * comb_i_d * comb_k_D,
-                            i + self.parameters["s"] * k,
-                        )
-        return sql
-
-    # ---#
-    def fpredict(self, L: list):
-        """
-    ---------------------------------------------------------------------------
-    Computes the prediction.
-
-    Parameters
-    ----------
-    L: list
-        List containing the data. It must be a two-dimensional list containing 
-        multiple rows. Each row must include as first element the ordered predictor
-        and as nth elements the nth - 1 exogenous variable (nth > 2). 
-
-    Returns
-    -------
-    float
-        the prediction.
-        """
-
-        def sub_arp(L: list):
-            L_final = []
-            for i in range(len(L)):
-                result = L[-i]
-                for i in range(len(self.coef_.values["coefficient"])):
-                    elem = self.coef_.values["predictor"][i]
-                    if elem.lower() == "intercept":
-                        result -= self.coef_.values["coefficient"][i]
-                    elif elem.lower()[0:2] == "ar":
-                        nb = int(elem[2:])
-                        try:
-                            result -= self.coef_.values["coefficient"][i] * L[-nb]
-                        except:
-                            result = None
-                    L_final = [result] + L_final
-            return L_final
-
-        def fepsilon(L: list):
-            if self.parameters["p"] > 0 or self.parameters["P"] > 0:
-                L_tmp = sub_arp(L)
-            else:
-                L_tmp = L
-            try:
-                result = L_tmp[-1] - self.ma_avg_
-                for i in range(1, self.parameters["max_pik"]):
-                    result -= self.ma_piq_.values["coefficient"][i] * (
-                        L_tmp[-i] - self.ma_avg_
-                    )
-                return result
-            except:
-                return 0
-
-        if (
-            self.parameters["p"] == 0
-            and self.parameters["q"] == 0
-            and self.parameters["d"] == 0
-            and self.parameters["s"] == 0
-            and not (self.exogenous)
-        ):
-            return self.ma_avg_
-        try:
-            yt = [elem[0] for elem in L]
-            yt_copy = [elem[0] for elem in L]
-            yt.reverse()
-            if self.parameters["d"] > 0:
-                for i in range(self.parameters["d"]):
-                    yt = [yt[i - 1] - yt[i] for i in range(1, len(yt))]
-            if self.parameters["D"] > 0 and self.parameters["s"] > 0:
-                for i in range(self.parameters["D"]):
-                    yt = [
-                        yt[i - self.parameters["s"]] - yt[i]
-                        for i in range(self.parameters["s"], len(yt))
-                    ]
-            yt.reverse()
-            result, j = 0, 1
-            for i in range(len(self.coef_.values["coefficient"])):
-                elem = self.coef_.values["predictor"][i]
-                if elem.lower() == "intercept":
-                    result += self.coef_.values["coefficient"][i]
-                elif elem.lower()[0:2] == "ar":
-                    nb = int(elem[2:])
-                    result += self.coef_.values["coefficient"][i] * yt[-nb]
-                elif elem.lower()[0:2] == "ma":
-                    nb = int(elem[2:])
-                    result += self.coef_.values["coefficient"][i] * fepsilon(
-                        yt[: -nb - 1]
-                    )
-                else:
-                    result += self.coef_.values["coefficient"][i] * L[-1][j]
-                    j += 1
-            for i in range(0, self.parameters["d"] + 1):
-                for k in range(
-                    0, max((self.parameters["D"] + 1) * min(1, self.parameters["s"]), 1)
-                ):
-                    if (k, i) != (0, 0):
-                        comb_i_d = (
-                            math.factorial(self.parameters["d"])
-                            / math.factorial(self.parameters["d"] - i)
-                            / math.factorial(i)
-                        )
-                        comb_k_D = (
-                            math.factorial(self.parameters["D"])
-                            / math.factorial(self.parameters["D"] - k)
-                            / math.factorial(k)
-                        )
-                        result += (
-                            (-1) ** (i + k + 1)
-                            * comb_i_d
-                            * comb_k_D
-                            * yt_copy[-(i + self.parameters["s"] * k)]
-                        )
-            return result
-        except:
-            return None
 
-    # ---#
-    def fit(
+class vDFPlot(vDFMachineLearning):
+    # Boxplots.
+
+    @save_verticapy_logs
+    def boxplot(
         self,
-        input_relation: Union[vDataFrame, str],
-        y: str,
-        ts: str,
-        X: list = [],
-        test_relation: Union[vDataFrame, str] = "",
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Trains the model.
-
-    Parameters
-    ----------
-    input_relation: str/vDataFrame
-        Training relation.
-    y: str
-        Response column.
-    ts: str
-        vcolumn used to order the data.
-    X: list, optional
-        exogenous columns used to fit the model.
-    test_relation: str/vDataFrame, optional
-        Relation used to test the model.
-
-    Returns
-    -------
-    object
-        model
-        """
-        check_types(
-            [
-                ("input_relation", input_relation, [str, vDataFrame]),
-                ("y", y, [str]),
-                ("test_relation", test_relation, [str, vDataFrame]),
-                ("ts", ts, [str]),
-            ]
-        )
-        # Initialization
-        if verticapy.options["overwrite_model"]:
-            self.drop()
+        columns: Optional[SQLColumns] = None,
+        q: tuple[float, float] = (0.25, 0.75),
+        max_nb_fliers: int = 30,
+        whis: float = 1.5,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the Box Plot of the input vDataColumns.
+
+        Parameters
+        ----------
+        columns: SQLColumns, optional
+            List  of the vDataColumns names.  If  empty, all
+            numerical vDataColumns are used.
+        q: tuple, optional
+            Tuple including the 2 quantiles used to draw the
+            BoxPlot.
+        max_nb_fliers: int, optional
+            Maximum number of points to use to represent the
+            fliers  of each category.  Drawing  fliers  will
+            slow down the graphic computation.
+        whis: float, optional
+            The position of the whiskers.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional parameter to  pass to the plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns = format_type(columns, dtype=list)
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="BoxPlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.BoxPlot(
+            vdf=self,
+            columns=columns,
+            q=q,
+            whis=whis,
+            max_nb_fliers=max_nb_fliers,
+        ).draw(**kwargs)
+
+    # 2D / ND CHARTS.
+
+    @save_verticapy_logs
+    def bar(
+        self,
+        columns: SQLColumns,
+        method: PlottingMethod = "density",
+        of: Optional[str] = None,
+        max_cardinality: tuple[int, int] = (6, 6),
+        h: tuple[PythonNumber, PythonNumber] = (None, None),
+        kind: Literal["auto", "drilldown", "stacked"] = "auto",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the bar chart of the input vDataColumns based
+        on an aggregation.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of  the vDataColumns names.  The list must
+            have one or two elements.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage  of  the  distribution.
+                mean    : Average  of the  vDataColumn 'of'.
+                min     : Minimum  of the  vDataColumn 'of'.
+                max     : Maximum  of the  vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of'
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation, for example:
+            AVG(column1) + 5
+        of: str, optional
+            The  vDataColumn used to compute the  aggregation.
+        max_cardinality: tuple, optional
+            Maximum number of distinct elements for vDataColumns
+            1  and  2  to be used as categorical. For these
+            elements, no  h is picked or computed.
+        h: tuple, optional
+            Interval width of  the vDataColumns 1 and 2 bars.
+            Only  valid if the  vDataColumns are  numerical.
+            Optimized  h will be  computed  if the parameter  is
+            empty or invalid.
+        kind: str, optional
+            The BarChart Type.
+                auto      : Regular  BarChart  based on  1  or 2
+                            vDataColumns.
+                drilldown : Drill   Down  BarChart  based  on  2
+                            vDataColumns.
+                stacked   : Stacked   BarChart    based    on  2
+                            vDataColumns.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns = format_type(columns, dtype=list)
+        columns, of = self.format_colnames(columns, of, expected_nb_of_cols=[1, 2])
+        if len(columns) == 1:
+            return self[columns[0]].bar(
+                method=method,
+                of=of,
+                max_cardinality=max_cardinality[0],
+                h=h[0],
+                **style_kwargs,
+            )
+        elif kind == "drilldown":
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="DrillDownBarChart",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            return vpy_plt.DrillDownBarChart(
+                vdf=self,
+                columns=columns,
+                method=method,
+                of=of,
+                h=h,
+                max_cardinality=max_cardinality,
+            ).draw(**kwargs)
         else:
-            does_model_exist(name=self.name, raise_error=True)
-        self.input_relation = (
-            input_relation
-            if isinstance(input_relation, str)
-            else input_relation.__genSQL__()
-        )
-        if isinstance(test_relation, vDataFrame):
-            self.test_relation = test_relation.__genSQL__()
-        elif test_relation:
-            self.test_relation = test_relation
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="BarChart2D",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            return vpy_plt.BarChart2D(
+                vdf=self,
+                columns=columns,
+                method=method,
+                of=of,
+                h=h,
+                max_cardinality=max_cardinality,
+                misc_layout={"kind": kind},
+            ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def barh(
+        self,
+        columns: SQLColumns,
+        method: PlottingMethod = "density",
+        of: Optional[str] = None,
+        max_cardinality: tuple[int, int] = (6, 6),
+        h: tuple[PythonNumber, PythonNumber] = (None, None),
+        kind: Literal[
+            "auto",
+            "fully_stacked",
+            "stacked",
+            "fully",
+            "fully stacked",
+            "pyramid",
+            "density",
+        ] = "auto",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws  the  horizontal  bar  chart  of  the  input
+        vDataColumns based on an aggregation.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of  the vDataColumns names.  The list must
+            have one or two elements.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage  of  the  distribution.
+                mean    : Average  of the  vDataColumn 'of'.
+                min     : Minimum  of the  vDataColumn 'of'.
+                max     : Maximum  of the  vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of'
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation, for example:
+            AVG(column1) + 5
+        of: str, optional
+            The  vDataColumn used to compute the  aggregation.
+        max_cardinality: tuple, optional
+            Maximum number of distinct elements for vDataColumns
+            1  and  2  to be used as categorical. For these
+            elements, no  h is picked or computed.
+        h: tuple, optional
+            Interval width of  the vDataColumns 1 and 2 bars.
+            Only  valid if the  vDataColumns are  numerical.
+            Optimized  h will be  computed  if the parameter  is
+            empty or invalid.
+        kind: str, optional
+            The BarChart Type.
+                auto          : Regular Bar Chart  based on 1 or 2
+                                vDataColumns.
+                pyramid       : Pyramid  Density  Bar  Chart. Only
+                                works   if    one   of   the   two
+                                vDataColumns  is  binary  and  the
+                                'method' is set to 'density'.
+                stacked       : Stacked  Bar  Chart   based  on  2
+                                vDataColumns.
+                fully_stacked : Fully Stacked Bar Chart based on 2
+                                vDataColumns.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns = format_type(columns, dtype=list)
+        columns, of = self.format_colnames(columns, of, expected_nb_of_cols=[1, 2])
+        if len(columns) == 1:
+            return self[columns[0]].barh(
+                method=method,
+                of=of,
+                max_cardinality=max_cardinality[0],
+                h=h[0],
+                chart=chart,
+                **style_kwargs,
+            )
+        elif kind == "drilldown":
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="DrillDownHorizontalBarChart",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            return vpy_plt.DrillDownHorizontalBarChart(
+                vdf=self,
+                columns=columns,
+                method=method,
+                of=of,
+                h=h,
+                max_cardinality=max_cardinality,
+            ).draw(**kwargs)
         else:
-            self.test_relation = self.input_relation
-        self.y, self.ts, self.deploy_predict_ = quote_ident(y), quote_ident(ts), ""
-        self.coef_ = tablesample({"predictor": [], "coefficient": []})
-        self.ma_avg_, self.ma_piq_ = None, None
-        X, schema = [quote_ident(elem) for elem in X], schema_relation(self.name)[0]
-        self.X, self.exogenous = [], X
-        relation = (
-            "(SELECT *, [VerticaPy_y] AS VerticaPy_y_copy FROM {}) VERTICAPY_SUBTABLE "
-        )
-        model = LinearRegression(
-            name=self.name,
-            solver=self.parameters["solver"],
-            max_iter=self.parameters["max_iter"],
-            tol=self.parameters["tol"],
-        )
-
-        if (
-            self.parameters["p"] == 0
-            and self.parameters["q"] == 0
-            and self.parameters["d"] == 0
-            and self.parameters["s"] == 0
-            and not (self.exogenous)
-        ):
-            query = "SELECT AVG({}) FROM {}".format(self.y, self.input_relation)
-            self.ma_avg_ = executeSQL(
-                query, method="fetchfirstelem", print_time_sql=False
-            )
-            self.deploy_predict_ = str(self.ma_avg_)
+            if kind in ("fully", "fully stacked"):
+                kind = "fully_stacked"
+            elif kind == "pyramid":
+                kind = "density"
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="HorizontalBarChart2D",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            return vpy_plt.HorizontalBarChart2D(
+                vdf=self,
+                columns=columns,
+                method=method,
+                of=of,
+                max_cardinality=max_cardinality,
+                h=h,
+                misc_layout={"kind": kind},
+            ).draw(**kwargs)
 
-        # I(d)
-        if self.parameters["d"] > 0:
-            for i in range(self.parameters["d"]):
-                relation = "(SELECT [VerticaPy_y] - LAG([VerticaPy_y], 1) OVER (ORDER BY [VerticaPy_ts]) AS [VerticaPy_y], VerticaPy_y_copy[VerticaPy_key_columns] FROM {}) VERTICAPY_SUBTABLE".format(
-                    relation
-                )
-        if self.parameters["D"] > 0 and self.parameters["s"] > 0:
-            for i in range(self.parameters["D"]):
-                relation = "(SELECT [VerticaPy_y] - LAG([VerticaPy_y], {}) OVER (ORDER BY [VerticaPy_ts]) AS [VerticaPy_y], VerticaPy_y_copy[VerticaPy_key_columns] FROM {}) VERTICAPY_SUBTABLE".format(
-                    self.parameters["s"], relation
-                )
-        # AR(p)
-        if self.parameters["p"] > 0 or self.parameters["P"] > 0:
-            columns = [
-                "LAG([VerticaPy_y], {}) OVER (ORDER BY [VerticaPy_ts]) AS AR{}".format(
-                    i, i
-                )
-                for i in range(1, self.parameters["p"] + 1)
-            ]
-            AR = ["AR{}".format(i) for i in range(1, self.parameters["p"] + 1)]
-            if self.parameters["s"] > 0:
-                for i in range(1, self.parameters["P"] + 1):
-                    if (i * self.parameters["s"]) not in (
-                        range(1, self.parameters["p"] + 1)
-                    ):
-                        columns += [
-                            "LAG([VerticaPy_y], {}) OVER (ORDER BY [VerticaPy_ts]) AS AR{}".format(
-                                i * self.parameters["s"], i * self.parameters["s"]
-                            )
-                        ]
-                        AR += ["AR{}".format(i * self.parameters["s"])]
-            relation = "(SELECT *, {} FROM {}) VERTICAPY_SUBTABLE".format(
-                ", ".join(columns), relation
-            )
-            view_name = gen_tmp_name(schema=schema, name="linear_reg")
-            drop(view_name, method="view")
-            query = "CREATE VIEW {} AS SELECT * FROM {}".format(
-                view_name,
-                relation.format(self.input_relation)
-                .replace("[VerticaPy_ts]", self.ts)
-                .replace("[VerticaPy_y]", self.y)
-                .replace("[VerticaPy_key_columns]", ", " + ", ".join([self.ts] + X)),
-            )
-            try:
-                executeSQL(query, print_time_sql=False)
-                self.X += AR + X
-                model.fit(
-                    input_relation=view_name, X=self.X, y=self.y,
-                )
-            except:
-                drop(view_name, method="view")
-                raise
-            drop(view_name, method="view")
-            self.coef_.values["predictor"] = model.coef_.values["predictor"]
-            self.coef_.values["coefficient"] = model.coef_.values["coefficient"]
-            alphaq = model.coef_.values["coefficient"]
-            model.drop()
-            epsilon_final = (
-                "[VerticaPy_y] - "
-                + str(alphaq[0])
-                + " - "
-                + " - ".join(
-                    [
-                        str(alphaq[i])
-                        + " * "
-                        + "LAG([VerticaPy_y], {}) OVER (ORDER BY [VerticaPy_ts])".format(
-                            i
-                        )
-                        for i in range(1, self.parameters["p"] + 1)
-                    ]
-                )
-            )
-            self.deploy_predict_ = (
-                str(alphaq[0])
-                + " + "
-                + " + ".join(
-                    [
-                        str(alphaq[i])
-                        + " * "
-                        + "LAG(VerticaPy_y_copy, {}) OVER (ORDER BY [VerticaPy_ts])".format(
-                            i
-                        )
-                        for i in range(1, self.parameters["p"] + 1)
-                    ]
-                )
-            )
-            if self.parameters["s"] > 0 and self.parameters["P"] > 0:
-                epsilon_final += " - " + " - ".join(
-                    [
-                        str(alphaq[i])
-                        + " * "
-                        + "LAG([VerticaPy_y], {}) OVER (ORDER BY [VerticaPy_ts])".format(
-                            i * self.parameters["s"]
-                        )
-                        for i in range(
-                            self.parameters["p"] + 1,
-                            self.parameters["p"]
-                            + (self.parameters["P"] if self.parameters["s"] > 0 else 0)
-                            + 1,
-                        )
-                    ]
-                )
-                self.deploy_predict_ += " + " + " + ".join(
-                    [
-                        str(alphaq[i])
-                        + " * "
-                        + "LAG(VerticaPy_y_copy, {}) OVER (ORDER BY [VerticaPy_ts])".format(
-                            i * self.parameters["s"]
-                        )
-                        for i in range(
-                            self.parameters["p"] + 1,
-                            self.parameters["p"]
-                            + (self.parameters["P"] if self.parameters["s"] > 0 else 0)
-                            + 1,
-                        )
-                    ]
-                )
-            for idx, elem in enumerate(X):
-                epsilon_final += " - {} * [X{}]".format(
-                    alphaq[
-                        idx
-                        + self.parameters["p"]
-                        + (self.parameters["P"] if self.parameters["s"] > 0 else 0)
-                        + 1
-                    ],
-                    idx,
-                )
-                self.deploy_predict_ += " + {} * [X{}]".format(
-                    alphaq[
-                        idx
-                        + self.parameters["p"]
-                        + (self.parameters["P"] if self.parameters["s"] > 0 else 0)
-                        + 1
-                    ],
-                    idx,
-                )
-            relation = "(SELECT {} AS [VerticaPy_y], {}, VerticaPy_y_copy[VerticaPy_key_columns] FROM {}) VERTICAPY_SUBTABLE".format(
-                epsilon_final, ", ".join(AR), relation
-            )
+    @save_verticapy_logs
+    def pie(
+        self,
+        columns: SQLColumns,
+        max_cardinality: Union[None, int, tuple] = None,
+        h: Union[None, int, tuple] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the nested density pie chart of the input
+        vDataColumns.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of the vDataColumns names.
+        max_cardinality: int / tuple, optional
+            Maximum number of distinct elements for
+            vDataColumns 1  and  2  to be used as
+            categorical. For these elements, no  h
+            is picked or computed.
+            If  of type tuple, represents the
+            'max_cardinality' of each column.
+        h: int / tuple, optional
+            Interval  width  of the bar. If empty,  an
+            optimized h will be computed.
+            If  of type tuple, it must represent  each
+            column's 'h'.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass to  the
+            plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="NestedPieChart",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.NestedPieChart(
+            vdf=self,
+            columns=columns,
+            max_cardinality=max_cardinality,
+            h=h,
+            method="count",
+        ).draw(**kwargs)
 
-        # MA(q)
-        if self.parameters["q"] > 0 or (
-            self.parameters["Q"] > 0 and self.parameters["s"] > 0
-        ):
-            transform_relation = relation.replace("[VerticaPy_y]", y).replace(
-                "[VerticaPy_ts]", ts
-            )
-            transform_relation = transform_relation.replace(
-                "[VerticaPy_key_columns]", ", " + ", ".join(X + [ts])
-            )
-            for idx, elem in enumerate(X):
-                transform_relation = transform_relation.replace(
-                    "[X{}]".format(idx), elem
-                )
-            query = "SELECT COUNT(*), AVG({}) FROM {}".format(
-                self.y, transform_relation.format(self.input_relation)
-            )
-            result = executeSQL(query, method="fetchrow", print_time_sql=False)
-            self.ma_avg_ = result[1]
-            n = result[0]
-            n = max(
-                max(
-                    min(max(n ** (1.0 / 3.0), 8), self.parameters["papprox_ma"]),
-                    self.parameters["q"],
-                ),
-                self.parameters["Q"] * self.parameters["s"] + 1,
-            )
-            n = int(n)
-            columns = [
-                "LAG([VerticaPy_y], {}) OVER (ORDER BY [VerticaPy_ts]) AS ARq{}".format(
-                    i, i
-                )
-                for i in range(1, n)
-            ]
-            ARq = ["ARq{}".format(i) for i in range(1, n)]
-            tmp_relation = "(SELECT *, {} FROM {}) VERTICAPY_SUBTABLE".format(
-                ", ".join(columns), relation
-            )
-            for idx, elem in enumerate(X):
-                tmp_relation = tmp_relation.replace("[X{}]".format(idx), elem)
-            drop(view_name, method="view")
-            query = "CREATE VIEW {} AS SELECT * FROM {}".format(
-                view_name,
-                tmp_relation.format(self.input_relation)
-                .replace("[VerticaPy_ts]", self.ts)
-                .replace("[VerticaPy_y]", self.y)
-                .replace("[VerticaPy_key_columns]", ", " + ", ".join([self.ts] + X)),
-            )
-            try:
-                executeSQL(query, print_time_sql=False)
-                model.fit(
-                    input_relation=view_name, X=ARq, y=self.y,
-                )
-            except:
-                drop(view_name, method="view")
-                raise
-            drop(view_name, method="view")
-            if not (self.coef_.values["predictor"]):
-                self.coef_.values["predictor"] += ["Intercept"]
-                self.coef_.values["coefficient"] += [self.ma_avg_]
-                self.deploy_predict_ = str(self.ma_avg_)
-            alphaq = model.coef_.values["coefficient"][1:]
-            model.drop()
-            thetaq, piq = [], [-1] + []
-            for j in range(0, len(alphaq)):
-                thetaq += [
-                    sum([alphaq[j - i - 1] * thetaq[i] for i in range(0, j)])
-                    + alphaq[j]
-                ]
-            for j in range(self.parameters["q"]):
-                self.coef_.values["predictor"] += ["ma{}".format(j + 1)]
-                self.coef_.values["coefficient"] += [thetaq[j]]
-                self.deploy_predict_ += " + {} * MA{}".format(thetaq[j], j + 1)
-            if self.parameters["s"] > 0:
-                for j in range(1, self.parameters["Q"] + 1):
-                    self.coef_.values["predictor"] += [
-                        "ma{}".format(self.parameters["s"] * j)
-                    ]
-                    self.coef_.values["coefficient"] += [
-                        thetaq[self.parameters["s"] * j - 1]
-                    ]
-                    self.deploy_predict_ += " + {} * MA{}".format(
-                        thetaq[self.parameters["s"] * j - 1], self.parameters["s"] * j
-                    )
-            for j in range(0, self.parameters["max_pik"]):
-                piq_tmp = 0
-                for i in range(0, self.parameters["q"]):
-                    if j - i > 0:
-                        piq_tmp -= thetaq[i] * piq[j - i]
-                    elif j - i == 0:
-                        piq_tmp -= thetaq[i]
-                piq = piq + [piq_tmp]
-            self.ma_piq_ = tablesample({"coefficient": piq})
-            epsilon = (
-                "[VerticaPy_y] - "
-                + str(self.ma_avg_)
-                + " - "
-                + " - ".join(
-                    [
-                        str((piq[i]))
-                        + " * "
-                        + "LAG([VerticaPy_y] - {}, {}) OVER (ORDER BY [VerticaPy_ts])".format(
-                            self.ma_avg_, i
-                        )
-                        for i in range(1, self.parameters["max_pik"])
-                    ]
-                )
-            )
-            epsilon += " AS MA0"
-            relation = "(SELECT *, {} FROM {}) VERTICAPY_SUBTABLE".format(
-                epsilon, relation
-            )
-            columns = [
-                "LAG(MA0, {}) OVER (ORDER BY [VerticaPy_ts]) AS MA{}".format(i, i)
-                for i in range(1, self.parameters["q"] + 1)
-            ]
-            MA = ["MA{}".format(i) for i in range(1, self.parameters["q"] + 1)]
-            if self.parameters["s"] > 0:
-                columns += [
-                    "LAG(MA0, {}) OVER (ORDER BY [VerticaPy_ts]) AS MA{}".format(
-                        i * self.parameters["s"], i * self.parameters["s"]
-                    )
-                    for i in range(1, self.parameters["Q"] + 1)
-                ]
-                MA += [
-                    "MA{}".format(i * self.parameters["s"])
-                    for i in range(1, self.parameters["Q"] + 1)
-                ]
-            relation = "(SELECT *, {} FROM {}) VERTICAPY_SUBTABLE".format(
-                ", ".join(columns), relation
-            )
-            self.X += MA
-            transform_relation = relation.replace("[VerticaPy_y]", y).replace(
-                "[VerticaPy_ts]", ts
-            )
-            transform_relation = transform_relation.replace(
-                "[VerticaPy_key_columns]", ", " + ", ".join(X + [ts])
-            )
-            for idx, elem in enumerate(X):
-                transform_relation = transform_relation.replace(
-                    "[X{}]".format(idx), elem
-                )
-        self.transform_relation = relation
-        model_save = {
-            "type": "SARIMAX",
-            "input_relation": self.input_relation,
-            "test_relation": self.test_relation,
-            "transform_relation": self.transform_relation,
-            "deploy_predict": self.deploy_predict_,
-            "ma_avg": self.ma_avg_,
-            "ma_piq": self.ma_piq_.values if (self.ma_piq_) else None,
-            "X": self.X,
-            "y": self.y,
-            "ts": self.ts,
-            "exogenous": self.exogenous,
-            "coef": self.coef_.values,
-            "p": self.parameters["p"],
-            "d": self.parameters["d"],
-            "q": self.parameters["q"],
-            "P": self.parameters["P"],
-            "D": self.parameters["D"],
-            "Q": self.parameters["Q"],
-            "s": self.parameters["s"],
-            "tol": self.parameters["tol"],
-            "max_iter": self.parameters["max_iter"],
-            "solver": self.parameters["solver"],
-            "max_pik": self.parameters["max_pik"],
-            "papprox_ma": self.parameters["papprox_ma"],
-        }
-        insert_verticapy_schema(
-            model_name=self.name, model_type="SARIMAX", model_save=model_save,
+    # Histogram & Density.
+
+    @save_verticapy_logs
+    def hist(
+        self,
+        columns: SQLColumns,
+        method: PlottingMethod = "density",
+        of: Optional[str] = None,
+        h: Optional[PythonNumber] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws  the  histograms  of  the  input vDataColumns
+        based on an aggregation.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of  the vDataColumns names.  The list must
+            have less than 5 elements.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage  of  the  distribution.
+                mean    : Average  of the  vDataColumn 'of'.
+                min     : Minimum  of the  vDataColumn 'of'.
+                max     : Maximum  of the  vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of'
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation, for example:
+            AVG(column1) + 5
+        of: str, optional
+            The  vDataColumn used to compute the  aggregation.
+        max_cardinality: tuple, optional
+            Maximum number of distinct elements for vDataColumns
+            to be used as categorical. For these elements, no
+            h is picked or computed.
+        h: tuple, optional
+            Interval width of the  input vDataColumns. Optimized
+            h  will be  computed if  the  parameter  is empty or
+            invalid.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="Histogram",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.Histogram(
+            vdf=self,
+            columns=columns,
+            method=method,
+            of=of,
+            h=h,
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def density(
+        self,
+        columns: Optional[SQLColumns] = None,
+        bandwidth: float = 1.0,
+        kernel: Literal["gaussian", "logistic", "sigmoid", "silverman"] = "gaussian",
+        nbins: int = 50,
+        xlim: list[tuple[float, float]] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the vDataColumns Density Plot.
+
+        Parameters
+        ----------
+        columns: SQLColumns, optional
+            List  of the vDataColumns names.  If  empty,
+            all numerical vDataColumns are selected.
+        bandwidth: float, optional
+            The bandwidth of the kernel.
+        kernel: str, optional
+            The method used for the plot.
+                gaussian  : Gaussian Kernel.
+                logistic  : Logistic Kernel.
+                sigmoid   : Sigmoid Kernel.
+                silverman : Silverman Kernel.
+        nbins: int, optional
+            Maximum  number of  points used to  evaluate
+            the approximate density function.
+            Increasing  this  parameter increases  the
+            precision  but also increases the time of the
+            learning and the scoring phases.
+        xlim: list of tuple, optional
+            Set the x limits of the current axes.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vml = get_vertica_mllib()
+        columns = format_type(columns, dtype=list)
+        columns = self.format_colnames(columns)
+        if not columns:
+            columns = self.numcol()
+        if not columns:
+            raise ValueError("No numerical columns found.")
+        name = gen_tmp_name(schema=conf.get_option("temp_schema"), name="kde")
+        if not xlim:
+            xmin = min(self[columns].min()["min"])
+            xmax = max(self[columns].max()["max"])
+            xlim_ = [(xmin, xmax)]
+        elif isinstance(xlim, tuple):
+            xlim_ = [xlim]
+        else:
+            xlim_ = xlim
+        model = vml.KernelDensity(
+            name=name,
+            bandwidth=bandwidth,
+            kernel=kernel,
+            nbins=nbins,
+            xlim=xlim_,
+            store=False,
         )
-        return self
+        if len(columns) == 1:
+            try:
+                model.fit(self, columns)
+                return model.plot(chart=chart, **style_kwargs)
+            finally:
+                model.drop()
+        else:
+            X, Y = [], []
+            for column in columns:
+                try:
+                    model.fit(self, [column])
+                    data, layout = model._compute_plot_params()
+                    X += [data["x"]]
+                    Y += [data["y"]]
+                finally:
+                    model.drop()
+            X = np.column_stack(X)
+            Y = np.column_stack(Y)
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="MultiDensityPlot",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            data = {"X": X, "Y": Y}
+            layout = {
+                "title": "KernelDensity",
+                "x_label": None,
+                "y_label": "density",
+                "labels": np.array(columns),
+                "labels_title": None,
+            }
+            return vpy_plt.MultiDensityPlot(data=data, layout=layout).draw(**kwargs)
+
+    # Time Series.
 
-    # ---#
+    @save_verticapy_logs
     def plot(
         self,
-        vdf: vDataFrame = None,
-        y: str = "",
-        ts: str = "",
-        X: list = [],
-        dynamic: bool = False,
-        one_step: bool = True,
-        observed: bool = True,
-        confidence: bool = True,
-        nlead: int = 10,
-        nlast: int = 0,
-        limit: int = 1000,
-        ax=None,
-        **style_kwds
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Draws the SARIMAX model.
-
-    Parameters
-    ----------
-    vdf: vDataFrame, optional
-        Object to use to run the prediction.
-    y: str, optional
-        Response column.
-    ts: str, optional
-        vcolumn used to order the data.
-    X: list, optional
-        exogenous vcolumns.
-    dynamic: bool, optional
-        If set to True, the dynamic forecast will be drawn.
-    one_step: bool, optional
-        If set to True, the one step ahead forecast will be drawn.
-    observed: bool, optional
-        If set to True, the observation will be drawn.
-    confidence: bool, optional
-        If set to True, the confidence ranges will be drawn.
-    nlead: int, optional
-        Number of predictions computed by the dynamic forecast after
-        the last ts date.
-    nlast: int, optional
-        The dynamic forecast will start nlast values before the last
-        ts date.
-    limit: int, optional
-        Maximum number of past elements to use.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    ax
-        Matplotlib axes object
-        """
-        if not (vdf):
-            vdf = vDataFrameSQL(relation=self.input_relation)
-        check_types(
-            [
-                ("limit", limit, [int, float]),
-                ("nlead", nlead, [int, float]),
-                ("dynamic", dynamic, [bool]),
-                ("observed", observed, [bool]),
-                ("one_step", one_step, [bool]),
-                ("confidence", confidence, [bool]),
-                ("vdf", vdf, [vDataFrame]),
-            ],
-        )
-        delta_limit, limit = (
-            limit,
-            max(
-                max(
-                    limit,
-                    self.parameters["p"] + 1 + nlast,
-                    self.parameters["P"] * self.parameters["s"] + 1 + nlast,
-                ),
-                200,
-            ),
-        )
-        delta_limit = max(limit - delta_limit - nlast, 0)
-        assert dynamic or one_step or observed, ParameterError(
-            "No option selected.\n You should set either dynamic, one_step or observed to True."
-        )
-        assert nlead + nlast > 0 or not (dynamic), ParameterError(
-            "Dynamic Plots are only possible if either parameter 'nlead' is greater than 0 or parameter 'nlast' is greater than 0, and parameter 'dynamic' is set to True."
-        )
-        if dynamic:
-            assert not (self.exogenous), Exception(
-                "Dynamic Plots are only possible for SARIMA models (no exegenous variables), not SARIMAX."
-            )
-        if not (y):
-            y = self.y
-        if not (ts):
-            ts = self.ts
-        if not (X):
-            X = self.exogenous
-        result = self.predict(
-            vdf=vdf, y=y, ts=ts, X=X, nlead=0, name="_verticapy_prediction_"
-        )
-        error_eps = 1.96 * math.sqrt(self.score(method="mse"))
-        print_info = verticapy.options["print_info"]
-        verticapy.options["print_info"] = False
-        try:
-            result = (
-                result.select([ts, y, "_verticapy_prediction_"])
-                .dropna()
-                .sort([ts])
-                .tail(limit)
-                .values
-            )
-        except:
-            verticapy.options["print_info"] = print_info
-            raise
-        verticapy.options["print_info"] = print_info
-        columns = [elem for elem in result]
-        if isinstance(result[columns[0]][0], str):
-            result[columns[0]] = [parse(elem) for elem in result[columns[0]]]
-        true_value = [result[columns[0]], result[columns[1]]]
-        one_step_ahead = [result[columns[0]], result[columns[2]]]
-        lower_osa, upper_osa = (
-            [
-                float(elem) - error_eps if elem != None else None
-                for elem in one_step_ahead[1]
-            ],
-            [
-                float(elem) + error_eps if elem != None else None
-                for elem in one_step_ahead[1]
-            ],
-        )
-        if dynamic:
-            deltat = result[columns[0]][-1] - result[columns[0]][-2]
-            lead_time_list = []
-            if nlast > 0:
-                lead_list = [[elem] for elem in result[columns[1]][:-nlast]]
-            else:
-                lead_list = [[elem] for elem in result[columns[1]]]
-            for i in range(nlast):
-                lead_list += [[self.fpredict(lead_list)]]
-                lead_time_list += [result[columns[0]][i - nlast]]
-            if lead_time_list:
-                start_time = lead_time_list[-1]
-            else:
-                start_time = result[columns[0]][-1]
-            for i in range(nlead):
-                lead_list += [[self.fpredict(lead_list)]]
-                lead_time_list += [start_time + (i + 1) * deltat]
-            dynamic_forecast = (
-                [result[columns[0]][-nlast - 1]] + lead_time_list,
-                [result[columns[1]][-nlast - 1]]
-                + [elem[0] for elem in lead_list[-nlast - nlead :]],
-            )
-            lower_d, upper_d = [], []
-            for i in range(len(dynamic_forecast[1])):
-                if (
-                    self.parameters["s"] > 0
-                    and self.parameters["p"] == 0
-                    and self.parameters["d"] == 0
-                    and self.parameters["q"] == 0
-                ):
-                    delta_error = error_eps * math.sqrt(
-                        int(i / self.parameters["s"]) + 1
-                    )
-                else:
-                    delta_error = error_eps * math.sqrt(i + 1)
-                lower_d += [float(dynamic_forecast[1][i]) - delta_error]
-                upper_d += [float(dynamic_forecast[1][i]) + delta_error]
+        ts: str,
+        columns: Optional[SQLColumns] = None,
+        start_date: Optional[PythonScalar] = None,
+        end_date: Optional[PythonScalar] = None,
+        kind: Literal[
+            "area_percent", "area_stacked", "line", "spline", "step"
+        ] = "line",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the time series.
+
+        Parameters
+        ----------
+        ts: str
+            TS (Time Series)  vDataColumn used to order
+            the data.  The vDataColumn type must be  date
+            (date, datetime, timestamp...) or numerical.
+        columns: SQLColumns, optional
+            List of the vDataColumns names. If empty, all
+            numerical vDataColumns are used.
+        start_date: PythonScalar, optional
+            Input   Start  Date.  For  example,   time  =
+            '03-11-1993'  will  filter the data when 'ts'
+            is less than the 3rd of November 1993.
+        end_date: PythonScalar, optional
+            Input   End   Date.   For   example,   time =
+            '03-11-1993'   will  filter  the  data   when
+            'ts' is greater than the 3rd of November 1993.
+        kind: str, optional
+            The plot type.
+                line         : Line Plot.
+                spline       : Spline Plot.
+                step         : Step Plot.
+                area_stacked : Stacked Area Plot.
+                area_percent : Fully Stacked Area Plot.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any   optional  parameter  to   pass  to  the
+            plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns = format_type(columns, dtype=list)
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="MultiLinePlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.MultiLinePlot(
+            vdf=self,
+            order_by=ts,
+            columns=columns,
+            order_by_start=start_date,
+            order_by_end=end_date,
+            misc_layout={"kind": kind},
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def range_plot(
+        self,
+        columns: SQLColumns,
+        ts: str,
+        q: tuple[float, float] = (0.25, 0.75),
+        start_date: Optional[PythonScalar] = None,
+        end_date: Optional[PythonScalar] = None,
+        plot_median: bool = False,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the range plot of the input vDataColumns. The
+        aggregations used to draw the plot are the median
+        and the two user-specified quantiles.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of vDataColumns names.
+        ts: str
+            TS (Time Series) vDataColumn used to order the
+            data.  The  vDataColumn  type must be date
+            (date, datetime, timestamp...) or numerical.
+        q: tuple, optional
+            Tuple that includes the 2 quantiles used to draw
+            the Plot.
+        start_date: str / PythonNumber / date, optional
+            Input Start Date. For example, time = '03-11-1993'
+            will  filter  the data when 'ts' is  less  than
+            the 3rd of November 1993.
+        end_date: str / PythonNumber / date, optional
+            Input End Date.  For example, time = '03-11-1993'
+            will  filter the  data when 'ts' is greater than
+            the 3rd of November 1993.
+        plot_median: bool, optional
+            If set to True, the Median is drawn.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional parameter to pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="RangeCurve",
+            chart=chart,
+            matplotlib_kwargs={
+                "plot_median": plot_median,
+            },
+            plotly_kwargs={
+                "plot_median": plot_median,
+            },
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.RangeCurve(
+            vdf=self,
+            columns=columns,
+            order_by=ts,
+            q=q,
+            order_by_start=start_date,
+            order_by_end=end_date,
+        ).draw(**kwargs)
+
+    # 2D MAP.
+
+    @save_verticapy_logs
+    def _pivot_table(
+        self,
+        columns: SQLColumns,
+        method: PlottingMethod = "count",
+        of: Optional[str] = None,
+        max_cardinality: tuple[int, int] = (20, 20),
+        h: tuple[PythonNumber, PythonNumber] = (None, None),
+        fill_none: float = 0.0,
+    ) -> TableSample:
+        """
+        Computes and  returns the pivot table of one or two
+        columns based on an aggregation.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List  of the vDataColumns names.  The list  must
+            have one or two elements.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the vDataColumn 'of'.
+                min     : Minimum of the vDataColumn 'of'.
+                max     : Maximum of the vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation,
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The vDataColumn used to compute the aggregation.
+        max_cardinality: tuple, optional
+            Maximum number of distinct elements for vDataColumns
+            1  and  2  to be used as categorical. For these
+            elements, no  h is picked or computed.
+        h: tuple, optional
+            Interval width of the vDataColumns 1 and 2 bars.
+            Only valid if  the  vDataColumns  are numerical.
+            Optimized h will be computed if the parameter is
+            empty or invalid.
+        fill_none: float, optional
+            The  empty  values  of the pivot table are
+            filled by this number.
+
+        Returns
+        -------
+        obj
+            TableSample.
+        """
+        columns = format_type(columns, dtype=list)
+        columns, of = self.format_colnames(columns, of, expected_nb_of_cols=[1, 2])
+        vpy_plt = self.get_plotting_lib(class_name="HeatMap")[0]
+        plt_obj = vpy_plt.HeatMap(
+            vdf=self,
+            columns=columns,
+            method=method,
+            of=of,
+            h=h,
+            max_cardinality=max_cardinality,
+            fill_none=fill_none,
+        )
+        values = {"index": plt_obj.layout["x_labels"]}
+        if len(plt_obj.data["X"].shape) == 1:
+            values[plt_obj.layout["aggregate"]] = list(plt_obj.data["X"])
         else:
-            lower_d, upper_d, dynamic_forecast = [], [], ([], [])
-        alpha = 0.3
-        if not (ax):
-            fig, ax = plt.subplots()
-            if isnotebook():
-                fig.set_size_inches(10, 6)
-            ax.grid()
-        colors = gen_colors()
-        param1 = {
-            "color": colors[2],
-            "linewidth": 2,
-        }
-        param2 = {
-            "color": colors[3],
-            "linewidth": 2,
-            "linestyle": ":",
-        }
-        param3 = {
-            "color": colors[0],
-            "linewidth": 2,
-            "linestyle": "dashed",
-        }
-        if dynamic:
-            ax.fill_between(
-                dynamic_forecast[0],
-                1.02
-                * float(min(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-                1.02
-                * float(max(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-                alpha=0.04,
-                color=updated_dict(param3, style_kwds, 2)["color"],
-            )
-            if confidence:
-                ax.fill_between(
-                    dynamic_forecast[0], lower_d, upper_d, alpha=0.08, color="#555555"
+            for idx in range(plt_obj.data["X"].shape[1]):
+                values[plt_obj.layout["y_labels"][idx]] = list(
+                    plt_obj.data["X"][:, idx]
                 )
-                ax.plot(dynamic_forecast[0], lower_d, alpha=0.08, color="#000000")
-                ax.plot(dynamic_forecast[0], upper_d, alpha=0.08, color="#000000")
-            ax.plot(
-                dynamic_forecast[0],
-                dynamic_forecast[1],
-                label="Dynamic Forecast",
-                **updated_dict(param3, style_kwds, 2)
+        return TableSample(values=values)
+
+    @save_verticapy_logs
+    def pivot_table(
+        self,
+        columns: SQLColumns,
+        method: PlottingMethod = "count",
+        of: Optional[str] = None,
+        max_cardinality: tuple[int, int] = (20, 20),
+        h: tuple[PythonNumber, PythonNumber] = (None, None),
+        fill_none: float = 0.0,
+        mround: int = 3,
+        with_numbers: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the pivot table of one or two columns based on
+        an aggregation.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List  of the vDataColumns names.  The list  must
+            have one or two elements.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the vDataColumn 'of'.
+                min     : Minimum of the vDataColumn 'of'.
+                max     : Maximum of the vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The vDataColumn used to compute the aggregation.
+        max_cardinality: tuple, optional
+            Maximum number of distinct elements for vDataColumns
+            1  and  2  to be used as categorical. For these
+            elements, no  h is picked or computed.
+        h: tuple, optional
+            Interval width of the vDataColumns 1 and 2 bars.
+            Only valid if the vDataColumns  are numerical.
+            Optimized h will be computed if the parameter is
+            empty or invalid.
+        fill_none: float, optional
+            The  empty  values  of the pivot table  are
+            filled by this number.
+        mround: int, optional
+            Rounds the coefficient using the input number of
+            digits.  It  is only  used to display the  final
+            pivot table.
+        with_numbers: bool, optional
+            If  set to True, no number is displayed in
+            the final drawing.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional parameter to pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns = format_type(columns, dtype=list)
+        columns, of = self.format_colnames(columns, of, expected_nb_of_cols=[1, 2])
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="HeatMap",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.HeatMap(
+            vdf=self,
+            columns=columns,
+            method=method,
+            of=of,
+            h=h,
+            max_cardinality=max_cardinality,
+            fill_none=fill_none,
+            misc_layout={
+                "mround": mround,
+                "with_numbers": with_numbers,
+            },
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def contour(
+        self,
+        columns: SQLColumns,
+        func: Union[Callable, str],
+        nbins: int = 100,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws  the  contour  plot of the input function
+        using two input vDataColumns.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List  of the  vDataColumns  names. The list must
+            have two elements.
+        func: function / str
+            Function  used to compute  the contour score. It
+            can also be a SQL expression.
+        nbins: int, optional
+            Number of bins used to  discretize the two input
+            numerical vDataColumns.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional parameter to pass to  the plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="ContourPlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        func_name = None
+        if "func_name" in kwargs:
+            func_name = kwargs["func_name"]
+            del kwargs["func_name"]
+        return vpy_plt.ContourPlot(
+            vdf=self,
+            columns=columns,
+            func=func,
+            nbins=nbins,
+            func_name=func_name,
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def heatmap(
+        self,
+        columns: SQLColumns,
+        method: PlottingMethod = "count",
+        of: Optional[str] = None,
+        h: tuple = (None, None),
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the Heatmap of  the two input vDataColumns.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of the vDataColumns names. The list must
+            have two elements.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the vDataColumn 'of'.
+                min     : Minimum of the vDataColumn 'of'.
+                max     : Maximum of the vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q  Quantile  of the  vDataColumn
+                          'of (ex: 50% to get the median).
+            It can also be a cutomized aggregation
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The vDataColumn used to compute the aggregation.
+        h: tuple, optional
+            Interval width  of  the vDataColumns 1  and  2
+            bars.  Optimized  h  will  be computed if  the
+            parameter is empty or invalid.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any optional parameter to pass to the plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns = format_type(columns, dtype=list)
+        columns, of = self.format_colnames(columns, of, expected_nb_of_cols=2)
+        for column in columns:
+            assert self[column].isnum(), TypeError(
+                f"vDataColumn {column} must be numerical to draw the Heatmap."
+            )
+        min_max = self.agg(func=["min", "max"], columns=columns).transpose()
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="HeatMap",
+            chart=chart,
+            matplotlib_kwargs={
+                "extent": min_max[columns[0]] + min_max[columns[1]],
+            },
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.HeatMap(
+            vdf=self,
+            columns=columns,
+            method=method,
+            of=of,
+            h=h,
+            max_cardinality=(0, 0),
+            fill_none=0.0,
+            misc_layout={
+                "with_numbers": False,
+            },
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def hexbin(
+        self,
+        columns: SQLColumns,
+        method: PlottingMethod = "count",
+        of: Optional[str] = None,
+        bbox: Optional[list] = None,
+        img: Optional[str] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the Hexbin of the  input vDataColumns based
+        on an aggregation.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of the vDataColumns names. The list must
+            have two elements.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of  the vDataColumn 'of'.
+                min     : Minimum of  the vDataColumn 'of'.
+                max     : Maximum of  the vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of
+                          (ex: 50% to get the median).
+        of: str, optional
+            The vDataColumn used to compute the aggregation.
+        bbox: list, optional
+            List of 4 elements  to delimit the boundaries of
+            the final Plot. It must be similar the following
+            list: [xmin, xmax, ymin, ymax]
+        img: str, optional
+            Path  to the  image used as a background.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional parameter to pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns, bbox = format_type(columns, bbox, dtype=list)
+        columns, of = self.format_colnames(columns, of, expected_nb_of_cols=2)
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="HexbinMap",
+            chart=chart,
+            matplotlib_kwargs={"bbox": bbox, "img": img},
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.HexbinMap(
+            vdf=self,
+            columns=columns,
+            method=method,
+            of=of,
+        ).draw(**kwargs)
+
+    # Scatters.
+
+    @save_verticapy_logs
+    def scatter(
+        self,
+        columns: SQLColumns,
+        by: Optional[str] = None,
+        size: Optional[str] = None,
+        cmap_col: Optional[str] = None,
+        max_cardinality: int = 6,
+        cat_priority: Union[None, PythonScalar, ArrayLike] = None,
+        max_nb_points: int = 20000,
+        dimensions: tuple = None,
+        bbox: Optional[tuple] = None,
+        img: Optional[str] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the scatter plot of the input vDataColumns.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List of the vDataColumns names.
+        by: str, optional
+            Categorical vDataColumn used to label the data.
+        size: str
+            Numerical  vDataColumn used to represent  the
+            Bubble size.
+        cmap_col: str, optional
+            Numerical  column used  to represent the  color
+            map.
+        max_cardinality: int, optional
+            Maximum  number  of  distinct elements for  'by'
+            to  be  used as categorical.  The less  frequent
+            elements are gathered together  to create a
+            new category: 'Others'.
+        cat_priority: PythonScalar / ArrayLike, optional
+            ArrayLike list of the different categories to
+            consider when  labeling  the  data using  the
+            vDataColumn 'by'.  The  other  categories  are
+            filtered.
+        max_nb_points: int, optional
+            Maximum number of points to display.
+        dimensions: tuple, optional
+            Tuple of two  elements representing the IDs of the
+            PCA's components. If empty and the number of input
+            columns  is greater  than 3, the first and  second
+            PCA are drawn.
+        bbox: list, optional
+            Tuple  of 4 elements to delimit the boundaries  of
+            the  final Plot. It must be similar the  following
+            list: [xmin, xmax, ymin, ymax]
+        img: str, optional
+            Path to the image to display as background.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vml = get_vertica_mllib()
+        if img and not bbox and len(columns) == 2:
+            aggr = self.agg(columns=columns, func=["min", "max"])
+            bbox = (
+                aggr.values["min"][0],
+                aggr.values["max"][0],
+                aggr.values["min"][1],
+                aggr.values["max"][1],
+            )
+        if len(columns) > 3 and isinstance(dimensions, NoneType):
+            dimensions = (1, 2)
+        if isinstance(dimensions, Iterable):
+            model_name = gen_tmp_name(
+                schema=conf.get_option("temp_schema"), name="pca_plot"
             )
-        if one_step:
-            if confidence:
-                ax.fill_between(
-                    one_step_ahead[0][delta_limit:],
-                    lower_osa[delta_limit:],
-                    upper_osa[delta_limit:],
-                    alpha=0.04,
-                    color="#555555",
-                )
-                ax.plot(
-                    one_step_ahead[0][delta_limit:],
-                    lower_osa[delta_limit:],
-                    alpha=0.04,
-                    color="#000000",
-                )
-                ax.plot(
-                    one_step_ahead[0][delta_limit:],
-                    upper_osa[delta_limit:],
-                    alpha=0.04,
-                    color="#000000",
+            model = vml.PCA(model_name)
+            model.drop()
+            try:
+                model.fit(self, columns)
+                vdf = model.transform(self)
+                ev_1 = model.explained_variance_[dimensions[0] - 1]
+                x_label = f"Dim{dimensions[0]} ({ev_1}%)"
+                ev_2 = model.explained_variance_[dimensions[1] - 1]
+                y_label = f"Dim{dimensions[1]} ({ev_2}%)"
+                vdf[f"col{dimensions[0]}"].rename(x_label)
+                vdf[f"col{dimensions[1]}"].rename(y_label)
+                chart = vdf.scatter(
+                    columns=[x_label, y_label],
+                    by=by,
+                    cmap_col=cmap_col,
+                    size=size,
+                    max_cardinality=max_cardinality,
+                    cat_priority=cat_priority,
+                    max_nb_points=max_nb_points,
+                    bbox=bbox,
+                    img=img,
+                    chart=chart,
+                    **style_kwargs,
                 )
-            ax.plot(
-                one_step_ahead[0][delta_limit:],
-                one_step_ahead[1][delta_limit:],
-                label="One-step ahead Forecast",
-                **updated_dict(param2, style_kwds, 1)
-            )
-        if observed:
-            ax.plot(
-                true_value[0][delta_limit:],
-                true_value[1][delta_limit:],
-                label="Observed",
-                **updated_dict(param1, style_kwds, 0)
-            )
-        ax.set_title(
-            "SARIMAX({},{},{})({},{},{})_{}".format(
-                self.parameters["p"],
-                self.parameters["d"],
-                self.parameters["q"],
-                self.parameters["P"],
-                self.parameters["D"],
-                self.parameters["Q"],
-                self.parameters["s"],
-            )
-        )
-        ax.set_xlabel(ts)
-        ax.legend(loc="center left", bbox_to_anchor=[1, 0.5])
-        ax.set_ylim(
-            1.02 * float(min(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-            1.02 * float(max(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-        )
-        for tick in ax.get_xticklabels():
-            tick.set_rotation(90)
-        return ax
-
-    # ---#
-    def predict(
-        self,
-        vdf: vDataFrame,
-        y: str = "",
-        ts: str = "",
-        X: list = [],
-        nlead: int = 0,
-        name: str = "",
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Predicts using the input relation.
-
-    Parameters
-    ----------
-    vdf: vDataFrame
-        Object to use to run the prediction.
-    y: str, optional
-        Response column.
-    ts: str, optional
-        vcolumn used to order the data.
-    X: list, optional
-        exogenous vcolumns.
-    nlead: int, optional
-        Number of records to predict after the last ts date.
-    name: str, optional
-        Name of the added vcolumn. If empty, a name will be generated.
-
-    Returns
-    -------
-    vDataFrame
-        object including the prediction.
-        """
-        check_types(
-            [
-                ("name", name, [str]),
-                ("y", y, [str]),
-                ("ts", ts, [str]),
-                ("X", X, [list]),
-                ("nlead", nlead, [int, float]),
-                ("vdf", vdf, [vDataFrame]),
-            ],
-        )
-        if not (y):
-            y = self.y
-        if not (ts):
-            ts = self.ts
-        if not (X):
-            X = self.exogenous
-        vdf.are_namecols_in([y, ts])
-        y, ts = vdf.format_colnames([y, ts])
-        name = (
-            "{}_".format(self.type) + "".join(ch for ch in self.name if ch.isalnum())
-            if not (name)
-            else name
-        )
-        key_columns = ", " + ", ".join(vdf.get_columns(exclude_columns=[y]))
-        transform_relation = self.transform_relation.replace(
-            "[VerticaPy_y]", y
-        ).replace("[VerticaPy_ts]", ts)
-        transform_relation = transform_relation.replace(
-            "[VerticaPy_key_columns]", key_columns
-        )
-        predictSQL = self.deploySQL().replace("[VerticaPy_y]", y).replace(
-            "[VerticaPy_ts]", ts
-        ) + " AS {}".format(name)
-        for idx, elem in enumerate(X):
-            transform_relation = transform_relation.replace("[X{}]".format(idx), elem)
-            predictSQL = predictSQL.replace("[X{}]".format(idx), elem)
-        columns = (
-            vdf.get_columns(exclude_columns=[y])
-            + [predictSQL]
-            + ["VerticaPy_y_copy AS {}".format(y)]
-        )
-        relation = vdf.__genSQL__()
-        for i in range(nlead):
-            query = "SELECT ({} - LAG({}, 1) OVER (ORDER BY {}))::VARCHAR FROM {} ORDER BY {} DESC LIMIT 1".format(
-                ts, ts, ts, relation, ts
-            )
-            deltat = executeSQL(query, method="fetchfirstelem", print_time_sql=False)
-            query = "SELECT (MAX({}) + '{}'::interval)::VARCHAR FROM {}".format(
-                ts, deltat, relation
-            )
-            next_t = executeSQL(query, method="fetchfirstelem", print_time_sql=False)
-            if i == 0:
-                first_t = next_t
-            new_line = "SELECT '{}'::TIMESTAMP AS {}, {}".format(
-                next_t,
-                ts,
-                ", ".join(
-                    [
-                        "NULL AS {}".format(column)
-                        for column in vdf.get_columns(exclude_columns=[ts])
-                    ]
-                ),
-            )
-            relation_tmp = "(SELECT {} FROM {} UNION ALL ({})) VERTICAPY_SUBTABLE".format(
-                ", ".join([ts] + vdf.get_columns(exclude_columns=[ts])),
-                relation,
-                new_line,
-            )
-            query = "SELECT {} FROM {} ORDER BY {} DESC LIMIT 1".format(
-                self.deploySQL()
-                .replace("[VerticaPy_y]", y)
-                .replace("[VerticaPy_ts]", ts),
-                transform_relation.format(relation_tmp),
-                ts,
-            )
-            prediction = executeSQL(
-                query, method="fetchfirstelem", print_time_sql=False
-            )
-            columns_tmp = vdf.get_columns(exclude_columns=[ts, y])
-            new_line = "SELECT '{}'::TIMESTAMP AS {}, {} AS {} {}".format(
-                next_t,
-                ts,
-                prediction,
-                y,
-                (", " if (columns_tmp) else "")
-                + ", ".join(["NULL AS {}".format(column) for column in columns_tmp]),
-            )
-            relation = "(SELECT {} FROM {} UNION ALL ({})) VERTICAPY_SUBTABLE".format(
-                ", ".join([ts, y] + vdf.get_columns(exclude_columns=[ts, y])),
-                relation,
-                new_line,
-            )
-        final_relation = "(SELECT {} FROM {}) VERTICAPY_SUBTABLE".format(
-            ", ".join(columns), transform_relation.format(relation)
+            finally:
+                model.drop()
+            return chart
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="ScatterPlot",
+            chart=chart,
+            matplotlib_kwargs={
+                "bbox": bbox,
+                "img": img,
+            },
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.ScatterPlot(
+            vdf=self,
+            columns=columns,
+            by=by,
+            cmap_col=cmap_col,
+            size=size,
+            max_cardinality=max_cardinality,
+            cat_priority=cat_priority,
+            max_nb_points=max_nb_points,
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def scatter_matrix(
+        self,
+        columns: Optional[SQLColumns] = None,
+        max_nb_points: int = 1000,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the scatter matrix of the vDataFrame.
+
+        Parameters
+        ----------
+        columns: SQLColumns, optional
+            List of the vDataColumns names. If empty,
+            all numerical  vDataColumns are used.
+        max_nb_points: int, optional
+            Maximum  number of points to display for
+            each scatter plot.
+        **style_kwargs
+            Any  optional  parameter  to pass to the
+            plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        columns = format_type(columns, dtype=list)
+        columns = self.format_colnames(columns)
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="ScatterMatrix",
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.ScatterMatrix(
+            vdf=self, columns=columns, max_nb_points=max_nb_points
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def outliers_plot(
+        self,
+        columns: SQLColumns,
+        threshold: float = 3.0,
+        max_nb_points: int = 500,
+        color: ColorType = "orange",
+        outliers_color: ColorType = "black",
+        inliers_color: ColorType = "white",
+        inliers_border_color: ColorType = "red",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the global  outliers plot of one or two
+        columns based on their ZSCORE.
+
+        Parameters
+        ----------
+        columns: SQLColumns
+            List  of  one or two  vDataColumn  names.
+        threshold: float, optional
+            ZSCORE threshold used to detect outliers.
+        max_nb_points: int, optional
+            Maximum number of points to display.
+        color: ColorType, optional
+            Inliers Area color.
+        outliers_color: ColorType, optional
+            Outliers color.
+        inliers_color: ColorType, optional
+            Inliers color.
+        inliers_border_color: ColorType, optional
+            Inliers border color.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter to pass to  the
+            plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self.get_plotting_lib(
+            class_name="OutliersPlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.OutliersPlot(
+            vdf=self,
+            columns=columns,
+            threshold=threshold,
+            max_nb_points=max_nb_points,
+            misc_layout={
+                "color": color,
+                "outliers_color": outliers_color,
+                "inliers_color": inliers_color,
+                "inliers_border_color": inliers_border_color,
+            },
+        ).draw(**kwargs)
+
+
+class vDCPlot(vDCNorm):
+    # Special Methods.
+
+    def numh(
+        self, method: Literal["sturges", "freedman_diaconis", "fd", "auto"] = "auto"
+    ) -> float:
+        """
+        Computes the optimal vDataColumn bar width.
+
+        Parameters
+        ----------
+        method: str, optional
+            Method used to compute the optimal h.
+                auto              : Combination of Freedman Diaconis
+                                    and Sturges.
+                freedman_diaconis : Freedman Diaconis
+                                    [2 * IQR / n ** (1 / 3)]
+                sturges           : Sturges [CEIL(log2(n)) + 1]
+
+        Returns
+        -------
+        float
+            optimal bar width.
+        """
+        if method == "auto":
+            pre_comp = self._parent._get_catalog_value(self._alias, "numh")
+            if pre_comp != "VERTICAPY_NOT_PRECOMPUTED":
+                return pre_comp
+        assert self.isnum() or self.isdate(), ValueError(
+            "numh is only available on type numeric|date"
         )
-        result = vDataFrameSQL(final_relation, "SARIMAX")
-        if nlead > 0:
-            result[y].apply(
-                "CASE WHEN {} >= '{}' THEN NULL ELSE {} END".format(ts, first_t, "{}")
+        if self.isnum():
+            result = (
+                self._parent.describe(
+                    method="numerical", columns=[self._alias], unique=False
+                )
+                .transpose()
+                .values[self._alias]
             )
-        return result
+            (
+                count,
+                vDataColumn_min,
+                vDataColumn_025,
+                vDataColumn_075,
+                vDataColumn_max,
+            ) = (
+                result[0],
+                result[3],
+                result[4],
+                result[6],
+                result[7],
+            )
+        elif self.isdate():
+            result = _executeSQL(
+                f"""
+                SELECT 
+                    /*+LABEL('vDataColumn.numh')*/ COUNT({self}) AS NAs, 
+                    MIN({self}) AS min, 
+                    APPROXIMATE_PERCENTILE({self} 
+                        USING PARAMETERS percentile = 0.25) AS Q1, 
+                    APPROXIMATE_PERCENTILE({self} 
+                        USING PARAMETERS percentile = 0.75) AS Q3, 
+                    MAX({self}) AS max 
+                FROM 
+                    (SELECT 
+                        DATEDIFF('second', 
+                                 '{self.min()}'::timestamp, 
+                                 {self}) AS {self} 
+                    FROM {self._parent}) VERTICAPY_OPTIMAL_H_TABLE""",
+                title="Different aggregations to compute the optimal h.",
+                method="fetchrow",
+                sql_push_ext=self._parent._vars["sql_push_ext"],
+                symbol=self._parent._vars["symbol"],
+            )
+            (
+                count,
+                vDataColumn_min,
+                vDataColumn_025,
+                vDataColumn_075,
+                vDataColumn_max,
+            ) = result
+        sturges = max(
+            float(vDataColumn_max - vDataColumn_min)
+            / int(math.floor(math.log(count, 2) + 2)),
+            1e-99,
+        )
+        fd = max(
+            2.0 * (vDataColumn_075 - vDataColumn_025) / (count) ** (1.0 / 3.0), 1e-99
+        )
+        if method.lower() == "sturges":
+            best_h = sturges
+        elif method.lower() in ("freedman_diaconis", "fd"):
+            best_h = fd
+        else:
+            best_h = max(sturges, fd)
+            self._parent._update_catalog({"index": ["numh"], self._alias: [best_h]})
+        if self.category() == "int":
+            best_h = max(math.floor(best_h), 1)
+        return best_h
 
+    # Boxplots.
 
-# ---#
-class VAR(Regressor):
-    """
----------------------------------------------------------------------------
-[Beta Version]
-Creates an VAR object using the Vertica Linear Regression algorithm on the 
-data.
-
-Parameters
-----------
-name: str
-    Name of the the model. The model will be stored in the DB.
-p: int, optional
-    Order of the AR (Auto-Regressive) part.
-tol: float, optional
-    Determines whether the algorithm has reached the specified accuracy result.
-max_iter: int, optional
-    Determines the maximum number of iterations the algorithm performs before 
-    achieving the specified accuracy result.
-solver: str, optional
-    The optimizer method to use to train the model. 
-        Newton : Newton Method
-        BFGS   : Broyden Fletcher Goldfarb Shanno
-    """
-
-    def __init__(
-        self,
-        name: str,
-        p: int = 1,
-        tol: float = 1e-4,
-        max_iter: int = 1000,
-        solver: str = "Newton",
-    ):
-        check_types([("name", name, [str])])
-        self.type, self.name = "VAR", name
-        assert p > 0, ParameterError(
-            "Parameter 'p' must be greater than 0 to build a VAR model."
-        )
-        self.set_params({"p": p, "tol": tol, "max_iter": max_iter, "solver": solver})
-        version(condition=[8, 0, 0])
-
-    # ---#
-    def deploySQL(self):
-        """
-    ---------------------------------------------------------------------------
-    Returns the SQL code needed to deploy the model.
-
-    Returns
-    -------
-    str
-        the SQL code needed to deploy the model.
-        """
-        sql = []
-        for idx, coefs in enumerate(self.coef_):
-            coefs_tmp = coefs.values["coefficient"]
-            predictors_tmp = coefs.values["predictor"]
-            sql += [
-                str(coefs_tmp[0])
-                + " + "
-                + " + ".join(
-                    [
-                        str(coefs_tmp[i]) + " * " + str(predictors_tmp[i])
-                        for i in range(1, len(coefs_tmp))
-                    ]
-                )
-            ]
-        return sql
+    @save_verticapy_logs
+    def boxplot(
+        self,
+        by: Optional[str] = None,
+        q: tuple[float, float] = (0.25, 0.75),
+        h: PythonNumber = 0,
+        max_cardinality: int = 8,
+        cat_priority: Union[None, PythonScalar, ArrayLike] = None,
+        max_nb_fliers: int = 30,
+        whis: float = 1.5,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the box plot of the vDataColumn.
+
+        Parameters
+        ----------
+        by: str, optional
+            vDataColumn used to partition  the  data.
+        q: tuple, optional
+            Tuple including the 2 quantiles used to draw
+            the BoxPlot.
+        h: PythonNumber, optional
+            Interval  width  if  the 'by'  vDataColumn is
+            numerical or of a date-like type. Optimized h
+            will be computed if the parameter is empty or
+            invalid.
+        max_cardinality: int, optional
+            Maximum   number   of   distinct vDataColumn
+            elements to be used as categorical.
+            The less frequent  elements  are gathered
+            together to create a new category : 'Others'.
+        cat_priority: PythonScalar / ArrayLike, optional
+            ArrayLike list of the different categories to
+            consider when drawing the box plot. The other
+            categories are filtered.
+        max_nb_fliers: int, optional
+            Maximum  number of points used to represent
+            the fliers of each category.
+            Drawing fliers slows down the  graphic
+            computation.
+        whis: float, optional
+            The position of the whiskers.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="BoxPlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.BoxPlot(
+            vdf=self._parent,
+            columns=[self._alias],
+            by=by,
+            q=q,
+            h=h,
+            max_cardinality=max_cardinality,
+            cat_priority=cat_priority,
+            max_nb_fliers=max_nb_fliers,
+            whis=whis,
+        ).draw(**kwargs)
 
-    # ---#
-    def features_importance(
-        self, X_idx: int = 0, ax=None, show: bool = True, **style_kwds
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Computes the model's features importance.
-
-    Parameters
-    ----------
-    X_idx: int/str, optional
-        Index of the main vector vcolumn used to draw the features importance.
-        It can also be the name of a predictor vcolumn.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    show: bool
-        If set to True, draw the features importance.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    ax
-        Matplotlib axes object
-        """
-        check_types([("X_idx", X_idx, [int, float, str]), ("show", show, [bool])])
-        if isinstance(X_idx, str):
-            X_idx = quote_ident(X_idx).lower()
-            for idx, elem in enumerate(self.X):
-                if quote_ident(elem).lower() == X_idx:
-                    X_idx = idx
-                    break
-        assert (
-            isinstance(X_idx, (float, int)) and len(self.X) > X_idx >= 0
-        ), ParameterError(
-            "The index of the vcolumn to draw 'X_idx' must be between 0 and {}. It can also be the name of a predictor vcolumn.".format(
-                len(self.X)
-            )
-        )
-        relation = self.transform_relation.replace("[VerticaPy_ts]", self.ts).format(
-            self.test_relation
-        )
-        for idx, elem in enumerate(self.X):
-            relation = relation.replace("[X{}]".format(idx), elem)
-        min_max = (
-            vDataFrameSQL(relation=self.input_relation)
-            .agg(func=["min", "max"], columns=self.X)
-            .transpose()
-        )
-        coefficient = self.coef_[X_idx].values
-        coeff_importances = {}
-        coeff_sign = {}
-        for idx, coef in enumerate(coefficient["predictor"]):
-            if idx > 0:
-                predictor = int(coef.split("_")[0].replace("ar", ""))
-                predictor = quote_ident(self.X[predictor])
-                minimum, maximum = min_max[predictor]
-                val = coefficient["coefficient"][idx]
-                coeff_importances[coef] = abs(val) * (maximum - minimum)
-                coeff_sign[coef] = 1 if val >= 0 else -1
-        total = sum([coeff_importances[elem] for elem in coeff_importances])
-        for elem in coeff_importances:
-            coeff_importances[elem] = 100 * coeff_importances[elem] / total
-        if show:
-            plot_importance(
-                coeff_importances, coeff_sign, print_legend=True, ax=ax, **style_kwds
-            )
-        importances = {"index": ["importance", "sign"]}
-        for elem in coeff_importances:
-            importances[elem] = [coeff_importances[elem], coeff_sign[elem]]
-        return tablesample(values=importances).transpose()
+    # 1D CHARTS.
 
-    # ---#
-    def fit(
+    @save_verticapy_logs
+    def bar(
         self,
-        input_relation: Union[vDataFrame, str],
-        X: list,
-        ts: str,
-        test_relation: Union[vDataFrame, str] = "",
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Trains the model.
+        method: PlottingMethod = "density",
+        of: Optional[str] = None,
+        max_cardinality: int = 6,
+        nbins: int = 0,
+        h: PythonNumber = 0,
+        categorical: bool = True,
+        bargap: float = 0.06,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the bar chart of the vDataColumn based on an
+        aggregation.
+
+        Parameters
+        ----------
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the  vDataColumn 'of'.
+                min     : Minimum of the  vDataColumn 'of'.
+                max     : Maximum of the  vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The vDataColumn  used to compute the aggregation.
+        max_cardinality: int, optional
+            Maximum number of distinct vDataColumns elements
+            to be used as categorical. For these elements, no
+            h is picked or computed.
+        nbins: int, optional
+            Number  of  bins. If empty, an  optimized number of
+            bins is computed.
+        h: PythonNumber, optional
+            Interval width of the bar. If empty, an optimized h
+            is computed.
+        categorical: bool, optional
+            If  set to False and the  vDataColumn is numerical,
+            the parmater  'max_cardinality' is ignored and
+            the bar  chart is represented as a histogram.
+        bargap: float, optional
+            A float between  (0, 1] that represents the
+            proportion  taken out of each bar to render the
+            chart. This proportion creates gaps between each
+            bar. The bigger the value, the bigger the gap.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="BarChart",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.BarChart(
+            vdc=self,
+            method=method,
+            of=of,
+            max_cardinality=max_cardinality,
+            nbins=nbins,
+            h=h,
+            pie=categorical,
+            bargap=bargap,
+        ).draw(**kwargs)
 
-    Parameters
-    ----------
-    input_relation: str/vDataFrame
-        Training relation.
-    X: list
-        List of the response columns.
-    ts: str
-        vcolumn used to order the data.
-    test_relation: str/vDataFrame, optional
-        Relation used to test the model.
-
-    Returns
-    -------
-    object
-        self
-        """
-        check_types(
-            [
-                ("input_relation", input_relation, [str, vDataFrame]),
-                ("X", X, [list]),
-                ("ts", ts, [str]),
-                ("test_relation", test_relation, [str, vDataFrame]),
-            ]
-        )
-        # Initialization
-        if verticapy.options["overwrite_model"]:
-            self.drop()
-        else:
-            does_model_exist(name=self.name, raise_error=True)
-        self.input_relation = (
-            input_relation
-            if isinstance(input_relation, str)
-            else input_relation.__genSQL__()
-        )
-        if isinstance(test_relation, vDataFrame):
-            self.test_relation = test_relation.__genSQL__()
-        elif test_relation:
-            self.test_relation = test_relation
+    @save_verticapy_logs
+    def barh(
+        self,
+        method: str = "density",
+        of: Optional[str] = None,
+        max_cardinality: int = 6,
+        nbins: int = 0,
+        h: PythonNumber = 0,
+        categorical: bool = True,
+        bargap: float = 0.06,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws  the horizontal bar  chart of the vDataColumn
+        based on an aggregation.
+
+        Parameters
+        ----------
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the  vDataColumn 'of'.
+                min     : Minimum of the  vDataColumn 'of'.
+                max     : Maximum of the  vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The vDataColumn  used to compute the aggregation.
+        max_cardinality: int, optional
+            Maximum number of distinct elements for vDataColumns
+            to be used as categorical. For these elements, no
+            h is picked or computed.
+        nbins: int, optional
+            Number  of  bins. If empty, an  optimized number of
+            bins is computed.
+        h: PythonNumber, optional
+            Interval width of the bar. If empty, an optimized h
+            is computed.
+        categorical: bool, optional
+            If  set to False and the  vDataColumn is numerical,
+            the parmater  'max_cardinality' is ignored and
+            the bar  chart is represented as a histogram.
+        bargap: float, optional
+            A float between  (0, 1] that represent the
+            proportion  taken out of each bar to render the
+            chart. This proportion creates between  each bar.
+            The  bigger the value,  the bigger the gap.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="HorizontalBarChart",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.HorizontalBarChart(
+            vdc=self,
+            method=method,
+            of=of,
+            max_cardinality=max_cardinality,
+            nbins=nbins,
+            h=h,
+            pie=categorical,
+            bargap=bargap,
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def pie(
+        self,
+        method: str = "density",
+        of: Optional[str] = None,
+        max_cardinality: int = 6,
+        h: PythonNumber = 0,
+        kind: Literal["auto", "donut", "rose", "3d"] = "auto",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the pie chart of the vDataColumn based on an
+        aggregation.
+
+        Parameters
+        ----------
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the vDataColumn 'of'.
+                min     : Minimum of the vDataColumn 'of'.
+                max     : Maximum of the vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of'
+                          (ex: 50% to get the median).
+            It   can  also  be  a  cutomized   aggregation
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The vDataColumn used to compute the aggregation.
+        max_cardinality: int, optional
+            Maximum number of distinct elements for vDataColumns
+            to be used as categorical. For these elements, no
+            h is picked or computed.
+        h: PythonNumber, optional
+            Interval width of the bar. If empty, an optimized
+            h is computed.
+        kind: str, optional
+            The type of pie chart.
+                auto   : Regular pie chart.
+                donut  : Donut chart.
+                rose   : Rose chart.
+                3d     : 3D Pie.
+            It   can    also   be  a  cutomized   aggregation
+            (ex: AVG(column1) + 5).
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional parameter to pass to  the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="PieChart",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.PieChart(
+            vdc=self,
+            method=method,
+            of=of,
+            max_cardinality=max_cardinality,
+            h=h,
+            pie=True,
+            misc_layout={"kind": kind},
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def spider(
+        self,
+        by: Optional[str] = None,
+        method: str = "density",
+        of: Optional[str] = None,
+        max_cardinality: tuple[int, int] = (6, 6),
+        h: tuple[PythonNumber, PythonNumber] = (None, None),
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the spider plot of the input vDataColumn based on
+        an aggregation.
+
+        Parameters
+        ----------
+        by: str, optional
+            vDataColumn used to partition the data.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the vDataColumn 'of'.
+                min     : Minimum of the vDataColumn 'of'.
+                max     : Maximum of the vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of'
+                          (ex: 50% to get the median).
+            It   can  also  be  a  cutomized   aggregation
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The vDataColumn used to compute the aggregation.
+        max_cardinality: int, optional
+            Maximum number of distinct elements for vDataColumns
+            to be used as categorical. For these elements, no
+            h is picked or computed.
+        h: PythonNumber, optional
+            Interval width of the bar. If empty, an optimized
+            h is computed.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional parameter to pass to  the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        by, of = self._parent.format_colnames(by, of)
+        columns = [self._alias]
+        if by:
+            columns += [by]
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="SpiderChart",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.SpiderChart(
+            vdf=self._parent,
+            columns=columns,
+            method=method,
+            of=of,
+            max_cardinality=max_cardinality,
+            h=h,
+        ).draw(**kwargs)
+
+    # Histogram & Density.
+
+    @save_verticapy_logs
+    def hist(
+        self,
+        by: Optional[str] = None,
+        method: str = "density",
+        of: Optional[str] = None,
+        h: Optional[PythonNumber] = None,
+        h_by: PythonNumber = 0,
+        max_cardinality: int = 8,
+        cat_priority: Union[None, PythonScalar, ArrayLike] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws  the histogram of the input vDataColumn based
+        on an aggregation.
+
+        Parameters
+        ----------
+        by: str, optional
+            vDataColumn  used  to  partition  the  data.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                density : Percentage of the distribution.
+                mean    : Average of the  vDataColumn 'of'.
+                min     : Minimum of the  vDataColumn 'of'.
+                max     : Maximum of the  vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of
+                          (ex: 50% to get the median).
+            It can also be a cutomized aggregation
+            (ex: AVG(column1) + 5).
+        of: str, optional
+            The  vDataColumn  used to compute the  aggregation.
+        h: PythonNumber, optional
+            Interval  width of the  input vDataColumns. Optimized
+            h  will be  computed  if  the  parameter is empty  or
+            invalid.
+        h_by: PythonNumber, optional
+            Interval  width if the 'by' vDataColumn is  numerical
+            or of a date-like type. Optimized  h will be computed
+            if the parameter is empty or invalid.
+        max_cardinality: int, optional
+            Maximum number of distinct elements for vDataColumns
+            to be used as categorical.
+            The less frequent  elements are gathered together
+            to create a new category : 'Others'.
+        cat_priority: PythonScalar / ArrayLike, optional
+            ArrayLike list of the different categories to consider
+            when drawing the box plot.  The other categories are
+            filtered.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass  to  the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="Histogram",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.Histogram(
+            vdf=self._parent,
+            columns=[self._alias],
+            by=by,
+            method=method,
+            of=of,
+            h=h,
+            h_by=h_by,
+            max_cardinality=max_cardinality,
+            cat_priority=cat_priority,
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def density(
+        self,
+        by: Optional[str] = None,
+        bandwidth: PythonNumber = 1.0,
+        kernel: Literal["gaussian", "logistic", "sigmoid", "silverman"] = "gaussian",
+        nbins: int = 200,
+        xlim: Optional[tuple[float, float]] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the vDataColumn Density Plot.
+
+        Parameters
+        ----------
+        by: str, optional
+            vDataColumn used to partition  the data.
+        bandwidth: PythonNumber, optional
+            The bandwidth of the kernel.
+        kernel: str, optional
+            The method used for the plot.
+                gaussian  : Gaussian kernel.
+                logistic  : Logistic kernel.
+                sigmoid   : Sigmoid kernel.
+                silverman : Silverman kernel.
+        nbins: int, optional
+            Maximum number of points  used to evaluate
+            the approximate density function.
+            Increasing this  parameter increases the
+            precision but also increases the time of
+            the learning and scoring phases.
+        xlim: tuple, optional
+            Set the x limits of the current axes.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to  pass   to  the
+            plotting functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        vml = get_vertica_mllib()
+        name = gen_tmp_name(schema=conf.get_option("temp_schema"), name="kde")
+        by = self._parent.format_colnames(by)
+        if not xlim:
+            xlim_ = [(self.min(), self.max())]
         else:
-            self.test_relation = self.input_relation
-        self.ts, self.deploy_predict_ = quote_ident(ts), []
-        self.X, schema = (
-            [quote_ident(elem) for elem in X],
-            schema_relation(self.name)[0],
-        )
-        model = LinearRegression(
-            name=self.name,
-            solver=self.parameters["solver"],
-            max_iter=self.parameters["max_iter"],
-            tol=self.parameters["tol"],
-        )
-
-        # AR(p)
-        columns, AR = [], []
-        for idx, elem in enumerate(self.X):
-            for i in range(1, self.parameters["p"] + 1):
-                columns += [
-                    "LAG([X{}], {}) OVER (ORDER BY [VerticaPy_ts]) AS AR{}_{}".format(
-                        idx, i, idx, i
-                    )
-                ]
-                AR += ["AR{}_{}".format(idx, i)]
-        self.transform_relation = "(SELECT *, {} FROM {}) VERTICAPY_SUBTABLE".format(
-            ", ".join(columns), "{}"
-        )
-        relation = self.transform_relation.replace("[VerticaPy_ts]", self.ts).format(
-            self.input_relation
-        )
-        for idx, elem in enumerate(self.X):
-            relation = relation.replace("[X{}]".format(idx), elem)
-        view_name = gen_tmp_name(schema=schema, name="linear_reg")
-        drop(view_name, method="view")
-        try:
-            query = "CREATE VIEW {} AS SELECT * FROM {}".format(view_name, relation)
-            executeSQL(query, print_time_sql=False)
-            self.coef_ = []
-            for elem in X:
-                model.fit(
-                    input_relation=view_name, X=AR, y=elem,
-                )
-                self.coef_ += [model.coef_]
+            xlim_ = [xlim]
+        model = vml.KernelDensity(
+            name=name,
+            bandwidth=bandwidth,
+            kernel=kernel,
+            nbins=nbins,
+            xlim=xlim_,
+            store=False,
+        )
+        if not by:
+            try:
+                model.fit(self._parent, [self._alias])
+                return model.plot(chart=chart, **style_kwargs)
+            finally:
                 model.drop()
-        except:
-            drop(view_name, method="view")
-            raise
-        drop(view_name, method="view")
-        model_save = {
-            "type": "VAR",
-            "input_relation": self.input_relation,
-            "test_relation": self.test_relation,
-            "transform_relation": self.transform_relation,
-            "deploy_predict": self.deploy_predict_,
-            "X": self.X,
-            "ts": self.ts,
-            "p": self.parameters["p"],
-            "tol": self.parameters["tol"],
-            "max_iter": self.parameters["max_iter"],
-            "solver": self.parameters["solver"],
-        }
-        for idx, elem in enumerate(self.coef_):
-            model_save["coef_{}".format(idx)] = elem.values
-        insert_verticapy_schema(
-            model_name=self.name, model_type="VAR", model_save=model_save,
-        )
-        return self
-
-    # ---#
-    def fpredict(self, L: list):
-        """
-    ---------------------------------------------------------------------------
-    Computes the prediction.
-
-    Parameters
-    ----------
-    L: list
-        List containing the data. It must be a two-dimensional list containing 
-        multiple rows. Each row must include as first element the ordered predictor 
-        and as nth elements the nth - 1 exogenous variable (nth > 2).
-
-    Returns
-    -------
-    float
-        the prediction.
-        """
-        try:
-            result = []
-            result_tmp = 0
-            for i in range(len(self.X)):
-                result_tmp = 0
-                for j in range(len(self.coef_[i].values["coefficient"])):
-                    elem = self.coef_[i].values["predictor"][j]
-                    if elem.lower() == "intercept":
-                        result_tmp += self.coef_[i].values["coefficient"][j]
-                    else:
-                        ni, nj = elem[2:].split("_")
-                        ni, nj = int(ni), int(nj)
-                        result_tmp += (
-                            self.coef_[i].values["coefficient"][j] * L[-nj][ni]
-                        )
-                result += [result_tmp]
-            return result
-        except:
-            return None
+        else:
+            categories = self._parent[by].distinct()
+            X, Y = [], []
+            for cat in categories:
+                vdf = self._parent[by].isin(cat)
+                try:
+                    model.fit(vdf, [self._alias])
+                    data, layout = model._compute_plot_params()
+                    X += [data["x"]]
+                    Y += [data["y"]]
+                finally:
+                    model.drop()
+            X = np.column_stack(X)
+            Y = np.column_stack(Y)
+            vpy_plt, kwargs = self._parent.get_plotting_lib(
+                class_name="MultiDensityPlot",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            data = {"X": X, "Y": Y}
+            layout = {
+                "title": "KernelDensity",
+                "x_label": self._alias,
+                "y_label": "density",
+                "labels_title": by,
+                "labels": np.array(categories),
+            }
+            return vpy_plt.MultiDensityPlot(data=data, layout=layout).draw(**kwargs)
 
-    # ---#
+    # Time Series.
+
+    @save_verticapy_logs
+    def candlestick(
+        self,
+        ts: str,
+        method: str = "sum",
+        q: tuple[float, float] = (0.25, 0.75),
+        start_date: Optional[PythonScalar] = None,
+        end_date: Optional[PythonScalar] = None,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the Time Series of the vDataColumn.
+
+        Parameters
+        ----------
+        ts: str
+            TS  (Time Series)  vDataColumn used to order the
+            data.  The  vDataColumn  type must  be  date
+            like (date, datetime, timestamp...) or  numerical.
+        method: str, optional
+            The method used to aggregate the data.
+                count   : Number of elements.
+                mean    : Average of the vDataColumn 'of'.
+                min     : Minimum of the vDataColumn 'of'.
+                max     : Maximum of the vDataColumn 'of'.
+                sum     : Sum of the vDataColumn 'of'.
+                q%      : q Quantile of the vDataColumn 'of'
+                          (ex: 50% to get the median).
+            It   can  also  be  a  cutomized   aggregation
+            (ex: AVG(column1) + 5).
+        q: tuple, optional
+            Tuple including the  2 quantiles used to draw the
+            Plot.
+        start_date: str / PythonNumber / date, optional
+            Input Start Date. For example, time = '03-11-1993'
+            will  filter  the  data  when 'ts' is less than
+            the 3rd of November 1993.
+        end_date: str / PythonNumber / date, optional
+            Input  End  Date. For example, time = '03-11-1993'
+            will filter  the data when 'ts' is  greater  than
+            the 3rd of November 1993.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        ts = self._parent.format_colnames(ts)
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="CandleStick",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.CandleStick(
+            vdf=self._parent,
+            order_by=ts,
+            method=method,
+            q=q,
+            column=self._alias,
+            order_by_start=start_date,
+            order_by_end=end_date,
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
     def plot(
         self,
-        vdf: vDataFrame = None,
-        X: list = [],
-        ts: str = "",
-        X_idx: int = 0,
-        dynamic: bool = False,
-        one_step: bool = True,
-        observed: bool = True,
-        confidence: bool = True,
-        nlead: int = 10,
-        nlast: int = 0,
-        limit: int = 1000,
-        ax=None,
-        **style_kwds
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Draws the VAR model.
-
-    Parameters
-    ----------
-    vdf: vDataFrame
-        Object to use to run the prediction.
-    X: list, optional
-        List of the response columns.
-    ts: str, optional
-        vcolumn used to order the data.
-    X_idx: int, optional
-        Index of the main vector vcolumn to draw. It can also be the name of a 
-        predictor vcolumn.
-    dynamic: bool, optional
-        If set to True, the dynamic forecast will be drawn.
-    one_step: bool, optional
-        If set to True, the one step ahead forecast will be drawn.
-    observed: bool, optional
-        If set to True, the observation will be drawn.
-    confidence: bool, optional
-        If set to True, the confidence ranges will be drawn.
-    nlead: int, optional
-        Number of predictions computed by the dynamic forecast after
-        the last ts date.
-    nlast: int, optional
-        The dynamic forecast will start nlast values before the last
-        ts date.
-    limit: int, optional
-        Maximum number of past elements to use.
-    ax: Matplotlib axes object, optional
-        The axes to plot on.
-    **style_kwds
-        Any optional parameter to pass to the Matplotlib functions.
-
-    Returns
-    -------
-    ax 
-        Matplotlib axes object
-        """
-        if not (vdf):
-            vdf = vDataFrameSQL(relation=self.input_relation)
-        check_types(
-            [
-                ("limit", limit, [int, float]),
-                ("nlead", nlead, [int, float]),
-                ("X_idx", X_idx, [int, float, str]),
-                ("dynamic", dynamic, [bool]),
-                ("observed", observed, [bool]),
-                ("one_step", one_step, [bool]),
-                ("confidence", confidence, [bool]),
-                ("vdf", vdf, [vDataFrame]),
-            ],
-        )
-        delta_limit, limit = (
-            limit,
-            max(max(limit, self.parameters["p"] + 1 + nlast), 200),
-        )
-        delta_limit = max(limit - delta_limit - nlast, 0)
-        if not (ts):
-            ts = self.ts
-        if not (X):
-            X = self.X
-        assert dynamic or one_step or observed, ParameterError(
-            "No option selected.\n You should set either dynamic, one_step or observed to True."
-        )
-        assert nlead + nlast > 0 or not (dynamic), ParameterError(
-            "Dynamic Plots are only possible if either parameter 'nlead' is greater than 0 or parameter 'nlast' is greater than 0, and parameter 'dynamic' is set to True."
-        )
-        if isinstance(X_idx, str):
-            X_idx = quote_ident(X_idx).lower()
-            for idx, elem in enumerate(X):
-                if quote_ident(elem).lower() == X_idx:
-                    X_idx = idx
-                    break
-        assert (
-            isinstance(X_idx, (float, int)) and len(self.X) > X_idx >= 0
-        ), ParameterError(
-            "The index of the vcolumn to draw 'X_idx' must be between 0 and {}. It can also be the name of a predictor vcolumn.".format(
-                len(self.X)
-            )
-        )
-        result_all = self.predict(
-            vdf=vdf,
-            X=X,
+        ts: str,
+        by: Optional[str] = None,
+        start_date: Optional[PythonScalar] = None,
+        end_date: Optional[PythonScalar] = None,
+        kind: Literal[
+            "area", "area_percent", "area_stacked", "line", "spline", "step"
+        ] = "line",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws the Time Series of the vDataColumn.
+
+        Parameters
+        ----------
+        ts: str
+            TS  (Time Series)  vDataColumn used to order the
+            data.  The  vDataColumn  type must  be  date
+            like (date, datetime, timestamp...) or  numerical.
+        by: str, optional
+            vDataColumn used to partition the TS.
+        start_date: str / PythonNumber / date, optional
+            Input Start Date. For example, time = '03-11-1993'
+            will  filter  the  data  when 'ts' is less than
+            the 3rd of November 1993.
+        end_date: str / PythonNumber / date, optional
+            Input  End  Date. For example, time = '03-11-1993'
+            will filter  the data when 'ts' is  greater  than
+            the 3rd of November 1993.
+        kind: str, optional
+            The plot type.
+                area_stacked : Stacked Area Plot.
+                area_percent : Fully Stacked Area Plot.
+                line         : Line Plot.
+                spline       : Spline Plot.
+                area         : Area Plot.
+                step         : Step Plot.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        ts, by = self._parent.format_colnames(ts, by)
+        vpy_plt, kwargs = self._parent.get_plotting_lib(
+            class_name="LinePlot",
+            chart=chart,
+            style_kwargs=style_kwargs,
+        )
+        return vpy_plt.LinePlot(
+            vdf=self._parent,
+            order_by=ts,
+            columns=[self._alias, by] if by else [self._alias],
+            order_by_start=start_date,
+            order_by_end=end_date,
+            misc_layout={"kind": kind},
+        ).draw(**kwargs)
+
+    @save_verticapy_logs
+    def range_plot(
+        self,
+        ts: str,
+        q: tuple[float, float] = (0.25, 0.75),
+        start_date: Optional[PythonScalar] = None,
+        end_date: Optional[PythonScalar] = None,
+        plot_median: bool = False,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Draws  the  range   plot  of  the  vDataColumn.  The
+        aggregations  used  to draw the plot are  the  median
+        and the two user-specified quantiles.
+
+        Parameters
+        ----------
+        ts: str
+            TS  (Time Series)  vDataColumn  used  to order
+            the  data.  The  vDataColumn  type must  be  date
+            like (date, datetime, timestamp...) or  numerical.
+        q: tuple, optional
+            Tuple including the  2 quantiles used to draw the
+            Plot.
+        start_date: str / PythonNumber / date, optional
+            Input Start Date. For example, time = '03-11-1993'
+            will  filter  the  data  when 'ts' is less than
+            the 3rd of November 1993.
+        end_date: str / PythonNumber / date, optional
+            Input  End  Date. For example, time = '03-11-1993'
+            will filter  the data when 'ts' is  greater  than
+            the 3rd of November 1993.
+        plot_median: bool, optional
+            If set to True, the Median is drawn.
+        chart: PlottingObject, optional
+            The chart object to plot on.
+        **style_kwargs
+            Any  optional  parameter  to pass to the  plotting
+            functions.
+
+        Returns
+        -------
+        obj
+            Plotting Object.
+        """
+        return self._parent.range_plot(
+            columns=[self._alias],
             ts=ts,
-            nlead=0,
-            name=[
-                "_verticapy_prediction_{}_".format(idx) for idx in range(len(self.X))
-            ],
-        )
-        y, prediction = X[X_idx], "_verticapy_prediction_{}_".format(X_idx)
-        error_eps = 1.96 * math.sqrt(self.score(method="mse").values["mse"][X_idx])
-        print_info = verticapy.options["print_info"]
-        verticapy.options["print_info"] = False
-        try:
-            result = (
-                result_all.select([ts, y, prediction])
-                .dropna()
-                .sort([ts])
-                .tail(limit)
-                .values
-            )
-        except:
-            verticapy.options["print_info"] = print_info
-            raise
-        verticapy.options["print_info"] = print_info
-        columns = [elem for elem in result]
-        if isinstance(result[columns[0]][0], str):
-            result[columns[0]] = [parse(elem) for elem in result[columns[0]]]
-        true_value = [result[columns[0]], result[columns[1]]]
-        one_step_ahead = [result[columns[0]], result[columns[2]]]
-        lower_osa, upper_osa = (
-            [
-                float(elem) - error_eps if elem != None else None
-                for elem in one_step_ahead[1]
-            ],
-            [
-                float(elem) + error_eps if elem != None else None
-                for elem in one_step_ahead[1]
-            ],
-        )
-        if dynamic:
-            print_info = verticapy.options["print_info"]
-            verticapy.options["print_info"] = False
-            try:
-                result = (
-                    result_all.select([ts] + X).dropna().sort([ts]).tail(limit).values
-                )
-            except:
-                verticapy.options["print_info"] = print_info
-                raise
-            verticapy.options["print_info"] = print_info
-            columns = [elem for elem in result]
-            if isinstance(result[columns[0]][0], str):
-                result[columns[0]] = [parse(elem) for elem in result[columns[0]]]
-            deltat = result[columns[0]][-1] - result[columns[0]][-2]
-            lead_time_list, lead_list = [], []
-            if nlast > 0:
-                for i in range(len(result[columns[0]][:-nlast])):
-                    lead_list += [[result[elem][i] for elem in columns[1:]]]
-            else:
-                for i in range(len(result[columns[0]])):
-                    lead_list += [[result[elem][i] for elem in columns[1:]]]
-            for i in range(nlast):
-                lead_list += [self.fpredict(lead_list)]
-                lead_time_list += [result[columns[0]][i - nlast]]
-            if lead_time_list:
-                start_time = lead_time_list[-1]
-            else:
-                start_time = result[columns[0]][-1]
-            for i in range(nlead):
-                lead_list += [self.fpredict(lead_list)]
-                lead_time_list += [start_time + (i + 1) * deltat]
-            dynamic_forecast = (
-                [result[columns[0]][-nlast - 1]] + lead_time_list,
-                [result[columns[1 + X_idx]][-nlast - 1]]
-                + [elem[X_idx] for elem in lead_list[-nlast - nlead :]],
-            )
-            lower_d, upper_d = [], []
-            for i in range(len(dynamic_forecast[1])):
-                delta_error = error_eps * math.sqrt(i + 1)
-                lower_d += [float(dynamic_forecast[1][i]) - delta_error]
-                upper_d += [float(dynamic_forecast[1][i]) + delta_error]
+            q=q,
+            start_date=start_date,
+            end_date=end_date,
+            plot_median=plot_median,
+            chart=chart,
+            **style_kwargs,
+        )
+
+    # Geospatial.
+
+    @save_verticapy_logs
+    def geo_plot(self, *args, **kwargs) -> PlottingObject:
+        """
+        Draws the Geospatial object.
+
+        Parameters
+        ----------
+        *args / **kwargs
+            Any optional parameter to pass to the geopandas
+            plot function.
+            For more information, see:
+            https://geopandas.readthedocs.io/en/latest/
+            docs/reference/api/geopandas.GeoDataFrame.plot.html
+
+        Returns
+        -------
+        ax
+            Axes
+        """
+        columns = [self._alias]
+        check = True
+        if len(args) > 0:
+            column = args[0]
+        elif "column" in kwargs:
+            column = kwargs["column"]
         else:
-            lower_d, upper_d, dynamic_forecast = [], [], ([], [])
-        alpha = 0.3
-        if not (ax):
-            fig, ax = plt.subplots()
-            if isnotebook():
-                fig.set_size_inches(10, 6)
-            ax.grid()
-        colors = gen_colors()
-        param1 = {
-            "color": colors[2],
-            "linewidth": 2,
-        }
-        param2 = {
-            "color": colors[3],
-            "linewidth": 2,
-            "linestyle": ":",
-        }
-        param3 = {
-            "color": colors[0],
-            "linewidth": 2,
-            "linestyle": "dashed",
-        }
-        if dynamic:
-            ax.fill_between(
-                dynamic_forecast[0],
-                1.02
-                * float(min(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-                1.02
-                * float(max(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-                alpha=0.04,
-                color=updated_dict(param3, style_kwds, 2)["color"],
-            )
-            if confidence:
-                ax.fill_between(
-                    dynamic_forecast[0], lower_d, upper_d, alpha=0.08, color="#555555"
-                )
-                ax.plot(dynamic_forecast[0], lower_d, alpha=0.08, color="#000000")
-                ax.plot(dynamic_forecast[0], upper_d, alpha=0.08, color="#000000")
-            ax.plot(
-                dynamic_forecast[0],
-                dynamic_forecast[1],
-                label="Dynamic Forecast",
-                **updated_dict(param3, style_kwds, 2)
-            )
-        if one_step:
-            if confidence:
-                ax.fill_between(
-                    one_step_ahead[0][delta_limit:],
-                    lower_osa[delta_limit:],
-                    upper_osa[delta_limit:],
-                    alpha=0.04,
-                    color="#555555",
-                )
-                ax.plot(
-                    one_step_ahead[0][delta_limit:],
-                    lower_osa[delta_limit:],
-                    alpha=0.04,
-                    color="#000000",
-                )
-                ax.plot(
-                    one_step_ahead[0][delta_limit:],
-                    upper_osa[delta_limit:],
-                    alpha=0.04,
-                    color="#000000",
-                )
-            ax.plot(
-                one_step_ahead[0][delta_limit:],
-                one_step_ahead[1][delta_limit:],
-                label="One-step ahead Forecast",
-                **updated_dict(param2, style_kwds, 1)
-            )
-        if observed:
-            ax.plot(
-                true_value[0][delta_limit:],
-                true_value[1][delta_limit:],
-                label="Observed",
-                **updated_dict(param1, style_kwds, 0)
-            )
-        ax.set_title("VAR({}) [{}]".format(self.parameters["p"], y))
-        ax.set_xlabel(ts)
-        ax.legend(loc="center left", bbox_to_anchor=[1, 0.5])
-        ax.set_ylim(
-            1.02 * float(min(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-            1.02 * float(max(true_value[1] + dynamic_forecast[1] + one_step_ahead[1])),
-        )
-        for tick in ax.get_xticklabels():
-            tick.set_rotation(90)
-        return ax
-
-    # ---#
-    def predict(
-        self,
-        vdf: vDataFrame,
-        X: list = [],
-        ts: str = "",
-        nlead: int = 0,
-        name: list = [],
-    ):
-        """
-    ---------------------------------------------------------------------------
-    Predicts using the input relation.
-
-    Parameters
-    ----------
-    vdf: vDataFrame
-        Object to use to run the prediction.
-    X: list, optional
-        List of the response columns.
-    ts: str, optional
-        vcolumn used to order the data.
-    nlead: int, optional
-        Number of records to predict after the last ts date.
-    name: list, optional
-        Names of the added vcolumns. If empty, names will be generated.
-
-    Returns
-    -------
-    vDataFrame
-        object including the prediction.
-        """
-        check_types(
-            [
-                ("name", name, [list]),
-                ("ts", ts, [str]),
-                ("nlead", nlead, [int, float]),
-                ("X", X, [list]),
-                ("vdf", vdf, [vDataFrame]),
-            ],
-        )
-        if not (ts):
-            ts = self.ts
-        if not (X):
-            X = self.X
-        vdf.are_namecols_in(X + [ts])
-        X = vdf.format_colnames(X)
-        ts = vdf.format_colnames(ts)
-        all_pred, names = [], []
-        transform_relation = self.transform_relation.replace("[VerticaPy_ts]", self.ts)
-        for idx, elem in enumerate(X):
-            name_tmp = (
-                "{}_".format(self.type) + "".join(ch for ch in elem if ch.isalnum())
-                if len(name) != len(X)
-                else name[idx]
-            )
-            all_pred += ["{} AS {}".format(self.deploySQL()[idx], name_tmp)]
-            transform_relation = transform_relation.replace("[X{}]".format(idx), elem)
-        columns = vdf.get_columns() + all_pred
-        relation = vdf.__genSQL__()
-        for i in range(nlead):
-            query = "SELECT ({} - LAG({}, 1) OVER (ORDER BY {}))::VARCHAR FROM {} ORDER BY {} DESC LIMIT 1".format(
-                ts, ts, ts, relation, ts
-            )
-            deltat = executeSQL(query, method="fetchfirstelem", print_time_sql=False)
-            query = "SELECT (MAX({}) + '{}'::interval)::VARCHAR FROM {}".format(
-                ts, deltat, relation
-            )
-            next_t = executeSQL(query, method="fetchfirstelem", print_time_sql=False)
-            if i == 0:
-                first_t = next_t
-            new_line = "SELECT '{}'::TIMESTAMP AS {}, {}".format(
-                next_t,
-                ts,
-                ", ".join(
-                    [
-                        "NULL AS {}".format(column)
-                        for column in vdf.get_columns(exclude_columns=[ts])
-                    ]
-                ),
-            )
-            relation_tmp = "(SELECT {} FROM {} UNION ALL ({})) VERTICAPY_SUBTABLE".format(
-                ", ".join([ts] + vdf.get_columns(exclude_columns=[ts])),
-                relation,
-                new_line,
-            )
-            query = "SELECT {} FROM {} ORDER BY {} DESC LIMIT 1".format(
-                ", ".join(self.deploySQL()), transform_relation.format(relation_tmp), ts
-            )
-            prediction = executeSQL(query, method="fetchrow", print_time_sql=False)
-            for idx, elem in enumerate(X):
-                prediction[idx] = "{} AS {}".format(prediction[idx], elem)
-            columns_tmp = vdf.get_columns(exclude_columns=[ts] + X)
-            new_line = "SELECT '{}'::TIMESTAMP AS {}, {} {}".format(
-                next_t,
-                ts,
-                ", ".join(prediction),
-                (", " if (columns_tmp) else "")
-                + ", ".join(["NULL AS {}".format(column) for column in columns_tmp]),
-            )
-            relation = "(SELECT {} FROM {} UNION ALL ({})) VERTICAPY_SUBTABLE".format(
-                ", ".join([ts] + X + vdf.get_columns(exclude_columns=[ts] + X)),
-                relation,
-                new_line,
-            )
-        final_relation = "(SELECT {} FROM {}) VERTICAPY_SUBTABLE".format(
-            ", ".join(columns), transform_relation.format(relation)
-        )
-        result = vDataFrameSQL(final_relation, "VAR")
-        if nlead > 0:
-            for elem in X:
-                result[elem].apply(
-                    "CASE WHEN {} >= '{}' THEN NULL ELSE {} END".format(
-                        ts, first_t, "{}"
-                    )
-                )
-        return result
+            check = False
+        if check:
+            column = self._parent.format_colnames(column)
+            columns += [column]
+            if "cmap" not in kwargs:
+                kwargs["cmap"] = PlottingBase().get_cmap(idx=0)
+        else:
+            if "color" not in kwargs:
+                kwargs["color"] = PlottingBase().get_colors(idx=0)
+        if "legend" not in kwargs:
+            kwargs["legend"] = True
+        if "figsize" not in kwargs:
+            kwargs["figsize"] = (14, 10)
+        return self._parent[columns].to_geopandas(self._alias).plot(*args, **kwargs)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/logo.py` & `verticapy-1.0.0b1/verticapy/_utils/_logo.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,64 +1,37 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# File used to generate the VerticaPy logo
-#
-# ---#
-def gen_verticapy_logo_html(size: str = "50%"):
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+
+
+def verticapy_logo_html(size: str = "50%") -> str:
+    """
+    Generates the HTML code for the VerticaPy logo.
+    """
     return f'<center><img src="data:image/png;base64,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" style="width:{size};"></center>'
 
 
-# ---#
-def gen_verticapy_logo_str():
+def verticapy_logo_str() -> str:
+    """
+    Generates the Python string that represents the
+    VerticaPy logo.
+    """
     img = "              ____________       ______\n"
     img += "             / __        `\\     /     /\n"
     img += "            |  \\/         /    /     /\n"
     img += "            |______      /    /     /\n"
     img += "                   |____/    /     /\n"
     img += "          _____________     /     /\n"
     img += "          \\           /    /     /\n"
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/sql.py` & `verticapy-1.0.0b1/verticapy/jupyter/extensions/chart_magic.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,303 +1,305 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-##
-#  _____  _____ _      ___  ___  ___  _____ _____ _____
-# /  ___||  _  | |     |  \/  | / _ \|  __ \_   _/  __ \
-# \ `--. | | | | |     | .  . |/ /_\ \ |  \/ | | | /  \/
-#  `--. \| | | | |     | |\/| ||  _  | | __  | | | |
-# /\__/ /\ \/' / |____ | |  | || | | | |_\ \_| |_| \__/\
-# \____/  \_/\_\_____/ \_|  |_/\_| |_/\____/\___/ \____/
-#
-##
-#
-# ---#
-# Jupyter Modules
-from IPython.core.magic import needs_local_scope
-from IPython.core.display import HTML, display
-
-# Standard Python Modules
-import warnings, re, time
-
-# Other modules
-import pandas as pd
-
-# VerticaPy Modules
-import verticapy
-from verticapy.errors import QueryError, ParameterError
-from verticapy import (
-    executeSQL,
-    vDataFrameSQL,
-    get_magic_options,
-    vDataFrame,
-    set_option,
-    tablesample,
-    clean_query,
-    replace_vars_in_query,
-)
-
-# ---#
-@needs_local_scope
-def sql(line, cell="", local_ns=None):
-
-    # We don't want to display the query/time twice if the options are still on
-    # So we save the previous configuration and turn them off.
-    sql_on, time_on = verticapy.options["sql_on"], verticapy.options["time_on"]
-    set_option("sql_on", False)
-    set_option("time_on", False)
-
-    try:
-
-        # Initialization
-        queries = "" if (not (cell) and (line)) else cell
-
-        # Options
-        options = {}
-        all_options_dict = get_magic_options(line)
-
-        for option in all_options_dict:
-
-            if option.lower() in ("-f", "--file", "-o", "--output", "-nrows", "-ncols", "-c", "--command"):
-
-                if option.lower() in ("-f", "--file"):
-                    if "-f" in options:
-                        raise ParameterError("Duplicate option '-f'.")
-                    options["-f"] = all_options_dict[option]
-                elif option.lower() in ("-o", "--output"):
-                    if "-o" in options:
-                        raise ParameterError("Duplicate option '-o'.")
-                    options["-o"] = all_options_dict[option]
-                elif option.lower() in ("-c", "--command"):
-                    if "-c" in options:
-                        raise ParameterError("Duplicate option '-c'.")
-                    options["-c"] = all_options_dict[option]
-                elif option.lower() in ("-nrows",):
-                    if "-nrows" in options:
-                        raise ParameterError("Duplicate option '-nrows'.")
-                    options["-nrows"] = int(all_options_dict[option])
-                elif option.lower() in ("-ncols",):
-                    if "-ncols" in options:
-                        raise ParameterError("Duplicate option '-ncols'.")
-                    options["-ncols"] = int(all_options_dict[option])
-
-            elif verticapy.options["print_info"]:
-                warning_message = (
-                    f"\u26A0 Warning : The option '{option}' doesn't "
-                    "exist, it was skipped."
-                )
-                warnings.warn(warning_message, Warning)
-
-        if "-f" in options and "-c" in options:
-            raise ParameterError("Do not find which query to run: One of "
-                                 "the options '-f' and '-c' must be empty.")
-
-        if cell and ("-f" in options or "-c" in options):
-            raise ParameterError("Cell must be empty when using options '-f' or '-c'.")
-
-        if "-f" in options:
-            f = open(options["-f"], "r")
-            queries = f.read()
-            f.close()
-
-        elif "-c" in options:
-            queries = options["-c"]
-
-        # Cleaning the Query
-        queries = clean_query(queries)
-        queries = replace_vars_in_query(queries, locals()["local_ns"])
-
-        n, i, all_split = len(queries), 0, []
-
-        while i < n and queries[n - i - 1] in (";", " ", "\n"):
-            i += 1
-
-        queries = queries[: n - i]
-        i, n = 0, n - i
-
-        while i < n:
-
-            if queries[i] == '"':
-                i += 1
-                while i < n and queries[i] != '"':
-                    i += 1
-            elif queries[i] == "'":
-                i += 1
-                while i < n and queries[i] != "'":
-                    i += 1
-            elif queries[i] == ";":
-                all_split += [i]
-            i += 1
-
-        all_split = [0] + all_split + [n]
-        m = len(all_split)
-        queries = [queries[all_split[i] : all_split[i + 1]] for i in range(m - 1)]
-        n = len(queries)
-
-        for i in range(n):
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import time
+import warnings
+from typing import Optional, Literal, Union
 
-            query = queries[i]
-            while len(query) > 0 and (query[-1] in (";", " ")):
-                query = query[0:-1]
-            while len(query) > 0 and (query[0] in (";", " ")):
-                query = query[1:]
-            queries[i] = query
-
-        queries_tmp, i = [], 0
-
-        while i < n:
-
-            query = queries[i]
-            if (i < n - 1) and (queries[i + 1].lower() == "end"):
-                query += "; {}".format(queries[i + 1])
-                i += 1
-            queries_tmp += [query]
-            i += 1
-
-        queries, n = queries_tmp, len(queries_tmp)
-        result, start_time = None, time.time()
-
-        # Executing the Queries
-
-        for i in range(n):
-
-            query = queries[i]
-
-            if query.split(" ")[0]:
-                query_type = query.split(" ")[0].upper()
-            else:
-                query_type = query.split(" ")[1].upper()
-
-            if len(query_type) > 1 and query_type[0:2] in ("/*", "--"):
-                query_type = "undefined"
+from IPython.core.magic import needs_local_scope
+from IPython.display import display, HTML
 
-            if (query_type == "COPY") and ("from local" in query.lower()):
+from vertica_highcharts import Highstock, Highchart
 
-                query = re.split("from local", query, flags=re.IGNORECASE)
-                if query[1].split(" ")[0]:
-                    file_name = query[1].split(" ")[0]
+import verticapy._config.config as conf
+from verticapy._typing import PlottingObject
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import clean_query, replace_vars_in_query
+
+from verticapy.core.vdataframe.base import vDataFrame
+
+from verticapy.jupyter.extensions._utils import get_magic_options
+
+from verticapy.plotting._utils import PlottingUtils
+
+CLASS_NAME_MAP = {
+    "auto": None,
+    "area": "MultiLinePlot",
+    "area_percent": "MultiLinePlot",
+    "area_stacked": "MultiLinePlot",
+    "bar": "BarChart",
+    "bar2D": "BarChart2D",
+    "barh": "HorizontalBarChart",
+    "barh2D": "HorizontalBarChart2D",
+    "biserial": None,
+    "boxplot": "BoxPlot",
+    "bubble": "ScatterPlot",
+    "candlestick": "CandleStick",
+    "cramer": None,
+    "fstacked_barh": "HorizontalBarChart2D",
+    "drilldown_bar": "DrillDownBarChart",
+    "drilldown_barh": "DrillDownHorizontalBarChart",
+    "donut": "PieChart",
+    "heatmap": "HeatMap",
+    "hist": "Histogram",
+    "kendall": None,
+    "line": "LinePlot",
+    "multi_area": "MultiLinePlot",
+    "multi_line": "MultiLinePlot",
+    "multi_spline": "MultiLinePlot",
+    "multi_step": "MultiLinePlot",
+    "negative_bar": "HorizontalBarChart2D",
+    "nested_pie": "NestedPieChart",
+    "outliers": "OutliersPlot",
+    "pearson": None,
+    "pie": "PieChart",
+    "rose": "PieChart",
+    "scatter": "ScatterPlot",
+    "scatter_matrix": "ScatterMatrix",
+    "spearman": None,
+    "spearmand": None,
+    "spider": "SpiderChart",
+    "spline": "LinePlot",
+    "stacked_bar": "BarChart2D",
+    "stacked_barh": "HorizontalBarChart2D",
+    "step": "LinePlot",
+}
+
+
+def get_kind_option(kind: Literal[tuple(CLASS_NAME_MAP)] = "auto") -> dict:
+    kind_option = {}
+    other_params = {}
+    if kind in [
+        "area",
+        "area_percent",
+        "area_stacked",
+        "bubble",
+        "donut",
+        "line",
+        "rose",
+        "spline",
+        "step",
+    ]:
+        kind_option["kind"] = kind
+        if kind == "bubble":
+            other_params["kind"] = "bubble"
+    elif kind in ["area"]:
+        kind_option["kind"] = "area_stacked"
+    elif kind in ["multi_area", "multi_line", "multi_spline", "multi_step"]:
+        kind_option["kind"] = kind[6:]
+    elif kind == "negative_bar":
+        kind_option["kind"] = "density"
+        kind_option["method"] = "density"
+    elif kind in ["fstacked_bar", "fstacked_barh"]:
+        kind_option["kind"] = "fully_stacked"
+    elif kind in ["stacked_bar", "stacked_barh"]:
+        kind_option["kind"] = "stacked"
+    elif kind in ["bar2D", "barh2D", "pie"]:
+        kind_option["kind"] = "regular"
+    return {"misc_layout": kind_option, **other_params}
+
+
+def chartSQL(
+    query: str,
+    kind: Literal[tuple(CLASS_NAME_MAP)] = "auto",
+) -> PlottingObject:
+    """
+    Helper Function:
+    Draws a custom High Chart graphic using the
+    input SQL query.
+    """
+    if kind in [
+        "auto",
+        "bar",
+        "barh",
+        "biserial",
+        "cramer",
+        "donut",
+        "line",
+        "kendall",
+        "pearson",
+        "pie",
+        "rose",
+        "spearman",
+        "spearmand",
+        "spline",
+        "step",
+    ]:
+        vdf = vDataFrame(input_relation=query)
+        cols = vdf.get_columns()
+        if kind == "auto":
+            if len(cols) == 1:
+                if vdf[cols[0]].isnum():
+                    return vdf[cols[0]].boxplot()
                 else:
-                    file_name = query[1].split(" ")[1]
-                query = (
-                    "".join(query[0])
-                    + "FROM"
-                    + "".join(query[1]).replace(file_name, "STDIN")
-                )
-                if (file_name[0] == file_name[-1]) and (file_name[0] in ('"', "'")):
-                    file_name = file_name[1:-1]
-
-                executeSQL(query, method="copy", path=file_name, print_time_sql=False)
-
-            elif (i < n - 1) or (
-                (i == n - 1)
-                and (query_type.lower() not in ("select", "with", "undefined"))
+                    return vdf[cols[0]].pie()
+            elif len(cols) > 1 and vdf[cols[0]].isdate():
+                kind = "line"
+            elif len(cols) == 2 and vdf[cols[0]].isnum() and vdf[cols[1]].isnum():
+                kind = "hist"
+            elif len(cols) == 2 and vdf[cols[1]].isnum():
+                kind = "barh"
+            elif (
+                len(cols) == 3
+                and not vdf[cols[0]].isnum()
+                and not vdf[cols[1]].isnum()
+                and vdf[cols[2]].isnum()
             ):
-
-                executeSQL(query, print_time_sql=False)
-                if verticapy.options["print_info"]:
-                    print(query_type)
-
+                kind = "barh2D"
+            elif len(cols) > 4:
+                return vdf.boxplot()
             else:
+                kind = "scatter"
+        elif kind in [
+            "biserial",
+            "cramer",
+            "kendall",
+            "pearson",
+            "spearman",
+            "spearmand",
+        ]:
+            return vdf.corr(method=kind)
+        elif kind in ["line", "spline", "step"]:
+            if len(cols) > 3 or ((len(cols) == 3) and (vdf[cols[2]].isnum())):
+                kind = "multi_" + kind
+        elif kind in ["bar", "barh"]:
+            if len(cols) > 2:
+                kind = kind + "2D"
+        elif kind in ["donut", "pie", "rose"]:
+            if len(cols) > 2:
+                kind = "nested_pie"
+    class_name = CLASS_NAME_MAP[kind]
+    vpy_plt, kwargs = PlottingUtils().get_plotting_lib(class_name=class_name)
+    graph = getattr(vpy_plt, class_name)
+    return graph(query=query, **get_kind_option(kind)).draw(**kwargs)
 
-                error = ""
-                try:
-                    result = vDataFrameSQL("({}) x".format(query))
-                    # Display parameters
-                    if "-nrows" in options:
-                        result._VERTICAPY_VARIABLES_["max_rows"] = options["-nrows"]
-                    if "-ncols" in options:
-                        result._VERTICAPY_VARIABLES_["max_columns"] = options["-ncols"]
-
-                except:
-                    try:
-                        final_result = executeSQL(
-                            query, method="fetchfirstelem", print_time_sql=False
-                        )
-                        if final_result and verticapy.options["print_info"]:
-                            print(final_result[0])
-                        elif verticapy.options["print_info"]:
-                            print(query_type)
-                    except Exception as e:
-                        error = e
-
-                if error:
-                    raise QueryError(error)
-
-        # Displaying the information
-
-        elapsed_time = round(time.time() - start_time, 3)
-
-        if verticapy.options["print_info"]:
-            display(HTML(f"<div><b>Execution: </b> {elapsed_time}s</div>"))
 
-        # Exporting the result
+@save_verticapy_logs
+@needs_local_scope
+def chart_magic(
+    line: str, cell: Optional[str] = None, local_ns: Optional[dict] = None
+) -> Union[Highstock, Highchart]:
+    """
+    Draws  responsive charts using the High Chart  API:
+    https://api.highcharts.com/highcharts/
+    The returned object can be customized using the API
+    parameters and the 'set_dict_options' method.
+
+    -c / --command : SQL Command to execute.
+
+    -f  /   --file : Input File. You can use this option
+                     if  you  want to execute the  input
+                     file.
+
+    -k  /  --kind  : Chart Type, one  of  the following:
+                     area  / area_range  / area_ts / bar
+                     biserial   /   boxplot   /   bubble
+                     candlestick   /   cramer  /   donut
+                     donut3d  / heatmap / hist / kendall
+                     line / negative_bar / pearson / pie
+                     pie_half / pie3d / scatter / spider
+                     spline / stacked_bar / stacked_hist
+                     spearman
+
+     -o / --output : Output File. You can use this option
+                     if  you want to export the result of
+                     the query to the HTML format.
+    """
+
+    # Initialization
+    query = "" if (not cell and (line)) else cell
+
+    # Options
+    options = {}
+    options_dict = get_magic_options(line)
+
+    for option in options_dict:
+        if option.lower() in (
+            "-f",
+            "--file",
+            "-o",
+            "--output",
+            "-c",
+            "--command",
+            "-k",
+            "--kind",
+        ):
+            if option.lower() in ("-f", "--file"):
+                if "-f" in options:
+                    raise ValueError("Duplicate option '-f'.")
+                options["-f"] = options_dict[option]
+            elif option.lower() in ("-o", "--output"):
+                if "-o" in options:
+                    raise ValueError("Duplicate option '-o'.")
+                options["-o"] = options_dict[option]
+            elif option.lower() in ("-c", "--command"):
+                if "-c" in options:
+                    raise ValueError("Duplicate option '-c'.")
+                options["-c"] = options_dict[option]
+            elif option.lower() in ("-k", "--kind"):
+                if "-k" in options:
+                    raise ValueError("Duplicate option '-k'.")
+                options["-k"] = options_dict[option]
+
+        elif conf.get_option("print_info"):
+            warning_message = (
+                f"\u26A0 Warning : The option '{option}' doesn't exist - skipping."
+            )
+            warnings.warn(warning_message, Warning)
+
+    if "-f" in options and "-c" in options:
+        raise ValueError(
+            "Could not find a query to run; the options"
+            "'-f' and '-c' cannot be used together."
+        )
+
+    if cell and ("-f" in options or "-c" in options):
+        raise ValueError("Cell must be empty when using options '-f' or '-c'.")
+
+    if "-f" in options:
+        with open(options["-f"], "r", encoding="utf-8") as f:
+            query = f.read()
+            f.close()
 
-        if isinstance(result, vDataFrame) and "-o" in options:
+    elif "-c" in options:
+        query = options["-c"]
 
-            if options["-o"][-4:] == "json":
-                result.to_json(options["-o"])
-            else:
-                result.to_csv(options["-o"])
+    if "-k" not in options:
+        options["-k"] = "auto"
+
+    # Cleaning the Query
+    query = clean_query(query)
+    query = replace_vars_in_query(query, locals()["local_ns"])
 
-        # we load the previous configuration before returning the result.
-        set_option("sql_on", sql_on)
-        set_option("time_on", time_on)
+    # Drawing the graphic
+    start_time = time.time()
+    chart = chartSQL(query, options["-k"])
 
-        return result
+    # Exporting the result
+    if "-o" in options:
+        chart.save_file(options["-o"])
 
-    except:
+    # Displaying the time
+    elapsed_time = round(time.time() - start_time, 3)
+    display(HTML(f"<div><b>Execution: </b> {elapsed_time}s</div>"))
 
-        # If it fails, we load the previous configuration before raising the error.
-        set_option("sql_on", sql_on)
-        set_option("time_on", time_on)
-        
-        raise
+    return chart
 
 
-# ---#
-def load_ipython_extension(ipython):
-    ipython.register_magic_function(sql, "cell")
-    ipython.register_magic_function(sql, "line")
+def load_ipython_extension(ipython) -> None:
+    ipython.register_magic_function(chart_magic, "cell", "chart")
+    ipython.register_magic_function(chart_magic, "line", "chart")
+    ipython.register_magic_function(chart_magic, "cell", "plot")
+    ipython.register_magic_function(chart_magic, "line", "plot")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/tests/__init__.py` & `verticapy-1.0.0b1/verticapy/core/string_sql/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,12 +1,17 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+from verticapy.core.string_sql.base import StringSQL
```

### Comparing `verticapy-0.9.0/verticapy/tests/base.py` & `verticapy-1.0.0b1/verticapy/tests/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,33 +1,37 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 from __future__ import print_function, division, absolute_import
 
 import os
 import sys
 import logging
 import unittest
 import inspect
 import getpass
 import vertica_python
 
 from configparser import ConfigParser
 
-from ..util.log import VerticaLogging
+from .utils.log import VerticaLogging
 
 
 default_configs = {
     "log_dir": "vp_test_log",
     "log_level": logging.WARNING,
     "host": "localhost",
     "port": 5433,
```

### Comparing `verticapy-0.9.0/verticapy/tests/conftest.py` & `verticapy-1.0.0b1/verticapy/tests/conftest.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 import vertica_python, pytest, os, verticapy
 from .base import VerticaPyTestBase
 from configparser import ConfigParser
 
 
 def create_conn_file():
@@ -31,15 +35,15 @@
         confparser.read(path)
         if confparser.has_section("vp_test_config"):
             confparser.remove_section("vp_test_config")
         confparser.add_section("vp_test_config")
         for elem in base_test.test_config:
             if elem != "log_level":
                 confparser.set("vp_test_config", elem, str(base_test.test_config[elem]))
-        f = open(path, "w+")
+        f = open(path, "w+", encoding="utf-8")
         confparser.write(f)
         f.close()
 
 
 def delete_conn_file():
     os.remove(os.path.dirname(verticapy.__file__) + "/tests/verticaPy_test_tmp.conf")
 
@@ -48,26 +52,32 @@
     base_class = VerticaPyTestBase()
     base_class.setUp()
     create_conn_file()
     verticapy.connect(
         "vp_test_config",
         os.path.dirname(verticapy.__file__) + "/tests/verticaPy_test_tmp.conf",
     )
-    result = verticapy.version()
+    result = verticapy.vertica_version()
     base_class.tearDown()
-    delete_conn_file()
+    try:
+        delete_conn_file()
+    except:
+        pass
     return result
 
 
 @pytest.fixture(scope="session")
 def base():
     base_class = VerticaPyTestBase()
     base_class.setUp()
     create_conn_file()
     verticapy.connect(
         "vp_test_config",
         os.path.dirname(verticapy.__file__) + "/tests/verticaPy_test_tmp.conf",
     )
     yield base_class
     base_class.tearDown()
-    delete_conn_file()
+    try:
+        delete_conn_file()
+    except:
+        pass
     verticapy.close_connection()
```

### Comparing `verticapy-0.9.0/verticapy/tests/connect/__init__.py` & `verticapy-1.0.0b1/verticapy/core/tablesample/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,12 +1,17 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+from verticapy.core.tablesample.base import TableSample
```

### Comparing `verticapy-0.9.0/verticapy/tests/connect/test_connect.py` & `verticapy-1.0.0b1/verticapy/tests/connect/test_connect.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,51 +1,64 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
 
-# VerticaPy
-from verticapy.connect import *
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import os
+import verticapy as vp
+from verticapy.connection import *
+from verticapy.connection.global_connection import get_global_connection
 
 
 class TestConnect:
     def test_auto_connection(self, base):
         # test for read_dsn / new_connection / available_connections /
-        #          read_auto_connect / change_auto_connection / delete_connection.
+        #          auto_connect / change_auto_connection / delete_connection.
         # read_dsn
+        gb_conn = get_global_connection()
         d = read_dsn(
             "vp_test_config",
-            os.path.dirname(verticapy.__file__) + "/tests/verticaPy_test_tmp.conf",
+            os.path.dirname(vp.__file__) + "/tests/verticaPy_test_tmp.conf",
         )
         assert int(d["port"]) > 0
         # new_auto_connection
         new_connection(d, "vp_test_config")
         # available_connections
         result = available_connections()
         assert "vp_test_config" in result
         # change_auto_connection
         change_auto_connection("vp_test_config")
-        # read_auto_connect
-        read_auto_connect()
-        cur = verticapy.options["connection"]["conn"].cursor()
+        # auto_connect
+        auto_connect()
+        cur = gb_conn.get_connection().cursor()
         cur.execute("SELECT 1;")
         result2 = cur.fetchone()
         assert result2 == [1]
         # delete_connection
         assert delete_connection("vp_test_config")
+        # connection label
+        current_cursor().execute(
+            "SELECT client_label FROM v_monitor.sessions WHERE client_label LIKE 'verticapy%' LIMIT 1;"
+        )
+        label = current_cursor().fetchone()[0].split("-")
+        assert label[1] == vp.__version__.split("-")[0]
+        # assert label[2] == str(gb_conn.vpy_session_identifier)
 
     def test_vertica_connection(self, base):
         cur = vertica_connection(
             "vp_test_config",
-            os.path.dirname(verticapy.__file__) + "/tests/verticaPy_test_tmp.conf",
+            os.path.dirname(vp.__file__) + "/tests/verticaPy_test_tmp.conf",
         ).cursor()
         cur.execute("SELECT 1;")
         result = cur.fetchone()
         assert result == [1]
```

### Comparing `verticapy-0.9.0/verticapy/tests/datasets/__init__.py` & `verticapy-1.0.0b1/verticapy/learn/memmodel/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,12 +1,17 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+from verticapy.machine_learning.memmodel.base import InMemoryModel
```

### Comparing `verticapy-0.9.0/verticapy/tests/datasets/test_datasets.py` & `verticapy-1.0.0b1/verticapy/tests/datasets/test_datasets.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import datetime
 
@@ -28,15 +32,18 @@
     def test_gen_dataset(self):
         result = gen_dataset(
             features_ranges={
                 "name": {"type": str, "values": ["Badr", "Badr", "Raghu", "Waqas"]},
                 "age": {"type": int, "range": [20, 40]},
                 "distance": {"type": float, "range": [1000, 4000]},
                 "date": {"type": datetime.date, "range": ["1993-11-03", 365]},
-                "datetime": {"type": datetime.datetime, "range": ["1993-11-03", 365]},
+                "datetime": {
+                    "type": datetime.datetime,
+                    "range": ["1993-11-03", 365],
+                },
             }
         )
         assert result["name"].mode() == "Badr"
         assert result["date"].max() < datetime.date(1995, 11, 2)
         assert result["datetime"].max() < datetime.datetime(1995, 11, 2)
         assert result["distance"].max() < 4001
         assert result["age"].max() < 41
```

### Comparing `verticapy-0.9.0/verticapy/tests/geo/__init__.py` & `verticapy-1.0.0b1/verticapy/_config/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/geo/test_geo.py` & `verticapy-1.0.0b1/verticapy/tests/geo/test_geo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
 from verticapy.datasets import load_cities, load_world
@@ -43,15 +47,17 @@
         result = create_index(world_copy, "id", "geometry", "world_polygons", True)
         assert result["polygons"][0] == 177
         assert result["min_x"][0] == pytest.approx(-180.0)
         assert result["min_y"][0] == pytest.approx(-90.0)
         assert result["max_x"][0] == pytest.approx(180.0)
         assert result["max_y"][0] == pytest.approx(83.64513)
         rename_index(
-            "world_polygons", "world_polygons_rename", True,
+            "world_polygons",
+            "world_polygons_rename",
+            True,
         )
         result2 = describe_index("world_polygons_rename", True)
         assert result2.shape() == (177, 3)
         cities_copy = cities_vd.copy()
         cities_copy["id"] = "ROW_NUMBER() OVER (ORDER BY city)"
         result3 = intersect(cities_copy, "world_polygons_rename", "id", "geometry")
         assert result3.shape() == (172, 2)
```

### Comparing `verticapy-0.9.0/verticapy/tests/hchart/__init__.py` & `verticapy-1.0.0b1/verticapy/_utils/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/hchart/test_hchart.py` & `verticapy-1.0.0b1/verticapy/tests/hchart/test_hchart.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,35 +1,39 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import os
 
 # Other Modules
-from highcharts.highcharts.highcharts import Highchart
-from highcharts.highstock.highstock import Highstock
+from vertica_highcharts.highcharts.highcharts import Highchart
+from vertica_highcharts.highstock.highstock import Highstock
 
 # VerticaPy
 import verticapy
 from verticapy import drop, set_option
 from verticapy.datasets import load_titanic, load_amazon
-from verticapy.hchart import hchart
+from verticapy.jupyter.extensions.chart_magic import chart_magic as hchart
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
@@ -41,38 +45,36 @@
 def amazon_vd():
     amazon = load_amazon()
     yield amazon
     drop(name="public.titanic")
 
 
 class Test_hchart:
+    @pytest.mark.skip(
+        reason="Deprecated, we need to implement the functions for each graphic"
+    )
     def test_hchart(self, titanic_vd, amazon_vd):
-
         # Test -k
         result = hchart("-k pearson", "SELECT * FROM titanic;")
         assert isinstance(result, Highchart)
         result = hchart("--kind scatter", "SELECT age, fare FROM titanic;")
         assert isinstance(result, Highchart)
         result = hchart("    -k     scatter", "SELECT age, fare, pclass FROM titanic;")
         assert isinstance(result, Highchart)
-        result = hchart(
-            "-k scatter", "SELECT age, fare, parch, pclass FROM titanic;"
-        )
+        result = hchart("-k scatter", "SELECT age, fare, parch, pclass FROM titanic;")
         assert isinstance(result, Highchart)
         result = hchart("   --kind bubble", "SELECT age, fare, pclass FROM titanic;")
         assert isinstance(result, Highchart)
         result = hchart("-k bubble", "SELECT age, fare, parch, pclass FROM titanic;")
         assert isinstance(result, Highchart)
         result = hchart("-k auto", "SELECT age, fare, pclass FROM titanic;")
         assert isinstance(result, Highchart)
         result = hchart("-k auto", "SELECT age, fare, parch, pclass FROM titanic;")
         assert isinstance(result, Highchart)
-        result = hchart(
-            "-k auto", "SELECT pclass, COUNT(*) FROM titanic GROUP BY 1;"
-        )
+        result = hchart("-k auto", "SELECT pclass, COUNT(*) FROM titanic GROUP BY 1;")
         assert isinstance(result, Highchart)
         result = hchart(
             "-k auto",
             "SELECT pclass, survived, COUNT(*) FROM titanic GROUP BY 1, 2;",
         )
         assert isinstance(result, Highchart)
         result = hchart("-k auto", "SELECT date, number FROM amazon;")
@@ -80,20 +82,24 @@
         result = hchart("    --kind auto", "SELECT date, number, state FROM amazon;")
         assert isinstance(result, Highstock)
         result = hchart("-k line", "SELECT date, number, state FROM amazon;")
         assert isinstance(result, Highstock)
 
         # Test -f
         result = hchart(
-            "   -k line  -f   {}/tests/hchart/queries.sql".format(os.path.dirname(verticapy.__file__)),
+            "   -k line  -f   {}/tests/hchart/queries.sql".format(
+                os.path.dirname(verticapy.__file__)
+            ),
             "",
         )
         assert isinstance(result, Highstock)
         result = hchart(
-            "   -k line  --file     {}/tests/hchart/queries.sql  ".format(os.path.dirname(verticapy.__file__)),
+            "   -k line  --file     {}/tests/hchart/queries.sql  ".format(
+                os.path.dirname(verticapy.__file__)
+            ),
             "",
         )
         assert isinstance(result, Highstock)
 
         # Test -c
         result = hchart(
             "   -k line  -c   'SELECT date, number, state FROM amazon;'",
@@ -108,15 +114,15 @@
 
         # Export to HTML --output
         result = hchart(
             "  --output   verticapy_test_hchart",
             "SELECT date, number, state FROM amazon;",
         )
         try:
-            file = open("verticapy_test_hchart.html", "r")
+            file = open("verticapy_test_hchart.html", "r", encoding="utf-8")
             result = file.read()
             assert "<!DOCTYPE html>" in result
         except:
             os.remove("verticapy_test_hchart.html")
             file.close()
             raise
         os.remove("verticapy_test_hchart.html")
@@ -124,23 +130,16 @@
 
         # Export to HTML -o
         result = hchart(
             "  -o   verticapy_test_hchart",
             "SELECT date, number, state FROM amazon;",
         )
         try:
-            file = open("verticapy_test_hchart.html", "r")
+            file = open("verticapy_test_hchart.html", "r", encoding="utf-8")
             result = file.read()
             assert "<!DOCTYPE html>" in result
         except:
             os.remove("verticapy_test_hchart.html")
             file.close()
             raise
         os.remove("verticapy_test_hchart.html")
         file.close()
-
-
-
-
-
-
-
```

### Comparing `verticapy-0.9.0/verticapy/tests/sql/__init__.py` & `verticapy-1.0.0b1/verticapy/_utils/_sql/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/stats/__init__.py` & `verticapy-1.0.0b1/verticapy/core/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/stats/test_stats.py` & `verticapy-1.0.0b1/verticapy/tests/stats/test_stats.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
 from verticapy.datasets import load_titanic, load_airline_passengers, load_amazon
@@ -44,162 +48,189 @@
     drop(name="public.amazon")
 
 
 class TestStats:
     def test_adfuller(self, amazon_vd):
         # testing without trend
         result = st.adfuller(
-            amazon_vd, column="number", ts="date", by=["state"], p=40, with_trend=False
+            amazon_vd,
+            column="number",
+            ts="date",
+            by=["state"],
+            p=40,
+            with_trend=False,
         )
         assert result["value"][0] == pytest.approx(-0.4059507552046538, 1e-2)
         assert result["value"][1] == pytest.approx(0.684795156687264, 1e-2)
-        assert result["value"][-1] == False
+        assert not result["value"][-1]
 
         # testing with trend
         result = st.adfuller(
-            amazon_vd, column="number", ts="date", by=["state"], p=40, with_trend=True
+            amazon_vd,
+            column="number",
+            ts="date",
+            by=["state"],
+            p=40,
+            with_trend=True,
         )
         assert result["value"][0] == pytest.approx(-0.4081159118011171, 1e-2)
         assert result["value"][1] == pytest.approx(0.683205052234998, 1e-2)
-        assert result["value"][-1] == False
+        assert not result["value"][-1]
 
     def test_cochrane_orcutt(self, airline_vd):
         airline_copy = airline_vd.copy()
         airline_copy["passengers_bias"] = (
             airline_copy["passengers"] ** 2 - 50 * st.random()
         )
         drop("lin_cochrane_orcutt_model_test", method="model")
         model = LinearRegression("lin_cochrane_orcutt_model_test")
         model.fit(airline_copy, ["passengers_bias"], "passengers")
-        result = st.cochrane_orcutt(model, airline_copy, ts="date", prais_winsten=True,)
-        assert result.coef_["coefficient"][0] == pytest.approx(25.8582027191416, 1e-2)
-        assert result.coef_["coefficient"][1] == pytest.approx(
-            0.00123563974547625, 1e-2
+        result = st.cochrane_orcutt(
+            model,
+            airline_copy,
+            ts="date",
+            prais_winsten=True,
         )
+        assert result.intercept_ == pytest.approx(25.8582027191416, 1e-2)
+        assert result.coef_[0] == pytest.approx(0.00123563974547625, 1e-2)
         model.drop()
 
     def test_durbin_watson(self, amazon_vd):
         result = st.durbin_watson(amazon_vd, eps="number", ts="date", by=["state"])
         assert result == pytest.approx(0.583991056156811, 1e-2)
 
     def test_endogtest(self, amazon_vd):
         result = amazon_vd.groupby(["date"], ["AVG(number) AS number"])
         result["lag_number"] = "LAG(number) OVER (ORDER BY date)"
         result = st.endogtest(result, eps="number", X=["lag_number"])
-        assert result["value"] == [
-            pytest.approx(110.77204182524278),
-            pytest.approx(6.638132056570419e-26),
-            pytest.approx(204.73673827671277),
-            pytest.approx(6.836198697261425e-34),
-        ]
+        assert result == (
+            pytest.approx(110.77336789258061),
+            pytest.approx(6.633693190527767e-26),
+            pytest.approx(204.74130653722867),
+            pytest.approx(6.827786109983712e-34),
+        )
 
     def test_het_arch(self, amazon_vd):
         result = st.het_arch(amazon_vd, eps="number", ts="date", by=["state"], p=2)
-        assert result["value"] == [
-            pytest.approx(883.1042774059952),
-            pytest.approx(1.7232277858576802e-192),
-            pytest.approx(511.3347213420665),
-            pytest.approx(7.463757606288815e-207),
-        ]
+        assert result == (
+            pytest.approx(883.1114423462593),
+            pytest.approx(1.7170654186230018e-192),
+            pytest.approx(511.33952787255066),
+            pytest.approx(7.432857276079368e-207),
+        )
 
     def test_het_breuschpagan(self, amazon_vd):
         result = amazon_vd.groupby(["date"], ["AVG(number) AS number"])
         result["lag_number"] = "LAG(number) OVER (ORDER BY date)"
         result = st.het_breuschpagan(result, eps="number", X=["lag_number"])
-        assert result["value"] == [
-            pytest.approx(68.30346484950417),
-            pytest.approx(1.4017446778018072e-16),
-            pytest.approx(94.83450355369129),
-            pytest.approx(4.572276908758215e-19),
-        ]
+        assert result == (
+            pytest.approx(68.30399583311137),
+            pytest.approx(1.4013672773820152e-16),
+            pytest.approx(94.83553579040095),
+            pytest.approx(4.570574529673344e-19),
+        )
 
     def test_het_goldfeldquandt(self, amazon_vd):
         vdf = amazon_vd.groupby(["date"], ["AVG(number) AS number"])
         vdf["lag_number"] = "LAG(number) OVER (ORDER BY date)"
         result = st.het_goldfeldquandt(vdf, y="number", X=["lag_number"])
-        assert result["value"] == [
+        assert result == (
             pytest.approx(30.17263128858259),
             pytest.approx(1.3988910574921388e-55),
-        ]
+        )
         result2 = st.het_goldfeldquandt(
             vdf, y="number", X=["lag_number"], alternative="decreasing"
         )
-        assert result2["value"] == [
+        assert result2 == (
             pytest.approx(30.17263128858259),
             pytest.approx(0.9999999999999999),
-        ]
+        )
         result3 = st.het_goldfeldquandt(
             vdf, y="number", X=["lag_number"], alternative="two-sided"
         )
-        assert result3["value"] == [
+        assert result3 == (
             pytest.approx(30.17263128858259),
             pytest.approx(0.0),
-        ]
+        )
 
     def test_het_white(self, amazon_vd):
         result = amazon_vd.groupby(["date"], ["AVG(number) AS number"])
         result["lag_number"] = "LAG(number) OVER (ORDER BY date)"
         result = st.het_white(result, eps="number", X=["lag_number"])
-        assert result["value"] == [
-            pytest.approx(72.93515335650999),
-            pytest.approx(1.3398039866815678e-17),
-            pytest.approx(104.08964747730063),
-            pytest.approx(1.7004013245871353e-20),
-        ]
+        assert result == (
+            pytest.approx(72.93566993238099),
+            pytest.approx(1.3394533546740618e-17),
+            pytest.approx(104.09070850396219),
+            pytest.approx(1.6997687814870292e-20),
+        )
 
     def test_jarque_bera(self, amazon_vd):
         result = st.jarque_bera(amazon_vd, column="number")
-        assert result["value"][0] == pytest.approx(930829.520860999, 1e-2)
-        assert result["value"][1] == pytest.approx(0.0, 1e-2)
-        assert result["value"][-1] == False
+        assert result[0] == pytest.approx(930829.520860999, 1e-2)
+        assert result[1] == pytest.approx(0.0, 1e-2)
 
     def test_kurtosistest(self, amazon_vd):
         result = st.kurtosistest(amazon_vd, column="number")
-        assert result["value"][0] == pytest.approx(47.31605467852915, 1e-2)
-        assert result["value"][1] == pytest.approx(0.0, 1e-2)
+        assert result[0] == pytest.approx(47.31605467852915, 1e-2)
+        assert result[1] == pytest.approx(0.0, 1e-2)
 
     def test_normaltest(self, amazon_vd):
         result = st.normaltest(amazon_vd, column="number")
-        assert result["value"][0] == pytest.approx(7645.980976250067, 1e-2)
-        assert result["value"][1] == pytest.approx(0.0, 1e-2)
+        assert result[0] == pytest.approx(7645.980976250067, 1e-2)
+        assert result[1] == pytest.approx(0.0, 1e-2)
 
     def test_ljungbox(self, amazon_vd):
         # testing LjungBox
         result = st.ljungbox(
-            amazon_vd, column="number", ts="date", by=["state"], p=40, box_pierce=False
+            amazon_vd,
+            column="number",
+            ts="date",
+            by=["state"],
+            p=40,
+            box_pierce=False,
         )
-        assert result["Serial Correlation"][-1] == True
+        assert result["Serial Correlation"][-1]
         assert result["p_value"][-1] == pytest.approx(0.0)
         assert result["LjungBox Test Statistic"][-1] == pytest.approx(
             33724.41181636157, 1e-2
         )
 
         # testing Box-Pierce
         result = st.ljungbox(
-            amazon_vd, column="number", ts="date", by=["state"], p=40, box_pierce=True
+            amazon_vd,
+            column="number",
+            ts="date",
+            by=["state"],
+            p=40,
+            box_pierce=True,
         )
-        assert result["Serial Correlation"][-1] == True
+        assert result["Serial Correlation"][-1]
         assert result["p_value"][-1] == pytest.approx(0.0)
         assert result["Box-Pierce Test Statistic"][-1] == pytest.approx(
             33601.96361200001, 1e-2
         )
 
     def test_mkt(self, amazon_vd):
         result = amazon_vd.groupby(["date"], ["AVG(number) AS number"])
         result = st.mkt(result, column="number", ts="date")
         assert result["value"][0] == pytest.approx(2.579654773618437, 1e-2)
         assert result["value"][1] == pytest.approx(3188.0, 1e-2)
         assert result["value"][2] == pytest.approx(1235.43662996799, 1e-2)
         assert result["value"][3] == pytest.approx(0.009889912917327177, 1e-2)
-        assert result["value"][4] == True
+        assert result["value"][4]
         assert result["value"][5] == "increasing"
 
     def test_seasonal_decompose(self, airline_vd):
         result = st.seasonal_decompose(
-            airline_vd, "Passengers", "date", period=12, mult=True, polynomial_order=-1
+            airline_vd,
+            "Passengers",
+            "date",
+            period=12,
+            mult=True,
+            polynomial_order=-1,
         )
         assert result["passengers_trend"].avg() == pytest.approx(266.398518668831)
         assert result["passengers_seasonal"].avg() == pytest.approx(1.0)
         assert result["passengers_epsilon"].avg() == pytest.approx(1.05417531440333)
         result2 = st.seasonal_decompose(
             airline_vd,
             "Passengers",
@@ -224,26 +255,40 @@
         )
         assert result2["passengers_trend"].avg() == pytest.approx(280.298611111111)
         assert result2["passengers_seasonal"].avg() == pytest.approx(1.0)
         assert result2["passengers_epsilon"].avg() == pytest.approx(1.00044316689124)
 
     def test_skewtest(self, amazon_vd):
         result = st.skewtest(amazon_vd, column="number")
-        assert result["value"][0] == pytest.approx(73.53347500226347, 1e-2)
-        assert result["value"][1] == pytest.approx(0.0, 1e-2)
+        assert result[0] == pytest.approx(73.53347500226347, 1e-2)
+        assert result[1] == pytest.approx(0.0, 1e-2)
 
     def test_variance_inflation_factor(self, titanic_vd):
         result = st.variance_inflation_factor(
             titanic_vd, ["pclass", "survived", "age", "fare"]
         )
         assert result["VIF"][0] == pytest.approx(1.8761429731878563, 1e-2)
         assert result["VIF"][1] == pytest.approx(1.1859478232661875, 1e-2)
         assert result["VIF"][2] == pytest.approx(1.250542149908016, 1e-2)
         assert result["VIF"][3] == pytest.approx(1.4940557668701793, 1e-2)
 
+    @pytest.mark.skip(reason="this test will be valid for Vertica v12.0.2")
+    def test_jaro_distance(self, titanic_vd):
+        assert (
+            str(st.edit_distance(titanic_vd["name"], "Laurent"))
+            == "JARO_DISTANCE(\"name\", 'Laurent')"
+        )
+
+    @pytest.mark.skip(reason="this test will be valid for Vertica v12.0.2")
+    def test_jaro_winkler_distance(self, titanic_vd):
+        assert (
+            str(st.edit_distance(titanic_vd["name"], "Laurent"))
+            == "JARO_WINKLER_DISTANCE(\"name\", 'Laurent')"
+        )
+
     def test_edit_distance(self, titanic_vd):
         assert (
             str(st.edit_distance(titanic_vd["name"], "Laurent"))
             == "EDIT_DISTANCE(\"name\", 'Laurent')"
         )
 
     def test_soundex(self, titanic_vd):
@@ -393,14 +438,20 @@
 
     def test_asin(self, titanic_vd):
         assert str(st.asin(titanic_vd["age"])) == 'ASIN("age")'
 
     def test_atan(self, titanic_vd):
         assert str(st.atan(titanic_vd["age"])) == 'ATAN("age")'
 
+    def test_atan2(self, titanic_vd):
+        assert (
+            str(st.atan2(titanic_vd["age"], titanic_vd["fare"]))
+            == 'ATAN2("age", "fare")'
+        )
+
     def test_case_when(self, titanic_vd):
         assert (
             str(st.case_when(titanic_vd["age"] > 5, 11, 1993))
             == 'CASE WHEN ("age") > (5) THEN 11 ELSE 1993 END'
         )
 
     def test_cbrt(self, titanic_vd):
@@ -586,12 +637,12 @@
     def test_year(self, amazon_vd):
         assert str(st.year(amazon_vd["date"])) == 'YEAR("date")'
 
     def test_zeroifnull(self, amazon_vd):
         assert str(st.zeroifnull(amazon_vd["date"])) == 'ZEROIFNULL("date")'
 
     def test_constants(self):
-        assert str(st.pi) == "PI()"
-        assert str(st.e) == "EXP(1)"
-        assert str(st.tau) == "2 * PI()"
-        assert str(st.inf) == "'inf'::float"
-        assert str(st.nan) == "'nan'::float"
+        assert str(st.PI) == "PI()"
+        assert str(st.E) == "EXP(1)"
+        assert str(st.TAU) == "2 * PI()"
+        assert str(st.INF) == "'inf'::float"
+        assert str(st.NAN) == "'nan'::float"
```

### Comparing `verticapy-0.9.0/verticapy/tests/udf/__init__.py` & `verticapy-1.0.0b1/verticapy/core/parsers/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/udf/test_udf.py` & `verticapy-1.0.0b1/verticapy/tests/udf/test_udf.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,40 +1,44 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import math, os
 
 # VerticaPy
 import verticapy
-from verticapy.udf import create_lib_udf
+from verticapy.udf import generate_lib_udf
 
 
 def normalize_titanic(age, fare):
     return (age - 30.15) / 14.44, (fare - 33.96) / 52.65
 
 
 class TestUdf:
-    def test_create_lib_udf(self):
+    def test_generate_lib_udf(self):
         file_path = os.path.dirname(verticapy.__file__) + "/python_math_lib.py"
         pmath_path = os.path.dirname(verticapy.__file__) + "/tests/udf/pmath.py"
-        udx_str, udx_sql = create_lib_udf(
+        udx_str, udx_sql = generate_lib_udf(
             [
                 (math.exp, [float], float, {}, "python_exp"),
                 (
                     math.isclose,
                     [float, float],
                     bool,
                     {"abs_tol": float},
```

### Comparing `verticapy-0.9.0/verticapy/tests/utilities/__init__.py` & `verticapy-1.0.0b1/verticapy/datasets/data/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/utilities/titanic-passengers.json` & `verticapy-1.0.0b1/verticapy/tests/utilities/titanic-passengers.json`

 * *Files identical despite different names*

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/__init__.py` & `verticapy-1.0.0b1/verticapy/jupyter/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_correlation.py` & `verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_correlation.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
@@ -44,16 +48,17 @@
         result1 = amazon_vd.acf(
             ts="date",
             column="number",
             p=20,
             by=["state"],
             unit="month",
             method="spearman",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result1["value"][0] == pytest.approx(1)
         assert result1["confidence"][0] == pytest.approx(0.024396841824873748, 1e-2)
         assert result1.values["value"][10] == pytest.approx(0.494663471420921, 1e-2)
         assert result1.values["confidence"][10] == pytest.approx(
             0.06977116419369607, 1e-2
         )
 
@@ -61,86 +66,90 @@
         result2 = amazon_vd.acf(
             ts="date",
             column="number",
             by=["state"],
             p=[1, 3, 6, 7],
             unit="year",
             method="pearson",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result2["value"][0] == pytest.approx(1)
         assert result2["confidence"][0] == pytest.approx(0.024396841824873748, 1e-2)
         assert result2["value"][4] == pytest.approx(0.367, 1e-2)
         assert result2["confidence"][4] == pytest.approx(0.04080280865931269, 1e-2)
 
         # Autocorrelation Heatmap for each 'month' lag
         result3 = amazon_vd.acf(
             ts="date",
             column="number",
             by=["state"],
             p=12,
             unit="month",
             method="pearson",
-            round_nb=3,
-            acf_type="heatmap",
+            mround=3,
+            kind="heatmap",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result3["index"][1].replace('"', "") == "lag_12_number"
         assert result3["number"][1] == pytest.approx(0.778, 1e-2)
         assert result3["index"][5].replace('"', "") == "lag_10_number"
         assert result3["number"][5] == pytest.approx(0.334, 1e-2)
 
         # Autocorrelation Line for each 'month' lag
         result4 = amazon_vd.acf(
             ts="date",
             column="number",
             by=["state"],
             p=12,
             unit="month",
             method="pearson",
-            acf_type="line",
+            kind="line",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result4["value"][1] == pytest.approx(0.752, 1e-2)
         assert result4["confidence"][1] == pytest.approx(0.03627598368700659, 1e-2)
         assert result4["value"][6] == pytest.approx(-0.06, 1e-2)
         assert result4["confidence"][6] == pytest.approx(0.05273251493184901, 1e-2)
 
         # spearmanD method
         result5 = amazon_vd.acf(
             ts="date",
             column="number",
             p=20,
             by=["state"],
             unit="month",
             method="spearmanD",
+            show=False,
         )
-        plt.close("all")
-        assert result5["value"][0] == pytest.approx(1)
+        # plt.close("all")
+        assert result5["value"][0] == pytest.approx(1.0)
         assert result5["confidence"][0] == pytest.approx(0.024396841824873748, 1e-2)
-        assert result5.values["value"][10] == pytest.approx(0.5, 1e-2)
+        assert result5.values["value"][10] == pytest.approx(0.494663471420921, 1e-2)
         assert result5.values["confidence"][10] == pytest.approx(
             0.06977116419369607, 1e-2
         )
 
     def test_vDF_chaid(self, titanic_vd):
         result = titanic_vd.chaid("survived", ["age", "fare", "sex"])
-        tree = result.attributes_["tree"]
+        tree = result.tree_
         assert tree["chi2"] == pytest.approx(345.12775126385327)
         assert tree["children"]["female"]["chi2"] == pytest.approx(10.472532457814179)
         assert tree["children"]["female"]["children"][127.6]["chi2"] == pytest.approx(
             3.479525088868805
         )
         assert tree["children"]["female"]["children"][127.6]["children"][19.0][
             "prediction"
         ][0] == pytest.approx(0.325581395348837)
         assert tree["children"]["female"]["children"][127.6]["children"][38.0][
             "prediction"
         ][1] == pytest.approx(0.720496894409938)
-        assert not (tree["split_is_numerical"])
+        assert not tree["split_is_numerical"]
         assert tree["split_predictor"] == '"sex"'
         assert tree["split_predictor_idx"] == 2
         pred = result.predict([[3.0, 11.0, "male"], [11.0, 1.0, "female"]])
         assert pred[0] == 0
         assert pred[1] == 1
         pred = result.predict_proba([[3.0, 11.0, "male"], [11.0, 1.0, "female"]])
         assert pred[0][0] == pytest.approx(0.75968992)
@@ -150,30 +159,36 @@
 
     def test_vDF_corr(self, titanic_vd):
         #
         # PEARSON
         #
         # testing vDataFrame.corr (method = 'pearson')
         result1 = titanic_vd.corr(
-            columns=["survived", "age", "fare"], method="pearson",
+            columns=["survived", "age", "fare"],
+            method="pearson",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result1["survived"][0] == 1.0
         assert result1["survived"][1] == pytest.approx(-0.0422446185581737, 1e-2)
         assert result1["survived"][2] == pytest.approx(0.264150360783869, 1e-2)
         assert result1["age"][0] == pytest.approx(-0.0422446185581737, 1e-2)
         assert result1["age"][1] == 1.0
         assert result1["age"][2] == pytest.approx(0.178575164117464, 1e-2)
         assert result1["fare"][0] == pytest.approx(0.264150360783869, 1e-2)
         assert result1["fare"][1] == pytest.approx(0.178575164117464, 1e-2)
         assert result1["fare"][2] == 1.0
 
         # testing vDataFrame.corr (method = 'pearson') with focus
-        result1_f = titanic_vd.corr(method="pearson", focus="survived")
-        plt.close("all")
+        result1_f = titanic_vd.corr(
+            method="pearson",
+            focus="survived",
+            show=False,
+        )
+        # plt.close("all")
         assert result1_f["survived"][1] == pytest.approx(-0.336, 1e-2)
         assert result1_f["survived"][2] == pytest.approx(0.264, 1e-2)
 
         #
         # SPEARMAN
         #
         # testing vDataFrame.corr (method = 'spearman')
@@ -183,124 +198,154 @@
         titanic_vd_gb = titanic_vd_gb.groupby(
             ["fare"], ["AVG(age) AS age", "AVG(survived) AS survived"]
         )
         titanic_vd_gb = titanic_vd_gb.groupby(
             ["survived"], ["AVG(age) AS age", "AVG(fare) AS fare"]
         )
         result2 = titanic_vd_gb.corr(
-            columns=["survived", "age", "fare"], method="spearman",
+            columns=["survived", "age", "fare"],
+            method="spearman",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result2["survived"][0] == 1.0
         assert result2["survived"][1] == pytest.approx(-0.221388367729831, 1e-2)
         assert result2["survived"][2] == pytest.approx(0.425515947467167, 1e-2)
         assert result2["age"][0] == pytest.approx(-0.221388367729831, 1e-2)
         assert result2["age"][1] == 1.0
         assert result2["age"][2] == pytest.approx(0.287617260787992, 1e-2)
         assert result2["fare"][0] == pytest.approx(0.425515947467167, 1e-2)
         assert result2["fare"][1] == pytest.approx(0.287617260787992, 1e-2)
         assert result2["fare"][2] == 1.0
 
         # testing vDataFrame.corr (method = 'spearman') with focus
-        result2_f = titanic_vd_gb.corr(focus="survived", method="spearman")
-        plt.close("all")
+        result2_f = titanic_vd_gb.corr(
+            focus="survived",
+            method="spearman",
+            show=False,
+        )
+        # plt.close("all")
         assert result2_f["survived"][1] == pytest.approx(0.425515947467167, 1e-2)
         assert result2_f["survived"][2] == pytest.approx(-0.221388367729831, 1e-2)
 
         #
         # KENDALL
         #
         # testing vDataFrame.corr (method = 'kendall')
         result3 = titanic_vd.corr(
-            columns=["survived", "age", "fare"], method="kendall",
+            columns=["survived", "age", "fare"],
+            method="kendall",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result3["survived"][0] == 1.0
         assert result3["survived"][1] == pytest.approx(-0.0149530691050183, 1e-2)
         assert result3["survived"][2] == pytest.approx(0.264138930414481, 1e-2)
         assert result3["age"][0] == pytest.approx(-0.0149530691050183, 1e-2)
         assert result3["age"][1] == 1.0
         assert result3["age"][2] == pytest.approx(0.0844989716189637, 1e-2)
         assert result3["fare"][0] == pytest.approx(0.264138930414481, 1e-2)
         assert result3["fare"][1] == pytest.approx(0.0844989716189637, 1e-2)
         assert result3["fare"][2] == 1.0
 
         # testing vDataFrame.corr (method = 'kendall') with focus
-        result3_f = titanic_vd.corr(focus="survived", method="kendall")
-        plt.close("all")
+        result3_f = titanic_vd.corr(
+            focus="survived",
+            method="kendall",
+            show=False,
+        )
+        # plt.close("all")
         assert result3_f["survived"][1] == pytest.approx(-0.317426126117454, 1e-2)
         assert result3_f["survived"][2] == pytest.approx(0.264138930414481, 1e-2)
 
         #
         # BISERIAL POINT
         #
         # testing vDataFrame.corr (method = 'biserial')
         result4 = titanic_vd.corr(
-            columns=["survived", "age", "fare"], method="biserial",
+            columns=["survived", "age", "fare"],
+            method="biserial",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result4["survived"][0] == 1.0
         assert result4["survived"][1] == pytest.approx(-0.0422234273762242, 1e-2)
         assert result4["survived"][2] == pytest.approx(0.264043222121672, 1e-2)
         assert result4["age"][0] == pytest.approx(-0.0422234273762242, 1e-2)
         assert result4["age"][1] == 1.0
         assert result4["fare"][0] == pytest.approx(0.264043222121672, 1e-2)
         assert result4["fare"][2] == 1.0
 
         # testing vDataFrame.corr (method = 'biserial') with focus
-        result4_f = titanic_vd.corr(focus="survived", method="biserial")
-        plt.close("all")
+        result4_f = titanic_vd.corr(
+            focus="survived",
+            method="biserial",
+            show=False,
+        )
+        # plt.close("all")
         assert result4_f["survived"][1] == pytest.approx(-0.335720838027055, 1e-2)
         assert result4_f["survived"][2] == pytest.approx(0.264043222121672, 1e-2)
 
         #
         # CRAMER'S V
         #
         # testing vDataFrame.corr (method = 'cramer')
         result5 = titanic_vd.corr(
-            columns=["survived", "pclass", "embarked"], method="cramer"
+            columns=["survived", "pclass", "embarked"],
+            method="cramer",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result5["survived"][0] == 1.0
         assert result5["survived"][1] == pytest.approx(0.3358661117846154, 1e-2)
         assert result5["survived"][2] == pytest.approx(0.18608072188932145, 1e-2)
         assert result5["pclass"][0] == pytest.approx(0.3358661117846154, 1e-2)
         assert result5["pclass"][1] == 1.0
         assert result5["pclass"][2] == pytest.approx(0.27453049870161333, 1e-2)
         assert result5["embarked"][0] == pytest.approx(0.18608072188932145, 1e-2)
         assert result5["embarked"][1] == pytest.approx(0.27453049870161333, 1e-2)
         assert result5["embarked"][2] == 1.0
 
         # testing vDataFrame.corr (method = 'cramer') with focus
-        result5_f = titanic_vd.corr(focus="survived", method="cramer")
-        plt.close("all")
-        assert result5_f["survived"][1] == pytest.approx(0.73190924565401, 1e-2)
-        assert result5_f["survived"][2] == pytest.approx(0.6707486879228794, 1e-2)
+        result5_f = titanic_vd.corr(
+            focus="survived",
+            method="cramer",
+            show=False,
+        )
+        # plt.close("all")
+        assert result5_f["survived"][1] == pytest.approx(0.5531019147439457, 1e-2)
+        assert result5_f["survived"][2] == pytest.approx(0.3358661117846154, 1e-2)
 
         #
         # DENSE SPEARMAN
         #
         # testing vDataFrame.corr (method = 'spearmanD')
         result6 = titanic_vd_gb.corr(
-            columns=["survived", "age", "fare"], method="spearmanD",
+            columns=["survived", "age", "fare"],
+            method="spearmanD",
+            show=False,
         )
-        plt.close("all")
+        # plt.close("all")
         assert result6["survived"][0] == 1.0
         assert result6["survived"][1] == pytest.approx(-0.221388367729831, 1e-2)
         assert result6["survived"][2] == pytest.approx(0.425515947467167, 1e-2)
         assert result6["age"][0] == pytest.approx(-0.221388367729831, 1e-2)
         assert result6["age"][1] == 1.0
         assert result6["age"][2] == pytest.approx(0.287617260787992, 1e-2)
         assert result6["fare"][0] == pytest.approx(0.425515947467167, 1e-2)
         assert result6["fare"][1] == pytest.approx(0.287617260787992, 1e-2)
         assert result6["fare"][2] == 1.0
 
         # testing vDataFrame.corr (method = 'spearmanD') with focus
-        result6_f = titanic_vd_gb.corr(focus="survived", method="spearmanD")
-        plt.close("all")
+        result6_f = titanic_vd_gb.corr(
+            focus="survived",
+            method="spearmanD",
+            show=False,
+        )
+        # plt.close("all")
         assert result6_f["survived"][1] == pytest.approx(0.425515947467167, 1e-2)
         assert result6_f["survived"][2] == pytest.approx(-0.221388367729831, 1e-2)
 
     def test_vDF_corr_pvalue(self, titanic_vd):
         assert titanic_vd.corr_pvalue("age", "fare", "pearson") == (
             pytest.approx(0.178575164117468, 1e-2),
             pytest.approx(1.3923308548466764e-08, 1e-2),
@@ -332,39 +377,39 @@
         assert titanic_vd.corr_pvalue("survived", "pclass", "cramer") == (
             pytest.approx(0.3358661117846154, 1e-2),
             pytest.approx(3.507947423216931e-61, 1e-2),
         )
 
     def test_vDF_cov(self, titanic_vd):
         # testing vDataFrame.cov
-        result = titanic_vd.cov(columns=["survived", "age", "fare"])
-        plt.close("all")
+        result = titanic_vd.cov(columns=["survived", "age", "fare"], show=False)
+        # plt.close("all")
         assert result["survived"][0] == pytest.approx(0.231685181342251, 1e-2)
         assert result["survived"][1] == pytest.approx(-0.297583583247234, 1e-2)
         assert result["survived"][2] == pytest.approx(6.69214075159394, 1e-2)
         assert result["age"][0] == pytest.approx(-0.297583583247234, 1e-2)
         assert result["age"][1] == pytest.approx(208.169014723609, 1e-2)
         assert result["age"][2] == pytest.approx(145.057125218791, 1e-2)
         assert result["fare"][0] == pytest.approx(6.69214075159394, 1e-2)
         assert result["fare"][1] == pytest.approx(145.057125218791, 1e-2)
         assert result["fare"][2] == pytest.approx(2769.36114247479, 1e-2)
 
         # testing vDataFrame.cov with focus
         result_f = titanic_vd.cov(
-            columns=["survived", "age", "fare"], focus="survived",
+            columns=["survived", "age", "fare"], focus="survived", show=False
         )
         assert result_f["survived"][0] == pytest.approx(6.69214075159394, 1e-2)
         assert result_f["survived"][1] == pytest.approx(-0.297583583247234, 1e-2)
         assert result_f["survived"][2] == pytest.approx(0.231685181342251, 1e-2)
-        plt.close("all")
+        # plt.close("all")
 
     def test_vDF_iv_woe(self, titanic_vd):
         # testing vDataFrame.iv_woe
-        result = titanic_vd.iv_woe("survived")
-        plt.close("all")
+        result = titanic_vd.iv_woe("survived", show=False)
+        # plt.close("all")
         assert result["iv"][0] == pytest.approx(1.272254799126849)
         assert result["iv"][1] == pytest.approx(1.148751293230747)
         assert result["iv"][2] == pytest.approx(0.4951028280802058)
         # testing vDataFrame[].iv_woe
         result2 = titanic_vd["pclass"].iv_woe("survived")
         assert result2["iv"][-1] == pytest.approx(0.4951028280802058)
         assert result2["non_events"][-1] == pytest.approx(784)
@@ -395,53 +440,67 @@
         assert result["chi2"][4] == pytest.approx(44.074789072247576)
         assert result["chi2"][5] == pytest.approx(42.65927519250441)
         assert result["chi2"][6] == pytest.approx(38.109904618027755)
         assert result["chi2"][7] == pytest.approx(0.0)
 
     def test_vDF_pacf(self, amazon_vd):
         # testing vDataFrame.pacf
-        result = amazon_vd.pacf(column="number", ts="date", by=["state"], p=5,)
-        plt.close("all")
+        result = amazon_vd.pacf(
+            column="number", ts="date", by=["state"], p=5, show=False
+        )
+        # plt.close("all")
         assert result["value"][0] == 1.0
         assert result["value"][1] == pytest.approx(0.672667529541858, 1e-2)
         assert result["value"][2] == pytest.approx(-0.188727403801382, 1e-2)
         assert result["value"][3] == pytest.approx(0.022206688265849, 1e-2)
         assert result["value"][4] == pytest.approx(-0.0819798501305434, 1e-2)
         assert result["value"][5] == pytest.approx(-0.00663606854011195, 1e-2)
 
     def test_vDF_regr(self, titanic_vd):
         # testing vDataFrame.regr (method = 'alpha')
-        result1 = titanic_vd.regr(columns=["survived", "age", "fare"], method="alpha",)
-        plt.close("all")
+        result1 = titanic_vd.regr(
+            columns=["survived", "age", "fare"],
+            method="alpha",
+            show=False,
+        ).transpose()
+        # plt.close("all")
         assert result1["survived"][0] == 0.0
         assert result1["survived"][1] == pytest.approx(0.435280333103508, 1e-2)
         assert result1["survived"][2] == pytest.approx(0.282890247028015, 1e-2)
         assert result1["age"][0] == pytest.approx(30.6420462046205, 1e-2)
         assert result1["age"][1] == 0.0
         assert result1["age"][2] == pytest.approx(28.4268042866199, 1e-2)
         assert result1["fare"][0] == pytest.approx(23.425595019157, 1e-2)
         assert result1["fare"][1] == pytest.approx(16.1080039795446, 1e-2)
         assert result1["fare"][2] == 0.0
 
         # testing vDataFrame.regr (method = 'beta')
-        result2 = titanic_vd.regr(columns=["survived", "age", "fare"], method="beta")
-        plt.close("all")
+        result2 = titanic_vd.regr(
+            columns=["survived", "age", "fare"],
+            method="beta",
+            show=False,
+        ).transpose()
+        # plt.close("all")
         assert result2["survived"][0] == 1.0
         assert result2["survived"][1] == pytest.approx(-0.00142952871080426, 1e-2)
         assert result2["survived"][2] == pytest.approx(0.00241649261591561, 1e-2)
         assert result2["age"][0] == pytest.approx(-1.2483889156179, 1e-2)
         assert result2["age"][1] == 1.0
         assert result2["age"][2] == pytest.approx(0.0456059549185254, 1e-2)
         assert result2["fare"][0] == pytest.approx(28.8746643141762, 1e-2)
         assert result2["fare"][1] == pytest.approx(0.69923081967147, 1e-2)
         assert result2["fare"][2] == 1.0
 
         # testing vDataFrame.regr (method = 'r2')
-        result3 = titanic_vd.regr(columns=["survived", "age", "fare"], method="r2",)
-        plt.close("all")
+        result3 = titanic_vd.regr(
+            columns=["survived", "age", "fare"],
+            method="r2",
+            show=False,
+        ).transpose()
+        # plt.close("all")
         assert result3["survived"][0] == 1.0
         assert result3["survived"][1] == pytest.approx(0.00178460779712559, 1e-2)
         assert result3["survived"][2] == pytest.approx(0.0697754131022489, 1e-2)
         assert result3["age"][0] == pytest.approx(0.00178460779712559, 1e-2)
         assert result3["age"][1] == 1.0
         assert result3["age"][2] == pytest.approx(0.0318890892395806, 1e-2)
         assert result3["fare"][0] == pytest.approx(0.0697754131022489, 1e-2)
```

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_descriptive_statistics.py` & `verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_descriptive_statistics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from vertica_python.errors import QueryError
 from verticapy import drop, set_option
@@ -796,25 +800,27 @@
         assert result["Name"][1] == "Watermelon"
         assert result["Form"][1] == "Fresh"
         assert result["Price"][1] == pytest.approx(0.33341203)
 
     def test_vDF_nunique(self, titanic_vd):
         # Exact Cardinality
         result = titanic_vd.nunique(
-            columns=["pclass", "embarked", "survived", "cabin"], approx=False,
+            columns=["pclass", "embarked", "survived", "cabin"],
+            approx=False,
         )
 
         assert result["unique"][0] == 3.0
         # assert result["unique"][1] == 3.0
         assert result["unique"][2] == 2.0
         # assert result["unique"][3] == 182.0
 
         # Approximate Cardinality
         result = titanic_vd.nunique(
-            columns=["pclass", "embarked", "survived", "cabin"], approx=True,
+            columns=["pclass", "embarked", "survived", "cabin"],
+            approx=True,
         )
 
         assert result["approx_unique"][0] == 3.0
         # assert result["approx_unique"][1] == 3.0
         assert result["approx_unique"][2] == 2.0
         # assert result["approx_unique"][3] == 181.0
 
@@ -890,49 +896,21 @@
         )
 
         model.drop()  # dropping the model in case of its existance
         model.fit("public.titanic", ["fare", "age"], "survived")
         model.predict_proba(titanic_vd, name="survived_pred", pos_label=1)
 
         # Computing AUC
-        auc = titanic_vd.score(y_true="survived", y_score="survived_pred", method="auc")
+        auc = titanic_vd.score(y_true="survived", y_score="survived_pred", metric="auc")
         assert auc == pytest.approx(0.7051784997146537)
 
         # Computing MSE
-        mse = titanic_vd.score(y_true="survived", y_score="survived_pred", method="mse")
+        mse = titanic_vd.score(y_true="survived", y_score="survived_pred", metric="mse")
         assert mse == pytest.approx(0.228082579110535)
 
-        # Drawing ROC Curve
-        roc_res = titanic_vd.score(
-            y_true="survived", y_score="survived_pred", method="roc", nbins=1000,
-        )
-        assert roc_res["threshold"][3] == 0.003
-        assert roc_res["false_positive"][3] == 1.0
-        assert roc_res["true_positive"][3] == 1.0
-        assert roc_res["threshold"][300] == 0.3
-        assert roc_res["false_positive"][300] == pytest.approx(1.0)
-        assert roc_res["true_positive"][300] == pytest.approx(1.0)
-        assert roc_res["threshold"][900] == 0.9
-        assert roc_res["false_positive"][900] == pytest.approx(0.0148760330578512)
-        assert roc_res["true_positive"][900] == pytest.approx(0.061381074168798)
-
-        # Drawing PRC Curve
-        prc_res = titanic_vd.score(
-            y_true="survived", y_score="survived_pred", method="prc", nbins=1000,
-        )
-        assert prc_res["threshold"][3] == 0.002
-        assert prc_res["recall"][3] == 1.0
-        assert prc_res["precision"][3] == pytest.approx(0.3925702811)
-        assert prc_res["threshold"][300] == 0.299
-        assert prc_res["recall"][300] == pytest.approx(1.0)
-        assert prc_res["precision"][300] == pytest.approx(0.392570281124498)
-        assert prc_res["threshold"][900] == 0.899
-        assert prc_res["recall"][900] == pytest.approx(0.061381074168798)
-        assert prc_res["precision"][900] == pytest.approx(0.727272727272727)
-
         # dropping the created model
         model.drop()
 
     def test_vDF_sem(self, titanic_vd):
         # testing vDataFrame.sem
         result = titanic_vd.sem(columns=["age", "fare"])
```

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_feature_engineering.py` & `verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_feature_engineering.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,29 +1,38 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import datetime
 
 # VerticaPy
 from verticapy import drop, errors, set_option
-from verticapy.datasets import load_amazon, load_iris, load_smart_meters, load_titanic
+from verticapy.datasets import (
+    load_amazon,
+    load_iris,
+    load_smart_meters,
+    load_titanic,
+)
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def amazon_vd():
     amazon = load_amazon()
@@ -803,15 +812,19 @@
         titanic_copy.regexp(column="name", pattern="son", method="count", name="name2")
 
         assert titanic_copy["name2"].max() == 2
 
         # method = "ilike"
         titanic_copy = titanic_vd.copy()
         titanic_copy.regexp(
-            column="name", pattern="mrs.", method="ilike", occurrence=1, name="name2"
+            column="name",
+            pattern="mrs.",
+            method="ilike",
+            occurrence=1,
+            name="name2",
         )
 
         assert titanic_copy["name2"].sum() == 185
 
         # method = "instr"
         titanic_copy = titanic_vd.copy()
         titanic_copy.regexp(
@@ -827,23 +840,31 @@
         )
 
         assert titanic_copy["name2"].sum() == 185
 
         # method = "not_ilike"
         titanic_copy = titanic_vd.copy()
         titanic_copy.regexp(
-            column="name", pattern="mrs.", method="not_ilike", position=2, name="name2"
+            column="name",
+            pattern="mrs.",
+            method="not_ilike",
+            position=2,
+            name="name2",
         )
 
         assert titanic_copy["name2"].sum() == 1049
 
         # method = "not_like"
         titanic_copy = titanic_vd.copy()
         titanic_copy.regexp(
-            column="name", pattern="Mrs.", method="not_like", position=2, name="name2"
+            column="name",
+            pattern="Mrs.",
+            method="not_like",
+            position=2,
+            name="name2",
         )
 
         assert titanic_copy["name2"].sum() == 1049
 
         # method = "replace"
         titanic_copy = titanic_vd.copy()
         titanic_copy.regexp(
@@ -857,15 +878,19 @@
         titanic_copy.sort(["name2"])
 
         assert titanic_copy["name2"][3] == "Abbott, Mr. Stanton (Rosa Hunt)"
 
         # method = "substr"
         titanic_copy = titanic_vd.copy()
         titanic_copy.regexp(
-            column="name", pattern="[^,]+", method="substr", occurrence=2, name="name2"
+            column="name",
+            pattern="[^,]+",
+            method="substr",
+            occurrence=2,
+            name="name2",
         )
         titanic_copy.sort(["name2"])
 
         assert titanic_copy["name2"][3] == " Col. John Jacob"
 
     def test_vDF_str_contains(self, titanic_vd):
         titanic_copy = titanic_vd.copy()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_filter_sample.py` & `verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_filter_sample.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
 from verticapy.datasets import load_smart_meters, load_titanic, load_amazon
@@ -78,35 +82,32 @@
         assert result2["pclass"][1] == 1
 
     def test_vDF_at_time(self, smart_meters_vd):
         result = smart_meters_vd.copy().at_time(ts="time", time="12:00")
         assert result.shape() == (140, 3)
 
     def test_vDF_balance(self, titanic_vd):
-        # hybrid
-        result = titanic_vd.balance("embarked", method="hybrid")["embarked"].topk()
-        assert 30 < result["percent"][0] < 40
-        assert 30 < result["percent"][1] < 40
-        assert 30 < result["percent"][2] < 40
         # under
-        result = titanic_vd.balance("embarked", method="under", x=0.5)[
+        result = titanic_vd.balance("embarked", method="under", x=1.0)[
             "embarked"
         ].topk()
-        assert 35 < result["percent"][0] < 55
-        assert 35 < result["percent"][1] < 55
-        assert 15 < result["percent"][2] < 30
+        assert 30 < result["percent"][0] < 36
+        assert 30 < result["percent"][1] < 36
+        assert 30 < result["percent"][2] < 36
         # over
-        result = titanic_vd.balance("embarked", method="over", x=0.5)["embarked"].topk()
-        assert 40 < result["percent"][0] < 55
-        assert 15 < result["percent"][1] < 35
-        assert 15 < result["percent"][2] < 35
+        result = titanic_vd.balance("embarked", method="over", x=1.0)["embarked"].topk()
+        assert 30 < result["percent"][0] < 36
+        assert 30 < result["percent"][1] < 36
+        assert 30 < result["percent"][2] < 36
 
     def test_vDF_between_time(self, smart_meters_vd):
         result = smart_meters_vd.copy().between_time(
-            ts="time", start_time="12:00", end_time="14:00",
+            ts="time",
+            start_time="12:00",
+            end_time="14:00",
         )
         assert result.shape() == (1151, 3)
 
     def test_vDF_filter(self, titanic_vd):
         result = titanic_vd.copy().filter(
             ["pclass = 1 OR age > 50", "embarked = 'S'", "boat IS NOT NULL"],
         )
@@ -116,15 +117,18 @@
         result = smart_meters_vd.copy().first(ts="time", offset="6 months")
         assert result.shape() == (3427, 3)
 
     def test_vDF_isin(self, amazon_vd):
         # testing vDataFrame.isin
         assert amazon_vd.isin(
             {"state": ["SERGIPE", "TOCANTINS"], "number": [0, 0]}
-        ).shape() == (90, 3)
+        ).shape() == (
+            90,
+            3,
+        )
 
         # testing vDataFrame[].isin
         assert amazon_vd["state"].isin(
             val=["SERGIPE", "TOCANTINS", "PARIS"]
         ).shape() == (478, 3)
 
     def test_vDF_last(self, smart_meters_vd):
@@ -147,15 +151,16 @@
     def test_vDF_drop_outliers(self, titanic_vd):
         # testing with threshold
         result1 = titanic_vd.copy()["age"].drop_outliers(threshold=3.0)
         assert result1.shape() == (994, 14)
 
         # testing without threshold
         result2 = titanic_vd.copy()["age"].drop_outliers(
-            use_threshold=False, alpha=0.05,
+            use_threshold=False,
+            alpha=0.05,
         )
         assert result2.shape() == (900, 14)
 
     def test_vDF_select(self, titanic_vd):
         result = titanic_vd.select(columns=["age", "fare", "pclass"])
         assert result.shape() == (1234, 3)
         result = titanic_vd.select(
```

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_plot.py` & `verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_plot.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,35 +1,39 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import datetime, os, sys
 
 # Other Modules
 import matplotlib.pyplot as plt
-import matplotlib.animation as animation
-from highcharts.highcharts.highcharts import Highchart
-from highcharts.highstock.highstock import Highstock
+from vertica_highcharts.highcharts.highcharts import Highchart
+from vertica_highcharts.highstock.highstock import Highstock
+from IPython.display import HTML
 
 # VerticaPy
 import verticapy
-from verticapy import drop, create_verticapy_schema, set_option
+from verticapy import drop, set_option
 from verticapy.datasets import (
     load_titanic,
     load_amazon,
     load_commodities,
     load_iris,
     load_world,
     load_pop_growth,
@@ -86,185 +90,182 @@
     gapminder = load_gapminder()
     yield gapminder
     drop(name="public.gapminder")
 
 
 class TestvDFPlot:
     def test_vDF_animated(self, pop_growth_vd, amazon_vd, commodities_vd, gapminder_vd):
-        result = pop_growth_vd.animated(
+        result = pop_growth_vd.animated_bar(
             "year",
             ["city", "population"],
             "continent",
             1970,
             1980,
-            "bar",
-            return_html=False,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = pop_growth_vd.animated(
+        result = pop_growth_vd.animated_pie(
             "year",
             ["city", "population"],
             "continent",
             1970,
             1980,
-            "pie",
-            return_html=False,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = pop_growth_vd.animated(
-            "year", ["city", "population"], "", 1970, 1980, "bar", return_html=False
+        result = pop_growth_vd.animated_bar(
+            "year",
+            ["city", "population"],
+            "",
+            1970,
+            1980,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = pop_growth_vd.animated(
-            "year", ["city", "population"], "", 1970, 1980, "pie", return_html=False
+        result = pop_growth_vd.animated_pie(
+            "year",
+            ["city", "population"],
+            "",
+            1970,
+            1980,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = amazon_vd.animated(
-            "date", "number", kind="ts", by="state", return_html=False
+        result = amazon_vd.animated_plot(
+            "date",
+            "number",
+            by="state",
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = commodities_vd.animated("date", kind="ts", color=["r", "g", "b"])
-        assert isinstance(result, animation.FuncAnimation)
+        result = commodities_vd.animated_plot("date", color=["r", "g", "b"])
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = gapminder_vd.animated(
+        result = gapminder_vd.animated_scatter(
             "year",
             ["lifeExp", "gdpPercap", "country", "pop"],
             "continent",
-            kind="bubble",
             limit_labels=10,
             limit_over=100,
-            return_html=False,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = gapminder_vd.animated(
+        result = gapminder_vd.animated_scatter(
             "year",
             ["lifeExp", "gdpPercap", "country"],
             "continent",
-            kind="bubble",
             limit_labels=10,
             limit_over=100,
-            return_html=False,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = gapminder_vd.animated(
+        result = gapminder_vd.animated_scatter(
             "year",
             ["lifeExp", "gdpPercap", "pop"],
             "continent",
-            kind="bubble",
             limit_labels=10,
             limit_over=100,
-            return_html=False,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
-        result = gapminder_vd.animated(
+        result = gapminder_vd.animated_scatter(
             "year",
             ["lifeExp", "gdpPercap"],
             "continent",
-            kind="bubble",
             limit_labels=10,
             limit_over=100,
-            return_html=False,
         )
-        assert isinstance(result, animation.FuncAnimation)
+        assert isinstance(result, HTML)
         plt.close("all")
 
     def test_vDF_stacked_area(self, amazon_vd):
         assert (
             len(
                 amazon_vd.pivot("date", "state", "number")
-                .stacked_area("date", ["ACRE", "BAHIA"], color="b")
+                .plot("date", ["ACRE", "BAHIA"], color="b", kind="area_stacked")
                 .get_default_bbox_extra_artists()
             )
             == 12
         )
         plt.close("all")
         assert (
             len(
                 amazon_vd.pivot("date", "state", "number")
-                .stacked_area("date", ["ACRE", "BAHIA"], fully=True, color="b")
+                .plot("date", ["ACRE", "BAHIA"], kind="area_percent", color="b")
                 .get_default_bbox_extra_artists()
             )
             == 12
         )
         plt.close("all")
 
-    def test_vDF_bar(self, titanic_vd, amazon_vd):
+    def test_vDF_barh(self, titanic_vd, amazon_vd):
         # testing vDataFrame[].bar
         # auto
-        result = titanic_vd["fare"].bar(color="b")
+        result = titanic_vd["fare"].barh(color="b", categorical=False)
         assert result.get_default_bbox_extra_artists()[0].get_width() == pytest.approx(
             0.7965964343598055
         )
         assert result.get_default_bbox_extra_artists()[1].get_width() == pytest.approx(
             0.12236628849270664
         )
         assert result.get_yticks()[1] == pytest.approx(42.694100000000006)
-        plt.close("all")
 
         # auto + date
-        result = amazon_vd["date"].bar(color="b")
+        result = amazon_vd["date"].barh(color="b", categorical=False)
         assert result.get_default_bbox_extra_artists()[0].get_width() == pytest.approx(
             0.07530213820886272
         )
         assert result.get_default_bbox_extra_artists()[1].get_width() == pytest.approx(
             0.06693523396343352
         )
         assert result.get_yticks()[1] == pytest.approx(44705828.571428575)
-        plt.close("all")
 
         # method=sum of=survived and nbins=5
-        result2 = titanic_vd["fare"].bar(method="sum", of="survived", nbins=5, color="b")
+        result2 = titanic_vd["fare"].barh(
+            method="sum", of="survived", nbins=5, color="b", categorical=False
+        )
         assert result2.get_default_bbox_extra_artists()[0].get_width() == pytest.approx(
             391
         )
         assert result2.get_default_bbox_extra_artists()[1].get_width() == pytest.approx(
             34
         )
         assert result2.get_yticks()[1] == pytest.approx(102.46583999999999)
-        plt.close("all")
 
         # testing vDataFrame.bar
         # auto & stacked
-        for hist_type in ["auto", "stacked"]:
-            result3 = titanic_vd.bar(
+        for kind in ["auto", "stacked"]:
+            result3 = titanic_vd.barh(
                 columns=["pclass", "survived"],
                 method="50%",
                 of="fare",
-                hist_type=hist_type,
+                kind=kind,
                 color="b",
             )
             assert result3.get_default_bbox_extra_artists()[
                 0
             ].get_width() == pytest.approx(50.0)
             assert result3.get_default_bbox_extra_artists()[
                 3
             ].get_width() == pytest.approx(77.9583)
-            plt.close("all")
         # fully_stacked
-        result4 = titanic_vd.bar(
-            columns=["pclass", "survived"], hist_type="fully_stacked", color="b",
+        result4 = titanic_vd.barh(
+            columns=["pclass", "survived"],
+            kind="fully_stacked",
+            color="b",
         )
         assert result4.get_default_bbox_extra_artists()[0].get_width() == pytest.approx(
             0.38782051282051283
         )
         assert result4.get_default_bbox_extra_artists()[3].get_width() == pytest.approx(
             0.6121794871794872
         )
-        plt.close("all")
         # pyramid
-        result5 = titanic_vd.bar(
-            columns=["pclass", "survived"], hist_type="pyramid", color="b"
+        result5 = titanic_vd.barh(
+            columns=["pclass", "survived"], kind="pyramid", color="b"
         )
         assert result5.get_default_bbox_extra_artists()[0].get_width() == pytest.approx(
             0.09805510534846029
         )
         assert result5.get_default_bbox_extra_artists()[3].get_width() == pytest.approx(
             -0.1547811993517018
         )
@@ -305,63 +306,62 @@
 
     @pytest.mark.skipif(
         sys.version_info > (3, 6),
         reason="this test is incompatible with newer versions of matplotlib",
     )
     def test_vDF_bubble(self, iris_vd, titanic_vd):
         # testing vDataFrame.bubble - img
-        result = titanic_vd.bubble(
+        result = titanic_vd.scatter(
             columns=["fare", "age"],
-            size_bubble_col="pclass",
+            size="pclass",
             color="b",
             img=os.path.dirname(verticapy.__file__) + "/tests/vDataFrame/img_test.png",
             bbox=[0, 10, 0, 10],
         )
         result = result.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result.get_offsets().data]) == 512.3292
         plt.close("all")
         # testing vDataFrame.bubble
-        result = iris_vd.bubble(
+        result = iris_vd.scatter(
             columns=["PetalLengthCm", "SepalLengthCm"],
-            size_bubble_col="PetalWidthCm",
+            size="PetalWidthCm",
             color="b",
         )
         result = result.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result.get_offsets().data]) == 6.9
         assert max([elem[1] for elem in result.get_offsets().data]) == 7.9
         plt.close("all")
-        # testing vDataFrame.scatter using parameter catcol
-        result2 = iris_vd.bubble(
+        # testing vDataFrame.scatter using parameter by
+        result2 = iris_vd.scatter(
             columns=["PetalLengthCm", "SepalLengthCm"],
-            size_bubble_col="PetalWidthCm",
-            catcol="Species",
+            size="PetalWidthCm",
+            by="Species",
             color="b",
         )
         result2 = result2.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result2.get_offsets().data]) <= 6.9
         assert max([elem[1] for elem in result2.get_offsets().data]) <= 7.9
         plt.close("all")
         # testing vDataFrame.scatter using parameter cmap_col
-        result3 = iris_vd.bubble(
+        result3 = iris_vd.scatter(
             columns=["PetalLengthCm", "SepalLengthCm"],
-            size_bubble_col="PetalWidthCm",
+            size="PetalWidthCm",
             cmap_col="SepalWidthCm",
         )
         result3 = result3.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result3.get_offsets().data]) <= 6.9
         assert max([elem[1] for elem in result3.get_offsets().data]) <= 7.9
         plt.close("all")
 
     @pytest.mark.skipif(
         sys.version_info >= (3, 7),
         reason="this test is incompatible with newer versions of matplotlib",
     )
     def test_vDF_density(self, iris_vd):
         # testing vDataFrame[].density
-        create_verticapy_schema()
         for kernel in ["gaussian", "logistic", "sigmoid", "silverman"]:
             result = iris_vd["PetalLengthCm"].density(
                 kernel=kernel, nbins=20, color="b"
             )
             assert max(result.get_default_bbox_extra_artists()[1].get_data()[1]) < 0.25
             plt.close("all")
         for kernel in ["gaussian", "logistic", "sigmoid", "silverman"]:
@@ -383,14 +383,15 @@
         result = titanic_vd.contour(["parch", "sibsp"], func)
         assert len(result.get_default_bbox_extra_artists()) == 32
         plt.close("all")
         result = titanic_vd.contour(["parch", "sibsp"], "parch + sibsp + 1")
         assert len(result.get_default_bbox_extra_artists()) == 32
         plt.close("all")
 
+    @pytest.mark.skip(reason="Python 3.6 VE could not install proper dependencies")
     def test_vDF_geo_plot(self, world_vd):
         assert (
             len(
                 world_vd["geometry"]
                 .geo_plot(column="pop_est", cmap="Reds")
                 .get_default_bbox_extra_artists()
             )
@@ -433,14 +434,17 @@
             80.00000007967, 1e-2
         )
         assert max([elem[1] for elem in result.get_offsets()]) == pytest.approx(
             512.3292, 1e-2
         )
         plt.close("all")
 
+    @pytest.mark.skip(
+        reason="Deprecated, we need to implement the functions for each graphic"
+    )
     def test_vDF_hchart(self, titanic_vd, amazon_vd):
         # boxplot
         result = titanic_vd.hchart(kind="boxplot")
         assert isinstance(result, Highchart)
         # kendall
         result = titanic_vd.hchart(kind="kendall")
         assert isinstance(result, Highchart)
@@ -469,15 +473,17 @@
         # spline
         result = amazon_vd.hchart(x="date", y="number", kind="spline")
         assert isinstance(result, Highchart)
         result = amazon_vd.hchart(x="date", y="number", z="state", kind="spline")
         assert isinstance(result, Highchart)
         # area_range
         result = amazon_vd.hchart(
-            x="date", y=["MIN(number)", "AVG(number)", "MAX(number)"], kind="area_range"
+            x="date",
+            y=["MIN(number)", "AVG(number)", "MAX(number)"],
+            kind="area_range",
         )
         assert isinstance(result, Highchart)
         # area_ts
         result = amazon_vd.hchart(x="date", y="number", kind="area_ts")
         assert isinstance(result, Highchart)
         result = amazon_vd.hchart(x="date", y="number", z="state", kind="area_ts")
         assert isinstance(result, Highchart)
@@ -516,23 +522,35 @@
         )
         assert isinstance(result, Highchart)
         result = titanic_vd.hchart(
             x="pclass", y="survived", z="COUNT(*) AS cnt", kind="stacked_bar"
         )
         assert isinstance(result, Highchart)
         result = titanic_vd.hchart(
-            x="pclass", y="survived", z="COUNT(*) AS cnt", kind="bar", drilldown=True
+            x="pclass",
+            y="survived",
+            z="COUNT(*) AS cnt",
+            kind="bar",
+            drilldown=True,
         )
         assert isinstance(result, Highchart)
         result = titanic_vd.hchart(
-            x="pclass", y="survived", z="COUNT(*) AS cnt", kind="hist", drilldown=True
+            x="pclass",
+            y="survived",
+            z="COUNT(*) AS cnt",
+            kind="hist",
+            drilldown=True,
         )
         assert isinstance(result, Highchart)
         result = titanic_vd.hchart(
-            x="pclass", y="survived", z="COUNT(*) AS cnt", kind="pie", drilldown=True
+            x="pclass",
+            y="survived",
+            z="COUNT(*) AS cnt",
+            kind="pie",
+            drilldown=True,
         )
         assert isinstance(result, Highchart)
         # bubble or scatter
         result = titanic_vd.hchart(x="age", y="fare", kind="scatter")
         assert isinstance(result, Highchart)
         result = titanic_vd.hchart(x="age", y="fare", c="survived", kind="scatter")
         assert isinstance(result, Highchart)
@@ -554,57 +572,59 @@
         # spider
         result = titanic_vd.hchart(x="pclass", kind="spider")
         assert isinstance(result, Highchart)
         # candlestick
         result = amazon_vd.hchart(x="date", y="number", kind="candlestick")
         assert isinstance(result, Highstock)
 
-    def test_vDF_hist(self, titanic_vd):
-        # testing vDataFrame[].hist
+    def test_vDF_bar(self, titanic_vd):
+        # testing vDataFrame[].bar
         # auto
-        result = titanic_vd["age"].hist(color="b")
+        result = titanic_vd["age"].bar(color="b", categorical=False)
         assert result.get_default_bbox_extra_artists()[0].get_height() == pytest.approx(
             0.050243111831442464
         )
         assert result.get_default_bbox_extra_artists()[1].get_height() == pytest.approx(
             0.029983792544570502
         )
         assert result.get_xticks()[1] == pytest.approx(7.24272727)
         plt.close("all")
 
         # method=avg of=survived and h=15
-        result2 = titanic_vd["age"].hist(method="avg", of="survived", h=15, color="b")
+        result2 = titanic_vd["age"].bar(
+            method="avg", of="survived", h=15, color="b", categorical=False
+        )
         assert result2.get_default_bbox_extra_artists()[
             0
         ].get_height() == pytest.approx(0.534653465346535)
         assert result2.get_default_bbox_extra_artists()[
             1
         ].get_height() == pytest.approx(0.354838709677419)
         assert result2.get_xticks()[1] == pytest.approx(15)
         plt.close("all")
 
-        # testing vDataFrame.hist
+        # testing vDataFrame.bar
         # auto & stacked
-        for hist_type in ["auto", "stacked"]:
-            result3 = titanic_vd.hist(
+        for kind in ["auto", "stacked"]:
+            result3 = titanic_vd.bar(
                 columns=["pclass", "sex"],
                 method="avg",
                 of="survived",
-                hist_type=hist_type,
+                kind=kind,
                 color="b",
             )
             assert result3.get_default_bbox_extra_artists()[
                 0
             ].get_height() == pytest.approx(0.964285714285714)
             assert result3.get_default_bbox_extra_artists()[
                 3
             ].get_height() == pytest.approx(0.325581395348837)
             plt.close("all")
-        # multi
-        result4 = titanic_vd.hist(columns=["fare", "age"], hist_type="multi")
+        # hist
+        result4 = titanic_vd.hist(columns=["fare", "age"])
         assert result4.get_default_bbox_extra_artists()[
             0
         ].get_height() == pytest.approx(0.07374392220421394)
         assert result4.get_default_bbox_extra_artists()[
             1
         ].get_height() == pytest.approx(0.4327390599675851)
         plt.close("all")
@@ -625,39 +645,42 @@
         plt.close("all")
         # testing vDataFrame.pie
         result = titanic_vd.pie(["sex", "pclass"], color="b")
         assert result.get_default_bbox_extra_artists()[9].get_text() == "11.3%"
         plt.close("all")
         # testing vDataFrame[].pie - donut
         result = titanic_vd["sex"].pie(
-            method="sum", of="survived", pie_type="donut", colors=["b", "r"]
+            method="sum", of="survived", kind="donut", colors=["b", "r"]
         )
         assert result.get_default_bbox_extra_artists()[6].get_text() == "female"
         assert int(
             result.get_default_bbox_extra_artists()[7].get_text()
         ) == pytest.approx(302)
         plt.close("all")
         # testing vDataFrame[].pie - rose
-        result = titanic_vd["sex"].pie(method="sum", of="survived", pie_type="rose")
+        result = titanic_vd["sex"].pie(method="sum", of="survived", kind="rose")
         assert len(result.get_default_bbox_extra_artists()) == 8
         plt.close("all")
 
     def test_vDF_pivot_table(self, titanic_vd):
-        result = titanic_vd.pivot_table(
-            columns=["age", "pclass"], method="avg", of="survived", cmap="Reds",
+        result = titanic_vd._pivot_table(
+            columns=["age", "pclass"],
+            method="avg",
+            of="survived",
         )
         assert result[1][0] == pytest.approx(0.75)
         assert result[1][1] == pytest.approx(1.0)
         assert result[1][2] == pytest.approx(0.782608695652174)
         assert result[2][0] == pytest.approx(1.0)
         assert result[2][1] == pytest.approx(0.875)
         assert result[2][2] == pytest.approx(0.375)
         assert len(result[1]) == 12
-        plt.close("all")
+        # plt.close("all")
 
+    @pytest.mark.skip(reason="implement new version later.")
     def test_vDF_outliers_plot(self, titanic_vd):
         assert (
             len(titanic_vd.outliers_plot(["fare"]).get_default_bbox_extra_artists())
             == 24
         )
         plt.close("all")
         assert (
@@ -724,32 +747,35 @@
         plt.close("all")
         result = iris_vd.scatter(columns=["PetalLengthCm", "SepalLengthCm"], color="b")
         result = result.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result.get_offsets().data]) == 6.9
         assert max([elem[1] for elem in result.get_offsets().data]) == 7.9
         plt.close("all")
         result2 = iris_vd.scatter(
-            columns=["PetalLengthCm", "SepalLengthCm", "SepalWidthCm"], color="b",
+            columns=["PetalLengthCm", "SepalLengthCm", "SepalWidthCm"],
+            color="b",
         )
         result2 = result2.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result2.get_offsets().data]) == 6.9
         assert max([elem[1] for elem in result2.get_offsets().data]) == 7.9
         plt.close("all")
 
-        # testing vDataFrame.scatter using parameter catcol
+        # testing vDataFrame.scatter using parameter by
         result3 = iris_vd.scatter(
-            columns=["PetalLengthCm", "SepalLengthCm"], catcol="Species", color="b",
+            columns=["PetalLengthCm", "SepalLengthCm"],
+            by="Species",
+            color="b",
         )
         result3 = result3.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result3.get_offsets().data]) <= 6.9
         assert max([elem[1] for elem in result3.get_offsets().data]) <= 7.9
         plt.close("all")
         result4 = iris_vd.scatter(
             columns=["PetalLengthCm", "SepalLengthCm", "SepalWidthCm"],
-            catcol="Species",
+            by="Species",
             color="b",
         )
         result4 = result4.get_default_bbox_extra_artists()[0]
         assert max([elem[0] for elem in result3.get_offsets().data]) <= 6.9
         assert max([elem[1] for elem in result3.get_offsets().data]) <= 7.9
         plt.close("all")
```

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_preprocessing.py` & `verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_preprocessing.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,61 +1,75 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
-from verticapy import vDataFrame, drop, errors, set_option, tablesample
+from verticapy.core.vdataframe.base import vDataFrame
+from verticapy.utilities import drop, TableSample
+from verticapy.errors import ConversionError
 from verticapy.datasets import load_titanic, load_iris, load_market
+from verticapy._config.config import set_option
+
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def iris_vd():
     iris = load_iris()
     yield iris
-    drop(name="public.iris",)
+    drop(
+        name="public.iris",
+    )
 
 
 @pytest.fixture(scope="module")
 def market_vd():
     market = load_market()
     yield market
-    drop(name="public.market",)
+    drop(
+        name="public.market",
+    )
 
 
 class TestvDFPreprocessing:
     def test_vDF_train_test_split(self, titanic_vd):
         train, test = titanic_vd.train_test_split(
             test_size=0.33, order_by={"name": "asc"}, random_state=1
         )
         assert train.shape() == (pytest.approx(827), 14)
         assert test.shape() == (pytest.approx(407), 14)
 
     def test_vDF_add_duplicates(self):
-        names = tablesample(
+        names = TableSample(
             {"name": ["Badr", "Waqas", "Pratibha"], "weight": [2, 4, 6]}
         ).to_vdf()
         result = (
             names.add_duplicates("weight")
             .groupby("name", "COUNT(*) AS cnt")
             .sort("cnt")
         )
@@ -155,15 +169,15 @@
             " Miss.",
             " Mr.",
             " Mrs.",
             " Rev.",
             "rare",
         ]
 
-        ### method = "auto" for numerical vcolumn
+        ### method = "auto" for numerical vDataColumn
         titanic_copy = titanic_vd.copy()
         titanic_copy["age"].discretize()
         assert titanic_copy["age"].distinct() == [
             "[0.00;7.24]",
             "[14.48;21.72]",
             "[21.72;28.96]",
             "[28.96;36.20]",
@@ -234,14 +248,28 @@
 
     def test_vDF_lable_encode(self, titanic_vd):
         titanic_copy = titanic_vd.copy()
         titanic_copy["embarked"].label_encode()
 
         assert titanic_copy["embarked"].distinct() == [0, 1, 2, 3]
 
+    def test_vDF_merge_similar_names(self):
+        x = TableSample(
+            {
+                "age": [50, None, None, None],
+                "information.age": [None, None, 30, None],
+                "dict.age": [None, 80, None, None],
+                "age.people": [None, None, None, 10],
+                "num": [1, 2, 3, 4],
+            }
+        ).to_vdf()
+        result = x.merge_similar_names(skip_word=["information.", "dict.", ".people"])
+        assert result["age"].avg() == 42.5
+        assert result.shape() == (4, 2)
+
     def test_vDF_mean_encode(self, titanic_vd):
         titanic_copy = titanic_vd.copy()
         titanic_copy["embarked"].mean_encode(response="survived")
         result = titanic_copy["embarked"].distinct()
 
         result[0] == pytest.approx(0.2735849)
         result[1] == pytest.approx(0.3241695)
@@ -415,27 +443,76 @@
         titanic_copy.astype({"fare": "int", "cabin": "varchar(1)"})
 
         assert titanic_copy["fare"].dtype() == "int"
         assert titanic_copy["cabin"].dtype() == "varchar(1)"
 
         ### Testing vDataFrame[].astype
         # expected exception
-        with pytest.raises(errors.ConversionError) as exception_info:
+        with pytest.raises(ConversionError) as exception_info:
             titanic_copy["sex"].astype("int")
         # checking the error message
         assert exception_info.match(
             'Could not convert "female" from column titanic.sex to an int8'
         )
 
         titanic_copy["sex"].astype("varchar(10)")
         assert titanic_copy["sex"].dtype() == "varchar(10)"
 
         titanic_copy["age"].astype("float")
         assert titanic_copy["age"].dtype() == "float"
 
+        # STR to VMAP
+        # tests on JSONs vdf
+        vdf = TableSample(
+            {
+                "str_test": [
+                    '{"name": "Badr", "information": {"age": 29, "numero": [0, 6, 3]}}'
+                ]
+            }
+        ).to_vdf()
+        vdf["str_test"].astype("vmap")
+        assert int(vdf["str_test"]["information"]["age"][0]) == 29
+        # tests on CSVs strings
+        vdf = TableSample({"str_test": ["a,b,c,d"]}).to_vdf()
+        vdf["str_test"].astype("vmap(val1,val2,val3,val4)")
+        assert vdf["str_test"]["val2"][0] == "b"
+
+        # STR to ARRAY
+        vdf = TableSample({"str_test": ["a,b,c,d"]}).to_vdf()
+        vdf["str_test"].astype("array")
+        assert vdf["str_test"][1][0] == "b"
+
+        # VMAP to STR
+        vdf = TableSample(
+            {
+                "str_test": [
+                    '{"name": "Badr", "information": {"age": 29, "numero": [0, 6, 3]}}'
+                ]
+            }
+        ).to_vdf()
+        vdf["str_test"].astype("vmap")
+        vdf["str_test"].astype(str)
+        assert (
+            vdf["str_test"][0]
+            == '{\n\t"information": {\n\t\t"age": "29",\n\t\t"numero": {\n\t\t\t"0": "0",\n\t\t\t"1": "6",\n\t\t\t"2": "3"\n\t\t}\n\t},\n\t"name": "Badr"\n}'
+        )
+        vdf = TableSample(
+            {
+                "str_test": [
+                    '{"name": "Badr", "information": {"age": 29, "numero": [0, 6, 3]}}'
+                ]
+            }
+        ).to_vdf()
+        vdf["str_test"].astype("vmap")
+        vdf["str_test"].astype("json")
+        assert (
+            vdf["str_test"][0]
+            == '{\n\t"information": {\n\t\t"age": "29",\n\t\t"numero": {\n\t\t\t"0": "0",\n\t\t\t"1": "6",\n\t\t\t"2": "3"\n\t\t}\n\t},\n\t"name": "Badr"\n}'
+        )
+
     def test_vDF_bool_to_int(self, titanic_vd):
         titanic_copy = titanic_vd.copy()
         titanic_copy["survived"].astype("bool")
         assert titanic_copy["survived"].dtype() == "bool"
 
         titanic_copy.bool_to_int()
         assert titanic_copy["survived"].dtype() == "int"
```

### Comparing `verticapy-0.9.0/verticapy/tests/vDataFrame/test_vDF_utilities.py` & `verticapy-1.0.0b1/verticapy/tests/vDataFrame/test_vDF_utilities.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,41 +1,47 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import os, pickle
 from math import ceil, floor
 
 # Other Modules
-import pandas, geopandas
+import pandas
 
 # VerticaPy
 from verticapy import (
     vDataFrame,
-    get_session,
     drop,
     set_option,
     read_shp,
+    read_csv,
+    read_json,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 import verticapy.stats as st
 from verticapy.datasets import load_titanic, load_cities, load_amazon, load_world
+from verticapy.sql.sys import current_session, username
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
@@ -154,25 +160,43 @@
     def test_vDF_to_csv(self, titanic_vd):
         titanic_vd.copy().select(["age", "fare"]).sort({"age": "desc", "fare": "desc"})[
             0:2
         ].to_csv("verticapy_test_to_csv.csv")
         try:
             file = open("verticapy_test_to_csv.csv", "r")
             result = file.read()
-            assert result == "age,fare\n80.000,30.00000\n76.000,78.85000"
+            assert result == '"age","fare"\n80.000,30.00000\n76.000,78.85000'
         except:
             os.remove("verticapy_test_to_csv.csv")
             file.close()
             raise
         os.remove("verticapy_test_to_csv.csv")
         file.close()
-        # TODO - test with multiple CSV files.
+        # multiple files
+        try:
+            titanic_vd.to_csv(
+                "titanic_verticapy_test_to_csv",
+                n_files=3,
+                order_by=["name", "age", "fare"],
+            )
+            titanic_test = read_csv("titanic_verticapy_test_to_csv/*.csv")
+            assert titanic_test.shape() == (1234, 14)
+        except:
+            os.remove("titanic_verticapy_test_to_csv/1.csv")
+            os.remove("titanic_verticapy_test_to_csv/2.csv")
+            os.remove("titanic_verticapy_test_to_csv/3.csv")
+            os.rmdir("titanic_verticapy_test_to_csv")
+            raise
+        os.remove("titanic_verticapy_test_to_csv/1.csv")
+        os.remove("titanic_verticapy_test_to_csv/2.csv")
+        os.remove("titanic_verticapy_test_to_csv/3.csv")
+        os.rmdir("titanic_verticapy_test_to_csv")
 
     def test_vDF_to_parquet(self, titanic_vd):
-        session_id = get_session()
+        session_id = f"{current_session()}_{username()}"
         name = "parquet_test_{}".format(session_id)
         result = titanic_vd.to_parquet(name)
         assert result["Rows Exported"][0] == 1234
         # TODO: erasing the folder
 
     def test_vDF_to_db(self, titanic_vd):
         drop("verticapy_titanic_tmp")
@@ -262,32 +286,50 @@
             assert result[0] == "verticapy_titanic_tmp"
         except:
             drop("verticapy_titanic_tmp")
             raise
         drop("verticapy_titanic_tmp")
 
     def test_vDF_to_json(self, titanic_vd):
-        session_id = get_session()
+        session_id = f"{current_session()}_{username()}"
         titanic_vd.copy().select(["age", "fare"]).sort({"age": "desc", "fare": "desc"})[
             0:2
         ].to_json("verticapy_test_to_json.json")
         try:
             file = open("verticapy_test_to_json.json", "r")
             result = file.read()
             assert (
                 result
-                == '[\n{"age": 80.000, "fare": 30.00000},\n{"age": 76.000, "fare": 78.85000},\n]'
+                == '[\n{"age": 80.000, "fare": 30.00000},\n{"age": 76.000, "fare": 78.85000}\n]'
             )
         except:
             os.remove("verticapy_test_to_json.json")
             file.close()
             raise
         os.remove("verticapy_test_to_json.json")
         file.close()
-        # TODO - test with multiple JSON files.
+        # multiple files
+        try:
+            titanic_vd.to_json(
+                "titanic_verticapy_test_to_json",
+                n_files=3,
+                order_by=["name", "age", "fare"],
+            )
+            titanic_test = read_json("titanic_verticapy_test_to_json/*.json")
+            assert titanic_test.shape() == (1234, 14)
+        except:
+            os.remove("titanic_verticapy_test_to_json/1.json")
+            os.remove("titanic_verticapy_test_to_json/2.json")
+            os.remove("titanic_verticapy_test_to_json/3.json")
+            os.rmdir("titanic_verticapy_test_to_json")
+            raise
+        os.remove("titanic_verticapy_test_to_json/1.json")
+        os.remove("titanic_verticapy_test_to_json/2.json")
+        os.remove("titanic_verticapy_test_to_json/3.json")
+        os.rmdir("titanic_verticapy_test_to_json")
 
     def test_vDF_to_list(self, titanic_vd):
         result = titanic_vd.select(["age", "survived"])[:20].to_list()
         assert len(result) == 20
         assert len(result[0]) == 2
 
     def test_vDF_to_numpy(self, titanic_vd):
@@ -302,15 +344,18 @@
     def test_vDF_to_pickle(self, titanic_vd):
         result = titanic_vd.select(["age", "survived"])[:20].to_pickle("save.p")
         pickle.DEFAULT_PROTOCOL = 4
         result_tmp = pickle.load(open("save.p", "rb"))
         assert result_tmp.shape() == (20, 2)
         os.remove("save.p")
 
+    @pytest.mark.skip(reason="Python 3.6 VE could not install proper dependencies")
     def test_vDF_to_geopandas(self, world_vd):
+        import geopandas
+
         result = world_vd.to_geopandas(geometry="geometry")
         assert isinstance(result, geopandas.GeoDataFrame)
         assert result.shape == (177, 4)
 
     def test_vDF_to_shp(self, cities_vd):
         drop(name="public.cities_test")
         cities_vd.to_shp("cities_test", "/home/dbadmin/", shape="Point")
@@ -323,37 +368,37 @@
         except:
             pass
         drop(name="public.cities_test")
 
     def test_vDF_del_catalog(self, titanic_vd):
         result = titanic_vd.copy()
         result.describe(method="numerical")
-        assert "max" in result["age"].catalog
-        assert "avg" in result["age"].catalog
+        assert "max" in result["age"]._catalog
+        assert "avg" in result["age"]._catalog
         result.del_catalog()
-        assert "max" not in result["age"].catalog
-        assert "avg" not in result["age"].catalog
+        assert "max" not in result["age"]._catalog
+        assert "avg" not in result["age"]._catalog
 
     def test_vDF_load(self, titanic_vd):
         result = titanic_vd.copy()
-        result._VERTICAPY_VARIABLES_["saving"] = []
+        result._vars["saving"] = []
         result.save()
-        assert len(result._VERTICAPY_VARIABLES_["saving"]) == 1
+        assert len(result._vars["saving"]) == 1
         result.filter("age < 40")
         result["embarked"].drop()
         assert result.shape() == (760, 13)
         result = result.load()
-        assert len(result._VERTICAPY_VARIABLES_["saving"]) == 0
+        assert len(result._vars["saving"]) == 0
         assert result.shape() == (1234, 14)
 
     def test_vDF_save(self, titanic_vd):
         result = titanic_vd.copy()
-        result._VERTICAPY_VARIABLES_["saving"] = []
+        result._vars["saving"] = []
         result.save()
-        assert len(result._VERTICAPY_VARIABLES_["saving"]) == 1
+        assert len(result._vars["saving"]) == 1
 
     def test_vDF_catcol(self, titanic_vd):
         result = [
             elem.replace('"', "").lower()
             for elem in titanic_vd.catcol(max_cardinality=4)
         ]
         result.sort()
@@ -413,19 +458,19 @@
         assert result == amazon_vd["number"].ctype()
         assert result2 == amazon_vd["date"].ctype()
         assert result3 == amazon_vd["state"].ctype()
 
     def test_vDF_empty(self, amazon_vd):
         # test for non-empty vDataFrame
         result = amazon_vd.empty()
-        assert result == False
+        assert not result
 
         # test for empty vDataFrame
         result2 = amazon_vd.copy().drop(["number", "date", "state"]).empty()
-        assert result2 == True
+        assert result2
 
     def test_vDF_expected_store_usage(self, titanic_vd):
         # test expected_size
         result = titanic_vd.expected_store_usage()["expected_size (b)"][-1]
         assert result == pytest.approx(85947.0)
 
         # test max_size
@@ -485,36 +530,36 @@
     def test_vDF_info(self, titanic_vd):
         result = titanic_vd.copy().filter("age > 0")
         result["fare"].drop_outliers()
         result = len(result.info().split("\n")) - 1
         assert result == 2
 
     def test_vDF_isdate(self, amazon_vd):
-        # test for date-like vcolumn
+        # test for date-like vDataColumn
         result = amazon_vd["date"].isdate()
-        assert result == True
+        assert result
 
-        # test for non-date-like vcolumn
+        # test for non-date-like vDataColumn
         result2 = amazon_vd["number"].isdate()
-        assert result2 == False
+        assert not result2
 
         result2 = amazon_vd["state"].isdate()
-        assert result2 == False
+        assert not result2
 
     def test_vDF_isnum(self, amazon_vd):
-        # test for numerical vcolumn
+        # test for numerical vDataColumn
         result = amazon_vd["number"].isnum()
-        assert result == True
+        assert result
 
-        # test for non-numerical vcolumn
+        # test for non-numerical vDataColumn
         result = amazon_vd["date"].isnum()
-        assert result == False
+        assert not result
 
         result = amazon_vd["state"].isnum()
-        assert result == False
+        assert not result
 
     @pytest.mark.skip(reason="test not stable")
     def test_vDF_memory_usage(self, amazon_vd):
         # testing vDataFrame[].memory_usage
         result = amazon_vd["number"].memory_usage()
         assert result == pytest.approx(1690, 5e-2)
 
@@ -525,15 +570,23 @@
         assert result2["value"][2] == pytest.approx(1689, 5e-2)
         assert result2["value"][3] == pytest.approx(1690, 5e-2)
         assert result2["value"][4] == pytest.approx(5751, 5e-2)
 
     def test_vDF_numcol(self, titanic_vd):
         result = [elem.replace('"', "") for elem in titanic_vd.numcol()]
         result.sort()
-        assert result == ["age", "body", "fare", "parch", "pclass", "sibsp", "survived"]
+        assert result == [
+            "age",
+            "body",
+            "fare",
+            "parch",
+            "pclass",
+            "sibsp",
+            "survived",
+        ]
 
     def test_vDF_tail(self, titanic_vd):
         # testing vDataFrame[].tail
         result = titanic_vd.copy().sort(["age"])["age"].tail(2)
         assert result["age"] == [76.0, 80.0]
 
         # testing vDataFrame.tail
@@ -544,17 +597,17 @@
     def test_vDF_sql(self, titanic_vd):
         sql = """-- Selecting some columns \n
                  SELECT 
                     age, 
                     fare 
                  FROM titanic 
                  WHERE age IS NOT NULL;;"""
-        vdf = vDataFrame(sql=sql)
+        vdf = vDataFrame(sql)
         assert vdf.shape() == (997, 2)
-        vdf = vDataFrame(sql=sql, usecols=["age"])
+        vdf = vDataFrame(sql, usecols=["age"])
         assert vdf.shape() == (997, 1)
 
     def test_vDF_store_usage(self, titanic_vd):
         result = titanic_vd["age"].store_usage()
         assert result == pytest.approx(5908, 1e-2)
 
     def test_vDF_swap(self, titanic_vd):
@@ -562,13 +615,7 @@
         result.swap("sex", 0)
         result.swap("pclass", 1)
         assert result.get_columns()[0].replace('"', "") == "sex"
         assert result.get_columns()[1].replace('"', "") == "pclass"
         result.swap("pclass", "sex")
         assert result.get_columns()[0].replace('"', "") == "pclass"
         assert result.get_columns()[1].replace('"', "") == "sex"
-
-    def test_vDF_version(self, titanic_vd):
-        result = titanic_vd.version()
-        assert 3 <= len(result) <= 4
-        assert 6 < result[0] < 20
-        assert 0 <= result[1] < 5
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/__init__.py` & `verticapy-1.0.0b1/verticapy/jupyter/extensions/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_balance.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_balance.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,37 +1,43 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
 from verticapy.learn.preprocessing import Balance
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 class TestBalance:
     def test_hybrid_method(self, titanic_vd):
         current_cursor().execute("DROP VIEW IF EXISTS public.hybrid_balanced")
 
         bvd = Balance(
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_bisecting_kmeans.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_bisecting_kmeans.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,45 +1,51 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 import verticapy
 from verticapy import (
     vDataFrame,
     drop,
     set_option,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality, load_dataset_num
 from verticapy.learn.cluster import BisectingKMeans
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def bsk_data_vd():
     bsk_data = load_dataset_num(table_name="bsk_data", schema="public")
     yield bsk_data
     drop(name="public.bsk_data", method="table")
@@ -50,53 +56,51 @@
     current_cursor().execute("DROP MODEL IF EXISTS bsk_model_test")
 
     current_cursor().execute(
         "SELECT BISECTING_KMEANS('bsk_model_test', 'public.bsk_data', '*', 3 USING PARAMETERS exclude_columns='id', kmeans_seed=11, id_column='id')"
     )
 
     model_class = BisectingKMeans("bsk_model_test", n_cluster=3, max_iter=10)
-    model_class.metrics_ = model_class.get_attr("Metrics")
-    model_class.cluster_centers_ = model_class.get_attr("BKTree")
     model_class.X = ["col1", "col2", "col3", "col4"]
+    model_class._compute_attributes()
 
     yield model_class
     model_class.drop()
 
 
 class TestBisectingKMeans:
     def test_repr(self, model):
-        assert "bisecting_kmeans('bsk_model_test'" in model.__repr__()
-        model_repr = BisectingKMeans("BisectingKMeans_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<BisectingKMeans>"
+        assert model.__repr__() == "<BisectingKMeans>"
 
     def test_deploySQL(self, model):
-        expected_sql = "APPLY_BISECTING_KMEANS(col1, col2, col3, col4 USING PARAMETERS model_name = 'bsk_model_test', match_by_pos = 'true')"
+        expected_sql = 'APPLY_BISECTING_KMEANS("col1", "col2", "col3", "col4" USING PARAMETERS model_name = \'bsk_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS bsk_model_test_drop")
-        model_test = BisectingKMeans("bsk_model_test_drop",)
+        model_test = BisectingKMeans(
+            "bsk_model_test_drop",
+        )
         model_test.fit("public.bsk_data", ["col1", "col2", "col3", "col4"])
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'bsk_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "bsk_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'bsk_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "num_of_clusters",
             "dimensions_of_dataset",
             "num_of_clusters_found",
             "height_of_BKTree",
             "BKTree",
@@ -110,20 +114,34 @@
             "height_of_BKTree",
             "center_id, col1, col2, col3, col4, withinss, totWithinss, bisection_level, cluster_size, parent, left_child, right_child",
             "Measure, Value",
             "call_string",
         ]
         assert m_att["#_of_rows"] == [1, 1, 1, 1, 5, 7, 1]
 
-        assert model.get_attr("num_of_clusters")["num_of_clusters"][0] == 3
-        assert model.get_attr("dimensions_of_dataset")["dimensions_of_dataset"][0] == 4
-        assert model.get_attr("num_of_clusters_found")["num_of_clusters_found"][0] == 3
-        assert model.get_attr("height_of_BKTree")["height_of_BKTree"][0] == 3
+        assert (
+            model.get_vertica_attributes("num_of_clusters")["num_of_clusters"][0] == 3
+        )
+        assert (
+            model.get_vertica_attributes("dimensions_of_dataset")[
+                "dimensions_of_dataset"
+            ][0]
+            == 4
+        )
+        assert (
+            model.get_vertica_attributes("num_of_clusters_found")[
+                "num_of_clusters_found"
+            ][0]
+            == 3
+        )
+        assert (
+            model.get_vertica_attributes("height_of_BKTree")["height_of_BKTree"][0] == 3
+        )
 
-        m_att_bktree = model.get_attr(attr_name="BKTree")
+        m_att_bktree = model.get_vertica_attributes(attr_name="BKTree")
         assert m_att_bktree["bisection_level"] == [0, 1, 1, 2, 2]
 
     def test_get_params(self, model):
         assert model.get_params() == {
             "max_iter": 10,
             "tol": 0.0001,
             "n_cluster": 3,
@@ -143,56 +161,60 @@
 
     def test_set_params(self, model):
         model.set_params({"max_iter": 200})
         assert model.get_params()["max_iter"] == 200
 
     def test_model_from_vDF(self, bsk_data_vd):
         current_cursor().execute("DROP MODEL IF EXISTS bsk_vDF")
-        model_test = BisectingKMeans("bsk_vDF",)
+        model_test = BisectingKMeans(
+            "bsk_vDF",
+        )
         model_test.fit(bsk_data_vd, ["col1", "col2", "col3", "col4"])
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'bsk_vDF'"
         )
         assert current_cursor().fetchone()[0] == "bsk_vDF"
         model_test.drop()
 
     def test_init_method(self):
         model_test_kmeanspp = BisectingKMeans("bsk_kmeanspp_test", init="kmeanspp")
         model_test_kmeanspp.drop()
         model_test_kmeanspp.fit("public.bsk_data", ["col1", "col2", "col3", "col4"])
 
         assert (
             "kmeans_center_init_method='kmeanspp'"
-            in model_test_kmeanspp.get_attr("call_string")["call_string"][0]
+            in model_test_kmeanspp.get_vertica_attributes("call_string")["call_string"][
+                0
+            ]
         )
         model_test_kmeanspp.drop()
 
         model_test_pseudo = BisectingKMeans("bsk_pseudo_test", init="pseudo")
         model_test_pseudo.drop()
         model_test_pseudo.fit("public.bsk_data", ["col1", "col2", "col3", "col4"])
         assert (
             "kmeans_center_init_method='pseudo'"
-            in model_test_pseudo.get_attr("call_string")["call_string"][0]
+            in model_test_pseudo.get_vertica_attributes("call_string")["call_string"][0]
         )
         model_test_pseudo.drop()
 
     def test_get_plot(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = BisectingKMeans("model_test_plot",)
+        model_test = BisectingKMeans(
+            "model_test_plot",
+        )
         model_test.fit(winequality_vd, ["alcohol", "quality"])
         result = model_test.plot()
-        assert len(result.get_default_bbox_extra_artists()) == 16
+        assert len(result.get_default_bbox_extra_artists()) > 7
         plt.close("all")
         model_test.drop()
 
     def test_to_graphviz(self, model):
         gvz_tree_0 = model.to_graphviz(
-            tree_id=0,
-            classes_color=["red", "blue", "green"],
-            round_pred=4,
+            round_score=4,
             percent=True,
             vertical=False,
             node_style={"shape": "box", "style": "filled"},
             arrow_style={"color": "blue"},
             leaf_style={"shape": "circle", "style": "filled"},
         )
         assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_0
@@ -201,26 +223,26 @@
     def test_plot_tree(self, model):
         result = model.plot_tree()
         assert model.to_graphviz() == result.source.strip()
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT APPLY_BISECTING_KMEANS(5.006, 3.418, 1.464, 0.244 USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[5.006, 3.418, 1.464, 0.244]])
+            model.to_python()([[5.006, 3.418, 1.464, 0.244]])
         )
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT APPLY_BISECTING_KMEANS(5.006, 3.418, 1.464, 0.244 USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([5.006, 3.418, 1.464, 0.244])
+                model.model_name, model.to_sql([5.006, 3.418, 1.464, 0.244])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model):
         mmodel = model.to_memmodel()
@@ -229,9 +251,9 @@
             [[5.006, 3.418, 1.464, 0.244], [3.0, 11.0, 1993.0, 0.0]]
         )
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
         vdf = vDataFrame("public.bsk_data")
         vdf["prediction_sql"] = mmodel.predict_sql(["col1", "col2", "col3", "col4"])
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
         assert score == pytest.approx(1.0)
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_countvectorizer.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_countvectorizer.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,88 +1,67 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import (
     drop,
     set_option,
-    create_verticapy_schema,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
 from verticapy.learn.preprocessing import CountVectorizer
+from verticapy._utils._sql._format import clean_query
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
-    create_verticapy_schema()
-    model_class = CountVectorizer("model_test_countvectorizer",)
+    model_class = CountVectorizer(
+        "model_test_countvectorizer",
+    )
     model_class.drop()
     model_class.fit("public.titanic", ["name"])
     yield model_class
     model_class.drop()
 
 
 class TestCountVectorizer:
     def test_repr(self, model):
-        assert "Vocabulary" in model.__repr__()
-        model_repr = CountVectorizer("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<CountVectorizer>"
-
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
-        assert m_att["attr_name"] == [
-            "lowercase",
-            "max_df",
-            "min_df",
-            "max_features",
-            "ignore_special",
-            "max_text_size",
-            "vocabulary",
-            "stop_words",
-        ]
-        m_att = model.get_attr("lowercase")
-        assert m_att == model.parameters["lowercase"]
-        m_att = model.get_attr("max_df")
-        assert m_att == model.parameters["max_df"]
-        m_att = model.get_attr("min_df")
-        assert m_att == model.parameters["min_df"]
-        m_att = model.get_attr("max_features")
-        assert m_att == model.parameters["max_features"]
-        m_att = model.get_attr("ignore_special")
-        assert m_att == model.parameters["ignore_special"]
-        m_att = model.get_attr("max_text_size")
-        assert m_att == model.parameters["max_text_size"]
-        m_att = model.get_attr("vocabulary")
-        assert m_att == model.parameters["vocabulary"]
-        m_att = model.get_attr("stop_words")
-        assert m_att == model.parameters["stop_words"]
+        assert model.__repr__() == "<CountVectorizer>"
+
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_attributes()
+        assert m_att == ["stop_words_", "vocabulary_", "n_errors_"]
 
     def test_deploySQL(self, model):
         expected_sql = (
             "SELECT \n                    * \n                 FROM"
             " (SELECT \n                          token, \n        "
             "                  cnt / SUM(cnt) OVER () AS df, \n    "
             "                      cnt, \n                         "
@@ -92,36 +71,19 @@
             " BY COUNT(*) DESC) AS rnk \n                       FROM"
             " model_test_countvectorizer GROUP BY 1) VERTICAPY_SUBTABLE)"
             " VERTICAPY_SUBTABLE \n                       WHERE (df "
             "BETWEEN 0.0 AND 1.0)"
         )
         result_sql = model.deploySQL()
 
-        assert result_sql == expected_sql
-
-    def test_drop(self, titanic_vd):
-        model_test = CountVectorizer("model_test_drop")
-        model_test.drop()
-        model_test.fit(titanic_vd, ["name"])
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone()[0] in (
-            "model_test_drop",
-            '"model_test_drop"',
-        )
-        model_test.drop()
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone() is None
+        assert result_sql == clean_query(expected_sql)
 
-    def test_get_attr(self, model):
+    def test_get_attributes(self, model):
         assert sorted(model.vocabulary_)[0:3] == ["a", "aaron", "abbing"]
-        assert model.stop_words_ == []
+        assert len(model.stop_words_) == 0
 
     def test_get_params(self, model):
         assert model.get_params() == {
             "ignore_special": True,
             "lowercase": True,
             "max_df": 1.0,
             "max_features": -1,
@@ -135,17 +97,19 @@
         assert result["df"][0] == pytest.approx(0.14816310052482842)
         assert result["cnt"][0] == pytest.approx(734)
         assert result["rnk"][0] == pytest.approx(1)
         assert result.shape() == (1841, 4)
 
     def test_set_params(self, model):
         model.set_params({"lowercase": False})
-        assert model.get_params()["lowercase"] == False
+        assert not model.get_params()["lowercase"]
         model.set_params({"lowercase": True})
-        assert model.get_params()["lowercase"] == True
+        assert model.get_params()["lowercase"]
 
     def test_model_from_vDF(self, titanic_vd):
-        model_class = CountVectorizer("model_test_vdf",)
+        model_class = CountVectorizer(
+            "model_test_vdf",
+        )
         model_class.drop()
         model_class.fit(titanic_vd, ["name"])
         assert model_class.transform().shape() == (1841, 4)
         model_class.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_dbscan.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_kde.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,105 +1,110 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-import verticapy
 from verticapy import (
     drop,
     set_option,
-    create_verticapy_schema,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
-from verticapy.learn.cluster import DBSCAN
+from verticapy.learn.neighbors import KernelDensity
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
-    create_verticapy_schema()
-    model_class = DBSCAN("DBSCAN_model_test",)
+    model_class = KernelDensity(
+        "KernelDensity_model_test",
+    )
     model_class.drop()
     model_class.fit("public.titanic", ["age", "fare"])
     yield model_class
     model_class.drop()
 
 
-class TestDBSCAN:
+class TestKernelDensity:
     def test_repr(self, model):
-        assert "Additional Info" in model.__repr__()
-        model_repr = DBSCAN("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<DBSCAN>"
-
-    def test_drop(self):
-        model_test = DBSCAN("model_test_drop",)
-        model_test.drop()
-        model_test.fit("public.titanic", ["age", "fare"], "survived")
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone()[0] in (
-            "model_test_drop",
-            '"model_test_drop"',
-        )
-
-        model_test.drop()
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone() is None
+        assert model.__repr__() == "<KernelDensity>"
 
     def test_get_params(self, model):
-        assert model.get_params() == {"eps": 0.5, "min_samples": 5, "p": 2}
+        assert model.get_params() == {
+            "bandwidth": 1,
+            "kernel": "gaussian",
+            "max_depth": 5,
+            "max_leaf_nodes": 1000000000,
+            "min_samples_leaf": 1,
+            "nbins": 5,
+            "p": 2,
+            "xlim": [],
+        }
 
     def test_get_predict(self, titanic_vd, model):
-        titanic_copy = model.predict()
+        titanic_copy = model.predict(titanic_vd.copy(), name="kde")
 
-        assert titanic_copy["dbscan_cluster"].min() == pytest.approx(-1, abs=1e-6)
+        assert titanic_copy["kde"].mean() == pytest.approx(
+            1.82115211838814e-06, abs=1e-6
+        )
 
-    def test_get_attr(self, model):
-        result = model.get_attr()
-        assert result["attr_name"] == ["n_cluster", "n_noise"]
+    def test_get_vertica_attributes(self, model):
+        result = model.get_vertica_attributes()
+        assert result["attr_name"][0] == "tree_count"
 
     def test_get_plot(self, model):
         result = model.plot()
-        assert len(result.get_default_bbox_extra_artists()) == 27
+        assert len(result.get_default_bbox_extra_artists()) == 8
         plt.close("all")
+        model_test = KernelDensity("model_test_plot_kde_plot")
+        model_test.drop()
+        model_test.fit("public.titanic", ["age"])
+        result = model_test.plot()
+        assert len(result.get_default_bbox_extra_artists()) == 9
+        model_test.drop()
 
     def test_set_params(self, model):
         model.set_params({"p": 1})
 
         assert model.get_params()["p"] == 1
 
     def test_model_from_vDF(self, titanic_vd):
-        model_test = DBSCAN("dbscan_from_vDF")
+        model_test = KernelDensity(
+            "KernelDensity_from_vDF_tmp",
+        )
         model_test.drop()
-        model_test.fit(titanic_vd, ["age", "fare"], "survived")
-        assert model_test.predict()["dbscan_cluster"].min() == pytest.approx(
-            -1, abs=1e-6
+        model_test.fit(titanic_vd, ["age", "fare"])
+        titanic_copy = model_test.predict(titanic_vd.copy(), name="kde")
+
+        assert titanic_copy["kde"].mean() == pytest.approx(
+            1.82115211838814e-06, abs=1e-6
         )
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_decision_tree_classifier.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_decision_tree_classifier.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,33 +1,37 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import (
     vDataFrame,
     drop,
     set_option,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic, load_dataset_cl
 from verticapy.learn.tree import DecisionTreeClassifier
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
@@ -37,15 +41,17 @@
     drop(name="public.dtc_data", method="table")
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(dtc_data_vd):
     current_cursor().execute("DROP MODEL IF EXISTS decision_tc_model_test")
 
     current_cursor().execute(
@@ -62,34 +68,23 @@
         min_info_gain=0,
         nbins=40,
     )
     model_class.input_relation = "public.dtc_data"
     model_class.test_relation = model_class.input_relation
     model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
     model_class.y = '"TransPortation"'
-    current_cursor().execute(
-        "SELECT DISTINCT {} FROM {} WHERE {} IS NOT NULL ORDER BY 1".format(
-            model_class.y, model_class.input_relation, model_class.y
-        )
-    )
-    classes = current_cursor().fetchall()
-    model_class.classes_ = [item[0] for item in classes]
+    model_class._compute_attributes()
 
     yield model_class
     model_class.drop()
 
 
 class TestDecisionTreeClassifier:
     def test_repr(self, model):
-        assert (
-            "SELECT rf_classifier('public.decision_tc_model_test'," in model.__repr__()
-        )
-        model_repr = DecisionTreeClassifier("RF_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<RandomForestClassifier>"
+        assert model.__repr__() == "<RandomForestClassifier>"
 
     def test_classification_report(self, model):
         cls_rep1 = model.classification_report().transpose()
 
         assert cls_rep1["auc"][0] == pytest.approx(1.0)
         assert cls_rep1["prc_auc"][0] == pytest.approx(1.0)
         assert cls_rep1["accuracy"][0] == pytest.approx(1.0)
@@ -97,52 +92,53 @@
         assert cls_rep1["precision"][0] == pytest.approx(1.0)
         assert cls_rep1["recall"][0] == pytest.approx(1.0)
         assert cls_rep1["f1_score"][0] == pytest.approx(1.0)
         assert cls_rep1["mcc"][0] == pytest.approx(1.0)
         assert cls_rep1["informedness"][0] == pytest.approx(1.0)
         assert cls_rep1["markedness"][0] == pytest.approx(1.0)
         assert cls_rep1["csi"][0] == pytest.approx(1.0)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.999)
-
-        cls_rep2 = model.classification_report(cutoff=0.2).transpose()
-
-        assert cls_rep2["cutoff"][0] == pytest.approx(0.2)
 
     def test_confusion_matrix(self, model):
         conf_mat1 = model.confusion_matrix()
 
-        assert conf_mat1["Bus"] == [4, 0, 0]
-        assert conf_mat1["Car"] == [0, 3, 0]
-        assert conf_mat1["Train"] == [0, 0, 3]
+        assert list(conf_mat1[:, 0]) == [4, 0, 0]
+        assert list(conf_mat1[:, 1]) == [0, 3, 0]
+        assert list(conf_mat1[:, 2]) == [0, 0, 3]
 
         conf_mat2 = model.confusion_matrix(cutoff=0.2)
 
-        assert conf_mat2["Bus"] == [4, 0, 0]
-        assert conf_mat2["Car"] == [0, 3, 0]
-        assert conf_mat2["Train"] == [0, 0, 3]
+        assert list(conf_mat2[:, 0]) == [4, 0, 0]
+        assert list(conf_mat2[:, 1]) == [0, 3, 0]
+        assert list(conf_mat2[:, 2]) == [0, 0, 3]
 
     def test_contour(self, titanic_vd):
-        model_test = DecisionTreeClassifier("model_contour",)
+        model_test = DecisionTreeClassifier(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 34
         model_test.drop()
 
     def test_deploySQL(self, model):
         expected_sql = 'PREDICT_RF_CLASSIFIER("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'decision_tc_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS decision_tc_model_test_drop")
-        model_test = DecisionTreeClassifier("decision_tc_model_test_drop",)
+        model_test = DecisionTreeClassifier(
+            "decision_tc_model_test_drop",
+        )
         model_test.fit(
             "public.dtc_data",
             ["Gender", '"owned cars"', "cost", "income"],
             "TransPortation",
         )
 
         current_cursor().execute(
@@ -153,23 +149,23 @@
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'decision_tc_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        f_imp = model.features_importance()
+        f_imp = model.features_importance(show=False)
 
         assert f_imp["index"] == ["cost", "owned cars", "gender", "income"]
         assert f_imp["importance"] == [75.76, 15.15, 9.09, 0.0]
         assert f_imp["sign"] == [1, 1, 1, 0]
         plt.close("all")
 
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000)
+        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000, show=False)
 
         assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
         assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(1.0)
         assert lift_ch["lift"][300] == pytest.approx(2.5)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(1.0)
         assert lift_ch["lift"][900] == pytest.approx(2.5)
@@ -179,26 +175,20 @@
         model_test = DecisionTreeClassifier("rfc_python_test")
         model_test.drop()
         model_test.fit(titanic_vd, ["age", "fare", "sex"], "embarked")
         current_cursor().execute(
             "SELECT PREDICT_RF_CLASSIFIER(30.0, 45.0, 'male' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 45.0, "male"]])[0]
-        )
+        assert prediction == model_test.to_python()([[30.0, 45.0, "male"]])[0]
         current_cursor().execute(
             "SELECT PREDICT_RF_CLASSIFIER(30.0, 145.0, 'female' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 145.0, "female"]])[0]
-        )
+        assert prediction == model_test.to_python()([[30.0, 145.0, "female"]])[0]
 
     def test_to_sql(self, model, titanic_vd):
         model_test = DecisionTreeClassifier("rfc_sql_test")
         model_test.drop()
         model_test.fit(titanic_vd, ["age", "fare", "sex"], "survived")
         current_cursor().execute(
             "SELECT PREDICT_RF_CLASSIFIER(* USING PARAMETERS model_name = 'rfc_sql_test', match_by_pos=True)::int, {}::int FROM (SELECT 30.0 AS age, 45.0 AS fare, 'male' AS sex) x".format(
@@ -250,31 +240,31 @@
         )
         model.predict_proba(
             vdf, name="prediction_proba_vertica_sql_1", pos_label=model.classes_[1]
         )
         model.predict_proba(
             vdf, name="prediction_proba_vertica_sql_2", pos_label=model.classes_[2]
         )
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", "r2"
+            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", metric="r2"
         )
         assert score == pytest.approx(1.0)
 
-    def test_get_attr(self, model):
-        attr = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        attr = model.get_vertica_attributes()
         assert attr["attr_name"] == [
             "tree_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "details",
         ]
@@ -283,47 +273,51 @@
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "predictor, type",
         ]
         assert attr["#_of_rows"] == [1, 1, 1, 1, 4]
 
-        details = model.get_attr("details")
+        details = model.get_vertica_attributes("details")
         assert details["predictor"] == ["gender", "owned cars", "cost", "income"]
         assert details["type"] == [
             "char or varchar",
             "int",
             "char or varchar",
             "char or varchar",
         ]
 
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("tree_count")["tree_count"][0] == 1
         assert (
-            model.get_attr("call_string")["call_string"][0]
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 1
+        assert (
+            model.get_vertica_attributes("call_string")["call_string"][0]
             == "SELECT rf_classifier('public.decision_tc_model_test', 'public.dtc_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, TransPortation', ntree=1, mtry=4, sampling_size=1, max_depth=6, max_breadth=100, min_leaf_size=1, min_info_gain=0, nbins=40);"
         )
 
     def test_get_params(self, model):
         params = model.get_params()
 
         assert params == {
-            "n_estimators": 1,
             "max_features": 4,
             "max_leaf_nodes": 100,
-            "sample": 1.0,
             "max_depth": 6,
             "min_samples_leaf": 1,
             "min_info_gain": 0,
             "nbins": 40,
         }
 
     def test_prc_curve(self, model):
-        prc = model.prc_curve(pos_label="Car", nbins=1000)
+        prc = model.prc_curve(pos_label="Car", nbins=1000, show=False)
 
         assert prc["threshold"][300] == pytest.approx(0.299)
         assert prc["recall"][300] == pytest.approx(1.0)
         assert prc["precision"][300] == pytest.approx(1.0)
         assert prc["threshold"][800] == pytest.approx(0.799)
         assert prc["recall"][800] == pytest.approx(1.0)
         assert prc["precision"][800] == pytest.approx(1.0)
@@ -349,120 +343,124 @@
         assert dtc_data_copy["prob_train"].avg() == 0.3
         assert dtc_data_copy["prob_car"].avg() == 0.3
 
         model.predict_proba(dtc_data_copy, name="prob_bus_2", pos_label="Bus")
         assert dtc_data_copy["prob_bus_2"].avg() == 0.4
 
     def test_roc_curve(self, model):
-        roc = model.roc_curve(pos_label="Train", nbins=1000)
+        roc = model.roc_curve(pos_label="Train", nbins=1000, show=False)
 
         assert roc["threshold"][100] == pytest.approx(0.1)
         assert roc["false_positive"][100] == pytest.approx(0.0)
         assert roc["true_positive"][100] == pytest.approx(1.0)
         assert roc["threshold"][700] == pytest.approx(0.7)
         assert roc["false_positive"][700] == pytest.approx(0.0)
         assert roc["true_positive"][700] == pytest.approx(1.0)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000)
+        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000, show=False)
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
         assert cutoff_curve["false_positive"][100] == pytest.approx(0.0)
         assert cutoff_curve["true_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
         assert cutoff_curve["false_positive"][700] == pytest.approx(0.0)
         assert cutoff_curve["true_positive"][700] == pytest.approx(1.0)
         plt.close("all")
 
     def test_score(self, model):
-        assert model.score(cutoff=0.9, method="accuracy") == pytest.approx(1.0)
-        assert model.score(cutoff=0.1, method="accuracy") == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="accuracy") == pytest.approx(1.0)
+        assert model.score(cutoff=0.1, metric="accuracy") == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="auc", pos_label="Train"
+            cutoff=0.9, metric="auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="auc", pos_label="Train"
+            cutoff=0.1, metric="auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="best_cutoff", pos_label="Train"
+            cutoff=0.9, metric="best_cutoff", pos_label="Train"
         ) == pytest.approx(0.999)
         assert model.score(
-            cutoff=0.1, method="best_cutoff", pos_label="Train"
+            cutoff=0.1, metric="best_cutoff", pos_label="Train"
         ) == pytest.approx(0.999)
-        assert model.score(cutoff=0.9, method="bm", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.9, metric="bm", pos_label="Train") == pytest.approx(
+            1.0
         )
-        assert model.score(cutoff=0.1, method="bm", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="bm", pos_label="Train") == pytest.approx(
+            1.0
         )
         assert model.score(
-            cutoff=0.9, method="csi", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="csi", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="csi", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="f1", pos_label="Train") == pytest.approx(
-            0.0
+            cutoff=0.1, metric="csi", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="f1", pos_label="Train") == pytest.approx(
+            1.0
         )
-        assert model.score(cutoff=0.1, method="f1", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="f1", pos_label="Train") == pytest.approx(
+            1.0
         )
         assert model.score(
-            cutoff=0.9, method="logloss", pos_label="Train"
+            cutoff=0.9, metric="logloss", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.1, method="logloss", pos_label="Train"
+            cutoff=0.1, metric="logloss", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.9, method="mcc", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="mcc", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="mcc", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="mk", pos_label="Train") == pytest.approx(
-            0.0
+            cutoff=0.1, metric="mcc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="mk", pos_label="Train") == pytest.approx(
+            1.0
         )
-        assert model.score(cutoff=0.1, method="mk", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="mk", pos_label="Train") == pytest.approx(
+            1.0
         )
         assert model.score(
-            cutoff=0.9, method="npv", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="npv", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="npv", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.1, metric="npv", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="prc_auc", pos_label="Train"
+            cutoff=0.9, metric="prc_auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="prc_auc", pos_label="Train"
+            cutoff=0.1, metric="prc_auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="precision", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="precision", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="precision", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.1, metric="precision", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="specificity", pos_label="Train"
+            cutoff=0.9, metric="specificity", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="specificity", pos_label="Train"
+            cutoff=0.1, metric="specificity", pos_label="Train"
         ) == pytest.approx(1.0)
 
     def test_set_params(self, model):
         model.set_params({"nbins": 1000})
 
         assert model.get_params()["nbins"] == 1000
 
     def test_model_from_vDF(self, dtc_data_vd):
         current_cursor().execute("DROP MODEL IF EXISTS tc_from_vDF")
-        model_test = DecisionTreeClassifier("tc_from_vDF",)
+        model_test = DecisionTreeClassifier(
+            "tc_from_vDF",
+        )
         model_test.fit(
-            dtc_data_vd, ["Gender", '"owned cars"', "cost", "income"], "TransPortation"
+            dtc_data_vd,
+            ["Gender", '"owned cars"', "cost", "income"],
+            "TransPortation",
         )
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'tc_from_vDF'"
         )
         assert current_cursor().fetchone()[0] == "tc_from_vDF"
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_decision_tree_regressor.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_elastic_net.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,227 +1,247 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
+# Standard Python Modules
+import warnings
+
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-from verticapy import (
-    vDataFrame,
-    drop,
-    set_option,
-)
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_titanic, load_dataset_reg
-from verticapy.learn.tree import DecisionTreeRegressor
+from verticapy.tests.conftest import get_version
+from verticapy import drop, set_option
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_winequality
+from verticapy.learn.linear_model import ElasticNet
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
-def tr_data_vd():
-    tr_data = load_dataset_reg(table_name="tr_data", schema="public")
-    yield tr_data
-    drop(name="public.tr_data", method="table")
+def winequality_vd():
+    winequality = load_winequality()
+    yield winequality
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
-def model(tr_data_vd):
-    current_cursor().execute("DROP MODEL IF EXISTS tr_model_test")
-
-    current_cursor().execute(
-        "SELECT rf_regressor('tr_model_test', 'public.tr_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, transportation', mtry=4, ntree=1, max_breadth=100, sampling_size=1, max_depth=6, min_leaf_size=1, min_info_gain=0.0, nbins=40, seed=1, id_column='id')"
+def model(winequality_vd):
+    model_class = ElasticNet(
+        "elasticnet_model_test",
     )
-
-    # I could use load_model but it is buggy
-    model_class = DecisionTreeRegressor(
-        "tr_model_test",
-        max_features=4,
-        max_leaf_nodes=100,
-        max_depth=6,
-        min_samples_leaf=1,
-        min_info_gain=0.0,
-        nbins=40,
+    model_class.drop()
+    model_class.fit(
+        "public.winequality",
+        ["total_sulfur_dioxide", "residual_sugar", "alcohol"],
+        "quality",
     )
-    model_class.input_relation = "public.tr_data"
-    model_class.test_relation = model_class.input_relation
-    model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
-    model_class.y = '"TransPortation"'
-
     yield model_class
     model_class.drop()
 
 
-@pytest.fixture(scope="module")
-def titanic_vd():
-    titanic = load_titanic()
-    yield titanic
-    drop(name="public.titanic",)
-
-
-class TestDecisionTreeRegressor:
+class TestElasticNet:
     def test_repr(self, model):
-        assert "SELECT rf_regressor('public.tr_model_test'," in model.__repr__()
-        model_repr = DecisionTreeRegressor("RF_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<RandomForestRegressor>"
+        assert model.__repr__() == "<LinearRegression>"
 
-    def test_contour(self, titanic_vd):
-        model_test = DecisionTreeRegressor("model_contour",)
+    def test_contour(self, winequality_vd):
+        model_test = ElasticNet(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
-        )
-        result = model_test.contour()
-        assert len(result.get_default_bbox_extra_artists()) == 34
+            winequality_vd,
+            ["residual_sugar", "alcohol"],
+            "quality",
+        )
+        with warnings.catch_warnings(record=True) as w:
+            result = model_test.contour()
+        assert len(result.get_default_bbox_extra_artists()) == 10
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_RF_REGRESSOR("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'tr_model_test\', match_by_pos = \'true\')'
+        expected_sql = 'PREDICT_LINEAR_REG("total_sulfur_dioxide", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'elasticnet_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS tr_model_test_drop")
-        model_test = DecisionTreeRegressor("tr_model_test_drop",)
-        model_test.fit(
-            "public.tr_data",
-            ["Gender", '"owned cars"', "cost", "income"],
-            "TransPortation",
+        current_cursor().execute("DROP MODEL IF EXISTS enet_model_test_drop")
+        model_test = ElasticNet(
+            "enet_model_test_drop",
         )
+        model_test.fit("public.winequality", ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'enet_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "tr_model_test_drop"
+        assert current_cursor().fetchone()[0] == "enet_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'enet_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
-        assert fim["index"] == ["cost", "owned cars", "gender", "income"]
-        assert fim["importance"] == [88.41, 7.25, 4.35, 0.0]
-        assert fim["sign"] == [1, 1, 1, 0]
+        assert fim["index"] == ["total_sulfur_dioxide", "residual_sugar", "alcohol"]
+        assert fim["importance"] == [100, 0, 0]
+        assert fim["sign"] == [-1, 0, 0]
         plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
-            "tree_count",
+            "details",
+            "regularization",
+            "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
-            "details",
         ]
         assert m_att["attr_fields"] == [
-            "tree_count",
+            "predictor, coefficient, std_err, t_value, p_value",
+            "type, lambda",
+            "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
-            "predictor, type",
         ]
-        assert m_att["#_of_rows"] == [1, 1, 1, 1, 4]
+        assert m_att["#_of_rows"] == [4, 1, 1, 1, 1, 1]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
-        assert m_att_details["predictor"] == ["gender", "owned cars", "cost", "income"]
-        assert m_att_details["type"] == [
-            "char or varchar",
-            "int",
-            "char or varchar",
-            "char or varchar",
+        assert m_att_details["predictor"] == [
+            "Intercept",
+            "total_sulfur_dioxide",
+            "residual_sugar",
+            "alcohol",
         ]
+        assert m_att_details["coefficient"][0] == pytest.approx(5.874258, abs=1e-6)
+        assert m_att_details["coefficient"][1] == pytest.approx(-0.000482, abs=1e-6)
+        assert m_att_details["coefficient"][2] == pytest.approx(0, abs=1e-6)
+        assert m_att_details["coefficient"][3] == pytest.approx(0, abs=1e-6)
+        assert m_att_details["std_err"][1] == pytest.approx(0.000221, abs=1e-6)
+        assert m_att_details["t_value"][1] == pytest.approx(-2.176118, abs=1e-6)
+        assert m_att_details["p_value"][1] == pytest.approx(0.029582, abs=1e-6)
 
-        assert model.get_attr("tree_count")["tree_count"][0] == 1
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
+        m_att_regularization = model.get_vertica_attributes("regularization")
+
+        assert m_att_regularization["type"][0] == "enet"
+        assert m_att_regularization["lambda"][0] == 1
+
+        assert (
+            model.get_vertica_attributes("iteration_count")["iteration_count"][0] == 1
+        )
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "SELECT rf_regressor('public.tr_model_test', 'public.tr_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, transportation', ntree=1, mtry=4, sampling_size=1, max_depth=6, max_breadth=100, min_leaf_size=1, min_info_gain=0, nbins=40);"
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
         )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 6497
+        )
+
+        if get_version()[0] < 12:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.elasticnet_model_test', 'public.winequality', '\"quality\"', '\"total_sulfur_dioxide\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='cgd', epsilon=1e-06, max_iterations=100, regularization='enet', lambda=1, alpha=0.5)"
+            )
+        else:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.elasticnet_model_test', 'public.winequality', '\"quality\"', '\"total_sulfur_dioxide\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='cgd', epsilon=1e-06, max_iterations=100, regularization='enet', lambda=1, alpha=0.5, fit_intercept=true)"
+            )
 
     def test_get_params(self, model):
         assert model.get_params() == {
-            "n_estimators": 1,
-            "max_features": 4,
-            "max_leaf_nodes": 100,
-            "sample": 1.0,
-            "max_depth": 6,
-            "min_samples_leaf": 1,
-            "min_info_gain": 0,
-            "nbins": 40,
+            "solver": "cgd",
+            "max_iter": 100,
+            "l1_ratio": 0.5,
+            "C": 1,
+            "tol": 1e-6,
+            "fit_intercept": True,
         }
 
+    def test_get_plot(self, winequality_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
+        model_test = ElasticNet(
+            "model_test_plot",
+        )
+        model_test.fit(winequality_vd, ["alcohol"], "quality")
+        result = model_test.plot()
+        assert len(result.get_default_bbox_extra_artists()) == 9
+        plt.close("all")
+        model_test.drop()
+
     def test_to_python(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_RF_REGRESSOR('Male', 0, 'Cheap', 'Low' USING PARAMETERS model_name = '{}', match_by_pos=True)::float".format(
-                model.name
+            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([["Male", 0, "Cheap", "Low"]])[0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0])
 
     def test_to_sql(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_RF_REGRESSOR(* USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float FROM (SELECT 'Male' AS \"Gender\", 0 AS \"owned cars\", 'Cheap' AS \"cost\", 'Low' AS \"income\") x".format(
-                model.name, model.to_sql()
+            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
+                model.model_name, model.to_sql([3.0, 11.0, 93.0])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
-    def test_to_memmodel(self, model):
+    def test_to_memmodel(self, model, winequality_vd):
         mmodel = model.to_memmodel()
-        res = mmodel.predict(
-            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
-        )
-        res_py = model.to_python()(
-            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
-        )
+        res = mmodel.predict([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
+        res_py = model.to_python()([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
-        vdf = vDataFrame("public.tr_data")
+        vdf = winequality_vd.copy()
         vdf["prediction_sql"] = mmodel.predict_sql(
-            ['"Gender"', '"owned cars"', '"cost"', '"income"']
+            ["total_sulfur_dioxide", "residual_sugar", "alcohol"]
         )
         model.predict(vdf, name="prediction_vertica_sql")
         score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
         assert score == pytest.approx(1.0)
 
-    def test_get_predicts(self, tr_data_vd, model):
-        tr_data_copy = tr_data_vd.copy()
+    def test_get_predicts(self, winequality_vd, model):
+        winequality_copy = winequality_vd.copy()
         model.predict(
-            tr_data_copy,
-            X=["Gender", '"owned cars"', "cost", "income"],
+            winequality_copy,
+            X=["total_sulfur_dioxide", "residual_sugar", "alcohol"],
             name="predicted_quality",
         )
 
-        assert tr_data_copy["predicted_quality"].mean() == pytest.approx(0.9, abs=1e-6)
+        assert winequality_copy["predicted_quality"].mean() == pytest.approx(
+            5.818377, abs=1e-6
+        )
 
     def test_regression_report(self, model):
         reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
             "max_error",
@@ -230,112 +250,86 @@
             "mean_squared_error",
             "root_mean_squared_error",
             "r2",
             "r2_adj",
             "aic",
             "bic",
         ]
-        assert reg_rep["value"][0] == pytest.approx(1.0, abs=1e-6)
-        assert reg_rep["value"][1] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][2] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][3] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][4] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][5] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][6] == pytest.approx(1.0, abs=1e-6)
-        assert reg_rep["value"][7] == pytest.approx(1.0, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-float("inf"), abs=1e-6)
-        assert reg_rep["value"][9] == pytest.approx(-float("inf"), abs=1e-6)
+        assert reg_rep["value"][0] == pytest.approx(0.001610, abs=1e-6)
+        assert reg_rep["value"][1] == pytest.approx(3.192849, abs=1e-6)
+        assert reg_rep["value"][2] == pytest.approx(0.788321, abs=1e-6)
+        assert reg_rep["value"][3] == pytest.approx(0.684299, abs=1e-6)
+        assert reg_rep["value"][4] == pytest.approx(0.761229, abs=1e-6)
+        assert reg_rep["value"][5] == pytest.approx(0.872484, abs=1e-6)
+        assert reg_rep["value"][6] == pytest.approx(0.001610, abs=1e-6)
+        assert reg_rep["value"][7] == pytest.approx(0.001148, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-1764.50012797112, abs=1e-6)
+        assert reg_rep["value"][9] == pytest.approx(-1737.394835300611, abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
-            10.0,
-            4,
-            pytest.approx(1.0),
-            pytest.approx(1.0),
-            float("inf"),
-            pytest.approx(0.0),
-            pytest.approx(-1.73372940858763),
-            pytest.approx(0.223450528977454),
-            pytest.approx(3.76564442746721),
+            6497.0,
+            3,
+            pytest.approx(0.00161000676361089),
+            pytest.approx(0.001148714605947454),
+            pytest.approx(2.116870802238774),
+            pytest.approx(0.0958537016435304),
+            pytest.approx(0.232322269343305),
+            pytest.approx(0.189622693372695),
+            pytest.approx(53.1115447611131),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
-            pytest.approx(6.9),
-            pytest.approx(0.0),
-            pytest.approx(6.9),
+            pytest.approx(4.83725377631033),
+            pytest.approx(4945.71023360928),
+            pytest.approx(4953.68570109281),
+        ]
+        assert reg_rep_anova["MS"][:-1] == [
+            pytest.approx(1.6124179254367768),
+            pytest.approx(0.7616987884813307),
         ]
-        assert reg_rep_anova["MS"][:-1] == [pytest.approx(1.725), pytest.approx(0.0)]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(3.192849, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0.684299, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="median") == pytest.approx(0.788321, abs=1e-6)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.761229, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.8724848619460168, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="msle") == pytest.approx(0.003171, abs=1e-6)
         # method = "r2"
-        assert model.score() == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="r2") == pytest.approx(0.001610, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="r2a") == pytest.approx(
+            0.001148714605947454, abs=1e-6
+        )
         # method = "var"
-        assert model.score(method="var") == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(0.001610, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-float("inf"), abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-1764.50012797112, abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-float("inf"), abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-1737.394835300611, abs=1e-6)
 
     def test_set_params(self, model):
-        model.set_params({"max_features": 1000})
+        model.set_params({"max_iter": 1000})
 
-        assert model.get_params()["max_features"] == 1000
+        assert model.get_params()["max_iter"] == 1000
 
-    def test_model_from_vDF(self, tr_data_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS tr_from_vDF")
-        model_test = DecisionTreeRegressor("tr_from_vDF",)
-        model_test.fit(tr_data_vd, ["gender"], "transportation")
+    def test_model_from_vDF(self, winequality_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS enet_from_vDF")
+        model_test = ElasticNet(
+            "enet_from_vDF",
+        )
+        model_test.fit(winequality_vd, ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'tr_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'enet_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "tr_from_vDF"
+        assert current_cursor().fetchone()[0] == "enet_from_vDF"
 
         model_test.drop()
-
-    def test_to_graphviz(self, model):
-        gvz_tree_0 = model.to_graphviz(
-            tree_id=0,
-            classes_color=["red", "blue", "green"],
-            round_pred=4,
-            percent=True,
-            vertical=False,
-            node_style={"shape": "box", "style": "filled"},
-            arrow_style={"color": "blue"},
-            leaf_style={"shape": "circle", "style": "filled"},
-        )
-        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_0
-        assert "0 -> 1" in gvz_tree_0
-
-    def test_get_tree(self, model):
-        tree_0 = model.get_tree()
-
-        assert tree_0["prediction"] == [
-            None,
-            "2.000000",
-            None,
-            None,
-            "1.000000",
-            None,
-            "0.000000",
-            "0.000000",
-            "1.000000",
-        ]
-
-    def test_plot_tree(self, model):
-        result = model.plot_tree()
-        assert model.to_graphviz() == result.source.strip()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_delphi.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_delphi.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
@@ -45,47 +49,58 @@
     winequality = load_winequality()
     yield winequality
     drop(name="public.winequality")
 
 
 class TestDelphi:
     def test_AutoML(self, titanic_vd):
-        model = AutoML("AutoML_test_ml",)
+        model = AutoML(
+            "AutoML_test_ml",
+        )
         model.drop()
         model.fit(titanic_vd, y="survived")
         assert model.model_grid_["avg_score"][0] < 0.1
         assert len(model.plot().get_default_bbox_extra_artists()) < 30
         plt.close("all")
         assert len(model.plot("stepwise").get_default_bbox_extra_artists()) < 200
         plt.close("all")
         model.drop()
 
     def test_AutoDataPrep(self, titanic_vd, amazon_vd):
-        model = AutoDataPrep("AutoML_test_dp",)
+        model = AutoDataPrep(
+            "AutoML_test_dp",
+        )
         model.drop()
         model.fit(titanic_vd)
         assert model.final_relation_.shape() == (1234, 56)
         model.drop()
-        model2 = AutoDataPrep("AutoML_test_dp", num_method="same_freq")
+        model2 = AutoDataPrep("AutoML_test_dp_2", num_method="same_freq")
         model2.drop()
         model2.fit(titanic_vd)
         assert model2.final_relation_.shape() == (1234, 101)
         model2.drop()
         model3 = AutoDataPrep(
-            "AutoML_test_dp", num_method="same_width", na_method="drop", apply_pca=True
+            "AutoML_test_dp_3",
+            num_method="same_width",
+            na_method="drop",
+            apply_pca=True,
         )
         model3.drop()
         model3.fit(titanic_vd)
         assert model3.final_relation_.shape() == (112, 122)
         model3.drop()
-        model4 = AutoDataPrep("AutoML_test_dp",)
+        model4 = AutoDataPrep(
+            "AutoML_test_dp_4",
+        )
         model4.drop()
         model4.fit(amazon_vd)
         assert model4.final_relation_.shape() == (6318, 3)
         model4.drop()
 
     def test_AutoClustering(self, titanic_vd):
-        model = AutoClustering("AutoML_test_cluster",)
+        model = AutoClustering(
+            "AutoML_test_cluster",
+        )
         model.drop()
         model.fit(titanic_vd)
         assert model.model_.parameters["n_cluster"] < 100
         model.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_dummy_tree_classifier.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_linear_svc.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,495 +1,415 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-from verticapy import (
-    vDataFrame,
-    drop,
-    set_option,
-)
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_titanic, load_dataset_cl
-from verticapy.learn.tree import DummyTreeClassifier
+from verticapy import drop, set_option
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_titanic, load_winequality
+from verticapy.learn.svm import LinearSVC
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
-def dtc_data_vd():
-    dtc_data = load_dataset_cl(table_name="dtc_data", schema="public")
-    yield dtc_data
-    drop(name="public.dtc_data", method="table")
-
-
-@pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
-def model(dtc_data_vd):
-    current_cursor().execute("DROP MODEL IF EXISTS decision_tc_model_test")
-
-    current_cursor().execute(
-        "SELECT rf_classifier('decision_tc_model_test', 'public.dtc_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, TransPortation', mtry=4, ntree=1, max_breadth=1e9, sampling_size=1, max_depth=100, nbins=1000, seed=1, id_column='id')"
+def winequality_vd():
+    winequality = load_winequality()
+    yield winequality
+    drop(
+        name="public.winequality",
     )
 
-    # I could use load_model but it is buggy
-    model_class = DummyTreeClassifier("decision_tc_model_test",)
-    model_class.input_relation = "public.dtc_data"
-    model_class.test_relation = model_class.input_relation
-    model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
-    model_class.y = "TransPortation"
-    current_cursor().execute(
-        "SELECT DISTINCT {} FROM {} WHERE {} IS NOT NULL ORDER BY 1".format(
-            model_class.y, model_class.input_relation, model_class.y
-        )
-    )
-    classes = current_cursor().fetchall()
-    model_class.classes_ = [item[0] for item in classes]
 
+@pytest.fixture(scope="module")
+def model(titanic_vd):
+    current_cursor().execute("DROP MODEL IF EXISTS lsvc_model_test")
+    model_class = LinearSVC(
+        "lsvc_model_test",
+    )
+    model_class.fit("public.titanic", ["age", "fare"], "survived")
     yield model_class
     model_class.drop()
 
 
-class TestDummyTreeClassifier:
+class TestLinearSVC:
     def test_repr(self, model):
-        assert (
-            "SELECT rf_classifier('public.decision_tc_model_test'," in model.__repr__()
-        )
-        model_repr = DummyTreeClassifier("RF_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<RandomForestClassifier>"
+        assert model.__repr__() == "<LinearSVC>"
 
     def test_classification_report(self, model):
         cls_rep1 = model.classification_report().transpose()
 
-        assert cls_rep1["auc"][0] == pytest.approx(1.0)
-        assert cls_rep1["prc_auc"][0] == pytest.approx(1.0)
-        assert cls_rep1["accuracy"][0] == pytest.approx(1.0)
-        assert cls_rep1["log_loss"][0] == pytest.approx(0.0)
-        assert cls_rep1["precision"][0] == pytest.approx(1.0)
-        assert cls_rep1["recall"][0] == pytest.approx(1.0)
-        assert cls_rep1["f1_score"][0] == pytest.approx(1.0)
-        assert cls_rep1["mcc"][0] == pytest.approx(1.0)
-        assert cls_rep1["informedness"][0] == pytest.approx(1.0)
-        assert cls_rep1["markedness"][0] == pytest.approx(1.0)
-        assert cls_rep1["csi"][0] == pytest.approx(1.0)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.999)
-
-        cls_rep2 = model.classification_report(cutoff=0.2).transpose()
-
-        assert cls_rep2["cutoff"][0] == pytest.approx(0.2)
+        assert cls_rep1["auc"][0] == pytest.approx(0.6933968844454788, 1e-2)
+        assert cls_rep1["prc_auc"][0] == pytest.approx(0.5976470350144453, 1e-2)
+        assert cls_rep1["accuracy"][0] == pytest.approx(0.6536144578313253, 1e-2)
+        assert cls_rep1["log_loss"][0] == pytest.approx(0.279724470067258, 1e-2)
+        assert cls_rep1["precision"][0] == pytest.approx(0.6916666666666667, 1e-2)
+        assert cls_rep1["recall"][0] == pytest.approx(0.21227621483375958, 1e-2)
+        assert cls_rep1["f1_score"][0] == pytest.approx(0.324853228962818, 1e-2)
+        assert cls_rep1["mcc"][0] == pytest.approx(0.22669555629341528, 1e-2)
+        assert cls_rep1["informedness"][0] == pytest.approx(0.151119190040371, 1e-2)
+        assert cls_rep1["markedness"][0] == pytest.approx(0.34006849315068477, 1e-2)
+        assert cls_rep1["csi"][0] == pytest.approx(0.1939252336448598, 1e-2)
 
     def test_confusion_matrix(self, model):
         conf_mat1 = model.confusion_matrix()
 
-        assert conf_mat1["Bus"] == [4, 0, 0]
-        assert conf_mat1["Car"] == [0, 3, 0]
-        assert conf_mat1["Train"] == [0, 0, 3]
+        assert conf_mat1[0][0] == 568
+        assert conf_mat1[1][0] == 308
+        assert conf_mat1[0][1] == 37
+        assert conf_mat1[1][1] == 83
 
         conf_mat2 = model.confusion_matrix(cutoff=0.2)
 
-        assert conf_mat2["Bus"] == [4, 0, 0]
-        assert conf_mat2["Car"] == [0, 3, 0]
-        assert conf_mat2["Train"] == [0, 0, 3]
+        assert conf_mat2[0][0] == 0
+        assert conf_mat2[1][0] == 0
+        assert conf_mat2[0][1] == 605
+        assert conf_mat2[1][1] == 391
 
     def test_contour(self, titanic_vd):
-        model_test = DummyTreeClassifier("model_contour",)
+        model_test = LinearSVC(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 34
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_RF_CLASSIFIER("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'decision_tc_model_test\', match_by_pos = \'true\')'
+        expected_sql = "PREDICT_SVM_CLASSIFIER(\"age\", \"fare\" USING PARAMETERS model_name = 'lsvc_model_test', type = 'probability', match_by_pos = 'true')"
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS decision_tc_model_test_drop")
-        model_test = DummyTreeClassifier("decision_tc_model_test_drop",)
-        model_test.fit(
-            "public.dtc_data",
-            ["Gender", '"owned cars"', "cost", "income"],
-            "TransPortation",
+        current_cursor().execute("DROP MODEL IF EXISTS lsvc_model_test_drop")
+        model_test = LinearSVC(
+            "lsvc_model_test_drop",
         )
+        model_test.fit("public.titanic", ["age", "fare"], "survived")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'decision_tc_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'lsvc_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "decision_tc_model_test_drop"
+        assert current_cursor().fetchone()[0] == "lsvc_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'decision_tc_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'lsvc_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        f_imp = model.features_importance()
+        f_imp = model.features_importance(show=False)
 
-        assert f_imp["index"] == ["cost", "owned cars", "gender", "income"]
-        assert f_imp["importance"] == [75.76, 15.15, 9.09, 0.0]
-        assert f_imp["sign"] == [1, 1, 1, 0]
+        assert f_imp["index"] == ["fare", "age"]
+        assert f_imp["importance"] == [85.09, 14.91]
+        assert f_imp["sign"] == [1, -1]
         plt.close("all")
 
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000)
+        lift_ch = model.lift_chart(nbins=1000, show=False)
 
-        assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
-        assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(1.0)
-        assert lift_ch["lift"][300] == pytest.approx(2.5)
+        assert lift_ch["decision_boundary"][10] == pytest.approx(0.01)
+        assert lift_ch["positive_prediction_ratio"][10] == pytest.approx(0.0)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(1.0)
-        assert lift_ch["lift"][900] == pytest.approx(2.5)
+        assert lift_ch["lift"][900] == pytest.approx(1.0)
         plt.close("all")
 
-    def test_to_python(self, model, titanic_vd):
-        model_test = DummyTreeClassifier("rfc_python_test")
+    def test_get_plot(self, winequality_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
+        model_test = LinearSVC(
+            "model_test_plot",
+        )
+        model_test.fit(winequality_vd, ["alcohol"], "good")
+        result = model_test.plot(color="r")
+        assert len(result.get_default_bbox_extra_artists()) == 11
+        plt.close("all")
         model_test.drop()
-        model_test.fit(titanic_vd, ["age", "fare", "sex"], "embarked")
-        current_cursor().execute(
-            "SELECT PREDICT_RF_CLASSIFIER(30.0, 45.0, 'male' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
-        )
-        prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 45.0, "male"]])[0]
+        model_test.fit(winequality_vd, ["alcohol", "residual_sugar"], "good")
+        result = model_test.plot(color="r")
+        assert len(result.get_default_bbox_extra_artists()) == 11
+        plt.close("all")
+        model_test.drop()
+        model_test.fit(
+            winequality_vd, ["alcohol", "residual_sugar", "fixed_acidity"], "good"
         )
+        result = model_test.plot(color="r")
+        assert len(result.get_default_bbox_extra_artists()) == 5
+        plt.close("all")
+        model_test.drop()
+
+    def test_to_python(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_RF_CLASSIFIER(30.0, 145.0, 'female' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
+            "SELECT PREDICT_SVM_CLASSIFIER(3.0, 11.0 USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
+                model.model_name
+            )
         )
         prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 145.0, "female"]])[0]
-        )
-
-    def test_to_sql(self, model, titanic_vd):
-        model_test = DummyTreeClassifier("rfc_sql_test")
-        model_test.drop()
-        model_test.fit(titanic_vd, ["age", "fare", "sex"], "survived")
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0]])[0])
         current_cursor().execute(
-            "SELECT PREDICT_RF_CLASSIFIER(* USING PARAMETERS model_name = 'rfc_sql_test', match_by_pos=True)::int, {}::int FROM (SELECT 30.0 AS age, 45.0 AS fare, 'male' AS sex) x".format(
-                model_test.to_sql()
+            "SELECT PREDICT_SVM_CLASSIFIER(3.0, 11.0 USING PARAMETERS model_name = '{}', type='probability', match_by_pos=True)".format(
+                model.model_name
             )
         )
-        prediction = current_cursor().fetchone()
-        assert prediction[0] == pytest.approx(prediction[1])
-        model_test.drop()
+        prediction = current_cursor().fetchone()[0]
+        assert prediction == pytest.approx(
+            model.to_python(
+                return_proba=True,
+            )([[3.0, 11.0]])[
+                0
+            ][1]
+        )
 
-    def test_to_memmodel(self, model):
+    def test_to_memmodel(self, model, titanic_vd):
         mmodel = model.to_memmodel()
-        res = mmodel.predict(
-            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
-        )
-        res_py = model.to_python()(
-            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
-        )
+        res = mmodel.predict([[3.0, 11.0], [11.0, 1.0]])
+        res_py = model.to_python()([[3.0, 11.0], [11.0, 1.0]])
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
-        res = mmodel.predict_proba(
-            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
-        )
-        res_py = model.to_python(return_proba=True)(
-            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
-        )
+        res = mmodel.predict_proba([[3.0, 11.0], [11.0, 1.0]])
+        res_py = model.to_python(return_proba=True)([[3.0, 11.0], [11.0, 1.0]])
         assert res[0][0] == res_py[0][0]
         assert res[0][1] == res_py[0][1]
-        assert res[0][2] == res_py[0][2]
         assert res[1][0] == res_py[1][0]
         assert res[1][1] == res_py[1][1]
-        assert res[1][2] == res_py[1][2]
-        vdf = vDataFrame("public.dtc_data")
-        vdf["prediction_sql"] = mmodel.predict_sql(
-            ['"Gender"', '"owned cars"', '"cost"', '"income"']
-        )
-        vdf["prediction_proba_sql_0"] = mmodel.predict_proba_sql(
-            ['"Gender"', '"owned cars"', '"cost"', '"income"']
-        )[0]
-        vdf["prediction_proba_sql_1"] = mmodel.predict_proba_sql(
-            ['"Gender"', '"owned cars"', '"cost"', '"income"']
-        )[1]
-        vdf["prediction_proba_sql_2"] = mmodel.predict_proba_sql(
-            ['"Gender"', '"owned cars"', '"cost"', '"income"']
-        )[2]
-        model.predict(vdf, name="prediction_vertica_sql")
-        model.predict_proba(
-            vdf, name="prediction_proba_vertica_sql_0", pos_label=model.classes_[0]
-        )
-        model.predict_proba(
-            vdf, name="prediction_proba_vertica_sql_1", pos_label=model.classes_[1]
-        )
-        model.predict_proba(
-            vdf, name="prediction_proba_vertica_sql_2", pos_label=model.classes_[2]
-        )
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        vdf = titanic_vd.copy()
+        vdf["prediction_sql"] = mmodel.predict_sql(["age", "fare"])
+        vdf["prediction_proba_sql_0"] = mmodel.predict_proba_sql(["age", "fare"])[0]
+        vdf["prediction_proba_sql_1"] = mmodel.predict_proba_sql(["age", "fare"])[1]
+        model.predict(vdf, name="prediction_vertica_sql", cutoff=0.5)
+        model.predict_proba(vdf, pos_label=0, name="prediction_proba_vertica_sql_0")
+        model.predict_proba(vdf, pos_label=1, name="prediction_proba_vertica_sql_1")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
-        score = vdf.score(
-            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", "r2"
+
+    def test_to_sql(self, model):
+        current_cursor().execute(
+            "SELECT PREDICT_SVM_CLASSIFIER(3.0, 11.0 USING PARAMETERS model_name = '{}', match_by_pos=True), {}".format(
+                model.model_name, model.to_sql([3.0, 11.0])
+            )
         )
-        assert score == pytest.approx(1.0)
+        prediction = current_cursor().fetchone()
+        assert prediction[0] == pytest.approx(prediction[1])
 
-    def test_get_attr(self, model):
-        attr = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        attr = model.get_vertica_attributes()
         assert attr["attr_name"] == [
-            "tree_count",
-            "rejected_row_count",
+            "details",
             "accepted_row_count",
+            "rejected_row_count",
+            "iteration_count",
             "call_string",
-            "details",
         ]
         assert attr["attr_fields"] == [
-            "tree_count",
-            "rejected_row_count",
+            "predictor, coefficient",
             "accepted_row_count",
+            "rejected_row_count",
+            "iteration_count",
             "call_string",
-            "predictor, type",
         ]
-        assert attr["#_of_rows"] == [1, 1, 1, 1, 4]
+        assert attr["#_of_rows"] == [3, 1, 1, 1, 1]
 
-        details = model.get_attr("details")
-        assert details["predictor"] == ["gender", "owned cars", "cost", "income"]
-        assert details["type"] == [
-            "char or varchar",
-            "int",
-            "char or varchar",
-            "char or varchar",
-        ]
+        details = model.get_vertica_attributes("details")
+        assert details["predictor"] == ["Intercept", "age", "fare"]
+        assert details["coefficient"][0] == pytest.approx(-0.226679636751873)
+        assert details["coefficient"][1] == pytest.approx(-0.00661256493751514)
+        assert details["coefficient"][2] == pytest.approx(0.00587052591948468)
 
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("tree_count")["tree_count"][0] == 1
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "SELECT rf_classifier('public.decision_tc_model_test', 'public.dtc_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, TransPortation', ntree=1, mtry=4, sampling_size=1, max_depth=100, max_breadth=1000000000, min_leaf_size=1, min_info_gain=0, nbins=1000);"
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 996
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 238
+        )
+        assert (
+            model.get_vertica_attributes("iteration_count")["iteration_count"][0] == 6
+        )
+        assert (
+            model.get_vertica_attributes("call_string")["call_string"][0]
+            == "SELECT svm_classifier('public.lsvc_model_test', 'public.titanic', '\"survived\"', '\"age\", \"fare\"'\nUSING PARAMETERS class_weights='1,1', C=1, max_iterations=100, intercept_mode='regularized', intercept_scaling=1, epsilon=0.0001);"
         )
 
     def test_get_params(self, model):
         params = model.get_params()
 
         assert params == {
-            "n_estimators": 1,
-            "max_features": "max",
-            "max_leaf_nodes": 1e9,
-            "sample": 1.0,
-            "max_depth": 100,
-            "min_samples_leaf": 1,
-            "min_info_gain": 0,
-            "nbins": 1000,
+            "tol": 0.0001,
+            "C": 1.0,
+            "max_iter": 100,
+            "intercept_scaling": 1.0,
+            "intercept_mode": "regularized",
+            "class_weight": [1, 1],
         }
 
     def test_prc_curve(self, model):
-        prc = model.prc_curve(pos_label="Car", nbins=1000)
+        prc = model.prc_curve(nbins=1000, show=False)
 
-        assert prc["threshold"][300] == pytest.approx(0.299)
-        assert prc["recall"][300] == pytest.approx(1.0)
-        assert prc["precision"][300] == pytest.approx(1.0)
-        assert prc["threshold"][800] == pytest.approx(0.799)
-        assert prc["recall"][800] == pytest.approx(1.0)
-        assert prc["precision"][800] == pytest.approx(1.0)
+        assert prc["threshold"][10] == pytest.approx(0.009)
+        assert prc["recall"][10] == pytest.approx(1.0)
+        assert prc["precision"][10] == pytest.approx(0.392570281124498)
+        assert prc["threshold"][900] == pytest.approx(0.899)
+        assert prc["recall"][900] == pytest.approx(0.010230179028133)
+        assert prc["precision"][900] == pytest.approx(1.0)
         plt.close("all")
 
-    def test_predict(self, dtc_data_vd, model):
-        dtc_data_copy = dtc_data_vd.copy()
-
-        model.predict(dtc_data_copy, name="pred")
-        assert dtc_data_copy["pred"].mode() == "Bus"
+    def test_predict(self, titanic_vd, model):
+        titanic_copy = titanic_vd.copy()
 
-        model.predict(dtc_data_copy, name="pred1", cutoff=0.7)
-        assert dtc_data_copy["pred1"].mode() == "Bus"
+        model.predict(titanic_copy, name="pred_class1", cutoff=0.7)
+        assert titanic_copy["pred_class1"].sum() == 23.0
 
-        model.predict(dtc_data_copy, name="pred2", cutoff=0.3)
-        assert dtc_data_copy["pred2"].mode() == "Bus"
+        model.predict(titanic_copy, name="pred_class2", cutoff=0.3)
+        assert titanic_copy["pred_class2"].sum() == 996.0
 
-    def test_predict_proba(self, dtc_data_vd, model):
-        dtc_data_copy = dtc_data_vd.copy()
+    def test_predict_proba(self, titanic_vd, model):
+        titanic_copy = titanic_vd.copy()
 
-        model.predict_proba(dtc_data_copy, name="prob")
-        assert dtc_data_copy["prob_bus"].avg() == 0.4
-        assert dtc_data_copy["prob_train"].avg() == 0.3
-        assert dtc_data_copy["prob_car"].avg() == 0.3
-
-        model.predict_proba(dtc_data_copy, name="prob_bus_2", pos_label="Bus")
-        assert dtc_data_copy["prob_bus_2"].avg() == 0.4
+        model.predict_proba(titanic_copy, name="probability", pos_label=1)
+        assert titanic_copy["probability"].min() == pytest.approx(0.33841486903496)
 
     def test_roc_curve(self, model):
-        roc = model.roc_curve(pos_label="Train", nbins=1000)
+        roc = model.roc_curve(nbins=1000, show=False)
 
         assert roc["threshold"][100] == pytest.approx(0.1)
-        assert roc["false_positive"][100] == pytest.approx(0.0)
+        assert roc["false_positive"][100] == pytest.approx(1.0)
         assert roc["true_positive"][100] == pytest.approx(1.0)
         assert roc["threshold"][700] == pytest.approx(0.7)
-        assert roc["false_positive"][700] == pytest.approx(0.0)
-        assert roc["true_positive"][700] == pytest.approx(1.0)
+        assert roc["false_positive"][700] == pytest.approx(0.00661157024793388)
+        assert roc["true_positive"][700] == pytest.approx(0.0485933503836317)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000)
+        cutoff_curve = model.cutoff_curve(nbins=1000, show=False)
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
-        assert cutoff_curve["false_positive"][100] == pytest.approx(0.0)
+        assert cutoff_curve["false_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["true_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
-        assert cutoff_curve["false_positive"][700] == pytest.approx(0.0)
-        assert cutoff_curve["true_positive"][700] == pytest.approx(1.0)
+        assert cutoff_curve["false_positive"][700] == pytest.approx(0.00661157024793388)
+        assert cutoff_curve["true_positive"][700] == pytest.approx(0.0485933503836317)
         plt.close("all")
 
     def test_score(self, model):
-        assert model.score(cutoff=0.9, method="accuracy") == pytest.approx(1.0)
-        assert model.score(cutoff=0.1, method="accuracy") == pytest.approx(1.0)
-        assert model.score(
-            cutoff=0.9, method="auc", pos_label="Train"
-        ) == pytest.approx(1.0)
-        assert model.score(
-            cutoff=0.1, method="auc", pos_label="Train"
-        ) == pytest.approx(1.0)
-        assert model.score(
-            cutoff=0.9, method="best_cutoff", pos_label="Train"
-        ) == pytest.approx(0.999)
-        assert model.score(
-            cutoff=0.1, method="best_cutoff", pos_label="Train"
-        ) == pytest.approx(0.999)
-        assert model.score(cutoff=0.9, method="bm", pos_label="Train") == pytest.approx(
-            0.0
-        )
-        assert model.score(cutoff=0.1, method="bm", pos_label="Train") == pytest.approx(
-            0.0
-        )
-        assert model.score(
-            cutoff=0.9, method="csi", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.1, method="csi", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="f1", pos_label="Train") == pytest.approx(
-            0.0
-        )
-        assert model.score(cutoff=0.1, method="f1", pos_label="Train") == pytest.approx(
-            0.0
-        )
-        assert model.score(
-            cutoff=0.9, method="logloss", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.1, method="logloss", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.9, method="mcc", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.1, method="mcc", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="mk", pos_label="Train") == pytest.approx(
-            0.0
-        )
-        assert model.score(cutoff=0.1, method="mk", pos_label="Train") == pytest.approx(
-            0.0
-        )
-        assert model.score(
-            cutoff=0.9, method="npv", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.1, method="npv", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.9, method="prc_auc", pos_label="Train"
-        ) == pytest.approx(1.0)
-        assert model.score(
-            cutoff=0.1, method="prc_auc", pos_label="Train"
-        ) == pytest.approx(1.0)
-        assert model.score(
-            cutoff=0.9, method="precision", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.1, method="precision", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(
-            cutoff=0.9, method="specificity", pos_label="Train"
-        ) == pytest.approx(1.0)
-        assert model.score(
-            cutoff=0.1, method="specificity", pos_label="Train"
-        ) == pytest.approx(1.0)
+        assert model.score(cutoff=0.7, metric="accuracy") == pytest.approx(
+            0.6224899598393574
+        )
+        assert model.score(cutoff=0.3, metric="accuracy") == pytest.approx(
+            0.392570281124498
+        )
+        assert model.score(cutoff=0.7, metric="auc") == pytest.approx(
+            0.6933968844454788
+        )
+        assert model.score(cutoff=0.3, metric="auc") == pytest.approx(
+            0.6933968844454788
+        )
+        assert model.score(cutoff=0.7, metric="best_cutoff") == pytest.approx(0.431)
+        assert model.score(cutoff=0.3, metric="best_cutoff") == pytest.approx(0.431)
+        assert model.score(cutoff=0.7, metric="bm") == pytest.approx(
+            0.041981780135697866
+        )
+        assert model.score(cutoff=0.3, metric="bm") == pytest.approx(0.0)
+        assert model.score(cutoff=0.7, metric="csi") == pytest.approx(
+            0.04810126582278481
+        )
+        assert model.score(cutoff=0.3, metric="csi") == pytest.approx(0.392570281124498)
+        assert model.score(cutoff=0.7, metric="f1") == pytest.approx(
+            0.09178743961352658
+        )
+        assert model.score(cutoff=0.3, metric="f1") == pytest.approx(0.5638067772170151)
+        assert model.score(cutoff=0.7, metric="logloss") == pytest.approx(
+            0.279724470067258
+        )
+        assert model.score(cutoff=0.3, metric="logloss") == pytest.approx(
+            0.279724470067258
+        )
+        assert model.score(cutoff=0.7, metric="mcc") == pytest.approx(
+            0.13649180522208684
+        )
+        assert model.score(cutoff=0.3, metric="mcc") == pytest.approx(0.0)
+        assert model.score(cutoff=0.7, metric="mk") == pytest.approx(
+            0.44376424326377406
+        )
+        assert model.score(cutoff=0.3, metric="mk") == pytest.approx(-0.607429718875502)
+        assert model.score(cutoff=0.7, metric="npv") == pytest.approx(
+            0.6176772867420349
+        )
+        assert model.score(cutoff=0.3, metric="npv") == pytest.approx(0.0)
+        assert model.score(cutoff=0.7, metric="prc_auc") == pytest.approx(
+            0.5976470350144453
+        )
+        assert model.score(cutoff=0.3, metric="prc_auc") == pytest.approx(
+            0.5976470350144453
+        )
+        assert model.score(cutoff=0.7, metric="precision") == pytest.approx(
+            0.8260869565217391
+        )
+        assert model.score(cutoff=0.3, metric="precision") == pytest.approx(
+            0.392570281124498
+        )
+        assert model.score(cutoff=0.7, metric="specificity") == pytest.approx(
+            0.9933884297520661
+        )
+        assert model.score(cutoff=0.3, metric="specificity") == pytest.approx(0.0)
 
     def test_set_params(self, model):
-        model.set_params({"nbins": 100})
-        # Nothing will change as Dummy Trees have no parameters
-        assert model.get_params()["nbins"] == 100
-
-    def test_model_from_vDF(self, dtc_data_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS tc_from_vDF")
-        model_test = DummyTreeClassifier("tc_from_vDF",)
-        model_test.fit(
-            dtc_data_vd, ["Gender", '"owned cars"', "cost", "income"], "TransPortation"
+        model.set_params({"max_iter": 1000})
+
+        assert model.get_params()["max_iter"] == 1000
+
+    def test_model_from_vDF(self, titanic_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS lsvc_from_vDF")
+        model_test = LinearSVC(
+            "lsvc_from_vDF",
         )
+        model_test.fit(titanic_vd, ["age", "fare"], "survived")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'tc_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'lsvc_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "tc_from_vDF"
+        assert current_cursor().fetchone()[0] == "lsvc_from_vDF"
 
         model_test.drop()
-
-    def test_to_graphviz(self, model):
-        gvz_tree_0 = model.to_graphviz(
-            tree_id=0,
-            classes_color=["red", "blue", "green"],
-            round_pred=4,
-            percent=True,
-            vertical=False,
-            node_style={"shape": "box", "style": "filled"},
-            arrow_style={"color": "blue"},
-            leaf_style={"shape": "circle", "style": "filled"},
-        )
-        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_0
-        assert "0 -> 1" in gvz_tree_0
-
-    def test_get_tree(self, model):
-        tree_1 = model.get_tree()
-
-        assert tree_1["prediction"] == [
-            None,
-            "Car",
-            None,
-            None,
-            "Train",
-            None,
-            "Bus",
-            "Bus",
-            "Train",
-        ]
-
-    def test_plot_tree(self, model):
-        result = model.plot_tree()
-        assert model.to_graphviz() == result.source.strip()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_dummy_tree_regressor.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_ridge.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,219 +1,249 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-from verticapy import (
-    vDataFrame,
-    drop,
-    set_option,
-)
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_titanic, load_dataset_reg
-from verticapy.learn.tree import DummyTreeRegressor
+from verticapy.tests.conftest import get_version
+from verticapy import drop, set_option
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_winequality
+from verticapy.learn.linear_model import Ridge
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
-def tr_data_vd():
-    tr_data = load_dataset_reg(table_name="tr_data", schema="public")
-    yield tr_data
-    drop(name="public.tr_data", method="table")
+def winequality_vd():
+    winequality = load_winequality()
+    yield winequality
+    drop(name="public.winequality", method="table")
 
 
 @pytest.fixture(scope="module")
-def model(tr_data_vd):
-    current_cursor().execute("DROP MODEL IF EXISTS tr_model_test")
-
-    current_cursor().execute(
-        "SELECT rf_regressor('tr_model_test', 'public.tr_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, transportation', mtry=4, ntree=1, max_breadth=1e9, sampling_size=1, max_depth=100, min_leaf_size=1, min_info_gain=0.0, nbins=1000, seed=1, id_column='id')"
+def model(winequality_vd):
+    current_cursor().execute("DROP MODEL IF EXISTS ridge_model_test")
+    model_class = Ridge(
+        "ridge_model_test",
+    )
+    model_class.fit(
+        "public.winequality",
+        ["citric_acid", "residual_sugar", "alcohol"],
+        "quality",
     )
-
-    # I could use load_model but it is buggy
-    model_class = DummyTreeRegressor("tr_model_test",)
-    model_class.input_relation = "public.tr_data"
-    model_class.test_relation = model_class.input_relation
-    model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
-    model_class.y = '"TransPortation"'
-
     yield model_class
     model_class.drop()
 
 
-@pytest.fixture(scope="module")
-def titanic_vd():
-    titanic = load_titanic()
-    yield titanic
-    drop(name="public.titanic",)
-
-
-class TestDummyTreeRegressor:
+class TestRidge:
     def test_repr(self, model):
-        assert "SELECT rf_regressor('public.tr_model_test'," in model.__repr__()
-        model_repr = DummyTreeRegressor("RF_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<RandomForestRegressor>"
+        assert model.__repr__() == "<LinearRegression>"
 
-    def test_contour(self, titanic_vd):
-        model_test = DummyTreeRegressor("model_contour",)
+    def test_contour(self, winequality_vd):
+        model_test = Ridge(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            winequality_vd,
+            ["citric_acid", "residual_sugar"],
+            "quality",
         )
         result = model_test.contour()
-        assert len(result.get_default_bbox_extra_artists()) == 34
+        assert len(result.get_default_bbox_extra_artists()) == 30
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_RF_REGRESSOR("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'tr_model_test\', match_by_pos = \'true\')'
+        expected_sql = 'PREDICT_LINEAR_REG("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'ridge_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS tr_model_test_drop")
-        model_test = DummyTreeRegressor("tr_model_test_drop",)
-        model_test.fit(
-            "public.tr_data",
-            ["Gender", '"owned cars"', "cost", "income"],
-            "TransPortation",
+        current_cursor().execute("DROP MODEL IF EXISTS ridge_model_test_drop")
+        model_test = Ridge(
+            "ridge_model_test_drop",
         )
+        model_test.fit("public.winequality", ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'ridge_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "tr_model_test_drop"
+        assert current_cursor().fetchone()[0] == "ridge_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'ridge_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
-        assert fim["index"] == ["cost", "owned cars", "gender", "income"]
-        assert fim["importance"] == [88.41, 7.25, 4.35, 0.0]
-        assert fim["sign"] == [1, 1, 1, 0]
-        plt.close("all")
+        assert fim["index"] == ["alcohol", "residual_sugar", "citric_acid"]
+        assert fim["importance"] == [52.3, 32.63, 15.07]
+        assert fim["sign"] == [1, 1, 1]
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
-            "tree_count",
+            "details",
+            "regularization",
+            "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
-            "details",
         ]
         assert m_att["attr_fields"] == [
-            "tree_count",
+            "predictor, coefficient, std_err, t_value, p_value",
+            "type, lambda",
+            "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
-            "predictor, type",
         ]
-        assert m_att["#_of_rows"] == [1, 1, 1, 1, 4]
+        assert m_att["#_of_rows"] == [4, 1, 1, 1, 1, 1]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
-        assert m_att_details["predictor"] == ["gender", "owned cars", "cost", "income"]
-        assert m_att_details["type"] == [
-            "char or varchar",
-            "int",
-            "char or varchar",
-            "char or varchar",
+        assert m_att_details["predictor"] == [
+            "Intercept",
+            "citric_acid",
+            "residual_sugar",
+            "alcohol",
         ]
+        assert m_att_details["coefficient"][0] == pytest.approx(
+            1.77574980319025, abs=1e-6
+        )
+        assert m_att_details["coefficient"][1] == pytest.approx(
+            0.431005879933288, abs=1e-6
+        )
+        assert m_att_details["coefficient"][2] == pytest.approx(
+            0.0237636413018576, abs=1e-6
+        )
+        assert m_att_details["coefficient"][3] == pytest.approx(
+            0.359894749137091, abs=1e-6
+        )
+        assert m_att_details["std_err"][3] == pytest.approx(
+            0.00860813464286587, abs=1e-6
+        )
+        assert m_att_details["t_value"][3] == pytest.approx(41.8086802853809, abs=1e-6)
+        assert m_att_details["p_value"][1] == pytest.approx(8.96677134128099e-11)
+
+        m_att_regularization = model.get_vertica_attributes("regularization")
+
+        assert m_att_regularization["type"][0] == "l2"
+        assert m_att_regularization["lambda"][0] == 1
 
-        assert model.get_attr("tree_count")["tree_count"][0] == 1
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "SELECT rf_regressor('public.tr_model_test', 'public.tr_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, transportation', ntree=1, mtry=4, sampling_size=1, max_depth=100, max_breadth=1000000000, min_leaf_size=1, min_info_gain=0, nbins=1000);"
+            model.get_vertica_attributes("iteration_count")["iteration_count"][0] == 1
         )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 6497
+        )
+
+        if get_version()[0] < 12:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.ridge_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='l2', lambda=1, alpha=0.5)"
+            )
+        else:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.ridge_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='l2', lambda=1, alpha=0.5, fit_intercept=true)"
+            )
 
     def test_get_params(self, model):
         assert model.get_params() == {
-            "n_estimators": 1,
-            "max_features": "max",
-            "max_leaf_nodes": 1000000000,
-            "sample": 1.0,
-            "max_depth": 100,
-            "min_samples_leaf": 1,
-            "min_info_gain": 0,
-            "nbins": 1000,
+            "solver": "newton",
+            "max_iter": 100,
+            "C": 1.0,
+            "tol": 1e-06,
+            "fit_intercept": True,
         }
 
+    def test_get_plot(self, winequality_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
+        model_test = Ridge(
+            "model_test_plot",
+        )
+        model_test.fit(winequality_vd, ["alcohol"], "quality")
+        result = model_test.plot()
+        assert len(result.get_default_bbox_extra_artists()) == 9
+        plt.close("all")
+        model_test.drop()
+
     def test_to_python(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_RF_REGRESSOR('Male', 0, 'Cheap', 'Low' USING PARAMETERS model_name = '{}', match_by_pos=True)::float".format(
-                model.name
+            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([["Male", 0, "Cheap", "Low"]])[0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0])
 
     def test_to_sql(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_RF_REGRESSOR(* USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float FROM (SELECT 'Male' AS \"Gender\", 0 AS \"owned cars\", 'Cheap' AS \"cost\", 'Low' AS \"income\") x".format(
-                model.name, model.to_sql()
+            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
+                model.model_name, model.to_sql([3.0, 11.0, 93.0])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
-    def test_to_memmodel(self, model):
+    def test_to_memmodel(self, model, winequality_vd):
         mmodel = model.to_memmodel()
-        res = mmodel.predict(
-            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
-        )
-        res_py = model.to_python()(
-            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
-        )
+        res = mmodel.predict([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
+        res_py = model.to_python()([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
-        vdf = vDataFrame("public.tr_data")
+        vdf = winequality_vd.copy()
         vdf["prediction_sql"] = mmodel.predict_sql(
-            ['"Gender"', '"owned cars"', '"cost"', '"income"']
+            ["citric_acid", "residual_sugar", "alcohol"]
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="r2")
         assert score == pytest.approx(1.0)
 
-    def test_get_predicts(self, tr_data_vd, model):
-        tr_data_copy = tr_data_vd.copy()
+    def test_get_predicts(self, winequality_vd, model):
+        winequality_copy = winequality_vd.copy()
         model.predict(
-            tr_data_copy,
-            X=["Gender", '"owned cars"', "cost", "income"],
+            winequality_copy,
+            X=["citric_acid", "residual_sugar", "alcohol"],
             name="predicted_quality",
         )
 
-        assert tr_data_copy["predicted_quality"].mean() == pytest.approx(0.9, abs=1e-6)
+        assert winequality_copy["predicted_quality"].mean() == pytest.approx(
+            5.8183779, abs=1e-6
+        )
 
     def test_regression_report(self, model):
         reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
             "max_error",
@@ -222,112 +252,88 @@
             "mean_squared_error",
             "root_mean_squared_error",
             "r2",
             "r2_adj",
             "aic",
             "bic",
         ]
-        assert reg_rep["value"][0] == pytest.approx(1.0, abs=1e-6)
-        assert reg_rep["value"][1] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][2] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][3] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][4] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][5] == pytest.approx(0.0, abs=1e-6)
-        assert reg_rep["value"][6] == pytest.approx(1.0, abs=1e-6)
-        assert reg_rep["value"][7] == pytest.approx(1.0, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-float("inf"), abs=1e-6)
-        assert reg_rep["value"][9] == pytest.approx(-float("inf"), abs=1e-6)
+        assert reg_rep["value"][0] == pytest.approx(0.219816244842147, abs=1e-6)
+        assert reg_rep["value"][1] == pytest.approx(3.59213874427945, abs=1e-6)
+        assert reg_rep["value"][2] == pytest.approx(0.495516023908698, abs=1e-3)
+        assert reg_rep["value"][3] == pytest.approx(0.60908330928705, abs=1e-6)
+        assert reg_rep["value"][4] == pytest.approx(0.594856874272792, abs=1e-6)
+        assert reg_rep["value"][5] == pytest.approx(0.7712696508179172, abs=1e-6)
+        assert reg_rep["value"][6] == pytest.approx(0.219816244842152, abs=1e-6)
+        assert reg_rep["value"][7] == pytest.approx(0.21945577183037412, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-3366.75452986445, abs=1e-6)
+        assert reg_rep["value"][9] == pytest.approx(-3339.6492371939366, abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
-            10.0,
-            4,
-            pytest.approx(1.0),
-            pytest.approx(1.0),
-            float("inf"),
+            6497.0,
+            3,
+            pytest.approx(0.219816244842152),
+            pytest.approx(0.21945577183037412),
+            pytest.approx(609.4456850593101),
             pytest.approx(0.0),
-            pytest.approx(-1.73372940858763),
-            pytest.approx(0.223450528977454),
-            pytest.approx(3.76564442746721),
+            pytest.approx(0.232322269343305),
+            pytest.approx(0.189622693372695),
+            pytest.approx(53.1115447611131),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
-            pytest.approx(6.9),
-            pytest.approx(0.0),
-            pytest.approx(6.9),
+            pytest.approx(1088.2688789226),
+            pytest.approx(3864.78511215033),
+            pytest.approx(4953.68570109281),
+        ]
+        assert reg_rep_anova["MS"][:-1] == [
+            pytest.approx(362.7562929742),
+            pytest.approx(0.5952233346912568),
         ]
-        assert reg_rep_anova["MS"][:-1] == [pytest.approx(1.725), pytest.approx(0.0)]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(3.59213874427945, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0.60908330928705, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="median") == pytest.approx(
+            0.495516023908698, abs=1e-3
+        )
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.594856874272793, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.7712696508179179, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="msle") == pytest.approx(
+            0.00250970549028931, abs=1e-6
+        )
         # method = "r2"
-        assert model.score() == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="r2") == pytest.approx(0.219816244842152, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="r2a") == pytest.approx(0.21945577183037412, abs=1e-6)
         # method = "var"
-        assert model.score(method="var") == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(0.219816244842147, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-float("inf"), abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-3366.75452986445, abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-float("inf"), abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-3339.6492371939366, abs=1e-6)
 
     def test_set_params(self, model):
-        # Nothing will change as Dummy Trees have no parameters
-        model.set_params({"max_features": 100})
-        assert model.get_params()["max_features"] == "max"
-
-    def test_model_from_vDF(self, tr_data_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS tr_from_vDF")
-        model_test = DummyTreeRegressor("tr_from_vDF",)
-        model_test.fit(tr_data_vd, ["gender"], "transportation")
+        model.set_params({"max_iter": 1000})
+
+        assert model.get_params()["max_iter"] == 1000
+
+    def test_model_from_vDF(self, winequality_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS ridge_from_vDF")
+        model_test = Ridge(
+            "ridge_from_vDF",
+        )
+        model_test.fit(winequality_vd, ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'tr_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'ridge_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "tr_from_vDF"
+        assert current_cursor().fetchone()[0] == "ridge_from_vDF"
 
         model_test.drop()
-
-    def test_to_graphviz(self, model):
-        gvz_tree_0 = model.to_graphviz(
-            tree_id=0,
-            classes_color=["red", "blue", "green"],
-            round_pred=4,
-            percent=True,
-            vertical=False,
-            node_style={"shape": "box", "style": "filled"},
-            arrow_style={"color": "blue"},
-            leaf_style={"shape": "circle", "style": "filled"},
-        )
-        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_0
-        assert "0 -> 1" in gvz_tree_0
-
-    def test_get_tree(self, model):
-        tree_0 = model.get_tree()
-
-        assert tree_0["prediction"] == [
-            None,
-            "2.000000",
-            None,
-            None,
-            "1.000000",
-            None,
-            "0.000000",
-            "0.000000",
-            "1.000000",
-        ]
-
-    def test_plot_tree(self, model):
-        result = model.plot_tree()
-        assert model.to_graphviz() == result.source.strip()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_elastic_net.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_lasso.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,108 +1,120 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import warnings
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
+from verticapy.tests.conftest import get_version
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality
-from verticapy.learn.linear_model import ElasticNet
+from verticapy.learn.linear_model import Lasso
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(winequality_vd):
-    model_class = ElasticNet("elasticnet_model_test",)
+    model_class = Lasso(
+        "lasso_model_test",
+    )
     model_class.drop()
     model_class.fit(
         "public.winequality",
         ["total_sulfur_dioxide", "residual_sugar", "alcohol"],
         "quality",
     )
     yield model_class
     model_class.drop()
 
 
-class TestElasticNet:
+class TestLasso:
     def test_repr(self, model):
-        assert "|coefficient|std_err |t_value |p_value" in model.__repr__()
-        model_repr = ElasticNet("lin_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<LinearRegression>"
+        assert model.__repr__() == "<LinearRegression>"
 
     def test_contour(self, winequality_vd):
-        model_test = ElasticNet("model_contour",)
+        model_test = Lasso(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            winequality_vd, ["residual_sugar", "alcohol"], "quality",
+            winequality_vd,
+            ["residual_sugar", "alcohol"],
+            "quality",
         )
         with warnings.catch_warnings(record=True) as w:
             result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 10
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_LINEAR_REG("total_sulfur_dioxide", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'elasticnet_model_test\', match_by_pos = \'true\')'
+        expected_sql = 'PREDICT_LINEAR_REG("total_sulfur_dioxide", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'lasso_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS enet_model_test_drop")
-        model_test = ElasticNet("enet_model_test_drop",)
+        current_cursor().execute("DROP MODEL IF EXISTS lasso_model_test_drop")
+        model_test = Lasso(
+            "lasso_model_test_drop",
+        )
         model_test.fit("public.winequality", ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'enet_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'lasso_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "enet_model_test_drop"
+        assert current_cursor().fetchone()[0] == "lasso_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'enet_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'lasso_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
         assert fim["index"] == ["total_sulfur_dioxide", "residual_sugar", "alcohol"]
         assert fim["importance"] == [100, 0, 0]
         assert fim["sign"] == [-1, 0, 0]
         plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "details",
             "regularization",
             "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
@@ -114,77 +126,91 @@
             "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
         ]
         assert m_att["#_of_rows"] == [4, 1, 1, 1, 1, 1]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
         assert m_att_details["predictor"] == [
             "Intercept",
             "total_sulfur_dioxide",
             "residual_sugar",
             "alcohol",
         ]
-        assert m_att_details["coefficient"][0] == pytest.approx(5.874258, abs=1e-6)
-        assert m_att_details["coefficient"][1] == pytest.approx(-0.000482, abs=1e-6)
+        assert m_att_details["coefficient"][0] == pytest.approx(5.856149, abs=1e-6)
+        assert m_att_details["coefficient"][1] == pytest.approx(-0.000326, abs=1e-6)
         assert m_att_details["coefficient"][2] == pytest.approx(0, abs=1e-6)
         assert m_att_details["coefficient"][3] == pytest.approx(0, abs=1e-6)
         assert m_att_details["std_err"][1] == pytest.approx(0.000221, abs=1e-6)
-        assert m_att_details["t_value"][1] == pytest.approx(-2.176118, abs=1e-6)
-        assert m_att_details["p_value"][1] == pytest.approx(0.029582, abs=1e-6)
+        assert m_att_details["t_value"][1] == pytest.approx(-1.470683, abs=1e-6)
+        assert m_att_details["p_value"][1] == pytest.approx(0.141425, abs=1e-6)
 
-        m_att_regularization = model.get_attr("regularization")
+        m_att_regularization = model.get_vertica_attributes("regularization")
 
-        assert m_att_regularization["type"][0] == "enet"
+        assert m_att_regularization["type"][0] == "l1"
         assert m_att_regularization["lambda"][0] == 1
 
-        assert model.get_attr("iteration_count")["iteration_count"][0] == 1
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 6497
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "linear_reg('public.elasticnet_model_test', 'public.winequality', '\"quality\"', '\"total_sulfur_dioxide\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='cgd', epsilon=1e-06, max_iterations=100, regularization='enet', lambda=1, alpha=0.5)"
+            model.get_vertica_attributes("iteration_count")["iteration_count"][0] == 1
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 6497
         )
 
+        if get_version()[0] < 12:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.lasso_model_test', 'public.winequality', '\"quality\"', '\"total_sulfur_dioxide\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='cgd', epsilon=1e-06, max_iterations=100, regularization='l1', lambda=1, alpha=1)"
+            )
+        else:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.lasso_model_test', 'public.winequality', '\"quality\"', '\"total_sulfur_dioxide\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='cgd', epsilon=1e-06, max_iterations=100, regularization='l1', lambda=1, alpha=1, fit_intercept=true)"
+            )
+
     def test_get_params(self, model):
         assert model.get_params() == {
             "solver": "cgd",
-            "penalty": "enet",
             "max_iter": 100,
-            "l1_ratio": 0.5,
-            "C": 1,
-            "tol": 1e-6,
+            "C": 1.0,
+            "tol": 1e-06,
+            "fit_intercept": True,
         }
 
     def test_get_plot(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = ElasticNet("model_test_plot",)
+        model_test = Lasso(
+            "model_test_plot",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "quality")
         result = model_test.plot()
         assert len(result.get_default_bbox_extra_artists()) == 9
         plt.close("all")
         model_test.drop()
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0])
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([3.0, 11.0, 93.0])
+                model.model_name, model.to_sql([3.0, 11.0, 93.0])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model, winequality_vd):
         mmodel = model.to_memmodel()
@@ -193,15 +219,15 @@
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
         vdf = winequality_vd.copy()
         vdf["prediction_sql"] = mmodel.predict_sql(
             ["total_sulfur_dioxide", "residual_sugar", "alcohol"]
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="r2")
         assert score == pytest.approx(1.0)
 
     def test_get_predicts(self, winequality_vd, model):
         winequality_copy = winequality_vd.copy()
         model.predict(
             winequality_copy,
             X=["total_sulfur_dioxide", "residual_sugar", "alcohol"],
@@ -223,84 +249,86 @@
             "mean_squared_error",
             "root_mean_squared_error",
             "r2",
             "r2_adj",
             "aic",
             "bic",
         ]
-        assert reg_rep["value"][0] == pytest.approx(0.001610, abs=1e-6)
-        assert reg_rep["value"][1] == pytest.approx(3.192849, abs=1e-6)
-        assert reg_rep["value"][2] == pytest.approx(0.788321, abs=1e-6)
-        assert reg_rep["value"][3] == pytest.approx(0.684299, abs=1e-6)
-        assert reg_rep["value"][4] == pytest.approx(0.761229, abs=1e-6)
-        assert reg_rep["value"][5] == pytest.approx(0.872484, abs=1e-6)
-        assert reg_rep["value"][6] == pytest.approx(0.001610, abs=1e-6)
-        assert reg_rep["value"][7] == pytest.approx(0.001148, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-1764.5050571146871, abs=1e-6)
-        assert reg_rep["value"][9] == pytest.approx(-1737.394835300611, abs=1e-6)
+        assert reg_rep["value"][0] == pytest.approx(0.001302, abs=1e-6)
+        assert reg_rep["value"][1] == pytest.approx(3.189211, abs=1e-6)
+        assert reg_rep["value"][2] == pytest.approx(0.798061, abs=1e-6)
+        assert reg_rep["value"][3] == pytest.approx(0.684704, abs=1e-6)
+        assert reg_rep["value"][4] == pytest.approx(0.761464, abs=1e-6)
+        assert reg_rep["value"][5] == pytest.approx(0.8726193656049263, abs=1e-6)
+        assert reg_rep["value"][6] == pytest.approx(0.001302, abs=1e-6)
+        assert reg_rep["value"][7] == pytest.approx(0.0008407218505677161, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-1762.49710658166, abs=1e-6)
+        assert reg_rep["value"][9] == pytest.approx(-1735.3918139111545, abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
             6497.0,
             3,
-            pytest.approx(0.00161000676361089),
-            pytest.approx(0.001148714605947454),
-            pytest.approx(2.116870802238774),
-            pytest.approx(0.0958537016435304),
+            pytest.approx(0.00130215624626484),
+            pytest.approx(0.0008407218505677161),
+            pytest.approx(0.9668703628035513),
+            pytest.approx(0.40727177684393245),
             pytest.approx(0.232322269343305),
             pytest.approx(0.189622693372695),
             pytest.approx(53.1115447611131),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
-            pytest.approx(4.83725377631033),
-            pytest.approx(4945.71023360928),
+            pytest.approx(2.21007321118539),
+            pytest.approx(4947.23522831515),
             pytest.approx(4953.68570109281),
         ]
         assert reg_rep_anova["MS"][:-1] == [
-            pytest.approx(1.6124179254367768),
-            pytest.approx(0.7616987884813307),
+            pytest.approx(0.73669107039513),
+            pytest.approx(0.7619336559857001),
         ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(3.192849, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(3.189211, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0.684299, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0.684704, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0.788321, abs=1e-6)
+        assert model.score(metric="median") == pytest.approx(0.798061, abs=1e-6)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.761229, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.761464557228739, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.8724848619460168, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.8726193656049234, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.003171, abs=1e-6)
+        assert model.score(metric="msle") == pytest.approx(0.003172, abs=1e-6)
         # method = "r2"
-        assert model.score(method="r2") == pytest.approx(0.001610, abs=1e-6)
+        assert model.score(metric="r2") == pytest.approx(0.001302, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(
-            0.001148714605947454, abs=1e-6
+        assert model.score(metric="r2a") == pytest.approx(
+            0.0008407218505677161, abs=1e-6
         )
         # method = "var"
-        assert model.score(method="var") == pytest.approx(0.001610, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(0.001302, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-1764.5050571146871, abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-1762.49710658166, abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-1737.394835300611, abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-1735.3918139111545, abs=1e-6)
 
     def test_set_params(self, model):
         model.set_params({"max_iter": 1000})
 
         assert model.get_params()["max_iter"] == 1000
 
     def test_model_from_vDF(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS enet_from_vDF")
-        model_test = ElasticNet("enet_from_vDF",)
+        current_cursor().execute("DROP MODEL IF EXISTS lasso_from_vDF")
+        model_test = Lasso(
+            "lasso_from_vDF",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'enet_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'lasso_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "enet_from_vDF"
+        assert current_cursor().fetchone()[0] == "lasso_from_vDF"
 
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_kde.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_lof.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,129 +1,104 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import (
     drop,
     set_option,
-    create_verticapy_schema,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
-from verticapy.learn.neighbors import KernelDensity
+from verticapy.learn.neighbors import LocalOutlierFactor
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
-    create_verticapy_schema()
-    model_class = KernelDensity("KernelDensity_model_test",)
+    model_class = LocalOutlierFactor(
+        "lof_model_test",
+    )
     model_class.drop()
     model_class.fit("public.titanic", ["age", "fare"])
     yield model_class
     model_class.drop()
 
 
-class TestKernelDensity:
+class TestLocalOutlierFactor:
     def test_repr(self, model):
-        assert "Additional Info" in model.__repr__()
-        model_repr = KernelDensity("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<KernelDensity>"
-
-    def test_drop(self):
-        model_test = KernelDensity("model_test_drop",)
-        model_test.drop()
-        model_test.fit("public.titanic", ["age", "fare"])
-        current_cursor().execute(
-            """SELECT 
-                    model_name 
-               FROM verticapy.models 
-               WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"""
-        )
-        assert current_cursor().fetchone()[0] in (
-            "model_test_drop",
-            '"model_test_drop"',
-        )
-
-        model_test.drop()
-        current_cursor().execute(
-            """SELECT 
-                    model_name 
-               FROM verticapy.models 
-               WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"""
-        )
-        assert current_cursor().fetchone() is None
+        assert model.__repr__() == "<LocalOutlierFactor>"
 
     def test_get_params(self, model):
-        assert model.get_params() == {
-            "bandwidth": 1,
-            "kernel": "gaussian",
-            "max_depth": 5,
-            "max_leaf_nodes": 1000000000,
-            "min_samples_leaf": 1,
-            "nbins": 5,
-            "p": 2,
-            "xlim": [],
-        }
+        assert model.get_params() == {"n_neighbors": 20, "p": 2}
 
     def test_get_predict(self, titanic_vd, model):
-        titanic_copy = model.predict(titanic_vd.copy(), name="kde")
+        titanic_copy = model.predict()
 
-        assert titanic_copy["kde"].mean() == pytest.approx(
-            1.82115211838814e-06, abs=1e-6
+        assert titanic_copy["lof_score"].mean() == pytest.approx(
+            1.17226637499694, abs=1e-6
         )
 
-    def test_get_attr(self, model):
-        result = model.get_attr()
-        assert result["attr_name"][0] == "tree_count"
+    def test_get_attributes(self, model):
+        result = model.get_attributes()
+        assert result == ["n_neighbors_", "p_", "n_errors_", "cnt_"]
 
     def test_get_plot(self, model):
-        result = model.plot()
-        assert len(result.get_default_bbox_extra_artists()) == 8
+        result = model.plot(color=["r", "b"])
+        assert len(result.get_default_bbox_extra_artists()) == 9
         plt.close("all")
-        model_test = KernelDensity("model_test_plot_kde_plot")
+        model_test = LocalOutlierFactor("model_test_plot_lof")
         model_test.drop()
         model_test.fit("public.titanic", ["age"])
-        result = model_test.plot()
+        result = model_test.plot(color=["r", "b"])
         assert len(result.get_default_bbox_extra_artists()) == 9
+        model_test = LocalOutlierFactor("model_test_plot_lof_2")
+        model_test.drop()
+        model_test.fit("public.titanic", ["age", "fare", "pclass"])
+        result = model_test.plot(color=["r", "b"])
+        assert len(result.get_default_bbox_extra_artists()) == 3
         model_test.drop()
 
     def test_set_params(self, model):
         model.set_params({"p": 1})
 
         assert model.get_params()["p"] == 1
 
     def test_model_from_vDF(self, titanic_vd):
-        model_test = KernelDensity("KernelDensity_from_vDF_tmp",)
+        model_test = LocalOutlierFactor(
+            "lof_from_vDF_tmp",
+        )
         model_test.drop()
         model_test.fit(titanic_vd, ["age", "fare"])
-        titanic_copy = model_test.predict(titanic_vd.copy(), name="kde")
-
-        assert titanic_copy["kde"].mean() == pytest.approx(
-            1.82115211838814e-06, abs=1e-6
+        assert model_test.predict()["lof_score"].mean() == pytest.approx(
+            1.17226637499694, abs=1e-6
         )
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_kmeans.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_kmeans.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,47 +1,55 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_iris, load_winequality
 from verticapy.learn.cluster import KMeans
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def iris_vd():
     iris = load_iris()
     yield iris
-    drop(name="public.iris",)
+    drop(
+        name="public.iris",
+    )
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(iris_vd):
     model_class = KMeans(
         "kmeans_model_test",
         n_cluster=3,
@@ -55,65 +63,68 @@
     )
     yield model_class
     model_class.drop()
 
 
 class TestKMeans:
     def test_repr(self, model):
-        assert "kmeans" in model.__repr__()
-        model_repr = KMeans("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<KMeans>"
+        assert model.__repr__() == "<KMeans>"
 
     def test_deploySQL(self, model):
         expected_sql = 'APPLY_KMEANS("SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm" USING PARAMETERS model_name = \'kmeans_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS kmeans_model_test_drop")
-        model_test = KMeans("kmeans_model_test_drop",)
+        model_test = KMeans(
+            "kmeans_model_test_drop",
+        )
         model_test.fit("public.iris", ["SepalLengthCm", "SepalWidthCm"])
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'kmeans_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "kmeans_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'kmeans_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == ["centers", "metrics"]
         assert m_att["attr_fields"] == [
             "sepallengthcm, sepalwidthcm, petallengthcm, petalwidthcm",
             "metrics",
         ]
         assert m_att["#_of_rows"] == [3, 1]
 
-        m_att_centers = model.get_attr(attr_name="centers")
+        m_att_centers = model.get_vertica_attributes(attr_name="centers")
 
         assert m_att_centers["sepallengthcm"] == [
             pytest.approx(5.006),
             pytest.approx(5.90161290322581),
             pytest.approx(6.85),
         ]
 
     def test_get_params(self, model):
         assert model.get_params() == {
             "max_iter": 10,
             "tol": 0.0001,
             "n_cluster": 3,
-            "init": [[7.2, 3.0, 5.8, 1.6], [6.9, 3.1, 4.9, 1.5], [5.7, 4.4, 1.5, 0.4]],
+            "init": [
+                [7.2, 3.0, 5.8, 1.6],
+                [6.9, 3.1, 4.9, 1.5],
+                [5.7, 4.4, 1.5, 0.4],
+            ],
         }
 
     def test_get_predict(self, iris_vd, model):
         iris_copy = iris_vd.copy()
 
         model.predict(
             iris_copy,
@@ -157,41 +168,43 @@
             "SELECT model_name FROM models WHERE model_name = 'random_test'"
         )
         assert current_cursor().fetchone()[0] == "random_test"
         model_test_random.drop()
 
     def test_get_plot(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = KMeans("model_test_plot",)
+        model_test = KMeans(
+            "model_test_plot",
+        )
         model_test.fit(winequality_vd, ["alcohol", "quality"])
         result = model_test.plot(color="b")
-        assert len(result.get_default_bbox_extra_artists()) == 16
+        assert len(result.get_default_bbox_extra_artists()) > 8
         plt.close("all")
         model_test.drop()
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT APPLY_KMEANS(5.006, 3.418, 1.464, 0.244 USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[5.006, 3.418, 1.464, 0.244]])[0]
+            model.to_python()([[5.006, 3.418, 1.464, 0.244]])[0]
         )
         assert 0.0 == pytest.approx(
-            model.to_python(return_str=False, return_distance_clusters=True)(
+            model.to_python(return_distance_clusters=True)(
                 [[5.006, 3.418, 1.464, 0.244]]
             )[0][0]
         )
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT APPLY_KMEANS(5.006, 3.418, 1.464, 0.244 USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([5.006, 3.418, 1.464, 0.244])
+                model.model_name, model.to_sql([5.006, 3.418, 1.464, 0.244])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model, iris_vd):
         mmodel = model.to_memmodel()
@@ -202,18 +215,20 @@
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
         vdf = iris_vd.copy()
         vdf["prediction_sql"] = mmodel.predict_sql(
             ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"]
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
         assert score == pytest.approx(1.0)
 
     def test_get_voronoi_plot(self, iris_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = KMeans("model_test_plot",)
+        model_test = KMeans(
+            "model_test_plot",
+        )
         model_test.fit(iris_vd, ["SepalLengthCm", "SepalWidthCm"])
         result = model_test.plot_voronoi(color="b")
         assert len(result.gca().get_default_bbox_extra_artists()) == 21
         plt.close("all")
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_knn_classifier.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_knn_classifier.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,161 +1,153 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import (
     drop,
     set_option,
-    create_verticapy_schema,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
 from verticapy.learn.neighbors import KNeighborsClassifier
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
-    create_verticapy_schema()
-    model_class = KNeighborsClassifier("knn_model_test",)
+    model_class = KNeighborsClassifier(
+        "knn_model_test",
+    )
     model_class.drop()
     model_class.fit("public.titanic", ["age", "fare"], "survived")
     yield model_class
     model_class.drop()
 
 
 class TestKNeighborsClassifier:
     def test_repr(self, model):
-        assert "Additional Info" in model.__repr__()
-        model_repr = KNeighborsClassifier("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<KNeighborsClassifier>"
-
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
-        assert m_att["attr_name"] == ["n_neighbors", "p", "classes"]
-        m_att = model.get_attr("n_neighbors")
+        assert model.__repr__() == "<KNeighborsClassifier>"
+
+    def test_get_attributes(self, model):
+        m_att = model.get_attributes()
+        assert m_att == ["classes_", "n_neighbors_", "p_"]
+        m_att = model.get_attributes("n_neighbors")
         assert m_att == model.parameters["n_neighbors"]
-        m_att = model.get_attr("p")
+        m_att = model.get_attributes("p")
         assert m_att == model.parameters["p"]
-        m_att = model.get_attr("classes")
-        assert m_att == model.classes_
+        m_att = model.get_attributes("classes")
+        assert m_att[1] == model.classes_[1]
 
     def test_contour(self, titanic_vd):
-        model_test = KNeighborsClassifier("model_contour",)
+        model_test = KNeighborsClassifier(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 34
         model_test.drop()
 
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(nbins=1000)
+        lift_ch = model.lift_chart(nbins=1000, show=False)
 
         assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
         assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(
             0.353846153846154
         )
         assert lift_ch["lift"][300] == pytest.approx(1.81819061441703)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(1.0)
         assert lift_ch["lift"][900] == pytest.approx(1.0)
         plt.close("all")
 
     def test_roc_curve(self, model):
-        roc_curve = model.roc_curve(nbins=1000)
+        roc_curve = model.roc_curve(nbins=1000, show=False)
 
         assert roc_curve["threshold"][100] == pytest.approx(0.1)
         assert roc_curve["false_positive"][100] == pytest.approx(1.0)
         assert roc_curve["true_positive"][100] == pytest.approx(1.0)
         assert roc_curve["threshold"][700] == pytest.approx(0.7)
         assert roc_curve["false_positive"][700] == pytest.approx(0.0491803278688525)
         assert roc_curve["true_positive"][700] == pytest.approx(0.353846153846154)
         plt.close("all")
 
     def test_prc_curve(self, model):
-        prc_curve = model.prc_curve(nbins=1000)
+        prc_curve = model.prc_curve(nbins=1000, show=False)
 
         assert prc_curve["threshold"][100] == pytest.approx(0.099)
         assert prc_curve["recall"][100] == pytest.approx(1.0)
         assert prc_curve["precision"][100] == pytest.approx(0.477356181150551)
         assert prc_curve["threshold"][700] == pytest.approx(0.699)
         assert prc_curve["recall"][700] == pytest.approx(0.353846153846154)
         assert prc_curve["precision"][700] == pytest.approx(0.867924528301887)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(nbins=1000)
+        cutoff_curve = model.cutoff_curve(nbins=1000, show=False)
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
         assert cutoff_curve["false_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["true_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
         assert cutoff_curve["false_positive"][700] == pytest.approx(0.0491803278688525)
         assert cutoff_curve["true_positive"][700] == pytest.approx(0.353846153846154)
         plt.close("all")
 
     def test_deploySQL(self, model):
-        expected_sql = '(SELECT row_id, "age", "fare", "survived", predict_neighbors, COUNT(*) / 5 AS proba_predict FROM (SELECT x."age", x."fare", x."survived", ROW_NUMBER() OVER(PARTITION BY x."age", x."fare", row_id ORDER BY POWER(POWER(ABS(x."age" - y."age"), 2) + POWER(ABS(x."fare" - y."fare"), 2), 1 / 2)) AS ordered_distance, y."survived" AS predict_neighbors, row_id FROM (SELECT *, ROW_NUMBER() OVER() AS row_id FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) x CROSS JOIN (SELECT * FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) y) z WHERE ordered_distance <= 5 GROUP BY "age", "fare", "survived", row_id, predict_neighbors) kneighbors_table'
+        expected_sql = '(SELECT row_id, "age", "fare", "survived", predict_neighbors, COUNT(*) / 5 AS proba_predict FROM ( SELECT x."age", x."fare", x."survived", ROW_NUMBER() OVER(PARTITION BY x."age", x."fare", row_id ORDER BY POWER(POWER(ABS(x."age" - y."age"), 2) + POWER(ABS(x."fare" - y."fare"), 2), 1 / 2)) AS ordered_distance, y."survived" AS predict_neighbors, row_id FROM (SELECT *, ROW_NUMBER() OVER() AS row_id FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) x CROSS JOIN (SELECT * FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) y) z WHERE ordered_distance <= 5 GROUP BY "age", "fare", "survived", row_id, predict_neighbors) kneighbors_table'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
-    def test_drop(self):
-        model_test = KNeighborsClassifier("model_test_drop",)
-        model_test.drop()
-        model_test.fit("public.titanic", ["age"], "survived")
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone()[0] in (
-            "model_test_drop",
-            '"model_test_drop"',
-        )
-
-        model_test.drop()
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone() is None
-
     def test_get_params(self, model):
         assert model.get_params() == {"n_neighbors": 5, "p": 2}
 
     def test_predict(self, titanic_vd, model):
         titanic_copy = titanic_vd.copy()
 
         titanic_copy = model.predict(
-            titanic_copy, X=["age", "fare"], name="predicted_quality", inplace=False,
+            titanic_copy,
+            X=["age", "fare"],
+            name="predicted_quality",
+            inplace=False,
         )
 
         assert titanic_copy["predicted_quality"].mean() == pytest.approx(
             0.381884944920441, abs=1e-6
         )
 
     def test_predict_proba(self, titanic_vd, model):
@@ -171,52 +163,53 @@
         assert titanic_copy["prob_quality"].mean() == pytest.approx(
             0.378313253012048, abs=1e-6
         )
 
     def test_classification_report(self, model):
         cls_rep1 = model.classification_report().transpose()
 
-        assert cls_rep1["auc"][0] == pytest.approx(0.7529724373986667)
-        assert cls_rep1["prc_auc"][0] == pytest.approx(0.7776321621297582)
-        assert cls_rep1["accuracy"][0] == pytest.approx(0.6658506731946144)
-        assert cls_rep1["log_loss"][0] == pytest.approx(0.248241359319007)
-        assert cls_rep1["precision"][0] == pytest.approx(0.8679245283018868)
-        assert cls_rep1["recall"][0] == pytest.approx(0.35384615384615387)
-        assert cls_rep1["f1_score"][0] == pytest.approx(0.5027322404371585)
-        assert cls_rep1["mcc"][0] == pytest.approx(0.38437795748893316)
-        assert cls_rep1["informedness"][0] == pytest.approx(0.3046658259773014)
-        assert cls_rep1["markedness"][0] == pytest.approx(0.4849458048976314)
-        assert cls_rep1["csi"][0] == pytest.approx(0.3357664233576642)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.6)
+        assert cls_rep1["auc"][0] == pytest.approx(0.696400048039392)
+        assert cls_rep1["prc_auc"][0] == pytest.approx(0.7591081348272292)
+        assert cls_rep1["accuracy"][0] == pytest.approx(0.7013463892288861)
+        assert cls_rep1["log_loss"][0] == pytest.approx(26.8788249694002)
+        assert cls_rep1["precision"][0] == pytest.approx(0.7339743589743589)
+        assert cls_rep1["recall"][0] == pytest.approx(0.5871794871794872)
+        assert cls_rep1["f1_score"][0] == pytest.approx(0.6524216524216524)
+        assert cls_rep1["mcc"][0] == pytest.approx(0.40382652359985155)
+        assert cls_rep1["informedness"][0] == pytest.approx(0.39280009607878474)
+        assert cls_rep1["markedness"][0] == pytest.approx(0.41516247778624016)
+        assert cls_rep1["csi"][0] == pytest.approx(0.48414376321353064)
 
     def test_score(self, model):
-        assert model.score(cutoff=0.9, method="accuracy") == pytest.approx(
+        assert model.score(cutoff=0.9, metric="accuracy") == pytest.approx(
             0.5691554467564259
         )
-        assert model.score(cutoff=0.1, method="accuracy") == pytest.approx(
+        assert model.score(cutoff=0.1, metric="accuracy") == pytest.approx(
             0.4773561811505508
         )
-        assert model.score(method="best_cutoff") == pytest.approx(0.6)
-        assert model.score(method="bm") == pytest.approx(0.0)
-        assert model.score(method="csi") == pytest.approx(0.4773561811505508)
-        assert model.score(method="f1") == pytest.approx(0.6462303231151615)
-        assert model.score(method="logloss") == pytest.approx(0.248241359319007)
-        assert model.score(method="mcc") == pytest.approx(0)
-        assert model.score(method="mk") == pytest.approx(-0.5226438188494492)
-        assert model.score(method="npv") == pytest.approx(0.4773561811505508)
-        assert model.score(method="prc_auc") == pytest.approx(0.7776321621297582)
-        assert model.score(method="precision") == pytest.approx(0.4773561811505508)
-        assert model.score(method="specificity") == pytest.approx(0.0)
+        assert model.score(metric="best_cutoff") == pytest.approx(0.999)
+        assert model.score(metric="bm") == pytest.approx(0.39280009607878474)
+        assert model.score(metric="csi") == pytest.approx(0.48414376321353064)
+        assert model.score(metric="f1") == pytest.approx(0.6524216524216524)
+        assert model.score(metric="logloss") == pytest.approx(26.8788249694002)
+        assert model.score(metric="mcc") == pytest.approx(0.40382652359985155)
+        assert model.score(metric="mk") == pytest.approx(0.41516247778624016)
+        assert model.score(metric="npv") == pytest.approx(0.6811881188118812)
+        assert model.score(metric="prc_auc") == pytest.approx(0.7591081348272292)
+        assert model.score(metric="precision") == pytest.approx(0.7339743589743589)
+        assert model.score(metric="specificity") == pytest.approx(0.8056206088992974)
 
     def test_set_params(self, model):
         model.set_params({"p": 1})
 
         assert model.get_params()["p"] == 1
 
     def test_model_from_vDF(self, titanic_vd):
-        model_test = KNeighborsClassifier("knn_from_vDF",)
+        model_test = KNeighborsClassifier(
+            "knn_from_vDF",
+        )
         model_test.drop()
         model_test.fit(titanic_vd, ["age"], "survived")
-        assert model_test.score(cutoff=0.9, method="accuracy") == pytest.approx(
+        assert model_test.score(cutoff=0.9, metric="accuracy") == pytest.approx(
             0.5890710382513661
         )
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_knn_regressor.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_knn_regressor.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,117 +1,109 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import (
     drop,
     set_option,
-    create_verticapy_schema,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
 from verticapy.learn.neighbors import KNeighborsRegressor
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
-    create_verticapy_schema()
-    model_class = KNeighborsRegressor("knn_model_test",)
+    model_class = KNeighborsRegressor(
+        "knn_model_test",
+    )
     model_class.drop()
     model_class.fit("public.titanic", ["age", "fare"], "survived")
     yield model_class
     model_class.drop()
 
 
 class TestKNeighborsRegressor:
     def test_repr(self, model):
-        assert "Additional Info" in model.__repr__()
-        model_repr = KNeighborsRegressor("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<KNeighborsRegressor>"
-
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
-        assert m_att["attr_name"] == ["n_neighbors", "p"]
-        m_att = model.get_attr("n_neighbors")
+        assert model.__repr__() == "<KNeighborsRegressor>"
+
+    def test_get_attributes(self, model):
+        m_att = model.get_attributes()
+        assert m_att == ["n_neighbors_", "p_"]
+        m_att = model.get_attributes("n_neighbors")
         assert m_att == model.parameters["n_neighbors"]
-        m_att = model.get_attr("p")
+        m_att = model.get_attributes("p")
         assert m_att == model.parameters["p"]
 
     def test_contour(self, titanic_vd):
-        model_test = KNeighborsRegressor("model_contour",)
+        model_test = KNeighborsRegressor(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 34
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = '(SELECT "age", "fare", "survived", AVG(predict_neighbors) AS predict_neighbors FROM (SELECT x."age", x."fare", x."survived", ROW_NUMBER() OVER(PARTITION BY x."age", x."fare", row_id ORDER BY POWER(POWER(ABS(x."age" - y."age"), 2) + POWER(ABS(x."fare" - y."fare"), 2), 1 / 2)) AS ordered_distance, y."survived" AS predict_neighbors, row_id FROM (SELECT *, ROW_NUMBER() OVER() AS row_id FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) x CROSS JOIN (SELECT * FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) y) z WHERE ordered_distance <= 5 GROUP BY "age", "fare", "survived", row_id) knr_table'
+        expected_sql = '(SELECT "age", "fare", "survived", AVG(predict_neighbors) AS predict_neighbors FROM ( SELECT x."age", x."fare", x."survived", ROW_NUMBER() OVER(PARTITION BY x."age", x."fare", row_id ORDER BY POWER(POWER(ABS(x."age" - y."age"), 2) + POWER(ABS(x."fare" - y."fare"), 2), 1 / 2)) AS ordered_distance, y."survived" AS predict_neighbors, row_id FROM (SELECT *, ROW_NUMBER() OVER() AS row_id FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) x CROSS JOIN (SELECT * FROM public.titanic WHERE "age" IS NOT NULL AND "fare" IS NOT NULL) y) z WHERE ordered_distance <= 5 GROUP BY "age", "fare", "survived", row_id) knr_table'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
-    def test_drop(self):
-        model_test = KNeighborsRegressor("model_test_drop",)
-        model_test.drop()
-        model_test.fit("public.titanic", ["age"], "survived")
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone()[0] in (
-            "model_test_drop",
-            '"model_test_drop"',
-        )
-
-        model_test.drop()
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone() is None
-
     def test_get_params(self, model):
         assert model.get_params() == {"n_neighbors": 5, "p": 2}
 
     def test_get_predicts(self, titanic_vd, model):
         titanic_copy = titanic_vd.copy()
         titanic_copy = model.predict(
-            titanic_copy, X=["age", "fare"], name="predicted_quality", inplace=False,
+            titanic_copy,
+            X=["age", "fare"],
+            name="predicted_quality",
+            inplace=False,
         )
 
         assert titanic_copy["predicted_quality"].mean() == pytest.approx(
             0.378313253012048, abs=1e-6
         )
 
     def test_regression_report(self, model):
-        reg_rep = model.regression_report("metrics")
+        reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
             "max_error",
             "median_absolute_error",
             "mean_absolute_error",
             "mean_squared_error",
@@ -125,71 +117,73 @@
         assert reg_rep["value"][1] == pytest.approx(1.0, abs=1e-6)
         assert reg_rep["value"][2] == pytest.approx(0.2, abs=1e-6)
         assert reg_rep["value"][3] == pytest.approx(0.319076305220884, abs=1e-6)
         assert reg_rep["value"][4] == pytest.approx(0.161887550200803, abs=1e-6)
         assert reg_rep["value"][5] == pytest.approx(0.40235251981415876, abs=1e-6)
         assert reg_rep["value"][6] == pytest.approx(0.321109086681745, abs=1e-6)
         assert reg_rep["value"][7] == pytest.approx(0.3197417333820103, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-1807.5457125099035, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-1807.52756734861, abs=1e-6)
         assert reg_rep["value"][9] == pytest.approx(-1792.8586642855382, abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
             1234.0,
             2,
             pytest.approx(0.321109086681745),
             pytest.approx(0.3200060957584009),
             pytest.approx(239.85598471783427),
             pytest.approx(9.340149253171836e-86),
             pytest.approx(-1.68576262213743),
             pytest.approx(0.56300284427369),
             pytest.approx(212.604974886025),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
             pytest.approx(77.894016064257),
             pytest.approx(161.24),
             pytest.approx(237.505020080321),
         ]
         assert reg_rep_anova["MS"][:-1] == [
             pytest.approx(38.9470080321285),
             pytest.approx(0.16237663645518632),
         ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(1.0, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0.319076305220884, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0.319076305220884, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0.2, abs=1e-6)
+        assert model.score(metric="median") == pytest.approx(0.2, abs=1e-6)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.161887550200803, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.161887550200803, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(
+        assert model.score(metric="rmse") == pytest.approx(
             0.40235251981415876, abs=1e-6
         )
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.0148862189812457, abs=1e-6)
+        assert model.score(metric="msle") == pytest.approx(0.0148862189812457, abs=1e-6)
         # method = "r2"
         assert model.score() == pytest.approx(0.321109086681745, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(0.3197417333820103, abs=1e-6)
+        assert model.score(metric="r2a") == pytest.approx(0.3197417333820103, abs=1e-6)
         # method = "var"
-        assert model.score(method="var") == pytest.approx(0.32196148887151, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(0.32196148887151, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-1807.5457125099035, abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-1807.52756734861, abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-1792.8586642855369, abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-1792.8586642855369, abs=1e-6)
 
     def test_set_params(self, model):
         model.set_params({"p": 1})
 
         assert model.get_params()["p"] == 1
 
     def test_model_from_vDF(self, titanic_vd):
-        model_test = KNeighborsRegressor("knn_from_vDF",)
+        model_test = KNeighborsRegressor(
+            "knn_from_vDF",
+        )
         model_test.drop()
         model_test.fit(titanic_vd, ["age"], "survived")
         assert model_test.score() == pytest.approx(-0.122616967579114)
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_lasso.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_linear_svr.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,218 +1,232 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
-# Standard Python Modules
-import warnings
-
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality
-from verticapy.learn.linear_model import Lasso
+from verticapy.learn.svm import LinearSVR
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(winequality_vd):
-    model_class = Lasso("lasso_model_test",)
+    model_class = LinearSVR(
+        "lsvr_model_test",
+    )
     model_class.drop()
     model_class.fit(
         "public.winequality",
-        ["total_sulfur_dioxide", "residual_sugar", "alcohol"],
+        ["citric_acid", "residual_sugar", "alcohol"],
         "quality",
     )
     yield model_class
     model_class.drop()
 
 
-class TestLasso:
+class TestLinearSVR:
     def test_repr(self, model):
-        assert "|coefficient|std_err |t_value |p_value" in model.__repr__()
-        model_repr = Lasso("lin_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<LinearRegression>"
+        assert model.__repr__() == "<LinearSVR>"
 
     def test_contour(self, winequality_vd):
-        model_test = Lasso("model_contour",)
+        model_test = LinearSVR(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            winequality_vd, ["residual_sugar", "alcohol"], "quality",
+            winequality_vd,
+            ["citric_acid", "residual_sugar"],
+            "quality",
         )
-        with warnings.catch_warnings(record=True) as w:
-            result = model_test.contour()
-        assert len(result.get_default_bbox_extra_artists()) == 10
+        result = model_test.contour()
+        assert len(result.get_default_bbox_extra_artists()) == 38
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_LINEAR_REG("total_sulfur_dioxide", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'lasso_model_test\', match_by_pos = \'true\')'
+        expected_sql = 'PREDICT_SVM_REGRESSOR("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'lsvr_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS lasso_model_test_drop")
-        model_test = Lasso("lasso_model_test_drop",)
+        current_cursor().execute("DROP MODEL IF EXISTS lsvr_model_test_drop")
+        model_test = LinearSVR(
+            "lsvr_model_test_drop",
+        )
         model_test.fit("public.winequality", ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lasso_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'lsvr_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "lasso_model_test_drop"
+        assert current_cursor().fetchone()[0] == "lsvr_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lasso_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'lsvr_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
-        assert fim["index"] == ["total_sulfur_dioxide", "residual_sugar", "alcohol"]
-        assert fim["importance"] == [100, 0, 0]
-        assert fim["sign"] == [-1, 0, 0]
+        assert fim["index"] == ["alcohol", "residual_sugar", "citric_acid"]
+        assert fim["importance"] == [52.68, 33.27, 14.05]
+        assert fim["sign"] == [1, 1, 1]
         plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "details",
-            "regularization",
-            "iteration_count",
-            "rejected_row_count",
             "accepted_row_count",
+            "rejected_row_count",
+            "iteration_count",
             "call_string",
         ]
         assert m_att["attr_fields"] == [
-            "predictor, coefficient, std_err, t_value, p_value",
-            "type, lambda",
-            "iteration_count",
-            "rejected_row_count",
+            "predictor, coefficient",
             "accepted_row_count",
+            "rejected_row_count",
+            "iteration_count",
             "call_string",
         ]
-        assert m_att["#_of_rows"] == [4, 1, 1, 1, 1, 1]
+        assert m_att["#_of_rows"] == [4, 1, 1, 1, 1]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
         assert m_att_details["predictor"] == [
             "Intercept",
-            "total_sulfur_dioxide",
+            "citric_acid",
             "residual_sugar",
             "alcohol",
         ]
-        assert m_att_details["coefficient"][0] == pytest.approx(5.856149, abs=1e-6)
-        assert m_att_details["coefficient"][1] == pytest.approx(-0.000326, abs=1e-6)
-        assert m_att_details["coefficient"][2] == pytest.approx(0, abs=1e-6)
-        assert m_att_details["coefficient"][3] == pytest.approx(0, abs=1e-6)
-        assert m_att_details["std_err"][1] == pytest.approx(0.000221, abs=1e-6)
-        assert m_att_details["t_value"][1] == pytest.approx(-1.470683, abs=1e-6)
-        assert m_att_details["p_value"][1] == pytest.approx(0.141425, abs=1e-6)
-
-        m_att_regularization = model.get_attr("regularization")
-
-        assert m_att_regularization["type"][0] == "l1"
-        assert m_att_regularization["lambda"][0] == 1
-
-        assert model.get_attr("iteration_count")["iteration_count"][0] == 1
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 6497
+        assert m_att_details["coefficient"][0] == pytest.approx(
+            1.67237120425236, abs=1e-6
+        )
+        assert m_att_details["coefficient"][1] == pytest.approx(
+            0.410055106076537, abs=1e-6
+        )
+        assert m_att_details["coefficient"][2] == pytest.approx(
+            0.0247255999942058, abs=1e-6
+        )
+        assert m_att_details["coefficient"][3] == pytest.approx(
+            0.369955366024044, abs=1e-6
+        )
+
+        assert (
+            model.get_vertica_attributes("iteration_count")["iteration_count"][0] == 5
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 6497
+        )
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "linear_reg('public.lasso_model_test', 'public.winequality', '\"quality\"', '\"total_sulfur_dioxide\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='cgd', epsilon=1e-06, max_iterations=100, regularization='l1', lambda=1, alpha=1)"
+            model.get_vertica_attributes("call_string")["call_string"][0]
+            == "SELECT svm_regressor('public.lsvr_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS error_tolerance=0.1, C=1, max_iterations=100, intercept_mode='regularized', intercept_scaling=1, epsilon=0.0001);"
         )
 
     def test_get_params(self, model):
         assert model.get_params() == {
-            "solver": "cgd",
-            "penalty": "l1",
-            "max_iter": 100,
+            "tol": 0.0001,
             "C": 1.0,
-            "tol": 1e-06,
+            "max_iter": 100,
+            "intercept_scaling": 1.0,
+            "intercept_mode": "regularized",
+            "acceptable_error_margin": 0.1,
         }
 
     def test_get_plot(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = Lasso("model_test_plot",)
-        model_test.fit(winequality_vd, ["alcohol"], "quality")
+        model_test = LinearSVR(
+            "model_test_plot",
+        )
+        model_test.fit("public.winequality", ["alcohol"], "quality")
         result = model_test.plot()
         assert len(result.get_default_bbox_extra_artists()) == 9
         plt.close("all")
         model_test.drop()
 
     def test_to_python(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+            "SELECT PREDICT_SVM_REGRESSOR(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0])
 
     def test_to_sql(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([3.0, 11.0, 93.0])
+            "SELECT PREDICT_SVM_REGRESSOR(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
+                model.model_name, model.to_sql([3.0, 11.0, 93.0])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model, winequality_vd):
         mmodel = model.to_memmodel()
         res = mmodel.predict([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
         res_py = model.to_python()([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
         vdf = winequality_vd.copy()
         vdf["prediction_sql"] = mmodel.predict_sql(
-            ["total_sulfur_dioxide", "residual_sugar", "alcohol"]
+            ["citric_acid", "residual_sugar", "alcohol"]
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="r2")
         assert score == pytest.approx(1.0)
 
     def test_get_predicts(self, winequality_vd, model):
         winequality_copy = winequality_vd.copy()
         model.predict(
             winequality_copy,
-            X=["total_sulfur_dioxide", "residual_sugar", "alcohol"],
+            X=["citric_acid", "residual_sugar", "alcohol"],
             name="predicted_quality",
         )
 
         assert winequality_copy["predicted_quality"].mean() == pytest.approx(
-            5.818377, abs=1e-6
+            5.8191136575806, abs=1e-6
         )
 
     def test_regression_report(self, model):
         reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
@@ -222,84 +236,86 @@
             "mean_squared_error",
             "root_mean_squared_error",
             "r2",
             "r2_adj",
             "aic",
             "bic",
         ]
-        assert reg_rep["value"][0] == pytest.approx(0.001302, abs=1e-6)
-        assert reg_rep["value"][1] == pytest.approx(3.189211, abs=1e-6)
-        assert reg_rep["value"][2] == pytest.approx(0.798061, abs=1e-6)
-        assert reg_rep["value"][3] == pytest.approx(0.684704, abs=1e-6)
-        assert reg_rep["value"][4] == pytest.approx(0.761464, abs=1e-6)
-        assert reg_rep["value"][5] == pytest.approx(0.8726193656049263, abs=1e-6)
-        assert reg_rep["value"][6] == pytest.approx(0.001302, abs=1e-6)
-        assert reg_rep["value"][7] == pytest.approx(0.0008407218505677161, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-1762.5020357252242, abs=1e-6)
-        assert reg_rep["value"][9] == pytest.approx(-1735.3918139111545, abs=1e-6)
+        assert reg_rep["value"][0] == pytest.approx(0.219641599658795, abs=1e-6)
+        assert reg_rep["value"][1] == pytest.approx(3.61156861855927, abs=1e-6)
+        assert reg_rep["value"][2] == pytest.approx(0.49469564704003, abs=1e-3)
+        assert reg_rep["value"][3] == pytest.approx(0.608521836351418, abs=1e-6)
+        assert reg_rep["value"][4] == pytest.approx(0.594990575399229, abs=1e-6)
+        assert reg_rep["value"][5] == pytest.approx(0.7713563219415707, abs=1e-6)
+        assert reg_rep["value"][6] == pytest.approx(0.219640889304706, abs=1e-6)
+        assert reg_rep["value"][7] == pytest.approx(0.21928033527235014, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-3365.29441626357, abs=1e-6)
+        assert reg_rep["value"][9] == pytest.approx(-3338.189123593071, abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
             6497.0,
             3,
-            pytest.approx(0.00130215624626484),
-            pytest.approx(0.0008407218505677161),
-            pytest.approx(0.9668703628035513),
-            pytest.approx(0.40727177684393245),
+            pytest.approx(0.219640889304706),
+            pytest.approx(0.21928033527235014),
+            pytest.approx(640.932567311251),
+            pytest.approx(0.0),
             pytest.approx(0.232322269343305),
             pytest.approx(0.189622693372695),
             pytest.approx(53.1115447611131),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
-            pytest.approx(2.21007321118539),
-            pytest.approx(4947.23522831515),
+            pytest.approx(1144.75129867412),
+            pytest.approx(3865.65376836879),
             pytest.approx(4953.68570109281),
         ]
         assert reg_rep_anova["MS"][:-1] == [
-            pytest.approx(0.73669107039513),
-            pytest.approx(0.7619336559857001),
+            pytest.approx(381.5837662247067),
+            pytest.approx(0.595357118184012),
         ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(3.189211, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(3.61156861855927, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0.684704, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0.608521836351418, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0.798061, abs=1e-6)
+        assert model.score(metric="median") == pytest.approx(0.49469564704003, abs=1e-3)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.761464557228739, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.594990575399229, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.8726193656049234, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.7713563219415712, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.003172, abs=1e-6)
+        assert model.score(metric="msle") == pytest.approx(
+            0.00251024411036473, abs=1e-6
+        )
         # method = "r2"
-        assert model.score(method="r2") == pytest.approx(0.001302, abs=1e-6)
+        assert model.score() == pytest.approx(0.219640889304706, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(
-            0.0008407218505677161, abs=1e-6
-        )
+        assert model.score(metric="r2a") == pytest.approx(0.21928033527235014, abs=1e-6)
         # method = "var"
-        assert model.score(method="var") == pytest.approx(0.001302, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(0.219641599658795, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-1762.5020357252242, abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-3365.29441626357, abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-1735.3918139111545, abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-3338.189123593071, abs=1e-6)
 
     def test_set_params(self, model):
         model.set_params({"max_iter": 1000})
 
         assert model.get_params()["max_iter"] == 1000
 
     def test_model_from_vDF(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS lasso_from_vDF")
-        model_test = Lasso("lasso_from_vDF",)
+        current_cursor().execute("DROP MODEL IF EXISTS lsvr_from_vDF")
+        model_test = LinearSVR(
+            "lsvr_from_vDF",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "quality")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lasso_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'lsvr_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "lasso_from_vDF"
+        assert current_cursor().fetchone()[0] == "lsvr_from_vDF"
 
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_linear_regression.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_linear_regression.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,102 +1,116 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Standard Python Modules
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
+from verticapy.tests.conftest import get_version
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality
 from verticapy.learn.linear_model import LinearRegression
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(winequality_vd):
-    model_class = LinearRegression("linreg_model_test",)
+    model_class = LinearRegression(
+        "linreg_model_test",
+    )
     model_class.drop()
     model_class.fit(
-        "public.winequality", ["citric_acid", "residual_sugar", "alcohol"], "quality"
+        "public.winequality",
+        ["citric_acid", "residual_sugar", "alcohol"],
+        "quality",
     )
     yield model_class
     model_class.drop()
 
 
 class TestLinearRegression:
     def test_repr(self, model):
-        assert "|coefficient|std_err |t_value |p_value" in model.__repr__()
-        model_repr = LinearRegression("lin_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<LinearRegression>"
+        assert model.__repr__() == "<LinearRegression>"
 
     def test_contour(self, winequality_vd):
-        model_test = LinearRegression("model_contour",)
+        model_test = LinearRegression(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            winequality_vd, ["citric_acid", "residual_sugar"], "quality",
+            winequality_vd,
+            ["citric_acid", "residual_sugar"],
+            "quality",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 32
         model_test.drop()
 
     def test_deploySQL(self, model):
         expected_sql = 'PREDICT_LINEAR_REG("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'linreg_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS linreg_model_test_drop")
-        model_test = LinearRegression("linreg_model_test_drop",)
+        model_test = LinearRegression(
+            "linreg_model_test_drop",
+        )
         model_test.fit("public.winequality", ["alcohol"], "quality")
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'linreg_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "linreg_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'linreg_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
         assert fim["index"] == ["alcohol", "residual_sugar", "citric_acid"]
         assert fim["importance"] == [52.25, 32.58, 15.17]
         assert fim["sign"] == [1, 1, 1]
         plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "details",
             "regularization",
             "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
@@ -108,15 +122,15 @@
             "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
         ]
         assert m_att["#_of_rows"] == [4, 1, 1, 1, 1, 1]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
         assert m_att_details["predictor"] == [
             "Intercept",
             "citric_acid",
             "residual_sugar",
             "alcohol",
         ]
@@ -124,64 +138,79 @@
         assert m_att_details["coefficient"][1] == pytest.approx(0.434204, abs=1e-6)
         assert m_att_details["coefficient"][2] == pytest.approx(0.023752, abs=1e-6)
         assert m_att_details["coefficient"][3] == pytest.approx(0.359921, abs=1e-6)
         assert m_att_details["std_err"][3] == pytest.approx(0.008608, abs=1e-6)
         assert m_att_details["t_value"][3] == pytest.approx(41.8089205202906, abs=1e-6)
         assert m_att_details["p_value"][3] == pytest.approx(0)
 
-        m_att_regularization = model.get_attr("regularization")
+        m_att_regularization = model.get_vertica_attributes("regularization")
 
         assert m_att_regularization["type"][0] == "none"
         assert m_att_regularization["lambda"][0] == 1
 
-        assert model.get_attr("iteration_count")["iteration_count"][0] == 1
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 6497
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "linear_reg('public.linreg_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='none', lambda=1, alpha=0.5)"
+            model.get_vertica_attributes("iteration_count")["iteration_count"][0] == 1
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 6497
         )
 
+        if get_version()[0] < 12:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.linreg_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='none', lambda=1, alpha=0.5)"
+            )
+        else:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "linear_reg('public.linreg_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='none', lambda=1, alpha=0.5, fit_intercept=true)"
+            )
+
     def test_get_params(self, model):
         assert model.get_params() == {
             "solver": "newton",
-            "penalty": "none",
             "max_iter": 100,
             "tol": 1e-06,
+            "fit_intercept": True,
         }
 
     def test_get_plot(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = LinearRegression("model_test_plot",)
+        model_test = LinearRegression(
+            "model_test_plot",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "quality")
         result = model_test.plot(color="r")
         assert len(result.get_default_bbox_extra_artists()) == 9
         plt.close("all")
         model_test.drop()
         model_test.fit(winequality_vd, ["alcohol", "residual_sugar"], "quality")
         result = model_test.plot(color="r")
         assert len(result.get_default_bbox_extra_artists()) == 3
         plt.close("all")
         model_test.drop()
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0])
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([3.0, 11.0, 93.0])
+                model.model_name, model.to_sql([3.0, 11.0, 93.0])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model, winequality_vd):
         mmodel = model.to_memmodel()
@@ -190,15 +219,15 @@
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
         vdf = winequality_vd.copy()
         vdf["prediction_sql"] = mmodel.predict_sql(
             ["citric_acid", "residual_sugar", "alcohol"]
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="r2")
         assert score == pytest.approx(1.0)
 
     def test_get_predicts(self, winequality_vd, model):
         winequality_copy = winequality_vd.copy()
         model.predict(
             winequality_copy,
             X=["citric_acid", "residual_sugar", "alcohol"],
@@ -206,15 +235,15 @@
         )
 
         assert winequality_copy["predicted_quality"].mean() == pytest.approx(
             5.818378, abs=1e-6
         )
 
     def test_regression_report(self, model):
-        reg_rep = model.regression_report("metrics")
+        reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
             "max_error",
             "median_absolute_error",
             "mean_absolute_error",
             "mean_squared_error",
@@ -228,73 +257,75 @@
         assert reg_rep["value"][1] == pytest.approx(3.592465, abs=1e-6)
         assert reg_rep["value"][2] == pytest.approx(0.496031, abs=1e-6)
         assert reg_rep["value"][3] == pytest.approx(0.609075, abs=1e-6)
         assert reg_rep["value"][4] == pytest.approx(0.594856, abs=1e-6)
         assert reg_rep["value"][5] == pytest.approx(0.7712695123858948, abs=1e-6)
         assert reg_rep["value"][6] == pytest.approx(0.219816, abs=1e-6)
         assert reg_rep["value"][7] == pytest.approx(0.21945605202370688, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-3366.7617912479104, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-3366.75686210436, abs=1e-6)
         assert reg_rep["value"][9] == pytest.approx(-3339.6515694338464, abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
             6497.0,
             3,
             pytest.approx(0.219816524906085),
             pytest.approx(0.21945605202370688),
             pytest.approx(609.8004472783862),
             pytest.approx(0.0),
             pytest.approx(0.232322269343305),
             pytest.approx(0.189622693372695),
             pytest.approx(53.1115447611131),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
             pytest.approx(1088.90197629059),
             pytest.approx(3864.78372480164),
             pytest.approx(4953.68570109281),
         ]
         assert reg_rep_anova["MS"][:-1] == [
             pytest.approx(362.9673254301967),
             pytest.approx(0.5952231210228923),
         ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(3.592465, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(3.592465, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0.609075, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0.609075, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0.496031, abs=1e-6)
+        assert model.score(metric="median") == pytest.approx(0.496031, abs=1e-6)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.594856660735976, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.594856660735976, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.7712695123858948, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.7712695123858948, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.002509, abs=1e-6)
+        assert model.score(metric="msle") == pytest.approx(0.002509, abs=1e-6)
         # method = "r2"
         assert model.score() == pytest.approx(0.219816, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(0.21945605202370688, abs=1e-6)
+        assert model.score(metric="r2a") == pytest.approx(0.21945605202370688, abs=1e-6)
         # method = "var"
-        assert model.score(method="var") == pytest.approx(0.219816, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(0.219816, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-3366.7617912479104, abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-3366.75686210436, abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-3339.6515694338464, abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-3339.6515694338464, abs=1e-6)
 
     def test_set_params(self, model):
         model.set_params({"max_iter": 1000})
 
         assert model.get_params()["max_iter"] == 1000
 
     def test_model_from_vDF(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS linreg_from_vDF")
-        model_test = LinearRegression("linreg_from_vDF",)
+        model_test = LinearRegression(
+            "linreg_from_vDF",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "quality")
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'linreg_from_vDF'"
         )
         assert current_cursor().fetchone()[0] == "linreg_from_vDF"
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_linear_svc.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_dummy_tree_classifier.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,394 +1,485 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-from verticapy import drop, set_option
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_titanic, load_winequality
-from verticapy.learn.svm import LinearSVC
+from verticapy import (
+    vDataFrame,
+    drop,
+    set_option,
+)
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_titanic, load_dataset_cl
+from verticapy.learn.tree import DummyTreeClassifier
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
+def dtc_data_vd():
+    dtc_data = load_dataset_cl(table_name="dtc_data", schema="public")
+    yield dtc_data
+    drop(name="public.dtc_data", method="table")
+
+
+@pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
-def winequality_vd():
-    winequality = load_winequality()
-    yield winequality
-    drop(name="public.winequality",)
+def model(dtc_data_vd):
+    current_cursor().execute("DROP MODEL IF EXISTS decision_tc_model_test")
 
+    current_cursor().execute(
+        "SELECT rf_classifier('decision_tc_model_test', 'public.dtc_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, TransPortation', mtry=4, ntree=1, max_breadth=1e9, sampling_size=1, max_depth=100, nbins=1000, seed=1, id_column='id')"
+    )
+
+    # I could use load_model but it is buggy
+    model_class = DummyTreeClassifier(
+        "decision_tc_model_test",
+    )
+    model_class.input_relation = "public.dtc_data"
+    model_class.test_relation = model_class.input_relation
+    model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
+    model_class.y = "TransPortation"
+    model_class._compute_attributes()
 
-@pytest.fixture(scope="module")
-def model(titanic_vd):
-    current_cursor().execute("DROP MODEL IF EXISTS lsvc_model_test")
-    model_class = LinearSVC("lsvc_model_test",)
-    model_class.fit("public.titanic", ["age", "fare"], "survived")
     yield model_class
     model_class.drop()
 
 
-class TestLinearSVC:
+class TestDummyTreeClassifier:
     def test_repr(self, model):
-        assert "predictor|coefficient" in model.__repr__()
-        model_repr = LinearSVC("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<LinearSVC>"
+        assert model.__repr__() == "<RandomForestClassifier>"
 
     def test_classification_report(self, model):
         cls_rep1 = model.classification_report().transpose()
 
-        assert cls_rep1["auc"][0] == pytest.approx(0.6933968844454788, 1e-2)
-        assert cls_rep1["prc_auc"][0] == pytest.approx(0.5976470350144453, 1e-2)
-        assert cls_rep1["accuracy"][0] == pytest.approx(0.6536144578313253, 1e-2)
-        assert cls_rep1["log_loss"][0] == pytest.approx(0.279724470067258, 1e-2)
-        assert cls_rep1["precision"][0] == pytest.approx(0.6916666666666667, 1e-2)
-        assert cls_rep1["recall"][0] == pytest.approx(0.21227621483375958, 1e-2)
-        assert cls_rep1["f1_score"][0] == pytest.approx(0.324853228962818, 1e-2)
-        assert cls_rep1["mcc"][0] == pytest.approx(0.22669555629341528, 1e-2)
-        assert cls_rep1["informedness"][0] == pytest.approx(0.151119190040371, 1e-2)
-        assert cls_rep1["markedness"][0] == pytest.approx(0.34006849315068477, 1e-2)
-        assert cls_rep1["csi"][0] == pytest.approx(0.1939252336448598, 1e-2)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.5, 1e-2)
-
-        cls_rep2 = model.classification_report(cutoff=0.2).transpose()
-
-        assert cls_rep2["cutoff"][0] == pytest.approx(0.2, 1e-2)
+        assert cls_rep1["auc"][0] == pytest.approx(1.0)
+        assert cls_rep1["prc_auc"][0] == pytest.approx(1.0)
+        assert cls_rep1["accuracy"][0] == pytest.approx(1.0)
+        assert cls_rep1["log_loss"][0] == pytest.approx(0.0)
+        assert cls_rep1["precision"][0] == pytest.approx(1.0)
+        assert cls_rep1["recall"][0] == pytest.approx(1.0)
+        assert cls_rep1["f1_score"][0] == pytest.approx(1.0)
+        assert cls_rep1["mcc"][0] == pytest.approx(1.0)
+        assert cls_rep1["informedness"][0] == pytest.approx(1.0)
+        assert cls_rep1["markedness"][0] == pytest.approx(1.0)
+        assert cls_rep1["csi"][0] == pytest.approx(1.0)
 
     def test_confusion_matrix(self, model):
         conf_mat1 = model.confusion_matrix()
 
-        assert conf_mat1[0][0] == 568
-        assert conf_mat1[0][1] == 308
-        assert conf_mat1[1][0] == 37
-        assert conf_mat1[1][1] == 83
+        assert list(conf_mat1[:, 0]) == [4, 0, 0]
+        assert list(conf_mat1[:, 1]) == [0, 3, 0]
+        assert list(conf_mat1[:, 2]) == [0, 0, 3]
 
         conf_mat2 = model.confusion_matrix(cutoff=0.2)
 
-        assert conf_mat2[0][0] == 0
-        assert conf_mat2[0][1] == 0
-        assert conf_mat2[1][0] == 605
-        assert conf_mat2[1][1] == 391
+        assert list(conf_mat2[:, 0]) == [4, 0, 0]
+        assert list(conf_mat2[:, 1]) == [0, 3, 0]
+        assert list(conf_mat2[:, 2]) == [0, 0, 3]
 
     def test_contour(self, titanic_vd):
-        model_test = LinearSVC("model_contour",)
+        model_test = DummyTreeClassifier(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 34
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = "PREDICT_SVM_CLASSIFIER(\"age\", \"fare\" USING PARAMETERS model_name = 'lsvc_model_test', type = 'probability', match_by_pos = 'true')"
+        expected_sql = 'PREDICT_RF_CLASSIFIER("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'decision_tc_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS lsvc_model_test_drop")
-        model_test = LinearSVC("lsvc_model_test_drop",)
-        model_test.fit("public.titanic", ["age", "fare"], "survived")
+        current_cursor().execute("DROP MODEL IF EXISTS decision_tc_model_test_drop")
+        model_test = DummyTreeClassifier(
+            "decision_tc_model_test_drop",
+        )
+        model_test.fit(
+            "public.dtc_data",
+            ["Gender", '"owned cars"', "cost", "income"],
+            "TransPortation",
+        )
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lsvc_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'decision_tc_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "lsvc_model_test_drop"
+        assert current_cursor().fetchone()[0] == "decision_tc_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lsvc_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'decision_tc_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        f_imp = model.features_importance()
+        f_imp = model.features_importance(show=False)
 
-        assert f_imp["index"] == ["fare", "age"]
-        assert f_imp["importance"] == [85.09, 14.91]
-        assert f_imp["sign"] == [1, -1]
+        assert f_imp["index"] == ["cost", "owned cars", "gender", "income"]
+        assert f_imp["importance"] == [75.76, 15.15, 9.09, 0.0]
+        assert f_imp["sign"] == [1, 1, 1, 0]
         plt.close("all")
 
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(nbins=1000)
+        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000, show=False)
 
-        assert lift_ch["decision_boundary"][10] == pytest.approx(0.01)
-        assert lift_ch["positive_prediction_ratio"][10] == pytest.approx(0.0)
+        assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
+        assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(1.0)
+        assert lift_ch["lift"][300] == pytest.approx(2.5)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(1.0)
-        assert lift_ch["lift"][900] == pytest.approx(1.0)
+        assert lift_ch["lift"][900] == pytest.approx(2.5)
         plt.close("all")
 
-    def test_get_plot(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = LinearSVC("model_test_plot",)
-        model_test.fit(winequality_vd, ["alcohol"], "good")
-        result = model_test.plot(color="r")
-        assert len(result.get_default_bbox_extra_artists()) == 11
-        plt.close("all")
+    def test_to_python(self, model, titanic_vd):
+        model_test = DummyTreeClassifier("rfc_python_test")
         model_test.drop()
-        model_test.fit(winequality_vd, ["alcohol", "residual_sugar"], "good")
-        result = model_test.plot(color="r")
-        assert len(result.get_default_bbox_extra_artists()) == 11
-        plt.close("all")
-        model_test.drop()
-        model_test.fit(
-            winequality_vd, ["alcohol", "residual_sugar", "fixed_acidity"], "good"
-        )
-        result = model_test.plot(color="r")
-        assert len(result.get_default_bbox_extra_artists()) == 5
-        plt.close("all")
-        model_test.drop()
-
-    def test_to_python(self, model):
+        model_test.fit(titanic_vd, ["age", "fare", "sex"], "embarked")
         current_cursor().execute(
-            "SELECT PREDICT_SVM_CLASSIFIER(3.0, 11.0 USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
-            )
+            "SELECT PREDICT_RF_CLASSIFIER(30.0, 45.0, 'male' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0]])[0]
-        )
+        assert prediction == model_test.to_python()([[30.0, 45.0, "male"]])[0]
         current_cursor().execute(
-            "SELECT PREDICT_SVM_CLASSIFIER(3.0, 11.0 USING PARAMETERS model_name = '{}', type='probability', class=1, match_by_pos=True)".format(
-                model.name
-            )
+            "SELECT PREDICT_RF_CLASSIFIER(30.0, 145.0, 'female' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_proba=True, return_str=False)([[3.0, 11.0]])[0][1]
+        assert prediction == model_test.to_python()([[30.0, 145.0, "female"]])[0]
+
+    def test_to_sql(self, model, titanic_vd):
+        model_test = DummyTreeClassifier("rfc_sql_test")
+        model_test.drop()
+        model_test.fit(titanic_vd, ["age", "fare", "sex"], "survived")
+        current_cursor().execute(
+            "SELECT PREDICT_RF_CLASSIFIER(* USING PARAMETERS model_name = 'rfc_sql_test', match_by_pos=True)::int, {}::int FROM (SELECT 30.0 AS age, 45.0 AS fare, 'male' AS sex) x".format(
+                model_test.to_sql()
+            )
         )
+        prediction = current_cursor().fetchone()
+        assert prediction[0] == pytest.approx(prediction[1])
+        model_test.drop()
 
-    def test_to_memmodel(self, model, titanic_vd):
+    def test_to_memmodel(self, model):
         mmodel = model.to_memmodel()
-        res = mmodel.predict([[3.0, 11.0], [11.0, 1.0]])
-        res_py = model.to_python()([[3.0, 11.0], [11.0, 1.0]])
+        res = mmodel.predict(
+            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
+        )
+        res_py = model.to_python()(
+            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
+        )
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
-        res = mmodel.predict_proba([[3.0, 11.0], [11.0, 1.0]])
-        res_py = model.to_python(return_proba=True)([[3.0, 11.0], [11.0, 1.0]])
+        res = mmodel.predict_proba(
+            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
+        )
+        res_py = model.to_python(return_proba=True)(
+            [["Male", 0, "Cheap", "Low"], ["Female", 3, "Expensive", "Hig"]]
+        )
         assert res[0][0] == res_py[0][0]
         assert res[0][1] == res_py[0][1]
+        assert res[0][2] == res_py[0][2]
         assert res[1][0] == res_py[1][0]
         assert res[1][1] == res_py[1][1]
-        vdf = titanic_vd.copy()
-        vdf["prediction_sql"] = mmodel.predict_sql(["age", "fare"])
-        vdf["prediction_proba_sql_0"] = mmodel.predict_proba_sql(["age", "fare"])[0]
-        vdf["prediction_proba_sql_1"] = mmodel.predict_proba_sql(["age", "fare"])[1]
-        model.predict(vdf, name="prediction_vertica_sql", cutoff=0.5)
-        model.predict_proba(vdf, pos_label=0, name="prediction_proba_vertica_sql_0")
-        model.predict_proba(vdf, pos_label=1, name="prediction_proba_vertica_sql_1")
+        assert res[1][2] == res_py[1][2]
+        vdf = vDataFrame("public.dtc_data")
+        vdf["prediction_sql"] = mmodel.predict_sql(
+            ['"Gender"', '"owned cars"', '"cost"', '"income"']
+        )
+        vdf["prediction_proba_sql_0"] = mmodel.predict_proba_sql(
+            ['"Gender"', '"owned cars"', '"cost"', '"income"']
+        )[0]
+        vdf["prediction_proba_sql_1"] = mmodel.predict_proba_sql(
+            ['"Gender"', '"owned cars"', '"cost"', '"income"']
+        )[1]
+        vdf["prediction_proba_sql_2"] = mmodel.predict_proba_sql(
+            ['"Gender"', '"owned cars"', '"cost"', '"income"']
+        )[2]
+        model.predict(vdf, name="prediction_vertica_sql")
+        model.predict_proba(
+            vdf, name="prediction_proba_vertica_sql_0", pos_label=model.classes_[0]
+        )
+        model.predict_proba(
+            vdf, name="prediction_proba_vertica_sql_1", pos_label=model.classes_[1]
+        )
+        model.predict_proba(
+            vdf, name="prediction_proba_vertica_sql_2", pos_label=model.classes_[2]
+        )
         score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
-
-    def test_to_sql(self, model):
-        current_cursor().execute(
-            "SELECT PREDICT_SVM_CLASSIFIER(3.0, 11.0 USING PARAMETERS model_name = '{}', match_by_pos=True), {}".format(
-                model.name, model.to_sql([3.0, 11.0])
-            )
+        score = vdf.score(
+            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", metric="r2"
         )
-        prediction = current_cursor().fetchone()
-        assert prediction[0] == pytest.approx(prediction[1])
+        assert score == pytest.approx(1.0)
 
-    def test_get_attr(self, model):
-        attr = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        attr = model.get_vertica_attributes()
         assert attr["attr_name"] == [
-            "details",
-            "accepted_row_count",
+            "tree_count",
             "rejected_row_count",
-            "iteration_count",
+            "accepted_row_count",
             "call_string",
+            "details",
         ]
         assert attr["attr_fields"] == [
-            "predictor, coefficient",
-            "accepted_row_count",
+            "tree_count",
             "rejected_row_count",
-            "iteration_count",
+            "accepted_row_count",
             "call_string",
+            "predictor, type",
         ]
-        assert attr["#_of_rows"] == [3, 1, 1, 1, 1]
+        assert attr["#_of_rows"] == [1, 1, 1, 1, 4]
 
-        details = model.get_attr("details")
-        assert details["predictor"] == ["Intercept", "age", "fare"]
-        assert details["coefficient"][0] == pytest.approx(-0.226679636751873)
-        assert details["coefficient"][1] == pytest.approx(-0.00661256493751514)
-        assert details["coefficient"][2] == pytest.approx(0.00587052591948468)
-
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 996
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 238
-        assert model.get_attr("iteration_count")["iteration_count"][0] == 6
+        details = model.get_vertica_attributes("details")
+        assert details["predictor"] == ["gender", "owned cars", "cost", "income"]
+        assert details["type"] == [
+            "char or varchar",
+            "int",
+            "char or varchar",
+            "char or varchar",
+        ]
+
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 1
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "SELECT svm_classifier('public.lsvc_model_test', 'public.titanic', '\"survived\"', '\"age\", \"fare\"'\nUSING PARAMETERS class_weights='1,1', C=1, max_iterations=100, intercept_mode='regularized', intercept_scaling=1, epsilon=0.0001);"
+            model.get_vertica_attributes("call_string")["call_string"][0]
+            == "SELECT rf_classifier('public.decision_tc_model_test', 'public.dtc_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, TransPortation', ntree=1, mtry=4, sampling_size=1, max_depth=100, max_breadth=1000000000, min_leaf_size=1, min_info_gain=0, nbins=1000);"
         )
 
     def test_get_params(self, model):
         params = model.get_params()
-
-        assert params == {
-            "tol": 0.0001,
-            "C": 1.0,
-            "max_iter": 100,
-            "fit_intercept": True,
-            "intercept_scaling": 1.0,
-            "intercept_mode": "regularized",
-            "class_weight": [1, 1],
-        }
+        assert params == {}
 
     def test_prc_curve(self, model):
-        prc = model.prc_curve(nbins=1000)
+        prc = model.prc_curve(pos_label="Car", nbins=1000, show=False)
 
-        assert prc["threshold"][10] == pytest.approx(0.009)
-        assert prc["recall"][10] == pytest.approx(1.0)
-        assert prc["precision"][10] == pytest.approx(0.392570281124498)
-        assert prc["threshold"][900] == pytest.approx(0.899)
-        assert prc["recall"][900] == pytest.approx(0.010230179028133)
-        assert prc["precision"][900] == pytest.approx(1.0)
+        assert prc["threshold"][300] == pytest.approx(0.299)
+        assert prc["recall"][300] == pytest.approx(1.0)
+        assert prc["precision"][300] == pytest.approx(1.0)
+        assert prc["threshold"][800] == pytest.approx(0.799)
+        assert prc["recall"][800] == pytest.approx(1.0)
+        assert prc["precision"][800] == pytest.approx(1.0)
         plt.close("all")
 
-    def test_predict(self, titanic_vd, model):
-        titanic_copy = titanic_vd.copy()
+    def test_predict(self, dtc_data_vd, model):
+        dtc_data_copy = dtc_data_vd.copy()
+
+        model.predict(dtc_data_copy, name="pred")
+        assert dtc_data_copy["pred"].mode() == "Bus"
 
-        model.predict(titanic_copy, name="pred_class1", cutoff=0.7)
-        assert titanic_copy["pred_class1"].sum() == 23.0
+        model.predict(dtc_data_copy, name="pred1", cutoff=0.7)
+        assert dtc_data_copy["pred1"].mode() == "Bus"
 
-        model.predict(titanic_copy, name="pred_class2", cutoff=0.3)
-        assert titanic_copy["pred_class2"].sum() == 996.0
+        model.predict(dtc_data_copy, name="pred2", cutoff=0.3)
+        assert dtc_data_copy["pred2"].mode() == "Bus"
 
-    def test_predict_proba(self, titanic_vd, model):
-        titanic_copy = titanic_vd.copy()
+    def test_predict_proba(self, dtc_data_vd, model):
+        dtc_data_copy = dtc_data_vd.copy()
 
-        model.predict_proba(titanic_copy, name="probability", pos_label=1)
-        assert titanic_copy["probability"].min() == pytest.approx(0.33841486903496)
+        model.predict_proba(dtc_data_copy, name="prob")
+        assert dtc_data_copy["prob_bus"].avg() == 0.4
+        assert dtc_data_copy["prob_train"].avg() == 0.3
+        assert dtc_data_copy["prob_car"].avg() == 0.3
+
+        model.predict_proba(dtc_data_copy, name="prob_bus_2", pos_label="Bus")
+        assert dtc_data_copy["prob_bus_2"].avg() == 0.4
 
     def test_roc_curve(self, model):
-        roc = model.roc_curve(nbins=1000)
+        roc = model.roc_curve(pos_label="Train", nbins=1000, show=False)
 
         assert roc["threshold"][100] == pytest.approx(0.1)
-        assert roc["false_positive"][100] == pytest.approx(1.0)
+        assert roc["false_positive"][100] == pytest.approx(0.0)
         assert roc["true_positive"][100] == pytest.approx(1.0)
         assert roc["threshold"][700] == pytest.approx(0.7)
-        assert roc["false_positive"][700] == pytest.approx(0.00661157024793388)
-        assert roc["true_positive"][700] == pytest.approx(0.0485933503836317)
+        assert roc["false_positive"][700] == pytest.approx(0.0)
+        assert roc["true_positive"][700] == pytest.approx(1.0)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(nbins=1000)
+        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000, show=False)
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
-        assert cutoff_curve["false_positive"][100] == pytest.approx(1.0)
+        assert cutoff_curve["false_positive"][100] == pytest.approx(0.0)
         assert cutoff_curve["true_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
-        assert cutoff_curve["false_positive"][700] == pytest.approx(0.00661157024793388)
-        assert cutoff_curve["true_positive"][700] == pytest.approx(0.0485933503836317)
+        assert cutoff_curve["false_positive"][700] == pytest.approx(0.0)
+        assert cutoff_curve["true_positive"][700] == pytest.approx(1.0)
         plt.close("all")
 
     def test_score(self, model):
-        assert model.score(cutoff=0.7, method="accuracy") == pytest.approx(
-            0.6224899598393574
-        )
-        assert model.score(cutoff=0.3, method="accuracy") == pytest.approx(
-            0.392570281124498
-        )
-        assert model.score(cutoff=0.7, method="auc") == pytest.approx(
-            0.6933968844454788
-        )
-        assert model.score(cutoff=0.3, method="auc") == pytest.approx(
-            0.6933968844454788
-        )
-        assert model.score(cutoff=0.7, method="best_cutoff") == pytest.approx(0.431)
-        assert model.score(cutoff=0.3, method="best_cutoff") == pytest.approx(0.431)
-        assert model.score(cutoff=0.7, method="bm") == pytest.approx(
-            0.041981780135697866
-        )
-        assert model.score(cutoff=0.3, method="bm") == pytest.approx(0.0)
-        assert model.score(cutoff=0.7, method="csi") == pytest.approx(
-            0.04810126582278481
-        )
-        assert model.score(cutoff=0.3, method="csi") == pytest.approx(0.392570281124498)
-        assert model.score(cutoff=0.7, method="f1") == pytest.approx(
-            0.09178743961352658
-        )
-        assert model.score(cutoff=0.3, method="f1") == pytest.approx(0.5638067772170151)
-        assert model.score(cutoff=0.7, method="logloss") == pytest.approx(
-            0.279724470067258
-        )
-        assert model.score(cutoff=0.3, method="logloss") == pytest.approx(
-            0.279724470067258
-        )
-        assert model.score(cutoff=0.7, method="mcc") == pytest.approx(
-            0.13649180522208684
-        )
-        assert model.score(cutoff=0.3, method="mcc") == pytest.approx(0.0)
-        assert model.score(cutoff=0.7, method="mk") == pytest.approx(
-            0.44376424326377406
-        )
-        assert model.score(cutoff=0.3, method="mk") == pytest.approx(-0.607429718875502)
-        assert model.score(cutoff=0.7, method="npv") == pytest.approx(
-            0.8260869565217391
-        )
-        assert model.score(cutoff=0.3, method="npv") == pytest.approx(0.392570281124498)
-        assert model.score(cutoff=0.7, method="prc_auc") == pytest.approx(
-            0.5976470350144453
-        )
-        assert model.score(cutoff=0.3, method="prc_auc") == pytest.approx(
-            0.5976470350144453
-        )
-        assert model.score(cutoff=0.7, method="precision") == pytest.approx(
-            0.8260869565217391
-        )
-        assert model.score(cutoff=0.3, method="precision") == pytest.approx(
-            0.392570281124498
-        )
-        assert model.score(cutoff=0.7, method="specificity") == pytest.approx(
-            0.9933884297520661
-        )
-        assert model.score(cutoff=0.3, method="specificity") == pytest.approx(0.0)
+        assert model.score(cutoff=0.9, metric="accuracy") == pytest.approx(1.0)
+        assert model.score(cutoff=0.1, metric="accuracy") == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.9, metric="auc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.1, metric="auc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.9, metric="best_cutoff", pos_label="Train"
+        ) == pytest.approx(0.999)
+        assert model.score(
+            cutoff=0.1, metric="best_cutoff", pos_label="Train"
+        ) == pytest.approx(0.999)
+        assert model.score(cutoff=0.9, metric="bm", pos_label="Train") == pytest.approx(
+            1.0
+        )
+        assert model.score(cutoff=0.1, metric="bm", pos_label="Train") == pytest.approx(
+            1.0
+        )
+        assert model.score(
+            cutoff=0.9, metric="csi", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.1, metric="csi", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="f1", pos_label="Train") == pytest.approx(
+            1.0
+        )
+        assert model.score(cutoff=0.1, metric="f1", pos_label="Train") == pytest.approx(
+            1.0
+        )
+        assert model.score(
+            cutoff=0.9, metric="logloss", pos_label="Train"
+        ) == pytest.approx(0.0)
+        assert model.score(
+            cutoff=0.1, metric="logloss", pos_label="Train"
+        ) == pytest.approx(0.0)
+        assert model.score(
+            cutoff=0.9, metric="mcc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.1, metric="mcc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="mk", pos_label="Train") == pytest.approx(
+            1.0
+        )
+        assert model.score(cutoff=0.1, metric="mk", pos_label="Train") == pytest.approx(
+            1.0
+        )
+        assert model.score(
+            cutoff=0.9, metric="npv", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.1, metric="npv", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.9, metric="prc_auc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.1, metric="prc_auc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.9, metric="precision", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.1, metric="precision", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.9, metric="specificity", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(
+            cutoff=0.1, metric="specificity", pos_label="Train"
+        ) == pytest.approx(1.0)
 
     def test_set_params(self, model):
-        model.set_params({"max_iter": 1000})
+        model.set_params({})
 
-        assert model.get_params()["max_iter"] == 1000
-
-    def test_model_from_vDF(self, titanic_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS lsvc_from_vDF")
-        model_test = LinearSVC("lsvc_from_vDF",)
-        model_test.fit(titanic_vd, ["age", "fare"], "survived")
+    def test_model_from_vDF(self, dtc_data_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS tc_from_vDF")
+        model_test = DummyTreeClassifier(
+            "tc_from_vDF",
+        )
+        model_test.fit(
+            dtc_data_vd,
+            ["Gender", '"owned cars"', "cost", "income"],
+            "TransPortation",
+        )
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lsvc_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'tc_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "lsvc_from_vDF"
+        assert current_cursor().fetchone()[0] == "tc_from_vDF"
 
         model_test.drop()
+
+    def test_to_graphviz(self, model):
+        gvz_tree_0 = model.to_graphviz(
+            tree_id=0,
+            classes_color=["red", "blue", "green"],
+            round_pred=4,
+            percent=True,
+            vertical=False,
+            node_style={"shape": "box", "style": "filled"},
+            arrow_style={"color": "blue"},
+            leaf_style={"shape": "circle", "style": "filled"},
+        )
+        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_0
+        assert "0 -> 1" in gvz_tree_0
+
+    def test_get_tree(self, model):
+        tree_1 = model.get_tree()
+
+        assert tree_1["prediction"] == [
+            None,
+            "Car",
+            None,
+            None,
+            "Train",
+            None,
+            "Bus",
+            "Bus",
+            "Train",
+        ]
+
+    def test_plot_tree(self, model):
+        result = model.plot_tree()
+        assert model.to_graphviz() == result.source.strip()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_linear_svr.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_decision_tree_regressor.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,213 +1,246 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-from verticapy import drop, set_option
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_winequality
-from verticapy.learn.svm import LinearSVR
+from verticapy import (
+    vDataFrame,
+    drop,
+    set_option,
+)
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_titanic, load_dataset_reg
+from verticapy.learn.tree import DecisionTreeRegressor
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
-def winequality_vd():
-    winequality = load_winequality()
-    yield winequality
-    drop(name="public.winequality",)
+def tr_data_vd():
+    tr_data = load_dataset_reg(table_name="tr_data", schema="public")
+    yield tr_data
+    drop(name="public.tr_data", method="table")
 
 
 @pytest.fixture(scope="module")
-def model(winequality_vd):
-    model_class = LinearSVR("lsvr_model_test",)
-    model_class.drop()
-    model_class.fit(
-        "public.winequality", ["citric_acid", "residual_sugar", "alcohol"], "quality"
+def model(tr_data_vd):
+    current_cursor().execute("DROP MODEL IF EXISTS tr_model_test")
+
+    current_cursor().execute(
+        "SELECT rf_regressor('tr_model_test', 'public.tr_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, transportation', mtry=4, ntree=1, max_breadth=100, sampling_size=1, max_depth=6, min_leaf_size=1, min_info_gain=0.0, nbins=40, seed=1, id_column='id')"
     )
+
+    # I could use load_model but it is buggy
+    model_class = DecisionTreeRegressor(
+        "tr_model_test",
+        max_features=4,
+        max_leaf_nodes=100,
+        max_depth=6,
+        min_samples_leaf=1,
+        min_info_gain=0.0,
+        nbins=40,
+    )
+    model_class.input_relation = "public.tr_data"
+    model_class.test_relation = model_class.input_relation
+    model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
+    model_class.y = '"TransPortation"'
+    model_class._compute_attributes()
+
     yield model_class
     model_class.drop()
 
 
-class TestLinearSVR:
+@pytest.fixture(scope="module")
+def titanic_vd():
+    titanic = load_titanic()
+    yield titanic
+    drop(
+        name="public.titanic",
+    )
+
+
+class TestDecisionTreeRegressor:
     def test_repr(self, model):
-        assert "predictor   |coefficient" in model.__repr__()
-        model_repr = LinearSVR("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<LinearSVR>"
+        assert model.__repr__() == "<RandomForestRegressor>"
 
-    def test_contour(self, winequality_vd):
-        model_test = LinearSVR("model_contour",)
+    def test_contour(self, titanic_vd):
+        model_test = DecisionTreeRegressor(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            winequality_vd, ["citric_acid", "residual_sugar"], "quality",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
-        assert len(result.get_default_bbox_extra_artists()) == 38
+        assert len(result.get_default_bbox_extra_artists()) == 34
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_SVM_REGRESSOR("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'lsvr_model_test\', match_by_pos = \'true\')'
+        expected_sql = 'PREDICT_RF_REGRESSOR("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'tr_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS lsvr_model_test_drop")
-        model_test = LinearSVR("lsvr_model_test_drop",)
-        model_test.fit("public.winequality", ["alcohol"], "quality")
+        current_cursor().execute("DROP MODEL IF EXISTS tr_model_test_drop")
+        model_test = DecisionTreeRegressor(
+            "tr_model_test_drop",
+        )
+        model_test.fit(
+            "public.tr_data",
+            ["Gender", '"owned cars"', "cost", "income"],
+            "TransPortation",
+        )
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lsvr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "lsvr_model_test_drop"
+        assert current_cursor().fetchone()[0] == "tr_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lsvr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
-        assert fim["index"] == ["alcohol", "residual_sugar", "citric_acid"]
-        assert fim["importance"] == [52.68, 33.27, 14.05]
-        assert fim["sign"] == [1, 1, 1]
+        assert fim["index"] == ["cost", "owned cars", "gender", "income"]
+        assert fim["importance"] == [88.41, 7.25, 4.35, 0.0]
+        assert fim["sign"] == [1, 1, 1, 0]
         plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
-            "details",
-            "accepted_row_count",
+            "tree_count",
             "rejected_row_count",
-            "iteration_count",
+            "accepted_row_count",
             "call_string",
+            "details",
         ]
         assert m_att["attr_fields"] == [
-            "predictor, coefficient",
-            "accepted_row_count",
+            "tree_count",
             "rejected_row_count",
-            "iteration_count",
+            "accepted_row_count",
             "call_string",
+            "predictor, type",
         ]
-        assert m_att["#_of_rows"] == [4, 1, 1, 1, 1]
+        assert m_att["#_of_rows"] == [1, 1, 1, 1, 4]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
         assert m_att_details["predictor"] == [
-            "Intercept",
-            "citric_acid",
-            "residual_sugar",
-            "alcohol",
+            "gender",
+            "owned cars",
+            "cost",
+            "income",
         ]
-        assert m_att_details["coefficient"][0] == pytest.approx(
-            1.67237120425236, abs=1e-6
-        )
-        assert m_att_details["coefficient"][1] == pytest.approx(
-            0.410055106076537, abs=1e-6
-        )
-        assert m_att_details["coefficient"][2] == pytest.approx(
-            0.0247255999942058, abs=1e-6
+        assert m_att_details["type"] == [
+            "char or varchar",
+            "int",
+            "char or varchar",
+            "char or varchar",
+        ]
+
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 1
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
         )
-        assert m_att_details["coefficient"][3] == pytest.approx(
-            0.369955366024044, abs=1e-6
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
         )
-
-        assert model.get_attr("iteration_count")["iteration_count"][0] == 5
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 6497
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "SELECT svm_regressor('public.lsvr_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS error_tolerance=0.1, C=1, max_iterations=100, intercept_mode='regularized', intercept_scaling=1, epsilon=0.0001);"
+            model.get_vertica_attributes("call_string")["call_string"][0]
+            == "SELECT rf_regressor('public.tr_model_test', 'public.tr_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, transportation', ntree=1, mtry=4, sampling_size=1, max_depth=6, max_breadth=100, min_leaf_size=1, min_info_gain=0, nbins=40);"
         )
 
     def test_get_params(self, model):
         assert model.get_params() == {
-            "tol": 0.0001,
-            "C": 1.0,
-            "max_iter": 100,
-            "fit_intercept": True,
-            "intercept_scaling": 1.0,
-            "intercept_mode": "regularized",
-            "acceptable_error_margin": 0.1,
+            "max_features": 4,
+            "max_leaf_nodes": 100,
+            "max_depth": 6,
+            "min_samples_leaf": 1,
+            "min_info_gain": 0,
+            "nbins": 40,
         }
 
-    def test_get_plot(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = LinearSVR("model_test_plot",)
-        model_test.fit("public.winequality", ["alcohol"], "quality")
-        result = model_test.plot()
-        assert len(result.get_default_bbox_extra_artists()) == 9
-        plt.close("all")
-        model_test.drop()
-
     def test_to_python(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_SVM_REGRESSOR(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+            "SELECT PREDICT_RF_REGRESSOR('Male', 0, 'Cheap', 'Low' USING PARAMETERS model_name = '{}', match_by_pos=True)::float".format(
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0]
+            model.to_python()([["Male", 0, "Cheap", "Low"]])[0]
         )
 
     def test_to_sql(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_SVM_REGRESSOR(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([3.0, 11.0, 93.0])
+            "SELECT PREDICT_RF_REGRESSOR(* USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float FROM (SELECT 'Male' AS \"Gender\", 0 AS \"owned cars\", 'Cheap' AS \"cost\", 'Low' AS \"income\") x".format(
+                model.model_name, model.to_sql()
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
-    def test_to_memmodel(self, model, winequality_vd):
+    def test_to_memmodel(self, model):
         mmodel = model.to_memmodel()
-        res = mmodel.predict([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
-        res_py = model.to_python()([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
+        res = mmodel.predict(
+            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
+        )
+        res_py = model.to_python()(
+            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
+        )
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
-        vdf = winequality_vd.copy()
+        vdf = vDataFrame("public.tr_data")
         vdf["prediction_sql"] = mmodel.predict_sql(
-            ["citric_acid", "residual_sugar", "alcohol"]
+            ['"Gender"', '"owned cars"', '"cost"', '"income"']
         )
         model.predict(vdf, name="prediction_vertica_sql")
         score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
         assert score == pytest.approx(1.0)
 
-    def test_get_predicts(self, winequality_vd, model):
-        winequality_copy = winequality_vd.copy()
+    def test_get_predicts(self, tr_data_vd, model):
+        tr_data_copy = tr_data_vd.copy()
         model.predict(
-            winequality_copy,
-            X=["citric_acid", "residual_sugar", "alcohol"],
+            tr_data_copy,
+            X=["Gender", '"owned cars"', "cost", "income"],
             name="predicted_quality",
         )
 
-        assert winequality_copy["predicted_quality"].mean() == pytest.approx(
-            5.8191136575806, abs=1e-6
-        )
+        assert tr_data_copy["predicted_quality"].mean() == pytest.approx(0.9, abs=1e-6)
 
     def test_regression_report(self, model):
         reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
             "max_error",
@@ -216,84 +249,117 @@
             "mean_squared_error",
             "root_mean_squared_error",
             "r2",
             "r2_adj",
             "aic",
             "bic",
         ]
-        assert reg_rep["value"][0] == pytest.approx(0.219641599658795, abs=1e-6)
-        assert reg_rep["value"][1] == pytest.approx(3.61156861855927, abs=1e-6)
-        assert reg_rep["value"][2] == pytest.approx(0.49469564704003, abs=1e-3)
-        assert reg_rep["value"][3] == pytest.approx(0.608521836351418, abs=1e-6)
-        assert reg_rep["value"][4] == pytest.approx(0.594990575399229, abs=1e-6)
-        assert reg_rep["value"][5] == pytest.approx(0.7713563219415707, abs=1e-6)
-        assert reg_rep["value"][6] == pytest.approx(0.219640889304706, abs=1e-6)
-        assert reg_rep["value"][7] == pytest.approx(0.21928033527235014, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-3365.2993454071307, abs=1e-6)
-        assert reg_rep["value"][9] == pytest.approx(-3338.189123593071, abs=1e-6)
+        assert reg_rep["value"][0] == pytest.approx(1.0, abs=1e-6)
+        assert reg_rep["value"][1] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][2] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][3] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][4] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][5] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][6] == pytest.approx(1.0, abs=1e-6)
+        assert reg_rep["value"][7] == pytest.approx(1.0, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-float("inf"), abs=1e-6)
+        assert reg_rep["value"][9] == pytest.approx(-float("inf"), abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
-            6497.0,
-            3,
-            pytest.approx(0.219640889304706),
-            pytest.approx(0.21928033527235014),
-            pytest.approx(640.932567311251),
+            10.0,
+            4,
+            pytest.approx(1.0),
+            pytest.approx(1.0),
+            float("inf"),
             pytest.approx(0.0),
-            pytest.approx(0.232322269343305),
-            pytest.approx(0.189622693372695),
-            pytest.approx(53.1115447611131),
+            pytest.approx(-1.73372940858763),
+            pytest.approx(0.223450528977454),
+            pytest.approx(3.76564442746721),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
-            pytest.approx(1144.75129867412),
-            pytest.approx(3865.65376836879),
-            pytest.approx(4953.68570109281),
+            pytest.approx(6.9),
+            pytest.approx(0.0),
+            pytest.approx(6.9),
         ]
         assert reg_rep_anova["MS"][:-1] == [
-            pytest.approx(381.5837662247067),
-            pytest.approx(0.595357118184012),
+            pytest.approx(1.725),
+            pytest.approx(0.0),
         ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(3.61156861855927, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(0, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0.608521836351418, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0.49469564704003, abs=1e-3)
+        assert model.score(metric="median") == pytest.approx(0, abs=1e-6)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.594990575399229, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.0, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.7713563219415712, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.0, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(
-            0.00251024411036473, abs=1e-6
-        )
+        assert model.score(metric="msle") == pytest.approx(0.0, abs=1e-6)
         # method = "r2"
-        assert model.score() == pytest.approx(0.219640889304706, abs=1e-6)
+        assert model.score() == pytest.approx(1.0, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(0.21928033527235014, abs=1e-6)
+        assert model.score(metric="r2a") == pytest.approx(1.0, abs=1e-6)
         # method = "var"
-        assert model.score(method="var") == pytest.approx(0.219641599658795, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(1.0, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-3365.2993454071307, abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-float("inf"), abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-3338.189123593071, abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-float("inf"), abs=1e-6)
 
     def test_set_params(self, model):
-        model.set_params({"max_iter": 1000})
+        model.set_params({"max_features": 1000})
 
-        assert model.get_params()["max_iter"] == 1000
+        assert model.get_params()["max_features"] == 1000
 
-    def test_model_from_vDF(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS lsvr_from_vDF")
-        model_test = LinearSVR("lsvr_from_vDF",)
-        model_test.fit(winequality_vd, ["alcohol"], "quality")
+    def test_model_from_vDF(self, tr_data_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS tr_from_vDF")
+        model_test = DecisionTreeRegressor(
+            "tr_from_vDF",
+        )
+        model_test.fit(tr_data_vd, ["gender"], "transportation")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'lsvr_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'tr_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "lsvr_from_vDF"
+        assert current_cursor().fetchone()[0] == "tr_from_vDF"
 
         model_test.drop()
+
+    def test_to_graphviz(self, model):
+        gvz_tree_0 = model.to_graphviz(
+            tree_id=0,
+            classes_color=["red", "blue", "green"],
+            round_pred=4,
+            percent=True,
+            vertical=False,
+            node_style={"shape": "box", "style": "filled"},
+            arrow_style={"color": "blue"},
+            leaf_style={"shape": "circle", "style": "filled"},
+        )
+        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_0
+        assert "0 -> 1" in gvz_tree_0
+
+    def test_get_tree(self, model):
+        tree_0 = model.get_tree()
+
+        assert tree_0["prediction"] == [
+            None,
+            "2.000000",
+            None,
+            None,
+            "1.000000",
+            None,
+            "0.000000",
+            "0.000000",
+            "1.000000",
+        ]
+
+    def test_plot_tree(self, model):
+        result = model.plot_tree()
+        assert model.to_graphviz() == result.source.strip()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_logistic_regression.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_logistic_regression.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,64 +1,72 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
+from verticapy.tests.conftest import get_version
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality, load_titanic
 from verticapy.learn.linear_model import LogisticRegression
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
-    model_class = LogisticRegression("logreg_model_test",)
+    model_class = LogisticRegression(
+        "logreg_model_test",
+    )
     model_class.drop()
     model_class.fit("public.titanic", ["age", "fare"], "survived")
     yield model_class
     model_class.drop()
 
 
 class TestLogisticRegression:
     def test_repr(self, model):
-        assert "predictor|coefficient|std_err" in model.__repr__()
-        model_repr = LogisticRegression("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<LogisticRegression>"
+        assert model.__repr__() == "<LogisticRegression>"
 
     def test_classification_report(self, model):
         cls_rep1 = model.classification_report().transpose()
 
         assert cls_rep1["auc"][0] == pytest.approx(0.6941239880788826)
         assert cls_rep1["prc_auc"][0] == pytest.approx(0.5979751713359676)
         assert cls_rep1["accuracy"][0] == pytest.approx(0.6586345381526104)
@@ -66,92 +74,95 @@
         assert cls_rep1["precision"][0] == pytest.approx(0.6758620689655173)
         assert cls_rep1["recall"][0] == pytest.approx(0.2506393861892583)
         assert cls_rep1["f1_score"][0] == pytest.approx(0.3656716417910448)
         assert cls_rep1["mcc"][0] == pytest.approx(0.2394674439996513)
         assert cls_rep1["informedness"][0] == pytest.approx(0.17295343577603517)
         assert cls_rep1["markedness"][0] == pytest.approx(0.3315612464038251)
         assert cls_rep1["csi"][0] == pytest.approx(0.2237442922374429)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.5)
-
-        cls_rep2 = model.classification_report(cutoff=0.2).transpose()
-
-        assert cls_rep2["cutoff"][0] == pytest.approx(0.2)
 
     def test_confusion_matrix(self, model):
         conf_mat1 = model.confusion_matrix()
 
         assert conf_mat1[0][0] == 558
-        assert conf_mat1[0][1] == 293
-        assert conf_mat1[1][0] == 47
+        assert conf_mat1[1][0] == 293
+        assert conf_mat1[0][1] == 47
         assert conf_mat1[1][1] == 98
 
         conf_mat2 = model.confusion_matrix(cutoff=0.2)
 
         assert conf_mat2[0][0] == 3
-        assert conf_mat2[0][1] == 0
-        assert conf_mat2[1][0] == 602
+        assert conf_mat2[1][0] == 0
+        assert conf_mat2[0][1] == 602
         assert conf_mat2[1][1] == 391
 
     def test_contour(self, titanic_vd):
-        model_test = LogisticRegression("model_contour",)
+        model_test = LogisticRegression(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 38
         model_test.drop()
 
     def test_deploySQL(self, model):
         expected_sql = "PREDICT_LOGISTIC_REG(\"age\", \"fare\" USING PARAMETERS model_name = 'logreg_model_test', type = 'probability', match_by_pos = 'true')"
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS logreg_model_test_drop")
-        model_test = LogisticRegression("logreg_model_test_drop",)
+        model_test = LogisticRegression(
+            "logreg_model_test_drop",
+        )
         model_test.fit("public.titanic", ["age", "fare"], "survived")
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'logreg_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "logreg_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'logreg_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        f_imp = model.features_importance()
+        f_imp = model.features_importance(show=False)
 
         assert f_imp["index"] == ["fare", "age"]
         assert f_imp["importance"] == [85.51, 14.49]
         assert f_imp["sign"] == [1, -1]
         plt.close("all")
 
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(nbins=1000)
+        lift_ch = model.lift_chart(nbins=1000, show=False)
 
         assert lift_ch["decision_boundary"][10] == pytest.approx(0.01)
         assert lift_ch["positive_prediction_ratio"][10] == pytest.approx(
             0.010230179028133
         )
         assert lift_ch["lift"][10] == pytest.approx(2.54731457800512)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(1.0)
         assert lift_ch["lift"][900] == pytest.approx(1.0)
         plt.close("all")
 
     def test_get_plot(self, winequality_vd):
         # 1D
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = LogisticRegression("model_test_plot",)
+        model_test = LogisticRegression(
+            "model_test_plot",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "good")
         result = model_test.plot(color="r")
         assert len(result.get_default_bbox_extra_artists()) == 11
         plt.close("all")
         model_test.drop()
         # 2D
         model_test.fit(winequality_vd, ["alcohol", "residual_sugar"], "good")
@@ -159,35 +170,37 @@
         assert len(result.get_default_bbox_extra_artists()) == 5
         plt.close("all")
         model_test.drop()
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT PREDICT_LOGISTIC_REG(3.0, 11.0 USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0]])[0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0]])[0])
         current_cursor().execute(
-            "SELECT PREDICT_LOGISTIC_REG(3.0, 11.0 USING PARAMETERS model_name = '{}', type='probability', class=1, match_by_pos=True)".format(
-                model.name
+            "SELECT PREDICT_LOGISTIC_REG(3.0, 11.0 USING PARAMETERS model_name = '{}', type='probability', match_by_pos=True)".format(
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
-            model.to_python(return_proba=True, return_str=False)([[3.0, 11.0]])[0][1]
+            model.to_python(
+                return_proba=True,
+            )([[3.0, 11.0]])[
+                0
+            ][1]
         )
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT PREDICT_LOGISTIC_REG(3.0, 11.0 USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([3.0, 11.0])
+                model.model_name, model.to_sql([3.0, 11.0])
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model, titanic_vd):
         mmodel = model.to_memmodel()
@@ -204,27 +217,27 @@
         vdf = titanic_vd.copy()
         vdf["prediction_sql"] = mmodel.predict_sql(["age", "fare"])
         vdf["prediction_proba_sql_0"] = mmodel.predict_proba_sql(["age", "fare"])[0]
         vdf["prediction_proba_sql_1"] = mmodel.predict_proba_sql(["age", "fare"])[1]
         model.predict(vdf, name="prediction_vertica_sql", cutoff=0.5)
         model.predict_proba(vdf, pos_label=0, name="prediction_proba_vertica_sql_0")
         model.predict_proba(vdf, pos_label=1, name="prediction_proba_vertica_sql_1")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
 
-    def test_get_attr(self, model):
-        attr = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        attr = model.get_vertica_attributes()
         assert attr["attr_name"] == [
             "details",
             "regularization",
             "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
@@ -235,53 +248,69 @@
             "iteration_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
         ]
         assert attr["#_of_rows"] == [3, 1, 1, 1, 1, 1]
 
-        details = model.get_attr("details")
+        details = model.get_vertica_attributes("details")
         assert details["predictor"] == ["Intercept", "age", "fare"]
         assert details["coefficient"][0] == pytest.approx(-0.477190254617772)
         assert details["coefficient"][1] == pytest.approx(-0.0152670631243078)
         assert details["coefficient"][2] == pytest.approx(0.0140086238717347)
         assert details["std_err"][0] == pytest.approx(0.157607831241612)
         assert details["std_err"][1] == pytest.approx(0.00487661936756958)
         assert details["std_err"][2] == pytest.approx(0.00183286928098778)
         assert details["z_value"][0] == pytest.approx(-3.02770649693316)
         assert details["z_value"][1] == pytest.approx(-3.13066531824005)
         assert details["z_value"][2] == pytest.approx(7.64300215898927)
         assert details["p_value"][0] == pytest.approx(0.00246417291934784)
         assert details["p_value"][1] == pytest.approx(0.00174410802172094)
         assert details["p_value"][2] == pytest.approx(2.99885239324552e-13)
 
-        reg = model.get_attr("regularization")
+        reg = model.get_vertica_attributes("regularization")
         assert reg["type"][0] == "none"
         assert reg["lambda"][0] == 1.0
 
-        assert model.get_attr("iteration_count")["iteration_count"][0] == 4
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 238
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 996
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "logistic_reg('public.logreg_model_test', 'public.titanic', '\"survived\"', '\"age\", \"fare\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='none', lambda=1, alpha=0.5)"
+            model.get_vertica_attributes("iteration_count")["iteration_count"][0] == 4
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 238
         )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 996
+        )
+
+        if get_version()[0] < 12:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "logistic_reg('public.logreg_model_test', 'public.titanic', '\"survived\"', '\"age\", \"fare\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='none', lambda=1, alpha=0.5)"
+            )
+        else:
+            assert (
+                model.get_vertica_attributes("call_string")["call_string"][0]
+                == "logistic_reg('public.logreg_model_test', 'public.titanic', '\"survived\"', '\"age\", \"fare\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='none', lambda=1, alpha=0.5, fit_intercept=true)"
+            )
 
     def test_get_params(self, model):
         params = model.get_params()
 
         assert params == {
             "solver": "newton",
             "penalty": "none",
             "max_iter": 100,
             "tol": 1e-06,
+            "fit_intercept": True,
         }
 
     def test_prc_curve(self, model):
-        prc = model.prc_curve(nbins=1000)
+        prc = model.prc_curve(nbins=1000, show=False)
 
         assert prc["threshold"][10] == pytest.approx(0.009)
         assert prc["recall"][10] == pytest.approx(1.0)
         assert prc["precision"][10] == pytest.approx(0.392570281124498)
         assert prc["threshold"][900] == pytest.approx(0.899)
         assert prc["recall"][900] == pytest.approx(0.0460358056265985)
         assert prc["precision"][900] == pytest.approx(0.818181818181818)
@@ -299,117 +328,117 @@
     def test_predict_proba(self, titanic_vd, model):
         titanic_copy = titanic_vd.copy()
 
         model.predict_proba(titanic_copy, name="probability", pos_label=1)
         assert titanic_copy["probability"].min() == pytest.approx(0.182718648793846)
 
     def test_roc_curve(self, model):
-        roc = model.roc_curve(nbins=1000)
+        roc = model.roc_curve(nbins=1000, show=False)
 
         assert roc["threshold"][100] == pytest.approx(0.1)
         assert roc["false_positive"][100] == pytest.approx(1.0)
         assert roc["true_positive"][100] == pytest.approx(1.0)
         assert roc["threshold"][900] == pytest.approx(0.9)
         assert roc["false_positive"][900] == pytest.approx(0.00661157024793388)
         assert roc["true_positive"][900] == pytest.approx(0.0434782608695652)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(nbins=1000)
+        cutoff_curve = model.cutoff_curve(nbins=1000, show=False)
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
         assert cutoff_curve["false_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["true_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["threshold"][900] == pytest.approx(0.9)
         assert cutoff_curve["false_positive"][900] == pytest.approx(0.00661157024793388)
         assert cutoff_curve["true_positive"][900] == pytest.approx(0.0434782608695652)
         plt.close("all")
 
     def test_score(self, model):
-        assert model.score(cutoff=0.7, method="accuracy") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="accuracy") == pytest.approx(
             0.6295180722891566
         )
-        assert model.score(cutoff=0.3, method="accuracy") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="accuracy") == pytest.approx(
             0.4929718875502008
         )
-        assert model.score(cutoff=0.7, method="auc") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="auc") == pytest.approx(
             0.6941239880788826
         )
-        assert model.score(cutoff=0.3, method="auc") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="auc") == pytest.approx(
             0.6941239880788826
         )
-        assert model.score(cutoff=0.7, method="best_cutoff") == pytest.approx(0.3602)
-        assert model.score(cutoff=0.3, method="best_cutoff") == pytest.approx(0.3602)
-        assert model.score(cutoff=0.7, method="bm") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="best_cutoff") == pytest.approx(0.3602)
+        assert model.score(cutoff=0.3, metric="best_cutoff") == pytest.approx(0.3602)
+        assert model.score(cutoff=0.7, metric="bm") == pytest.approx(
             0.07164507197057768
         )
-        assert model.score(cutoff=0.3, method="bm") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="bm") == pytest.approx(
             0.13453108156665472
         )
-        assert model.score(cutoff=0.7, method="csi") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="csi") == pytest.approx(
             0.09558823529411764
         )
-        assert model.score(cutoff=0.3, method="csi") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="csi") == pytest.approx(
             0.41415313225058004
         )
-        assert model.score(cutoff=0.7, method="f1") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="f1") == pytest.approx(
             0.17449664429530198
         )
-        assert model.score(cutoff=0.3, method="f1") == pytest.approx(0.5857260049220673)
-        assert model.score(cutoff=0.7, method="logloss") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="f1") == pytest.approx(0.5857260049220673)
+        assert model.score(cutoff=0.7, metric="logloss") == pytest.approx(
             0.271495668573431
         )
-        assert model.score(cutoff=0.3, method="logloss") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="logloss") == pytest.approx(
             0.271495668573431
         )
-        assert model.score(cutoff=0.7, method="mcc") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="mcc") == pytest.approx(
             0.15187785294188016
         )
-        assert model.score(cutoff=0.3, method="mcc") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="mcc") == pytest.approx(
             0.17543607019922353
         )
-        assert model.score(cutoff=0.7, method="mk") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="mk") == pytest.approx(
             0.32196048632218854
         )
-        assert model.score(cutoff=0.3, method="mk") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="mk") == pytest.approx(
             0.22877846790890288
         )
-        assert model.score(cutoff=0.7, method="npv") == pytest.approx(
-            0.6964285714285714
-        )
-        assert model.score(cutoff=0.3, method="npv") == pytest.approx(
-            0.4311594202898551
+        assert model.score(cutoff=0.7, metric="npv") == pytest.approx(0.625531914893617)
+        assert model.score(cutoff=0.3, metric="npv") == pytest.approx(
+            0.7976190476190477
         )
-        assert model.score(cutoff=0.7, method="prc_auc") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="prc_auc") == pytest.approx(
             0.5979751713359676
         )
-        assert model.score(cutoff=0.3, method="prc_auc") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="prc_auc") == pytest.approx(
             0.5979751713359676
         )
-        assert model.score(cutoff=0.7, method="precision") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="precision") == pytest.approx(
             0.6964285714285714
         )
-        assert model.score(cutoff=0.3, method="precision") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="precision") == pytest.approx(
             0.4311594202898551
         )
-        assert model.score(cutoff=0.7, method="specificity") == pytest.approx(
+        assert model.score(cutoff=0.7, metric="specificity") == pytest.approx(
             0.971900826446281
         )
-        assert model.score(cutoff=0.3, method="specificity") == pytest.approx(
+        assert model.score(cutoff=0.3, metric="specificity") == pytest.approx(
             0.22148760330578512
         )
 
     def test_set_params(self, model):
         model.set_params({"max_iter": 1000})
 
         assert model.get_params()["max_iter"] == 1000
 
     def test_model_from_vDF(self, titanic_vd):
         current_cursor().execute("DROP MODEL IF EXISTS logreg_from_vDF")
-        model_test = LogisticRegression("logreg_from_vDF",)
+        model_test = LogisticRegression(
+            "logreg_from_vDF",
+        )
         model_test.fit(titanic_vd, ["age", "fare"], "survived")
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'logreg_from_vDF'"
         )
         assert current_cursor().fetchone()[0] == "logreg_from_vDF"
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_mca.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_mca.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,57 +1,59 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_market
 from verticapy.learn.decomposition import MCA
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def market_vd():
     market = load_market()
     yield market
-    drop(name="public.market",)
+    drop(
+        name="public.market",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(market_vd):
-    model_class = MCA("mca_model_test",)
+    model_class = MCA(
+        "mca_model_test",
+    )
     model_class.drop()
     model_class.fit(market_vd.cdt())
     yield model_class
     model_class.drop()
 
 
 class TestMCA:
     def test_repr(self, model):
-        assert (
-            "index|                name                 |  mean  |   sd   "
-            in model.__repr__()
-        )
-        model_repr = MCA("mca_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<MCA>"
+        assert model.__repr__() == "<MCA>"
 
     def test_deploySQL(self, model):
         expected_sql = 'APPLY_PCA("Form_Boiled", "Form_Canned",'
         result_sql = model.deploySQL()
 
         assert expected_sql in result_sql
 
@@ -59,30 +61,32 @@
         expected_sql = 'APPLY_INVERSE_PCA("Form_Boiled", "Form_Canned",'
         result_sql = model.deployInverseSQL()
 
         assert expected_sql in result_sql
 
     def test_drop(self, market_vd):
         current_cursor().execute("DROP MODEL IF EXISTS mca_model_test_drop")
-        model_test = MCA("mca_model_test_drop",)
+        model_test = MCA(
+            "mca_model_test_drop",
+        )
         model_test.fit(market_vd.cdt())
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'mca_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "mca_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'mca_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "columns",
             "singular_values",
             "principal_components",
             "counters",
             "call_string",
@@ -92,15 +96,15 @@
             "index, value, explained_variance, accumulated_explained_variance",
             "index, PC1, PC2, PC3, PC4, PC5, PC6, PC7, PC8, PC9, PC10, PC11, PC12, PC13, PC14, PC15, PC16, PC17, PC18, PC19, PC20, PC21, PC22, PC23, PC24, PC25, PC26, PC27, PC28, PC29, PC30, PC31, PC32, PC33, PC34, PC35, PC36, PC37, PC38, PC39, PC40, PC41, PC42, PC43, PC44, PC45, PC46, PC47, PC48, PC49, PC50, PC51, PC52",
             "counter_name, counter_value",
             "call_string",
         ]
         assert m_att["#_of_rows"] == [52, 52, 52, 3, 1]
 
-        m_att_details = model.get_attr(attr_name="principal_components")
+        m_att_details = model.get_vertica_attributes(attr_name="principal_components")
 
         assert m_att_details["PC1"][0] == pytest.approx(-8.40681285066429e-18, abs=1)
         assert m_att_details["PC1"][1] == pytest.approx(6.69930488797486e-17, abs=1)
         assert m_att_details["PC1"][2] == pytest.approx(-8.57930855866453e-17, abs=1)
         assert m_att_details["PC2"][0] == pytest.approx(-0.00490495400370651, abs=1)
         assert m_att_details["PC2"][1] == pytest.approx(-0.00591611533838104, abs=1)
         assert m_att_details["PC2"][2] == pytest.approx(-0.00508231564856379, abs=1)
@@ -125,27 +129,27 @@
         result = model.plot_var(dimensions=(2, 3), method="cos2")
         assert len(result.get_default_bbox_extra_artists()) == 62
         result = model.plot_var(dimensions=(2, 3), method="contrib")
         assert len(result.get_default_bbox_extra_artists()) == 62
 
     def test_plot_contrib(self, model):
         result = model.plot_contrib()
-        assert len(result.get_default_bbox_extra_artists()) == 113
+        assert len(result.get_default_bbox_extra_artists()) == 114
         result = model.plot_contrib(dimension=2)
-        assert len(result.get_default_bbox_extra_artists()) == 113
+        assert len(result.get_default_bbox_extra_artists()) == 114
 
     def test_plot_cos2(self, model):
         result = model.plot_cos2()
-        assert len(result.get_default_bbox_extra_artists()) == 59
+        assert len(result.get_default_bbox_extra_artists()) == 111
         result = model.plot_cos2(dimensions=(2, 3))
-        assert len(result.get_default_bbox_extra_artists()) == 59
+        assert len(result.get_default_bbox_extra_artists()) == 111
 
     def test_plot_scree(self, model):
         result = model.plot_scree()
-        assert len(result.get_default_bbox_extra_artists()) == 112
+        assert len(result.get_default_bbox_extra_artists()) == 113
 
     def test_plot_circle(self, model):
         result = model.plot_circle()
         assert len(result.get_default_bbox_extra_artists()) == 114
         result = model.plot_circle(dimensions=(2, 3))
         assert len(result.get_default_bbox_extra_artists()) == 114
 
@@ -182,14 +186,16 @@
 
     def test_set_params(self, model):
         model.set_params({})
         assert model.get_params() == {}
 
     def test_model_from_vDF(self, market_vd):
         current_cursor().execute("DROP MODEL IF EXISTS mca_vDF")
-        model_test = MCA("mca_vDF",)
+        model_test = MCA(
+            "mca_vDF",
+        )
         model_test.fit(market_vd.cdt())
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'mca_vDF'"
         )
         assert current_cursor().fetchone()[0] == "mca_vDF"
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_memmodel.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_memmodel.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,73 +1,58 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop
 from verticapy.datasets import load_titanic
-from verticapy.learn.memmodel import memModel
+import verticapy.machine_learning.memmodel as mm
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
     drop(name="public.titanic")
 
 
-class Test_memModel:
-    def test_LinearRegression(self):
-        model = memModel(
-            "LinearRegression", {"coefficients": [0.5, 0.6], "intercept": 0.8}
-        )
+class Test_InMemoryModel:
+    def test_LinearModel(self):
+        model = mm.LinearModel(**{"coef": [0.5, 0.6], "intercept": 0.8})
         assert model.predict([[0.4, 0.5]])[0] == pytest.approx(1.3)
         assert model.predict_sql([0.4, 0.5]) == "0.8 + 0.5 * 0.4 + 0.6 * 0.5"
         attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.5
-        assert attributes["coefficients"][1] == 0.6
+        assert attributes["coef"][0] == 0.5
+        assert attributes["coef"][1] == 0.6
         assert attributes["intercept"] == 0.8
-        model.set_attributes({"coefficients": [0.4, 0.5]})
+        model.set_attributes(**{"coef": [0.4, 0.5]})
         attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.4
-        assert attributes["coefficients"][1] == 0.5
+        assert attributes["coef"][0] == 0.4
+        assert attributes["coef"][1] == 0.5
         assert attributes["intercept"] == 0.8
-        assert model.model_type_ == "LinearRegression"
+        assert model.object_type == "LinearModel"
 
-    def test_LinearSVR(self):
-        model = memModel("LinearSVR", {"coefficients": [0.5, 0.6], "intercept": 0.8})
-        assert model.predict([[0.4, 0.5]])[0] == pytest.approx(1.3)
-        assert model.predict_sql([0.4, 0.5]) == "0.8 + 0.5 * 0.4 + 0.6 * 0.5"
-        attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.5
-        assert attributes["coefficients"][1] == 0.6
-        assert attributes["intercept"] == 0.8
-        model.set_attributes({"coefficients": [0.4, 0.5]})
-        attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.4
-        assert attributes["coefficients"][1] == 0.5
-        assert attributes["intercept"] == 0.8
-        assert model.model_type_ == "LinearSVR"
-
-    def test_LogisticRegression(self):
-        model = memModel(
-            "LogisticRegression", {"coefficients": [0.5, 0.6], "intercept": 0.8}
-        )
+    def test_LinearModelClassifier(self):
+        model = mm.LinearModelClassifier(**{"coef": [0.5, 0.6], "intercept": 0.8})
         assert model.predict([[0.4, 0.5]])[0] == pytest.approx(1)
         assert (
             model.predict_sql([0.4, 0.5])
             == "((1 / (1 + EXP(- (0.8 + 0.5 * 0.4 + 0.6 * 0.5)))) > 0.5)::int"
         )
         predict_proba_val = model.predict_proba([[0.4, 0.5]])
         assert predict_proba_val[0][0] == pytest.approx(0.21416502)
@@ -77,57 +62,27 @@
             predict_proba_val_sql[0]
             == "1 - (1 / (1 + EXP(- (0.8 + 0.5 * 0.4 + 0.6 * 0.5))))"
         )
         assert (
             predict_proba_val_sql[1] == "1 / (1 + EXP(- (0.8 + 0.5 * 0.4 + 0.6 * 0.5)))"
         )
         attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.5
-        assert attributes["coefficients"][1] == 0.6
+        assert attributes["coef"][0] == 0.5
+        assert attributes["coef"][1] == 0.6
         assert attributes["intercept"] == 0.8
-        model.set_attributes({"coefficients": [0.4, 0.5]})
+        model.set_attributes(**{"coef": [0.4, 0.5]})
         attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.4
-        assert attributes["coefficients"][1] == 0.5
+        assert attributes["coef"][0] == 0.4
+        assert attributes["coef"][1] == 0.5
         assert attributes["intercept"] == 0.8
-        assert model.model_type_ == "LogisticRegression"
-
-    def test_LinearSVC(self):
-        model = memModel("LinearSVC", {"coefficients": [0.5, 0.6], "intercept": 0.8})
-        assert model.predict([[0.4, 0.5]])[0] == pytest.approx(1)
-        assert (
-            model.predict_sql([0.4, 0.5])
-            == "((1 / (1 + EXP(- (0.8 + 0.5 * 0.4 + 0.6 * 0.5)))) > 0.5)::int"
-        )
-        predict_proba_val = model.predict_proba([[0.4, 0.5]])
-        assert predict_proba_val[0][0] == pytest.approx(0.21416502)
-        assert predict_proba_val[0][1] == pytest.approx(0.78583498)
-        predict_proba_val_sql = model.predict_proba_sql([0.4, 0.5])
-        assert (
-            predict_proba_val_sql[0]
-            == "1 - (1 / (1 + EXP(- (0.8 + 0.5 * 0.4 + 0.6 * 0.5))))"
-        )
-        assert (
-            predict_proba_val_sql[1] == "1 / (1 + EXP(- (0.8 + 0.5 * 0.4 + 0.6 * 0.5)))"
-        )
-        attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.5
-        assert attributes["coefficients"][1] == 0.6
-        assert attributes["intercept"] == 0.8
-        model.set_attributes({"coefficients": [0.4, 0.5]})
-        attributes = model.get_attributes()
-        assert attributes["coefficients"][0] == 0.4
-        assert attributes["coefficients"][1] == 0.5
-        assert attributes["intercept"] == 0.8
-        assert model.model_type_ == "LinearSVC"
+        assert model.object_type == "LinearModelClassifier"
 
     def test_PCA(self):
-        model = memModel(
-            "PCA",
-            {"principal_components": [[0.4, 0.5], [0.3, 0.2]], "mean": [0.1, 0.3]},
+        model = mm.PCA(
+            **{"principal_components": [[0.4, 0.5], [0.3, 0.2]], "mean": [0.1, 0.3]},
         )
         transformation = model.transform([[0.4, 0.5]])
         assert transformation[0][0] == pytest.approx(0.18)
         assert transformation[0][1] == pytest.approx(0.19)
         transformation_sql = model.transform_sql([0.4, 0.5])
         assert transformation_sql[0] == "(0.4 - 0.1) * 0.4 + (0.5 - 0.3) * 0.3"
         assert transformation_sql[1] == "(0.4 - 0.1) * 0.5 + (0.5 - 0.3) * 0.2"
@@ -135,114 +90,108 @@
         assert attributes["principal_components"][0][0] == 0.4
         assert attributes["principal_components"][0][1] == 0.5
         assert attributes["principal_components"][1][0] == 0.3
         assert attributes["principal_components"][1][1] == 0.2
         assert attributes["mean"][0] == 0.1
         assert attributes["mean"][1] == 0.3
         model.set_attributes(
-            {"principal_components": [[0.1, 0.2], [0.7, 0.8]], "mean": [0.9, 0.8]}
+            **{"principal_components": [[0.1, 0.2], [0.7, 0.8]], "mean": [0.9, 0.8]}
         )
         attributes = model.get_attributes()
         assert attributes["principal_components"][0][0] == 0.1
         assert attributes["principal_components"][0][1] == 0.2
         assert attributes["principal_components"][1][0] == 0.7
         assert attributes["principal_components"][1][1] == 0.8
-        model = model.rotate()
+        model.rotate()
         attributes = model.get_attributes()
         assert attributes["principal_components"][0][0] == pytest.approx(0.05887149)
         assert attributes["principal_components"][0][1] == pytest.approx(0.21571775)
         assert attributes["principal_components"][1][0] == pytest.approx(0.01194755)
         assert attributes["principal_components"][1][1] == pytest.approx(1.06294744)
         assert attributes["mean"][0] == 0.9
         assert attributes["mean"][1] == 0.8
-        assert model.model_type_ == "PCA"
+        assert model.object_type == "PCA"
 
     def test_SVD(self):
-        model = memModel(
-            "SVD", {"vectors": [[0.4, 0.5], [0.3, 0.2]], "values": [0.1, 0.3]}
-        )
+        model = mm.SVD(**{"vectors": [[0.4, 0.5], [0.3, 0.2]], "values": [0.1, 0.3]})
         transformation = model.transform([[0.4, 0.5]])
         assert transformation[0][0] == pytest.approx(3.1)
         assert transformation[0][1] == pytest.approx(1.0)
         transformation_sql = model.transform_sql([0.4, 0.5])
         assert transformation_sql[0] == "0.4 * 0.4 / 0.1 + 0.5 * 0.3 / 0.1"
         assert transformation_sql[1] == "0.4 * 0.5 / 0.3 + 0.5 * 0.2 / 0.3"
         attributes = model.get_attributes()
         assert attributes["vectors"][0][0] == 0.4
         assert attributes["vectors"][0][1] == 0.5
         assert attributes["vectors"][1][0] == 0.3
         assert attributes["vectors"][1][1] == 0.2
         assert attributes["values"][0] == 0.1
         assert attributes["values"][1] == 0.3
         model.set_attributes(
-            {"vectors": [[0.1, 0.2], [0.7, 0.8]], "values": [0.9, 0.8]}
+            **{"vectors": [[0.1, 0.2], [0.7, 0.8]], "values": [0.9, 0.8]}
         )
         attributes = model.get_attributes()
         assert attributes["vectors"][0][0] == 0.1
         assert attributes["vectors"][0][1] == 0.2
         assert attributes["vectors"][1][0] == 0.7
         assert attributes["vectors"][1][1] == 0.8
         assert attributes["values"][0] == 0.9
         assert attributes["values"][1] == 0.8
-        assert model.model_type_ == "SVD"
+        assert model.object_type == "SVD"
 
-    def test_Normalizer(self):
-        model = memModel(
-            "Normalizer", {"values": [(0.4, 0.5), (0.3, 0.2)], "method": "minmax"}
+    def test_MinMaxScaler(self):
+        model = mm.MinMaxScaler(
+            **{
+                "min_": [0.4, 0.3],
+                "max_": [0.5, 0.2],
+            }
         )
         transformation = model.transform([[0.4, 0.5]])
         assert transformation[0][0] == pytest.approx(0.0)
         assert transformation[0][1] == pytest.approx(-2.0)
         transformation_sql = model.transform_sql([0.4, 0.5])
         assert transformation_sql[0] == "(0.4 - 0.4) / 0.09999999999999998"
         assert transformation_sql[1] == "(0.5 - 0.3) / -0.09999999999999998"
         attributes = model.get_attributes()
-        assert attributes["values"][0][0] == 0.4
-        assert attributes["values"][0][1] == 0.5
-        assert attributes["values"][1][0] == 0.3
-        assert attributes["values"][1][1] == 0.2
-        assert attributes["method"] == "minmax"
-        model.set_attributes({"method": "zscore"})
+        assert model.sub_[0] == 0.4
+        assert model.sub_[1] == 0.3
+        assert model.den_[0] == pytest.approx(0.1)
+        assert model.den_[1] == pytest.approx(-0.1)
+        assert model.object_type == "MinMaxScaler"
+
+    def test_StandardScaler(self):
+        model = mm.StandardScaler(
+            **{
+                "mean": [0.4, 0.3],
+                "std": [0.5, 0.2],
+            }
+        )
         transformation = model.transform([[0.4, 0.5]])
         assert transformation[0][0] == pytest.approx(0.0)
         assert transformation[0][1] == pytest.approx(1.0)
         transformation_sql = model.transform_sql([0.4, 0.5])
         assert transformation_sql[0] == "(0.4 - 0.4) / 0.5"
         assert transformation_sql[1] == "(0.5 - 0.3) / 0.2"
         attributes = model.get_attributes()
-        assert attributes["values"][0][0] == 0.4
-        assert attributes["values"][0][1] == 0.5
-        assert attributes["values"][1][0] == 0.3
-        assert attributes["values"][1][1] == 0.2
-        assert attributes["method"] == "zscore"
-        model.set_attributes({"method": "robust_zscore"})
-        transformation = model.transform([[0.4, 0.5]])
-        assert transformation[0][0] == pytest.approx(0.0)
-        assert transformation[0][1] == pytest.approx(1.0)
-        transformation_sql = model.transform_sql([0.4, 0.5])
-        assert transformation_sql[0] == "(0.4 - 0.4) / 0.5"
-        assert transformation_sql[1] == "(0.5 - 0.3) / 0.2"
+        model = mm.StandardScaler(
+            **{
+                "mean": [0.5, 0.6],
+                "std": [0.4, 0.3],
+            }
+        )
         attributes = model.get_attributes()
-        assert attributes["values"][0][0] == 0.4
-        assert attributes["values"][0][1] == 0.5
-        assert attributes["values"][1][0] == 0.3
-        assert attributes["values"][1][1] == 0.2
-        assert attributes["method"] == "robust_zscore"
-        model.set_attributes({"values": [(0.5, 0.6), (0.4, 0.3)]})
-        attributes = model.get_attributes()
-        assert attributes["values"][0][0] == 0.5
-        assert attributes["values"][0][1] == 0.6
-        assert attributes["values"][1][0] == 0.4
-        assert attributes["values"][1][1] == 0.3
-        assert model.model_type_ == "Normalizer"
+        assert model.sub_[0] == 0.5
+        assert model.sub_[1] == 0.6
+        assert model.den_[0] == 0.4
+        assert model.den_[1] == 0.3
+        assert model.object_type == "StandardScaler"
 
     def test_OneHotEncoder(self):
-        model = memModel(
-            "OneHotEncoder",
-            {
+        model = mm.OneHotEncoder(
+            **{
                 "categories": [["male", "female"], [1, 2, 3]],
                 "drop_first": False,
                 "column_naming": None,
             },
         )
         transformation = model.transform([["male", 1], ["female", 3]])
         assert transformation[0][0] == pytest.approx(1)
@@ -262,63 +211,61 @@
         assert (
             transformation_sql[0][1]
             == "(CASE WHEN 'male' = 'female' THEN 1 ELSE 0 END)"
         )
         assert transformation_sql[1][0] == "(CASE WHEN 1 = 1 THEN 1 ELSE 0 END)"
         assert transformation_sql[1][1] == "(CASE WHEN 1 = 2 THEN 1 ELSE 0 END)"
         assert transformation_sql[1][2] == "(CASE WHEN 1 = 3 THEN 1 ELSE 0 END)"
-        model.set_attributes({"drop_first": True})
+        model.set_attributes(**{"drop_first": True})
         transformation = model.transform([["male", 1], ["female", 3]])
         assert transformation[0][0] == pytest.approx(0)
         assert transformation[0][1] == pytest.approx(0)
         assert transformation[0][2] == pytest.approx(0)
         assert transformation[1][0] == pytest.approx(1)
         assert transformation[1][1] == pytest.approx(0)
         assert transformation[1][2] == pytest.approx(1)
         transformation_sql = model.transform_sql(["'male'", 1])
         assert (
             transformation_sql[0][0]
             == "(CASE WHEN 'male' = 'female' THEN 1 ELSE 0 END)"
         )
         assert transformation_sql[1][0] == "(CASE WHEN 1 = 2 THEN 1 ELSE 0 END)"
         assert transformation_sql[1][1] == "(CASE WHEN 1 = 3 THEN 1 ELSE 0 END)"
-        model.set_attributes({"column_naming": "indices"})
+        model.set_attributes(**{"column_naming": "indices"})
         transformation_sql = model.transform_sql(["sex", "pclass"])
         assert (
             transformation_sql[0][0]
             == "(CASE WHEN sex = 'female' THEN 1 ELSE 0 END) AS \"sex_1\""
         )
         assert (
             transformation_sql[1][0]
             == '(CASE WHEN pclass = 2 THEN 1 ELSE 0 END) AS "pclass_1"'
         )
         assert (
             transformation_sql[1][1]
             == '(CASE WHEN pclass = 3 THEN 1 ELSE 0 END) AS "pclass_2"'
         )
-        model.set_attributes({"column_naming": "values"})
+        model.set_attributes(**{"column_naming": "values"})
         transformation_sql = model.transform_sql(["sex", "pclass"])
         assert (
             transformation_sql[0][0]
             == "(CASE WHEN sex = 'female' THEN 1 ELSE 0 END) AS \"sex_female\""
         )
         assert (
             transformation_sql[1][0]
             == '(CASE WHEN pclass = 2 THEN 1 ELSE 0 END) AS "pclass_2"'
         )
         assert (
             transformation_sql[1][1]
             == '(CASE WHEN pclass = 3 THEN 1 ELSE 0 END) AS "pclass_3"'
         )
-        assert model.model_type_ == "OneHotEncoder"
+        assert model.object_type == "OneHotEncoder"
 
     def test_KMeans(self):
-        model = memModel(
-            "KMeans", {"clusters": [[0.5, 0.6], [1, 2], [100, 200]], "p": 2}
-        )
+        model = mm.KMeans(**{"clusters": [[0.5, 0.6], [1, 2], [100, 200]], "p": 2})
         assert model.predict([[0.2, 0.3]])[0] == 0
         assert model.predict([[2, 2]])[0] == 1
         assert model.predict([[100, 201]])[0] == 2
         assert (
             model.predict_sql([0.4, 0.5])
             == "CASE WHEN 0.4 IS NULL OR 0.5 IS NULL THEN NULL WHEN POWER(POWER(0.4 - 100.0, 2) + POWER(0.5 - 200.0, 2), 1 / 2) <= POWER(POWER(0.4 - 0.5, 2) + POWER(0.5 - 0.6, 2), 1 / 2) AND POWER(POWER(0.4 - 100.0, 2) + POWER(0.5 - 200.0, 2), 1 / 2) <= POWER(POWER(0.4 - 1.0, 2) + POWER(0.5 - 2.0, 2), 1 / 2) THEN 2 WHEN POWER(POWER(0.4 - 1.0, 2) + POWER(0.5 - 2.0, 2), 1 / 2) <= POWER(POWER(0.4 - 0.5, 2) + POWER(0.5 - 0.6, 2), 1 / 2) THEN 1 ELSE 0 END"
         )
@@ -356,25 +303,24 @@
             transform_val_sql[2]
             == "POWER(POWER(0.2 - 100.0, 2) + POWER(0.3 - 200.0, 2), 1 / 2)"
         )
         attributes = model.get_attributes()
         assert attributes["clusters"][0][0] == 0.5
         assert attributes["clusters"][0][1] == 0.6
         assert attributes["p"] == 2
-        model.set_attributes({"clusters": [[0.1, 0.2]], "p": 3})
+        model.set_attributes(**{"clusters": [[0.1, 0.2]], "p": 3})
         attributes = model.get_attributes()
         assert attributes["clusters"][0][0] == 0.1
         assert attributes["clusters"][0][1] == 0.2
         assert attributes["p"] == 3
-        assert model.model_type_ == "KMeans"
+        assert model.object_type == "KMeans"
 
     def test_NearestCentroid(self):
-        model = memModel(
-            "NearestCentroid",
-            {
+        model = mm.NearestCentroid(
+            **{
                 "clusters": [[0.5, 0.6], [1, 2], [100, 200]],
                 "p": 2,
                 "classes": ["a", "b", "c"],
             },
         )
         assert model.predict([[0.2, 0.3]])[0] == "a"
         assert model.predict([[2, 2]])[0] == "b"
@@ -420,29 +366,34 @@
         attributes = model.get_attributes()
         assert attributes["clusters"][0][0] == 0.5
         assert attributes["clusters"][0][1] == 0.6
         assert attributes["classes"][0] == "a"
         assert attributes["classes"][1] == "b"
         assert attributes["classes"][2] == "c"
         assert attributes["p"] == 2
-        model.set_attributes({"clusters": [[0.1, 0.2]], "p": 3})
+        model.set_attributes(**{"clusters": [[0.1, 0.2]], "p": 3})
         attributes = model.get_attributes()
         assert attributes["clusters"][0][0] == 0.1
         assert attributes["clusters"][0][1] == 0.2
         assert attributes["p"] == 3
-        assert model.model_type_ == "NearestCentroid"
+        assert model.object_type == "NearestCentroid"
 
     def test_BisectingKMeans(self):
-        model = memModel(
-            "BisectingKMeans",
-            {
-                "clusters": [[0.5, 0.6], [1, 2], [100, 200], [10, 700], [-100, -200]],
+        model = mm.BisectingKMeans(
+            **{
+                "clusters": [
+                    [0.5, 0.6],
+                    [1, 2],
+                    [100, 200],
+                    [10, 700],
+                    [-100, -200],
+                ],
                 "p": 2,
-                "left_child": [1, 3, None, None, None],
-                "right_child": [2, 4, None, None, None],
+                "children_left": [1, 3, None, None, None],
+                "children_right": [2, 4, None, None, None],
             },
         )
         assert model.predict([[0.2, 0.3]])[0] == 4
         assert model.predict([[2, 2]])[0] == 4
         assert model.predict([[100, 201]])[0] == 2
         assert (
             model.predict_sql([0.4, 0.5])
@@ -465,25 +416,24 @@
             transform_val_sql[2]
             == "POWER(POWER(0.2 - 100.0, 2) + POWER(0.3 - 200.0, 2), 1 / 2)"
         )
         attributes = model.get_attributes()
         assert attributes["clusters"][0][0] == 0.5
         assert attributes["clusters"][0][1] == 0.6
         assert attributes["p"] == 2
-        model.set_attributes({"clusters": [[0.1, 0.2]], "p": 3})
+        model.set_attributes(**{"clusters": [[0.1, 0.2]], "p": 3})
         attributes = model.get_attributes()
         assert attributes["clusters"][0][0] == 0.1
         assert attributes["clusters"][0][1] == 0.2
         assert attributes["p"] == 3
-        assert model.model_type_ == "BisectingKMeans"
+        assert model.object_type == "BisectingKMeans"
 
     def test_BinaryTreeRegressor(self):
-        model = memModel(
-            "BinaryTreeRegressor",
-            {
+        model = mm.BinaryTreeRegressor(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [None, None, 3, 11, 1993],
             },
         )
@@ -502,20 +452,19 @@
         assert attributes["children_right"][1] == 4
         assert attributes["feature"][0] == 0
         assert attributes["feature"][1] == 1
         assert attributes["threshold"][0] == "female"
         assert attributes["threshold"][1] == 30
         assert attributes["value"][2] == 3
         assert attributes["value"][3] == 11
-        assert model.model_type_ == "BinaryTreeRegressor"
+        assert model.object_type == "BinaryTreeRegressor"
 
     def test_BinaryTreeClassifier(self):
-        model = memModel(
-            "BinaryTreeClassifier",
-            {
+        model = mm.BinaryTreeClassifier(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [
                     None,
                     None,
@@ -553,24 +502,24 @@
         assert attributes["children_right"][1] == 4
         assert attributes["feature"][0] == 0
         assert attributes["feature"][1] == 1
         assert attributes["threshold"][0] == "female"
         assert attributes["threshold"][1] == 30
         assert attributes["value"][2][0] == 0.8
         assert attributes["value"][3][0] == 0.1
-        model.set_attributes({"classes": [0, 1, 2]})
+        model.set_attributes(**{"classes": [0, 1, 2]})
         attributes = model.get_attributes()
         assert attributes["classes"][0] == 0
         assert attributes["classes"][1] == 1
         assert attributes["classes"][2] == 2
-        assert model.model_type_ == "BinaryTreeClassifier"
+        assert model.object_type == "BinaryTreeClassifier"
 
-    def test_CHAID(self, titanic_vd):
-        tree = titanic_vd.chaid("survived", ["sex", "fare"]).attributes_["tree"]
-        model = memModel("CHAID", {"tree": tree, "classes": ["a", "b"]})
+    def test_NonBinaryTree(self, titanic_vd):
+        tree = titanic_vd.chaid("survived", ["sex", "fare"]).tree_
+        model = mm.NonBinaryTree(**{"tree": tree, "classes": ["a", "b"]})
         prediction = model.predict([["male", 100], ["female", 20], ["female", 50]])
         assert prediction[0] == "a"
         assert prediction[1] == "b"
         assert prediction[2] == "b"
         assert (
             model.predict_sql(["sex", "fare"])
             == "(CASE WHEN sex = 'female' THEN (CASE WHEN fare <= 127.6 THEN 'b' WHEN fare <= 255.2 THEN 'b' WHEN fare <= 382.8 THEN 'b' WHEN fare <= 638.0 THEN 'b' ELSE NULL END) WHEN sex = 'male' THEN (CASE WHEN fare <= 129.36 THEN 'a' WHEN fare <= 258.72 THEN 'a' WHEN fare <= 388.08 THEN 'a' WHEN fare <= 517.44 THEN 'b' ELSE NULL END) ELSE NULL END)"
@@ -582,59 +531,56 @@
         assert prediction[0][1] == pytest.approx(0.17870722)
         assert prediction[1][0] == pytest.approx(0.3042328)
         assert prediction[1][1] == pytest.approx(0.6957672)
         assert prediction[2][0] == pytest.approx(0.3042328)
         assert prediction[2][1] == pytest.approx(0.6957672)
         attributes = model.get_attributes()
         assert attributes["tree"]["chi2"] == pytest.approx(345.12775126385327)
-        assert not (attributes["tree"]["is_leaf"])
-        assert not (attributes["tree"]["split_is_numerical"])
+        assert not attributes["tree"]["is_leaf"]
+        assert not attributes["tree"]["split_is_numerical"]
         assert attributes["tree"]["split_predictor"] == '"sex"'
         assert attributes["tree"]["split_predictor_idx"] == 0
         assert attributes["tree"]["children"]["female"]["chi2"] == pytest.approx(
             10.472532457814179
         )
-        model.set_attributes({"classes": [0, 1]})
+        model.set_attributes(**{"classes": [0, 1]})
         attributes = model.get_attributes()
         assert attributes["classes"][0] == 0
         assert attributes["classes"][1] == 1
-        assert model.model_type_ == "CHAID"
+        assert model.object_type == "NonBinaryTree"
 
     def test_RandomForestRegressor(self):
-        model1 = memModel(
-            "BinaryTreeRegressor",
-            {
+        model1 = mm.BinaryTreeRegressor(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [None, None, 3, 11, 1993],
             },
         )
-        model2 = memModel(
-            "BinaryTreeRegressor",
-            {
+        model2 = mm.BinaryTreeRegressor(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [None, None, -3, -11, -1993],
             },
         )
-        model3 = memModel(
-            "BinaryTreeRegressor",
-            {
+        model3 = mm.BinaryTreeRegressor(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [None, None, 0, 3, 6],
             },
         )
-        model = memModel("RandomForestRegressor", {"trees": [model1, model2, model3]})
+        model = mm.RandomForestRegressor(**{"trees": [model1, model2, model3]})
         prediction = model.predict([["male", 100], ["female", 20], ["female", 50]])
         assert prediction[0] == pytest.approx(0.0)
         assert prediction[1] == pytest.approx(1.0)
         assert prediction[2] == pytest.approx(2.0)
         assert (
             model.predict_sql(["sex", "fare"])
             == "((CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 11 ELSE 1993 END) ELSE 3 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN -11 ELSE -1993 END) ELSE -3 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 3 ELSE 6 END) ELSE 0 END)) / 3"
@@ -646,69 +592,66 @@
         assert attributes["children_right"][1] == 4
         assert attributes["feature"][0] == 0
         assert attributes["feature"][1] == 1
         assert attributes["threshold"][0] == "female"
         assert attributes["threshold"][1] == 30
         assert attributes["value"][2] == 3
         assert attributes["value"][3] == 11
-        assert model.model_type_ == "RandomForestRegressor"
+        assert model.object_type == "RandomForestRegressor"
 
     def test_RandomForestClassifier(self):
-        model1 = memModel(
-            "BinaryTreeClassifier",
-            {
+        model1 = mm.BinaryTreeClassifier(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [
                     None,
                     None,
                     [0.8, 0.1, 0.1],
                     [0.1, 0.8, 0.1],
                     [0.1, 0.1, 0.8],
                 ],
                 "classes": ["a", "b", "c"],
             },
         )
-        model2 = memModel(
-            "BinaryTreeClassifier",
-            {
+        model2 = mm.BinaryTreeClassifier(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [
                     None,
                     None,
                     [0.7, 0.15, 0.15],
                     [0.2, 0.6, 0.2],
                     [0.2, 0.2, 0.6],
                 ],
                 "classes": ["a", "b", "c"],
             },
         )
-        model3 = memModel(
-            "BinaryTreeClassifier",
-            {
+        model3 = mm.BinaryTreeClassifier(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [
                     None,
                     None,
                     [0.3, 0.7, 0.0],
                     [0.0, 0.4, 0.6],
                     [0.9, 0.1, 0.0],
                 ],
                 "classes": ["a", "b", "c"],
             },
         )
-        model = memModel("RandomForestClassifier", {"trees": [model1, model2, model3]})
+        model = mm.RandomForestClassifier(**{"trees": [model1, model2, model3]})
         prediction = model.predict([["male", 100], ["female", 20], ["female", 50]])
         assert prediction[0] == "a"
         assert prediction[1] == "b"
         assert prediction[2] == "c"
         assert (
             model.predict_sql(["sex", "fare"])
             == "CASE WHEN sex IS NULL OR fare IS NULL THEN NULL WHEN ((CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 1.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 1.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 1.0 ELSE 0.0 END) ELSE 0.0 END)) / 3 >= ((CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 0.0 END) ELSE 1.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 0.0 END) ELSE 1.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 1.0 END) ELSE 0.0 END)) / 3 AND ((CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 1.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 1.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 1.0 ELSE 0.0 END) ELSE 0.0 END)) / 3 >= ((CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 1.0 ELSE 0.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 1.0 ELSE 0.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 0.0 END) ELSE 1.0 END)) / 3 THEN 'c' WHEN ((CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 1.0 ELSE 0.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 1.0 ELSE 0.0 END) ELSE 0.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 0.0 END) ELSE 1.0 END)) / 3 >= ((CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 0.0 END) ELSE 1.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 0.0 END) ELSE 1.0 END) + (CASE WHEN sex = 'female' THEN (CASE WHEN fare < 30 THEN 0.0 ELSE 1.0 END) ELSE 0.0 END)) / 3 THEN 'b' ELSE 'a' END"
@@ -745,50 +688,46 @@
         assert attributes["children_right"][1] == 4
         assert attributes["feature"][0] == 0
         assert attributes["feature"][1] == 1
         assert attributes["threshold"][0] == "female"
         assert attributes["threshold"][1] == 30
         assert attributes["value"][2][0] == 0.8
         assert attributes["value"][3][0] == 0.1
-        assert model.model_type_ == "RandomForestClassifier"
+        assert model.object_type == "RandomForestClassifier"
 
-    def test_XGBoostRegressor(self):
-        model1 = memModel(
-            "BinaryTreeRegressor",
-            {
+    def test_XGBRegressor(self):
+        model1 = mm.BinaryTreeRegressor(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [None, None, 3, 11, 1993],
             },
         )
-        model2 = memModel(
-            "BinaryTreeRegressor",
-            {
+        model2 = mm.BinaryTreeRegressor(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [None, None, -3, -11, -1993],
             },
         )
-        model3 = memModel(
-            "BinaryTreeRegressor",
-            {
+        model3 = mm.BinaryTreeRegressor(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [None, None, 0, 3, 6],
             },
         )
-        model = memModel(
-            "XGBoostRegressor",
-            {"trees": [model1, model2, model3], "learning_rate": 0.1, "mean": 1.0},
+        model = mm.XGBRegressor(
+            **{"trees": [model1, model2, model3], "eta": 0.1, "mean": 1.0},
         )
         prediction = model.predict([["male", 100], ["female", 20], ["female", 50]])
         assert prediction[0] == pytest.approx(1.0)
         assert prediction[1] == pytest.approx(1.3)
         assert prediction[2] == pytest.approx(1.6)
         assert (
             model.predict_sql(["sex", "fare"])
@@ -802,77 +741,73 @@
         assert attributes["feature"][0] == 0
         assert attributes["feature"][1] == 1
         assert attributes["threshold"][0] == "female"
         assert attributes["threshold"][1] == 30
         assert attributes["value"][2] == 3
         assert attributes["value"][3] == 11
         attributes = model.get_attributes()
-        assert attributes["learning_rate"] == 0.1
+        assert attributes["eta"] == 0.1
         assert attributes["mean"] == 1.0
-        model.set_attributes({"learning_rate": 0.2, "mean": 2.0})
+        model.set_attributes(**{"eta": 0.2, "mean": 2.0})
         attributes = model.get_attributes()
-        assert attributes["learning_rate"] == 0.2
+        assert attributes["eta"] == 0.2
         assert attributes["mean"] == 2.0
-        assert model.model_type_ == "XGBoostRegressor"
+        assert model.object_type == "XGBRegressor"
 
-    def test_XGBoostClassifier(self):
-        model1 = memModel(
-            "BinaryTreeClassifier",
-            {
+    def test_XGBClassifier(self):
+        model1 = mm.BinaryTreeClassifier(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [
                     None,
                     None,
                     [0.8, 0.1, 0.1],
                     [0.1, 0.8, 0.1],
                     [0.1, 0.1, 0.8],
                 ],
                 "classes": ["a", "b", "c"],
             },
         )
-        model2 = memModel(
-            "BinaryTreeClassifier",
-            {
+        model2 = mm.BinaryTreeClassifier(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [
                     None,
                     None,
                     [0.7, 0.15, 0.15],
                     [0.2, 0.6, 0.2],
                     [0.2, 0.2, 0.6],
                 ],
                 "classes": ["a", "b", "c"],
             },
         )
-        model3 = memModel(
-            "BinaryTreeClassifier",
-            {
+        model3 = mm.BinaryTreeClassifier(
+            **{
                 "children_left": [1, 3, None, None, None],
                 "children_right": [2, 4, None, None, None],
                 "feature": [0, 1, None, None, None],
                 "threshold": ["female", 30, None, None, None],
                 "value": [
                     None,
                     None,
                     [0.3, 0.7, 0.0],
                     [0.0, 0.4, 0.6],
                     [0.9, 0.1, 0.0],
                 ],
                 "classes": ["a", "b", "c"],
             },
         )
-        model = memModel(
-            "XGBoostClassifier",
-            {
+        model = mm.XGBClassifier(
+            **{
                 "trees": [model1, model2, model3],
                 "learning_rate": 0.1,
                 "logodds": [0.1, 0.12, 0.15],
             },
         )
         prediction = model.predict([["male", 100], ["female", 20], ["female", 50]])
         assert prediction[0] == "a"
@@ -914,20 +849,19 @@
         assert attributes["children_right"][1] == 4
         assert attributes["feature"][0] == 0
         assert attributes["feature"][1] == 1
         assert attributes["threshold"][0] == "female"
         assert attributes["threshold"][1] == 30
         assert attributes["value"][2][0] == 0.8
         assert attributes["value"][3][0] == 0.1
-        assert model.model_type_ == "XGBoostClassifier"
+        assert model.object_type == "XGBClassifier"
 
     def test_NaiveBayes(self):
-        model = memModel(
-            "NaiveBayes",
-            {
+        model = mm.NaiveBayes(
+            **{
                 "attributes": [
                     {
                         "type": "gaussian",
                         "C": {"mu": 63.9878308300395, "sigma_sq": 7281.87598377196},
                         "Q": {"mu": 13.0217386792453, "sigma_sq": 211.626862330204},
                         "S": {"mu": 27.6928120412844, "sigma_sq": 1428.57067393938},
                     },
@@ -941,17 +875,26 @@
                         "type": "bernoulli",
                         "C": 0.771666666666667,
                         "Q": 0.910714285714286,
                         "S": 0.878216123499142,
                     },
                     {
                         "type": "categorical",
-                        "C": {"female": 0.407843137254902, "male": 0.592156862745098},
-                        "Q": {"female": 0.416666666666667, "male": 0.583333333333333},
-                        "S": {"female": 0.406666666666667, "male": 0.593333333333333},
+                        "C": {
+                            "female": 0.407843137254902,
+                            "male": 0.592156862745098,
+                        },
+                        "Q": {
+                            "female": 0.416666666666667,
+                            "male": 0.583333333333333,
+                        },
+                        "S": {
+                            "female": 0.406666666666667,
+                            "male": 0.593333333333333,
+                        },
                     },
                 ],
                 "classes": ["C", "Q", "S"],
                 "prior": [0.8, 0.1, 0.1],
             },
         )
         prediction = model.predict(
@@ -1004,8 +947,8 @@
         assert attributes["classes"][0] == "C"
         assert attributes["classes"][1] == "Q"
         assert attributes["classes"][2] == "S"
         assert attributes["attributes"][0]["type"] == "gaussian"
         assert attributes["attributes"][1]["type"] == "multinomial"
         assert attributes["attributes"][2]["type"] == "bernoulli"
         assert attributes["attributes"][3]["type"] == "categorical"
-        assert model.model_type_ == "NaiveBayes"
+        assert model.object_type == "NaiveBayes"
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_model_selection.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_model_selection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,30 +1,36 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 import verticapy
-from verticapy import vDataFrame, set_option
-from verticapy.connect import current_cursor
+from verticapy import drop
+from verticapy.core.vdataframe.base import vDataFrame
+from verticapy._config.config import set_option
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic, load_amazon, load_winequality
 from verticapy.learn.model_selection import *
 from verticapy.learn.linear_model import *
 from verticapy.learn.naive_bayes import *
 from verticapy.learn.ensemble import *
 from verticapy.learn.tree import *
 from verticapy.learn.svm import *
@@ -95,42 +101,43 @@
             "good",
             "auc",
             cv=3,
             training_score=True,
         )
         assert result2[0]["auc"][3] == pytest.approx(0.7604040062168419, 5e-1)
         assert result2[1]["auc"][3] == pytest.approx(0.7749948214599245, 5e-1)
-        result3 = cross_validate(
-            NaiveBayes("model_test"),
-            "public.winequality",
-            ["residual_sugar", "alcohol"],
-            "quality",
-            "auc",
-            cv=3,
-            training_score=True,
-            pos_label=7,
-        )
-        assert result3[0]["auc"][3] == pytest.approx(0.7405650946597986, 5e-1)
-        assert result3[1]["auc"][3] == pytest.approx(0.7386519406866139, 5e-1)
+        # result3 = cross_validate(
+        #    NaiveBayes("model_test"),
+        #    "public.winequality",
+        #    ["residual_sugar", "alcohol"],
+        #    "quality",
+        #    "auc",
+        #    cv=3,
+        #    training_score=True,
+        #    pos_label=7,
+        # )
+        # assert result3[0]["auc"][3] == pytest.approx(0.7405650946597986, 5e-1)
+        # assert result3[1]["auc"][3] == pytest.approx(0.7386519406866139, 5e-1)
 
     def test_enet_search_cv(self, titanic_vd):
         result = enet_search_cv(titanic_vd, ["age", "fare"], "survived", small=True)
         assert len(result["parameters"]) == 19
 
+    @pytest.mark.skip(reason="needs some investigation")
     def test_bayesian_search_cv(self, titanic_vd):
         model = LinearRegression("LR_bs_test")
         model.drop()
         result = bayesian_search_cv(model, titanic_vd, ["age", "fare"], "survived")
         assert len(result["parameters"]) == 25
         model = NaiveBayes("NB_bs_test")
         model.drop()
         result = bayesian_search_cv(
             model, titanic_vd, ["age", "fare"], "embarked", pos_label="C", lmax=4
         )
-        assert len(result["parameters"]) == 14
+        assert 12 <= len(result["parameters"]) <= 14
 
     def test_randomized_features_search_cv(self, titanic_vd):
         model = LogisticRegression("Logit_fs_test")
         model.drop()
         result = randomized_features_search_cv(
             model, titanic_vd, ["age", "fare", "pclass"], "survived"
         )
@@ -138,26 +145,28 @@
 
     def test_elbow(self, winequality_vd):
         result = elbow(
             "public.winequality",
             ["residual_sugar", "alcohol"],
             n_cluster=(1, 5),
             init="kmeanspp",
+            show=False,
         )
         plt.close("all")
-        assert result["Within-Cluster SS"][0] == pytest.approx(0.0)
-        assert len(result["Within-Cluster SS"]) == 4
+        assert result["elbow_score"][0] == pytest.approx(0.0)
+        assert len(result["elbow_score"]) == 4
         result2 = elbow(
             winequality_vd,
             ["residual_sugar", "alcohol"],
             n_cluster=(1, 5),
             init="kmeanspp",
+            show=False,
         )
-        assert result2["Within-Cluster SS"][0] == pytest.approx(0.0)
-        assert len(result2["Within-Cluster SS"]) == 4
+        assert result2["elbow_score"][0] == pytest.approx(0.0)
+        assert len(result2["elbow_score"]) == 4
         plt.close("all")
 
     def test_gen_params_grid(self):
         assert len(gen_params_grid(LogisticRegression("model_test"), lmax=3)) == 3
         assert len(gen_params_grid(LinearSVC("model_test"), lmax=3)) == 3
         assert len(gen_params_grid(LinearSVR("model_test"), lmax=3)) == 3
         assert len(gen_params_grid(ElasticNet("model_test"), lmax=3)) == 3
@@ -169,20 +178,16 @@
             len(gen_params_grid(RandomForestClassifier("model_test"), lmax=3, nbins=3))
             == 3
         )
         assert (
             len(gen_params_grid(RandomForestRegressor("model_test"), lmax=3, nbins=3))
             == 3
         )
-        assert (
-            len(gen_params_grid(XGBoostClassifier("model_test"), lmax=3, nbins=3)) == 3
-        )
-        assert (
-            len(gen_params_grid(XGBoostRegressor("model_test"), lmax=3, nbins=3)) == 3
-        )
+        assert len(gen_params_grid(XGBClassifier("model_test"), lmax=3, nbins=3)) == 3
+        assert len(gen_params_grid(XGBRegressor("model_test"), lmax=3, nbins=3)) == 3
         assert (
             len(gen_params_grid(DecisionTreeRegressor("model_test"), lmax=3, nbins=3))
             == 3
         )
         assert (
             len(gen_params_grid(DecisionTreeClassifier("model_test"), lmax=3, nbins=3))
             == 3
@@ -204,31 +209,33 @@
         assert (
             len(gen_params_grid(LocalOutlierFactor("model_test"), lmax=3, nbins=3)) == 3
         )
 
     def test_grid_search_cv(self, winequality_vd):
         result = grid_search_cv(
             LogisticRegression("model_test"),
-            {"solver": ["Newton", "BFGS", "CGD"], "tol": [0.1, 0.01]},
+            {"solver": ["newton", "bfgs"], "tol": [0.1, 0.01]},
             winequality_vd,
             ["residual_sugar", "alcohol"],
             "good",
             "auc",
             cv=3,
         )
         assert len(result.values) == 6
-        assert len(result["parameters"]) == 6
+        assert len(result["parameters"]) == 4
 
     def test_lift_chart(self, winequality_vd):
         model = LogisticRegression("model_test")
         model.drop()
         model.fit("public.winequality", ["residual_sugar", "alcohol"], "good")
         data = winequality_vd.copy()
         data = model.predict(data, name="prediction")
-        result = lift_chart("good", "prediction", data, pos_label=1, nbins=30,)
+        result = lift_chart(
+            "good", "prediction", data, pos_label=1, nbins=30, show=False
+        )
         assert result["lift"][0] == pytest.approx(2.95543129990643)
         assert len(result["lift"]) == 31
         model.drop()
         plt.close("all")
 
     def test_parameter_grid(self):
         assert parameter_grid({"param1": [1, 2, 3], "param2": ["a", "b", "c"]}) == [
@@ -240,36 +247,40 @@
             {"param1": 2, "param2": "c"},
             {"param1": 3, "param2": "a"},
             {"param1": 3, "param2": "b"},
             {"param1": 3, "param2": "c"},
         ]
 
     def test_plot_acf_pacf(self, amazon_vd):
-        result = plot_acf_pacf(amazon_vd, ts="date", by=["state"], column="number", p=3)
+        result = plot_acf_pacf(
+            amazon_vd, ts="date", by=["state"], column="number", p=3, show=False
+        )
         plt.close("all")
         assert result["acf"] == [
             pytest.approx(1.0),
-            pytest.approx(0.673),
-            pytest.approx(0.349),
-            pytest.approx(0.165),
+            pytest.approx(0.672667529541858),
+            pytest.approx(0.349231212451282),
+            pytest.approx(0.164707818699449),
         ]
         assert result["pacf"] == [
             pytest.approx(1.0),
             pytest.approx(0.672667529541858),
             pytest.approx(-0.188727403801382),
             pytest.approx(0.022206688265849),
         ]
 
     def test_prc_curve(self, winequality_vd):
         model = LogisticRegression("model_test")
         model.drop()
         model.fit("public.winequality", ["residual_sugar", "alcohol"], "good")
         data = winequality_vd.copy()
         data = model.predict_proba(data, name="prediction", pos_label=1)
-        result = prc_curve("good", "prediction", data, pos_label=1, nbins=30,)
+        result = prc_curve(
+            "good", "prediction", data, pos_label=1, nbins=30, show=False
+        )
         assert result["precision"][1] == pytest.approx(0.196552254886871)
         assert len(result["precision"]) == 30
         model.drop()
         plt.close("all")
 
     def test_randomized_search_cv(self, winequality_vd):
         result = randomized_search_cv(
@@ -287,15 +298,17 @@
 
     def test_roc_curve(self, winequality_vd):
         model = LogisticRegression("model_test")
         model.drop()
         model.fit("public.winequality", ["residual_sugar", "alcohol"], "good")
         data = winequality_vd.copy()
         data = model.predict_proba(data, name="prediction", pos_label=1)
-        result = roc_curve("good", "prediction", data, pos_label=1, nbins=30,)
+        result = roc_curve(
+            "good", "prediction", data, pos_label=1, nbins=30, show=False
+        )
         assert result["true_positive"][2] == pytest.approx(0.945967110415035)
         assert len(result["true_positive"]) == 31
         model.drop()
         plt.close("all")
 
     def test_validation_curve(self, winequality_vd):
         result = validation_curve(
@@ -303,15 +316,14 @@
             "tol",
             [0.1, 0.01, 0.001],
             winequality_vd,
             ["residual_sugar", "alcohol"],
             "good",
             "auc",
             cv=3,
-            ax=None,
         )
         plt.close("all")
         assert len(result["tol"]) == 3
         assert len(result["test_score"]) == 3
         assert len(result.values) == 7
 
     def test_learning_curve(self, winequality_vd):
@@ -321,15 +333,14 @@
                 winequality_vd,
                 ["residual_sugar", "alcohol"],
                 "good",
                 [0.1, 0.33, 0.55],
                 elem,
                 "auc",
                 cv=3,
-                ax=None,
             )
             plt.close("all")
             assert len(result["n"]) == 3
 
     def test_stepwise(self, titanic_vd):
         titanic = titanic_vd.copy()
         titanic["boat"].fillna(method="0ifnull")
@@ -345,16 +356,16 @@
             100,
             3,
             True,
             "pearson",
             True,
             True,
         )
-        assert result["importance"][-1] == pytest.approx(99.99999999999999, 1e-2)
-        assert result["importance"][-4] == pytest.approx(0.0, 1e-2)
+        assert result["importance"][-1] == pytest.approx(99.40699725013596, 1e-4)
+        assert result["importance"][-4] == pytest.approx(0.5930027498640466, 1e-4)
         plt.close("all")
         result = stepwise(
             model,
             titanic,
             ["age", "fare", "boat", "pclass"],
             "survived",
             "aic",
@@ -362,15 +373,15 @@
             100,
             3,
             True,
             "spearman",
             True,
             True,
         )
-        assert result["importance"][-1] == pytest.approx(0.0, 1e-2)
-        assert result["importance"][-4] == pytest.approx(99.99999999999999, 1e-2)
+        assert result["importance"][-1] == pytest.approx(0.7255807088358904, 1e-4)
+        assert result["importance"][-4] == pytest.approx(99.2744192911641096, 1e-4)
         plt.close("all")
         model = LinearRegression("LR_stepwise_test")
         model.drop()
-        assert result["importance"][-1] == pytest.approx(0.0, 1e-2)
-        assert result["importance"][-4] == pytest.approx(99.99999999999999, 1e-2)
+        assert result["importance"][-1] == pytest.approx(0.7255807088358904, 1e-4)
+        assert result["importance"][-4] == pytest.approx(99.2744192911641096, 1e-4)
         plt.close("all")
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_naive_bayes.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_naive_bayes.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,75 +1,84 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality, load_titanic, load_iris
 from verticapy.learn.naive_bayes import *
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def iris_vd():
     iris = load_iris()
     yield iris
-    drop(name="public.iris",)
+    drop(
+        name="public.iris",
+    )
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(iris_vd):
-    model_class = NaiveBayes("nb_model_test",)
+    model_class = NaiveBayes(
+        "nb_model_test",
+    )
     model_class.drop()
     model_class.fit(
         "public.iris",
         ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"],
         "Species",
     )
     yield model_class
     model_class.drop()
 
 
 class TestNB:
     def test_repr(self, model):
-        assert "predictor  |  type" in model.__repr__()
-        model_repr = NaiveBayes("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<NaiveBayes>"
+        assert model.__repr__() == "<NaiveBayes>"
 
     def test_NB_subclasses(self, winequality_vd):
         model_test = BernoulliNB("model_test")
         assert model_test.parameters["nbtype"] == "bernoulli"
         model_test.drop()
         model_test.fit(winequality_vd, ["good"], "quality")
         model_test.drop()
@@ -99,52 +108,53 @@
         assert cls_rep1["precision"][0] == pytest.approx(1.0)
         assert cls_rep1["recall"][0] == pytest.approx(1.0)
         assert cls_rep1["f1_score"][0] == pytest.approx(1.0)
         assert cls_rep1["mcc"][0] == pytest.approx(1.0)
         assert cls_rep1["informedness"][0] == pytest.approx(1.0)
         assert cls_rep1["markedness"][0] == pytest.approx(1.0)
         assert cls_rep1["csi"][0] == pytest.approx(1.0)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.999)
-
-        cls_rep2 = model.classification_report(cutoff=0.999).transpose()
-
-        assert cls_rep2["cutoff"][0] == pytest.approx(0.999)
 
     def test_confusion_matrix(self, model):
         conf_mat1 = model.confusion_matrix()
 
-        assert conf_mat1["Iris-setosa"] == [50, 0, 0]
-        assert conf_mat1["Iris-versicolor"] == [0, 47, 3]
-        assert conf_mat1["Iris-virginica"] == [0, 3, 47]
+        assert list(conf_mat1[:, 0]) == [50, 0, 0]
+        assert list(conf_mat1[:, 1]) == [0, 47, 3]
+        assert list(conf_mat1[:, 2]) == [0, 3, 47]
 
         conf_mat2 = model.confusion_matrix(cutoff=0.2)
 
-        assert conf_mat2["Iris-setosa"] == [50, 0, 0]
-        assert conf_mat2["Iris-versicolor"] == [0, 47, 3]
-        assert conf_mat2["Iris-virginica"] == [0, 3, 47]
+        assert list(conf_mat1[:, 0]) == [50, 0, 0]
+        assert list(conf_mat1[:, 1]) == [0, 47, 3]
+        assert list(conf_mat1[:, 2]) == [0, 3, 47]
 
     def test_contour(self, titanic_vd):
-        model_test = NaiveBayes("model_contour",)
+        model_test = NaiveBayes(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 36
         model_test.drop()
 
     def test_deploySQL(self, model):
         expected_sql = 'PREDICT_NAIVE_BAYES("SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm" USING PARAMETERS model_name = \'nb_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS nb_model_test_drop")
-        model_test = NaiveBayes("nb_model_test_drop",)
+        model_test = NaiveBayes(
+            "nb_model_test_drop",
+        )
         model_test.fit(
             "public.iris",
             ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"],
             "Species",
         )
 
         current_cursor().execute(
@@ -155,28 +165,30 @@
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'nb_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(pos_label="Iris-versicolor", nbins=1000)
+        lift_ch = model.lift_chart(pos_label="Iris-versicolor", nbins=1000, show=False)
 
         assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
         assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(0.9)
         assert lift_ch["lift"][300] == pytest.approx(2.8125)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(0.98)
         assert lift_ch["lift"][900] == pytest.approx(2.57894736842105)
         plt.close()
 
     def test_to_python(self, titanic_vd):
         titanic = titanic_vd.copy()
         titanic["has_children"] = "parch > 0"
-        model_class = NaiveBayes("nb_model_test_to_python",)
+        model_class = NaiveBayes(
+            "nb_model_test_to_python",
+        )
         model_class.drop()
         model_class.fit(
             titanic,
             ["age", "fare", "survived", "pclass", "sex", "has_children"],
             "embarked",
         )
         predict_function = model_class.to_python()
@@ -209,35 +221,49 @@
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1], 1e-3)
         model_test.drop()
 
     def test_to_memmodel(self, titanic_vd):
         titanic = titanic_vd.copy()
         titanic["has_children"] = "parch > 0"
-        model_class = NaiveBayes("nb_model_test_to_memmodel",)
+        model_class = NaiveBayes(
+            "nb_model_test_to_memmodel",
+        )
         model_class.drop()
         model_class.fit(
             titanic,
             ["age", "fare", "survived", "pclass", "sex", "has_children"],
             "embarked",
         )
         mmodel = model_class.to_memmodel()
         res = mmodel.predict(
-            [[11.0, 1993.0, 1, 3, "male", False], [1.0, 1999.0, 1, 1, "female", True]]
+            [
+                [11.0, 1993.0, 1, 3, "male", False],
+                [1.0, 1999.0, 1, 1, "female", True],
+            ]
         )
         res_py = model_class.to_python()(
-            [[11.0, 1993.0, 1, 3, "male", False], [1.0, 1999.0, 1, 1, "female", True]]
+            [
+                [11.0, 1993.0, 1, 3, "male", False],
+                [1.0, 1999.0, 1, 1, "female", True],
+            ]
         )
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
         res = mmodel.predict_proba(
-            [[11.0, 1993.0, 1, 3, "male", False], [1.0, 1999.0, 1, 1, "female", True]]
+            [
+                [11.0, 1993.0, 1, 3, "male", False],
+                [1.0, 1999.0, 1, 1, "female", True],
+            ]
         )
         res_py = model_class.to_python(return_proba=True)(
-            [[11.0, 1993.0, 1, 3, "male", False], [1.0, 1999.0, 1, 1, "female", True]]
+            [
+                [11.0, 1993.0, 1, 3, "male", False],
+                [1.0, 1999.0, 1, 1, "female", True],
+            ]
         )
         assert res[0][0] == res_py[0][0]
         assert res[0][1] == res_py[0][1]
         assert res[0][2] == res_py[0][2]
         assert res[1][0] == res_py[1][0]
         assert res[1][1] == res_py[1][1]
         assert res[1][2] == res_py[1][2]
@@ -265,31 +291,33 @@
             pos_label=model_class.classes_[1],
         )
         model_class.predict_proba(
             titanic,
             name="prediction_proba_vertica_sql_2",
             pos_label=model_class.classes_[2],
         )
-        score = titanic.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        score = titanic.score(
+            "prediction_sql", "prediction_vertica_sql", metric="accuracy"
+        )
         assert score == pytest.approx(1.0)
         score = titanic.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = titanic.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = titanic.score(
-            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", "r2"
+            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", metric="r2"
         )
         assert score == pytest.approx(1.0)
 
-    def test_get_attr(self, model):
-        attr = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        attr = model.get_vertica_attributes()
         assert attr["attr_name"] == [
             "details",
             "alpha",
             "prior",
             "accepted_row_count",
             "rejected_row_count",
             "call_string",
@@ -306,15 +334,15 @@
             "call_string",
             "index, mu, sigma_sq",
             "index, mu, sigma_sq",
             "index, mu, sigma_sq",
         ]
         assert attr["#_of_rows"] == [5, 1, 3, 1, 1, 1, 4, 4, 4]
 
-        details = model.get_attr("details")
+        details = model.get_vertica_attributes("details")
         assert details["predictor"] == [
             "Species",
             "SepalLengthCm",
             "SepalWidthCm",
             "PetalLengthCm",
             "PetalWidthCm",
         ]
@@ -322,55 +350,61 @@
             "ResponseC",
             "Gaussian",
             "Gaussian",
             "Gaussian",
             "Gaussian",
         ]
 
-        assert model.get_attr("alpha")["alpha"][0] == 1.0
+        assert model.get_vertica_attributes("alpha")["alpha"][0] == 1.0
 
-        assert model.get_attr("prior")["class"] == [
+        assert model.get_vertica_attributes("prior")["class"] == [
             "Iris-setosa",
             "Iris-versicolor",
             "Iris-virginica",
         ]
-        assert model.get_attr("prior")["probability"] == [
+        assert model.get_vertica_attributes("prior")["probability"] == [
             pytest.approx(0.333333333333333),
             pytest.approx(0.333333333333333),
             pytest.approx(0.333333333333333),
         ]
 
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 150
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 150
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
 
-        assert model.get_attr("gaussian.Iris-setosa")["mu"] == [
+        assert model.get_vertica_attributes("gaussian.Iris-setosa")["mu"] == [
             pytest.approx(5.006),
             pytest.approx(3.418),
             pytest.approx(1.464),
             pytest.approx(0.244),
         ]
-        assert model.get_attr("gaussian.Iris-setosa")["sigma_sq"] == [
+        assert model.get_vertica_attributes("gaussian.Iris-setosa")["sigma_sq"] == [
             pytest.approx(0.12424897959183),
             pytest.approx(0.145179591836736),
             pytest.approx(0.0301061224489805),
             pytest.approx(0.0114938775510204),
         ]
 
         assert (
-            model.get_attr("call_string")["call_string"][0]
+            model.get_vertica_attributes("call_string")["call_string"][0]
             == "naive_bayes('public.nb_model_test', 'public.iris', '\"species\"', '\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"' USING PARAMETERS exclude_columns='', alpha=1)"
         )
 
     def test_get_params(self, model):
         params = model.get_params()
 
         assert params == {"alpha": 1.0, "nbtype": "auto"}
 
     def test_prc_curve(self, model):
-        prc = model.prc_curve(pos_label="Iris-virginica", nbins=1000)
+        prc = model.prc_curve(pos_label="Iris-virginica", nbins=1000, show=False)
 
         assert prc["threshold"][300] == pytest.approx(0.299)
         assert prc["recall"][300] == pytest.approx(0.94)
         assert prc["precision"][300] == pytest.approx(0.903846153846154)
         assert prc["threshold"][800] == pytest.approx(0.799)
         assert prc["recall"][800] == pytest.approx(0.9)
         assert prc["precision"][800] == pytest.approx(0.957446808510638)
@@ -385,87 +419,90 @@
         model.predict(iris_copy, name="pred_class1", cutoff=0.7)
         assert iris_copy["pred_class1"][0] == "Iris-setosa"
 
         model.predict(iris_copy, name="pred_class2", cutoff=0.3)
         assert iris_copy["pred_class2"][0] == "Iris-setosa"
 
     def test_roc_curve(self, model):
-        roc = model.roc_curve(pos_label="Iris-virginica", nbins=1000)
+        roc = model.roc_curve(pos_label="Iris-virginica", nbins=1000, show=False)
 
         assert roc["threshold"][100] == pytest.approx(0.1)
         assert roc["false_positive"][100] == pytest.approx(0.08)
         assert roc["true_positive"][100] == pytest.approx(0.96)
         assert roc["threshold"][700] == pytest.approx(0.7)
         assert roc["false_positive"][700] == pytest.approx(0.02)
         assert roc["true_positive"][700] == pytest.approx(0.92)
         plt.close()
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(pos_label="Iris-virginica", nbins=1000)
+        cutoff_curve = model.cutoff_curve(
+            pos_label="Iris-virginica", nbins=1000, show=False
+        )
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
         assert cutoff_curve["false_positive"][100] == pytest.approx(0.08)
         assert cutoff_curve["true_positive"][100] == pytest.approx(0.96)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
         assert cutoff_curve["false_positive"][700] == pytest.approx(0.02)
         assert cutoff_curve["true_positive"][700] == pytest.approx(0.92)
         plt.close()
 
     def test_score(self, model):
         # the value of cutoff has no impact on the result
-        assert model.score(cutoff=0.9, method="accuracy") == pytest.approx(0.96)
-        assert model.score(cutoff=0.1, method="accuracy") == pytest.approx(0.96)
+        assert model.score(metric="accuracy") == pytest.approx(0.96)
         assert model.score(
-            cutoff=0.9, method="auc", pos_label="Iris-virginica"
+            cutoff=0.9, metric="auc", pos_label="Iris-virginica"
         ) == pytest.approx(0.9923999999999998)
         assert model.score(
-            cutoff=0.1, method="auc", pos_label="Iris-virginica"
+            cutoff=0.1, metric="auc", pos_label="Iris-virginica"
         ) == pytest.approx(0.9923999999999998)
         assert model.score(
-            cutoff=0.9, method="best_cutoff", pos_label="Iris-virginica"
+            cutoff=0.9, metric="best_cutoff", pos_label="Iris-virginica"
         ) == pytest.approx(0.5099, 1e-2)
         assert model.score(
-            cutoff=0.9, method="bm", pos_label="Iris-virginica"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="bm", pos_label="Iris-virginica"
+        ) == pytest.approx(0.8300000000000001)
         assert model.score(
-            cutoff=0.9, method="csi", pos_label="Iris-virginica"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="csi", pos_label="Iris-virginica"
+        ) == pytest.approx(0.8235294117647058)
         assert model.score(
-            cutoff=0.9, method="f1", pos_label="Iris-virginica"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="f1", pos_label="Iris-virginica"
+        ) == pytest.approx(0.9032258064516129)
         assert model.score(
-            cutoff=0.9, method="logloss", pos_label="Iris-virginica"
+            cutoff=0.9, metric="logloss", pos_label="Iris-virginica"
         ) == pytest.approx(0.0479202007517544)
         assert model.score(
-            cutoff=0.9, method="mcc", pos_label="Iris-virginica"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="mcc", pos_label="Iris-virginica"
+        ) == pytest.approx(0.8652407755372198)
         assert model.score(
-            cutoff=0.9, method="mk", pos_label="Iris-virginica"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="mk", pos_label="Iris-virginica"
+        ) == pytest.approx(0.9019778309063247)
         assert model.score(
-            cutoff=0.9, method="npv", pos_label="Iris-virginica"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="npv", pos_label="Iris-virginica"
+        ) == pytest.approx(0.9252336448598131)
         assert model.score(
-            cutoff=0.9, method="prc_auc", pos_label="Iris-virginica"
+            cutoff=0.9, metric="prc_auc", pos_label="Iris-virginica"
         ) == pytest.approx(0.9864010713921592)
         assert model.score(
-            cutoff=0.9, method="precision", pos_label="Iris-virginica"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="precision", pos_label="Iris-virginica"
+        ) == pytest.approx(0.9767441860465116)
         assert model.score(
-            cutoff=0.9, method="specificity", pos_label="Iris-virginica"
-        ) == pytest.approx(1.0)
+            cutoff=0.9, metric="specificity", pos_label="Iris-virginica"
+        ) == pytest.approx(0.99)
 
     def test_set_params(self, model):
         model.set_params({"alpha": 0.5})
 
         assert model.get_params()["alpha"] == 0.5
 
     def test_model_from_vDF(self, iris_vd):
         current_cursor().execute("DROP MODEL IF EXISTS nb_from_vDF")
-        model_test = NaiveBayes("nb_from_vDF",)
+        model_test = NaiveBayes(
+            "nb_from_vDF",
+        )
         model_test.fit(
             iris_vd,
             ["SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm"],
             "Species",
         )
 
         current_cursor().execute(
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_nearestcentroid.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_nearestcentroid.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,111 +1,108 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import (
     drop,
     set_option,
-    create_verticapy_schema,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
 from verticapy.learn.neighbors import NearestCentroid
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
-    create_verticapy_schema()
-    model_class = NearestCentroid("nc_model_test",)
+    model_class = NearestCentroid(
+        "nc_model_test",
+    )
     model_class.drop()
     model_class.fit("public.titanic", ["age", "fare"], "survived")
     yield model_class
     model_class.drop()
 
 
 class TestNearestCentroid:
     def test_repr(self, model):
-        assert "Additional Info" in model.__repr__()
-        model_repr = NearestCentroid("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<NearestCentroid>"
-
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
-        assert m_att["attr_name"] == ["centroids", "classes", "p"]
-        m_att = model.get_attr("centroids")
-        assert m_att == model.centroids_
-        m_att = model.get_attr("p")
-        assert m_att == model.parameters["p"]
-        m_att = model.get_attr("classes")
-        assert m_att == model.classes_
+        assert model.__repr__() == "<NearestCentroid>"
 
     def test_contour(self, titanic_vd):
-        model_test = NearestCentroid("model_contour",)
+        model_test = NearestCentroid(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 40
         model_test.drop()
 
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(nbins=1000)
+        lift_ch = model.lift_chart(nbins=1000, show=False)
 
         assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
         assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(
             0.0895140664961637
         )
         assert lift_ch["lift"][300] == pytest.approx(1.89693638787615)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(
             0.979539641943734
         )
         assert lift_ch["lift"][900] == pytest.approx(0.998589031091053)
         plt.close("all")
 
     def test_roc_curve(self, model):
-        roc_curve = model.roc_curve(nbins=1000)
+        roc_curve = model.roc_curve(nbins=1000, show=False)
 
         assert roc_curve["threshold"][100] == pytest.approx(0.1)
         assert roc_curve["false_positive"][100] == pytest.approx(0.981818181818182)
         assert roc_curve["true_positive"][100] == pytest.approx(0.979539641943734)
         assert roc_curve["threshold"][700] == pytest.approx(0.7)
         assert roc_curve["false_positive"][700] == pytest.approx(0.0198347107438017)
         assert roc_curve["true_positive"][700] == pytest.approx(0.0895140664961637)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(nbins=1000)
+        cutoff_curve = model.cutoff_curve(nbins=1000, show=False)
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
         assert cutoff_curve["false_positive"][100] == pytest.approx(0.981818181818182)
         assert cutoff_curve["true_positive"][100] == pytest.approx(0.979539641943734)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
         assert cutoff_curve["false_positive"][700] == pytest.approx(0.0198347107438017)
         assert cutoff_curve["true_positive"][700] == pytest.approx(0.0895140664961637)
@@ -131,58 +128,49 @@
         assert res[1][0] == res_py[1][0]
         assert res[1][1] == res_py[1][1]
         titanic["prediction_sql"] = mmodel.predict_sql(["age", "fare"])
         titanic["prediction_proba_sql_0"] = mmodel.predict_proba_sql(["age", "fare"])[0]
         titanic["prediction_proba_sql_1"] = mmodel.predict_proba_sql(["age", "fare"])[1]
         titanic = model.predict(titanic, name="prediction_vertica_sql", cutoff=0.5)
         titanic = model.predict_proba(
-            titanic, name="prediction_proba_vertica_sql_0", pos_label=model.classes_[0]
+            titanic,
+            name="prediction_proba_vertica_sql_0",
+            pos_label=model.classes_[0],
         )
         titanic = model.predict_proba(
-            titanic, name="prediction_proba_vertica_sql_1", pos_label=model.classes_[1]
+            titanic,
+            name="prediction_proba_vertica_sql_1",
+            pos_label=model.classes_[1],
+        )
+        score = titanic.score(
+            "prediction_sql", "prediction_vertica_sql", metric="accuracy"
         )
-        score = titanic.score("prediction_sql", "prediction_vertica_sql", "accuracy")
         print(titanic[["prediction_sql", "prediction_vertica_sql"]])
         print(titanic.current_relation())
         assert score == pytest.approx(1.0)
         score = titanic.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = titanic.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
 
-    def test_drop(self):
-        model_test = NearestCentroid("model_test_drop",)
-        model_test.drop()
-        model_test.fit("public.titanic", ["age"], "survived")
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone()[0] in (
-            "model_test_drop",
-            '"model_test_drop"',
-        )
-
-        model_test.drop()
-        current_cursor().execute(
-            "SELECT model_name FROM verticapy.models WHERE model_name IN ('model_test_drop', '\"model_test_drop\"')"
-        )
-        assert current_cursor().fetchone() is None
-
     def test_get_params(self, model):
         assert model.get_params() == {"p": 2}
 
     def test_predict(self, titanic_vd, model):
         titanic_copy = titanic_vd.copy()
 
         titanic_copy = model.predict(
-            titanic_copy, X=["age", "fare"], name="predicted_quality", inplace=False,
+            titanic_copy,
+            X=["age", "fare"],
+            name="predicted_quality",
+            inplace=False,
         )
         assert titanic_copy["predicted_quality"].mean() == pytest.approx(
             0.245983935742972, abs=1e-6
         )
 
     def test_predict_proba(self, titanic_vd, model):
         titanic_copy = titanic_vd.copy()
@@ -199,63 +187,65 @@
         )
 
     def test_classification_report(self, model):
         cls_rep1 = model.classification_report().transpose()
 
         assert cls_rep1["auc"][0] == pytest.approx(0.6325400012682033)
         assert cls_rep1["prc_auc"][0] == pytest.approx(0.5442487908406839)
-        assert cls_rep1["accuracy"][0] == pytest.approx(0.6596385542168675)
+        assert cls_rep1["accuracy"][0] == pytest.approx(0.6746987951807228)
         assert cls_rep1["log_loss"][0] == pytest.approx(0.282873255537287)
-        assert cls_rep1["precision"][0] == pytest.approx(0.5680628272251309)
-        assert cls_rep1["recall"][0] == pytest.approx(0.5549872122762148)
-        assert cls_rep1["f1_score"][0] == pytest.approx(0.5614489003880982)
-        assert cls_rep1["mcc"][0] == pytest.approx(0.28346499991292595)
-        assert cls_rep1["informedness"][0] == pytest.approx(0.282259939548942)
-        assert cls_rep1["markedness"][0] == pytest.approx(0.28467520507529365)
-        assert cls_rep1["csi"][0] == pytest.approx(0.3902877697841727)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.352)
+        assert cls_rep1["precision"][0] == pytest.approx(0.636734693877551)
+        assert cls_rep1["recall"][0] == pytest.approx(0.3989769820971867)
+        assert cls_rep1["f1_score"][0] == pytest.approx(0.49056603773584906)
+        assert cls_rep1["mcc"][0] == pytest.approx(0.28558718217018486)
+        assert cls_rep1["informedness"][0] == pytest.approx(0.25186954408065776)
+        assert cls_rep1["markedness"][0] == pytest.approx(0.32381858202668545)
+        assert cls_rep1["csi"][0] == pytest.approx(0.325)
 
     def test_score(self, model):
-        assert model.score(cutoff=0.9, method="accuracy") == pytest.approx(
+        assert model.score(cutoff=0.9, metric="accuracy") == pytest.approx(
             0.607429718875502
         )
-        assert model.score(cutoff=0.1, method="accuracy") == pytest.approx(
+        assert model.score(cutoff=0.1, metric="accuracy") == pytest.approx(
             0.39558232931726905
         )
-        assert model.score(method="best_cutoff") == pytest.approx(0.352)
-        assert model.score(method="bm") == pytest.approx(0.282259939548942)
-        assert model.score(method="csi") == pytest.approx(0.3902877697841727)
-        assert model.score(method="f1") == pytest.approx(0.5614489003880982)
-        assert model.score(method="logloss") == pytest.approx(0.282873255537287)
-        assert model.score(method="mcc") == pytest.approx(0.28346499991292595)
-        assert model.score(method="mk") == pytest.approx(0.28467520507529365)
-        assert model.score(method="npv") == pytest.approx(0.5680628272251309)
-        assert model.score(method="prc_auc") == pytest.approx(0.5442487908406839)
-        assert model.score(method="precision") == pytest.approx(0.5680628272251309)
-        assert model.score(method="specificity") == pytest.approx(0.7272727272727273)
+        assert model.score(metric="best_cutoff") == pytest.approx(0.352)
+        assert model.score(metric="bm") == pytest.approx(0.25186954408065776)
+        assert model.score(metric="csi") == pytest.approx(0.325)
+        assert model.score(metric="f1") == pytest.approx(0.49056603773584906)
+        assert model.score(metric="logloss") == pytest.approx(0.282873255537287)
+        assert model.score(metric="mcc") == pytest.approx(0.28558718217018486)
+        assert model.score(metric="mk") == pytest.approx(0.32381858202668545)
+        assert model.score(metric="npv") == pytest.approx(0.6870838881491345)
+        assert model.score(metric="prc_auc") == pytest.approx(0.5442487908406839)
+        assert model.score(metric="precision") == pytest.approx(0.636734693877551)
+        assert model.score(metric="specificity") == pytest.approx(0.8528925619834711)
 
     def test_set_params(self, model):
         model.set_params({"p": 1})
         assert model.get_params()["p"] == 1
         model.set_params({"p": 2})
 
     def test_to_python(self, model):
-        assert 0 == pytest.approx(
-            model.to_python(return_str=False)([[5.006, 3.418]])[0]
+        assert 0 == pytest.approx(model.to_python()([[5.006, 3.418]])[0])
+        assert model.to_python(return_distance_clusters=True)([[5.006, 3.418]])[0][
+            0
+        ] in (
+            pytest.approx(32.519389961314424),
+            pytest.approx(45.6436412237776),
         )
-        assert model.to_python(return_str=False, return_distance_clusters=True)(
-            [[5.006, 3.418]]
-        )[0][0] in (pytest.approx(32.519389961314424), pytest.approx(45.6436412237776))
 
     def test_to_sql(self, model):
         current_cursor().execute("SELECT {}::int".format(model.to_sql([3.0, 11.0])))
         prediction = current_cursor().fetchone()
         assert prediction[0] == 0
 
     def test_model_from_vDF(self, titanic_vd):
-        model_test = NearestCentroid("nc_from_vDF",)
+        model_test = NearestCentroid(
+            "nc_from_vDF",
+        )
         model_test.drop()
         model_test.fit(titanic_vd, ["age"], "survived")
-        assert model_test.score(cutoff=0.9, method="accuracy") == pytest.approx(
+        assert model_test.score(cutoff=0.9, metric="accuracy") == pytest.approx(
             0.6078234704112337
         )
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_normalizer.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_normalizer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,61 +1,66 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality
 from verticapy.learn.preprocessing import (
-    Normalizer,
+    Scaler,
     StandardScaler,
     RobustScaler,
     MinMaxScaler,
 )
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(winequality_vd):
     current_cursor().execute("DROP MODEL IF EXISTS norm_model_test")
-    model_class = Normalizer("norm_model_test",)
+    model_class = Scaler(
+        "norm_model_test",
+    )
     model_class.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
     yield model_class
     model_class.drop()
 
 
-class TestNormalizer:
+class TestScaler:
     def test_repr(self, model):
-        assert "column_name  |  avg   |std_dev" in model.__repr__()
-        model_repr = Normalizer("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<Normalizer>"
+        assert model.__repr__() == "<Scaler>"
 
-    def test_Normalizer_subclasses(self):
+    def test_Scaler_subclasses(self):
         result = StandardScaler("model_test")
         assert result.parameters["method"] == "zscore"
         result = RobustScaler("model_test")
         assert result.parameters["method"] == "robust_zscore"
         result = MinMaxScaler("model_test")
         assert result.parameters["method"] == "minmax"
 
@@ -69,40 +74,42 @@
         expected_sql = 'REVERSE_NORMALIZE("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'norm_model_test\', match_by_pos = \'true\')'
         result_sql = model.deployInverseSQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS norm_model_test_drop")
-        model_test = Normalizer("norm_model_test_drop",)
+        model_test = Scaler(
+            "norm_model_test_drop",
+        )
         model_test.fit("public.winequality", ["alcohol", "quality"])
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'norm_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "norm_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'norm_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "details",
         ]
         assert m_att["attr_fields"] == [
             "column_name, avg, std_dev",
         ]
         assert m_att["#_of_rows"] == [3]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
         assert m_att_details["column_name"] == [
             "citric_acid",
             "residual_sugar",
             "alcohol",
         ]
         assert m_att_details["avg"][0] == pytest.approx(0.318633215330152, abs=1e-6)
@@ -115,90 +122,88 @@
     def test_get_params(self, model):
         assert model.get_params() == {"method": "zscore"}
 
     def test_to_python(self, model):
         # Zscore
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0][0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0][0])
         # Minmax
-        model2 = Normalizer("norm_model_test2", method="minmax")
+        model2 = Scaler("norm_model_test2", method="minmax")
         model2.drop()
         model2.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model2.name
+                model2.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
-            model2.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0][0]
+            model2.to_python()([[3.0, 11.0, 93.0]])[0][0]
         )
         model2.drop()
         # Robust Zscore
-        model3 = Normalizer("norm_model_test2", method="robust_zscore")
+        model3 = Scaler("norm_model_test2", method="robust_zscore")
         model3.drop()
         model3.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model3.name
+                model3.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
-            model3.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0][0]
+            model3.to_python()([[3.0, 11.0, 93.0]])[0][0]
         )
         model3.drop()
 
     def test_to_sql(self, model):
         # Zscore
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(model.to_sql())
             )
         )
         prediction2 = [float(elem) for elem in current_cursor().fetchone()]
         assert prediction == pytest.approx(prediction2)
         # Minmax
-        model2 = Normalizer("norm_model_test2", method="minmax")
+        model2 = Scaler("norm_model_test2", method="minmax")
         model2.drop()
         model2.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model2.name
+                model2.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(model2.to_sql())
             )
         )
         prediction2 = [float(elem) for elem in current_cursor().fetchone()]
         assert prediction == pytest.approx(prediction2)
         model2.drop()
         # Robust Zscore
-        model3 = Normalizer("norm_model_test2", method="robust_zscore")
+        model3 = Scaler("norm_model_test2", method="robust_zscore")
         model3.drop()
         model3.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model3.name
+                model3.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(model3.to_sql())
             )
@@ -207,15 +212,15 @@
         assert prediction == pytest.approx(prediction2)
         model3.drop()
 
     def test_to_memmodel(self, model):
         # Zscore
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(
                     model.to_memmodel().transform_sql(
@@ -227,20 +232,20 @@
         prediction2 = [float(elem) for elem in current_cursor().fetchone()]
         assert prediction == pytest.approx(prediction2)
         prediction3 = model.to_memmodel().transform([[3.0, 11.0, 93.0]])
         assert prediction[0] == pytest.approx(prediction3[0][0])
         assert prediction[1] == pytest.approx(prediction3[0][1])
         assert prediction[2] == pytest.approx(prediction3[0][2])
         # Minmax
-        model2 = Normalizer("norm_model_test2", method="minmax")
+        model2 = Scaler("norm_model_test2", method="minmax")
         model2.drop()
         model2.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model2.name
+                model2.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(
                     model2.to_memmodel().transform_sql(
@@ -253,20 +258,20 @@
         assert prediction == pytest.approx(prediction2)
         prediction3 = model2.to_memmodel().transform([[3.0, 11.0, 93.0]])
         assert prediction[0] == pytest.approx(prediction3[0][0])
         assert prediction[1] == pytest.approx(prediction3[0][1])
         assert prediction[2] == pytest.approx(prediction3[0][2])
         model2.drop()
         # Robust Zscore
-        model3 = Normalizer("norm_model_test2", method="robust_zscore")
+        model3 = Scaler("norm_model_test2", method="robust_zscore")
         model3.drop()
         model3.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         current_cursor().execute(
             "SELECT APPLY_NORMALIZE(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model3.name
+                model3.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(
                     model3.to_memmodel().transform_sql(
@@ -290,26 +295,26 @@
         )
         assert winequality_trans["citric_acid"].mean() == pytest.approx(0.0, abs=1e-6)
         assert winequality_trans["residual_sugar"].mean() == pytest.approx(
             0.0, abs=1e-6
         )
         assert winequality_trans["alcohol"].mean() == pytest.approx(0.0, abs=1e-6)
         # Minmax
-        model2 = Normalizer("norm_model_test2", method="minmax")
+        model2 = Scaler("norm_model_test2", method="minmax")
         model2.drop()
         model2.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         winequality_trans = model2.transform(
             winequality_vd, X=["citric_acid", "residual_sugar", "alcohol"]
         )
         assert winequality_trans["citric_acid"].min() == pytest.approx(0.0, abs=1e-6)
         assert winequality_trans["residual_sugar"].max() == pytest.approx(1.0, abs=1e-6)
         assert winequality_trans["alcohol"].min() == pytest.approx(0.0, abs=1e-6)
         model2.drop()
         # Robust Zscore
-        model3 = Normalizer("norm_model_test2", method="robust_zscore")
+        model3 = Scaler("norm_model_test2", method="robust_zscore")
         model3.drop()
         model3.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
         winequality_trans = model3.transform(
             winequality_vd, X=["citric_acid", "residual_sugar", "alcohol"]
         )
         assert winequality_trans["citric_acid"].median() == pytest.approx(0.0, abs=1e-6)
         assert winequality_trans["residual_sugar"].median() == pytest.approx(
@@ -336,14 +341,16 @@
         model.set_params({"method": "robust_zscore"})
         assert model.get_params()["method"] == "robust_zscore"
         model.set_params({"method": "zscore"})
         assert model.get_params()["method"] == "zscore"
 
     def test_model_from_vDF(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS norm_vDF")
-        model_test = Normalizer("norm_vDF",)
+        model_test = Scaler(
+            "norm_vDF",
+        )
         model_test.fit(winequality_vd, ["alcohol", "quality"])
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'norm_vDF'"
         )
         assert current_cursor().fetchone()[0] == "norm_vDF"
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_onehotencoder.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_onehotencoder.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,93 +1,98 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
 from verticapy.learn.preprocessing import OneHotEncoder
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(titanic_vd):
     model_class = OneHotEncoder("ohe_model_test", drop_first=False)
     model_class.drop()
     model_class.fit("public.titanic", ["pclass", "sex", "embarked"])
     yield model_class
     model_class.drop()
 
 
 class TestOneHotEncoder:
     def test_repr(self, model):
-        assert "one_hot_encoder_fit" in model.__repr__()
-        model_repr = OneHotEncoder("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<OneHotEncoder>"
+        assert model.__repr__() == "<OneHotEncoder>"
 
     def test_deploySQL(self, model):
-        expected_sql = "APPLY_ONE_HOT_ENCODER(\"pclass\", \"sex\", \"embarked\" USING PARAMETERS model_name = 'ohe_model_test', match_by_pos = 'true', drop_first = False, ignore_null = True, separator = '_', column_naming = 'indices')"
+        expected_sql = "APPLY_ONE_HOT_ENCODER(\"pclass\", \"sex\", \"embarked\" USING PARAMETERS model_name = 'ohe_model_test', match_by_pos = 'true', drop_first = 'false', ignore_null = 'true', separator = '_', column_naming = 'indices')"
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS ohe_model_test_drop")
-        model_test = OneHotEncoder("ohe_model_test_drop",)
+        model_test = OneHotEncoder(
+            "ohe_model_test_drop",
+        )
         model_test.fit("public.titanic", ["pclass", "embarked"])
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'ohe_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "ohe_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'ohe_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "call_string",
             "integer_categories",
             "varchar_categories",
         ]
         assert m_att["attr_fields"] == [
             "call_string",
             "category_name, category_level, category_level_index",
             "category_name, category_level, category_level_index",
         ]
         assert m_att["#_of_rows"] == [1, 3, 6]
 
-        m_att_details = model.get_attr(attr_name="integer_categories")
+        m_att_details = model.get_vertica_attributes(attr_name="integer_categories")
 
         assert m_att_details["category_name"] == [
             "pclass",
             "pclass",
             "pclass",
         ]
         assert m_att_details["category_level"][0] == pytest.approx(1, abs=1e-6)
@@ -106,30 +111,30 @@
             "null_column_name": "null",
             "separator": "_",
         }
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT pclass_1, pclass_2, sex_1, embarked_1, embarked_2 FROM (SELECT APPLY_ONE_HOT_ENCODER(pclass, sex, embarked USING PARAMETERS model_name = '{}', match_by_pos=True, drop_first=True) FROM (SELECT 1 AS pclass, 'female' AS sex, 'S' AS embarked) x) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT pclass_1, pclass_2, sex_1, embarked_1, embarked_2 FROM (SELECT {} FROM (SELECT 1 AS pclass, 'female' AS sex, 'S' AS embarked) x) x".format(
                 ", ".join([", ".join(elem) for elem in model.to_sql()])
             )
         )
         prediction2 = [float(elem) for elem in current_cursor().fetchone()]
         assert prediction == pytest.approx(prediction2)
 
     def test_to_memmodel(self, model):
         current_cursor().execute(
             "SELECT pclass_0, pclass_1, pclass_2, sex_0, sex_1, embarked_0, embarked_1, embarked_2 FROM (SELECT APPLY_ONE_HOT_ENCODER(pclass, sex, embarked USING PARAMETERS model_name = '{}', match_by_pos=True, drop_first=False) FROM (SELECT 1 AS pclass, 'female' AS sex, 'S' AS embarked) x) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT pclass_0, pclass_1, pclass_2, sex_0, sex_1, embarked_0, embarked_1, embarked_2 FROM (SELECT {} FROM (SELECT 1 AS pclass, 'female' AS sex, 'S' AS embarked) x) x".format(
                 ", ".join(
                     [
@@ -152,19 +157,19 @@
         assert prediction[5] == pytest.approx(prediction3[0][5])
         assert prediction[6] == pytest.approx(prediction3[0][6])
         assert prediction[7] == pytest.approx(prediction3[0][7])
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT pclass_0, pclass_1, pclass_2, sex_0, sex_1, embarked_0, embarked_1, embarked_2, 0 FROM (SELECT APPLY_ONE_HOT_ENCODER(pclass, sex, embarked USING PARAMETERS model_name = '{}', match_by_pos=True, drop_first=False) FROM (SELECT 1 AS pclass, 'female' AS sex, 'S' AS embarked) x) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = [int(elem) for elem in current_cursor().fetchone()]
-        prediction2 = model.to_python(return_str=False)([[1, "female", "S"]])[0]
+        prediction2 = model.to_python()([[1, "female", "S"]])[0]
         assert len(prediction) == len(prediction2)
         assert prediction[0] == prediction2[0]
         assert prediction[1] == prediction2[1]
         assert prediction[2] == prediction2[2]
         assert prediction[3] == prediction2[3]
         assert prediction[4] == prediction2[4]
         assert prediction[5] == prediction2[5]
@@ -185,15 +190,15 @@
 
     @pytest.mark.skip(reason="Vertica OHE has no inverse transform from now")
     def test_get_inverse_transform(self, titanic_vd, model):
         pass
 
     def test_set_params(self, model):
         model.set_params({"ignore_null": False})
-        assert model.get_params()["ignore_null"] == False
+        assert not model.get_params()["ignore_null"]
 
     def test_model_from_vDF(self, titanic_vd):
         current_cursor().execute("DROP MODEL IF EXISTS ohe_vDF")
         model_test = OneHotEncoder("ohe_vDF", drop_first=False)
         model_test.fit(titanic_vd, ["pclass", "embarked"])
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'ohe_vDF'"
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_pca.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_pca.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,54 +1,59 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality
 from verticapy.learn.decomposition import PCA
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(winequality_vd):
-    model_class = PCA("pca_model_test",)
+    model_class = PCA(
+        "pca_model_test",
+    )
     model_class.drop()
     model_class.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
     yield model_class
     model_class.drop()
 
 
 class TestPCA:
     def test_repr(self, model):
-        assert "index|     name     |  mean  |   sd" in model.__repr__()
-        model_repr = PCA("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<PCA>"
+        assert model.__repr__() == "<PCA>"
 
     def test_deploySQL(self, model):
         expected_sql = 'APPLY_PCA("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'pca_model_test\', match_by_pos = \'true\', cutoff = 1)'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
@@ -56,30 +61,32 @@
         expected_sql = 'APPLY_INVERSE_PCA("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'pca_model_test\', match_by_pos = \'true\')'
         result_sql = model.deployInverseSQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS pca_model_test_drop")
-        model_test = PCA("pca_model_test_drop",)
+        model_test = PCA(
+            "pca_model_test_drop",
+        )
         model_test.fit("public.winequality", ["alcohol", "quality"])
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'pca_model_test_drop'"
         )
         assert current_cursor().fetchone()[0] == "pca_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'pca_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "columns",
             "singular_values",
             "principal_components",
             "counters",
             "call_string",
@@ -89,15 +96,15 @@
             "index, value, explained_variance, accumulated_explained_variance",
             "index, PC1, PC2, PC3",
             "counter_name, counter_value",
             "call_string",
         ]
         assert m_att["#_of_rows"] == [3, 3, 3, 3, 1]
 
-        m_att_details = model.get_attr(attr_name="principal_components")
+        m_att_details = model.get_vertica_attributes(attr_name="principal_components")
 
         assert m_att_details["PC1"][0] == pytest.approx(0.00430584055130197, abs=1e-6)
         assert m_att_details["PC1"][1] == pytest.approx(0.995483456627961, abs=1e-6)
         assert m_att_details["PC1"][2] == pytest.approx(-0.0948374784417728, abs=1e-6)
         assert m_att_details["PC2"][0] == pytest.approx(0.00623540848865928, abs=1e-6)
         assert m_att_details["PC2"][1] == pytest.approx(0.0948097859779201, abs=1e-6)
         assert m_att_details["PC2"][2] == pytest.approx(0.995475878243064, abs=1e-6)
@@ -116,52 +123,50 @@
         result = model.plot()
         assert len(result.get_default_bbox_extra_artists()) == 8
         result = model.plot(dimensions=(2, 3))
         assert len(result.get_default_bbox_extra_artists()) == 8
 
     def test_plot_scree(self, model):
         result = model.plot_scree()
-        assert len(result.get_default_bbox_extra_artists()) == 14
+        assert len(result.get_default_bbox_extra_artists()) == 15
 
     def test_plot_circle(self, model):
         result = model.plot_circle()
         assert len(result.get_default_bbox_extra_artists()) == 16
         result = model.plot_circle(dimensions=(2, 3))
         assert len(result.get_default_bbox_extra_artists()) == 16
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT APPLY_PCA(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0]
-        )
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0])
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT APPLY_PCA(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(model.to_sql())
             )
         )
         prediction2 = [float(elem) for elem in current_cursor().fetchone()]
         assert prediction == pytest.approx(prediction2)
 
     def test_to_memmodel(self, model):
         current_cursor().execute(
             "SELECT APPLY_PCA(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
+                model.model_name
             )
         )
         prediction = [float(elem) for elem in current_cursor().fetchone()]
         current_cursor().execute(
             "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
                 ", ".join(
                     model.to_memmodel().transform_sql(
@@ -208,14 +213,16 @@
 
     def test_set_params(self, model):
         model.set_params({"n_components": 3})
         assert model.get_params()["n_components"] == 3
 
     def test_model_from_vDF(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS pca_vDF")
-        model_test = PCA("pca_vDF",)
+        model_test = PCA(
+            "pca_vDF",
+        )
         model_test.fit(winequality_vd, ["alcohol", "quality"])
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'pca_vDF'"
         )
         assert current_cursor().fetchone()[0] == "pca_vDF"
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_pipeline.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_svd.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,238 +1,224 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # VerticaPy
-from verticapy import drop, set_option, tablesample
-from verticapy.connect import current_cursor
+from verticapy import drop, set_option
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality
-from verticapy.learn.linear_model import LinearRegression, LogisticRegression
-from verticapy.learn.preprocessing import StandardScaler, MinMaxScaler
-from verticapy.learn.pipeline import Pipeline
+from verticapy.learn.decomposition import SVD
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality",)
+    drop(
+        name="public.winequality",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(winequality_vd):
-    model_class = Pipeline(
-        [
-            ("NormalizerWine", StandardScaler("std_model_test",)),
-            ("LinearRegressionWine", LinearRegression("linreg_model_test",)),
-        ]
+    model_class = SVD(
+        "SVD_model_test",
     )
     model_class.drop()
-    model_class.fit(
-        "public.winequality", ["citric_acid", "residual_sugar", "alcohol"], "quality"
-    )
+    model_class.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
     yield model_class
     model_class.drop()
 
 
-class TestPipeline:
-    def test_index(self, model):
-        assert model[0].type == "Normalizer"
-        assert model[0:][0][0] == "NormalizerWine"
-
-    def test_drop(self, winequality_vd):
-        model_class = Pipeline(
-            [
-                ("NormalizerWine", StandardScaler("std_model_test_drop",),),
-                ("LinearRegressionWine", LinearRegression("linreg_model_test_drop",),),
-            ]
-        )
-        model_class.drop()
-        model_class.fit(winequality_vd, ["alcohol"], "quality")
+class TestSVD:
+    def test_repr(self, model):
+        assert model.__repr__() == "<SVD>"
+
+    def test_deploySQL(self, model):
+        expected_sql = 'APPLY_SVD("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'SVD_model_test\', match_by_pos = \'true\', cutoff = 1)'
+        result_sql = model.deploySQL()
+
+        assert result_sql == expected_sql
+
+    def test_deployInverseSQL(self, model):
+        expected_sql = 'APPLY_INVERSE_SVD("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'SVD_model_test\', match_by_pos = \'true\')'
+        result_sql = model.deployInverseSQL()
+
+        assert result_sql == expected_sql
+
+    def test_plot(self, model):
+        result = model.plot()
+        assert len(result.get_default_bbox_extra_artists()) == 8
+        result = model.plot(dimensions=(2, 3))
+        assert len(result.get_default_bbox_extra_artists()) == 8
+
+    def test_plot_scree(self, model):
+        result = model.plot_scree()
+        assert len(result.get_default_bbox_extra_artists()) == 15
+
+    def test_plot_circle(self, model):
+        result = model.plot_circle()
+        assert len(result.get_default_bbox_extra_artists()) == 16
+        result = model.plot_circle(dimensions=(2, 3))
+        assert len(result.get_default_bbox_extra_artists()) == 16
+
+    def test_drop(self):
+        current_cursor().execute("DROP MODEL IF EXISTS SVD_model_test_drop")
+        model_test = SVD(
+            "SVD_model_test_drop",
+        )
+        model_test.fit("public.winequality", ["alcohol", "quality"])
+
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name IN ('linreg_model_test_drop', 'std_model_test_drop')"
+            "SELECT model_name FROM models WHERE model_name = 'SVD_model_test_drop'"
         )
-        assert len(current_cursor().fetchall()) == 2
-        model_class.drop()
+        assert current_cursor().fetchone()[0] == "SVD_model_test_drop"
+
+        model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name IN ('linreg_model_test_drop', 'std_model_test_drop')"
+            "SELECT model_name FROM models WHERE model_name = 'SVD_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_params(self, model):
-        assert model.get_params() == {
-            "LinearRegressionWine": {
-                "max_iter": 100,
-                "penalty": "none",
-                "solver": "newton",
-                "tol": 1e-06,
-            },
-            "NormalizerWine": {"method": "zscore"},
-        }
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
-    def test_set_params(self, model):
-        model.set_params({"NormalizerWine": {"method": "robust_zscore"}})
-        assert model.get_params()["NormalizerWine"] == {"method": "robust_zscore"}
-        model.set_params({"NormalizerWine": {"method": "zscore"}})
+        assert m_att["attr_name"] == [
+            "columns",
+            "singular_values",
+            "right_singular_vectors",
+            "counters",
+            "call_string",
+        ]
+        assert m_att["attr_fields"] == [
+            "index, name",
+            "index, value, explained_variance, accumulated_explained_variance",
+            "index, vector1, vector2, vector3",
+            "counter_name, counter_value",
+            "call_string",
+        ]
+        assert m_att["#_of_rows"] == [3, 3, 3, 3, 1]
+
+        m_att_details = model.get_vertica_attributes(attr_name="singular_values")
+
+        assert m_att_details["value"][0] == pytest.approx(968.964362586858, abs=1e-6)
+        assert m_att_details["value"][1] == pytest.approx(354.585184720344, abs=1e-6)
+        assert m_att_details["value"][2] == pytest.approx(11.7281921567471, abs=1e-6)
+
+    def test_get_params(self, model):
+        assert model.get_params() == {"method": "lapack", "n_components": 0}
 
     def test_to_python(self, model):
-        predict_function = model.to_python()
-        test_record = tablesample(
-            {"citric_acid": [3.0], "residual_sugar": [11.0], "alcohol": [93.0]}
-        ).to_vdf()
-        prediction = model.predict(
-            test_record, ["citric_acid", "residual_sugar", "alcohol"]
-        )[0][-1]
-        assert prediction == pytest.approx(predict_function([[3.0, 11.0, 93.0]])[0])
-
-    def test_get_predicts(self, winequality_vd, model):
-        winequality_copy = winequality_vd.copy()
-        winequality_copy = model.predict(
-            winequality_copy,
-            X=["citric_acid", "residual_sugar", "alcohol"],
-            name="predicted_quality",
-        )
-
-        assert winequality_copy["predicted_quality"].mean() == pytest.approx(
-            5.818378, abs=1e-6
-        )
-
-    def test_report(self, model):
-        reg_rep = model.report()
-
-        assert reg_rep["index"] == [
-            "explained_variance",
-            "max_error",
-            "median_absolute_error",
-            "mean_absolute_error",
-            "mean_squared_error",
-            "root_mean_squared_error",
-            "r2",
-            "r2_adj",
-            "aic",
-            "bic",
-        ]
-        assert reg_rep["value"][0] == pytest.approx(0.219816, abs=1e-6)
-        assert reg_rep["value"][1] == pytest.approx(3.592465, abs=1e-6)
-        assert reg_rep["value"][2] == pytest.approx(0.496031, abs=1e-6)
-        assert reg_rep["value"][3] == pytest.approx(0.609075, abs=1e-6)
-        assert reg_rep["value"][4] == pytest.approx(0.594856, abs=1e-6)
-        assert reg_rep["value"][5] == pytest.approx(0.7712695123858948, abs=1e-6)
-        assert reg_rep["value"][6] == pytest.approx(0.219816, abs=1e-6)
-        assert reg_rep["value"][7] == pytest.approx(0.21945605202370688, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-3366.7617912479104, abs=1e-6)
-        assert reg_rep["value"][9] == pytest.approx(-3339.65156943384, abs=1e-6)
-
-        model_class = Pipeline(
-            [
-                ("NormalizerWine", StandardScaler("logstd_model_test"),),
-                ("LogisticRegressionWine", LogisticRegression("logreg_model_test"),),
-            ]
-        )
-        model_class.drop()
-        model_class.fit("public.winequality", ["alcohol"], "good")
-        cls_rep1 = model_class.report().transpose()
-        assert cls_rep1["auc"][0] == pytest.approx(0.7642901826299067)
-        assert cls_rep1["prc_auc"][0] == pytest.approx(0.45326090911518313)
-        assert cls_rep1["accuracy"][0] == pytest.approx(0.8131445282438048)
-        assert cls_rep1["log_loss"][0] == pytest.approx(0.182720882885624)
-        assert cls_rep1["precision"][0] == pytest.approx(0.5595463137996219)
-        assert cls_rep1["recall"][0] == pytest.approx(0.2317932654659358)
-        assert cls_rep1["f1_score"][0] == pytest.approx(0.3277962347729789)
-        assert cls_rep1["mcc"][0] == pytest.approx(0.2719537880298097)
-        assert cls_rep1["informedness"][0] == pytest.approx(0.18715725014026519)
-        assert cls_rep1["markedness"][0] == pytest.approx(0.3951696381964047)
-        assert cls_rep1["csi"][0] == pytest.approx(0.19602649006622516)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.5)
-
-        model_class.drop()
-
-    def test_score(self, model):
-        # method = "max"
-        assert model.score(method="max") == pytest.approx(3.592465, abs=1e-6)
-        # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0.609075, abs=1e-6)
-        # method = "median"
-        assert model.score(method="median") == pytest.approx(0.496031, abs=1e-6)
-        # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.594856660735976, abs=1e-6)
-        # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.7712695123858948, abs=1e-6)
-        # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.002509, abs=1e-6)
-        # method = "r2"
-        assert model.score() == pytest.approx(0.219816, abs=1e-6)
-        # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(0.21945605202370688, abs=1e-6)
-        # method = "var"
-        assert model.score(method="var") == pytest.approx(0.219816, abs=1e-6)
-        # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-3366.7617912479104, abs=1e-6)
-        # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-3339.65156943384, abs=1e-6)
-
-    def test_transform(self, winequality_vd, model):
-        model_class = Pipeline(
-            [
-                ("NormalizerWine", StandardScaler("logstd_model_test"),),
-                ("NormalizerWine", MinMaxScaler("logmm_model_test"),),
-            ]
-        )
-        model_class.drop()
-        model_class.fit("public.winequality", ["alcohol"])
-        winequality_copy = winequality_vd.copy()
-        winequality_copy = model_class.transform(winequality_copy, X=["alcohol"])
-        assert winequality_copy["alcohol"].mean() == pytest.approx(
-            0.361130555239542, abs=1e-6
-        )
-
-        model_class.drop()
-
-    def test_inverse_transform(self, winequality_vd, model):
-        model_class = Pipeline(
-            [
-                ("NormalizerWine", StandardScaler("logstd_model_test"),),
-                ("NormalizerWine", MinMaxScaler("logmm_model_test"),),
-            ]
-        )
-        model_class.drop()
-        model_class.fit("public.winequality", ["alcohol"])
-        winequality_copy = winequality_vd.copy()
-        winequality_copy = model_class.inverse_transform(
-            winequality_copy, X=["alcohol"],
+        current_cursor().execute(
+            "SELECT APPLY_SVD(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
+                model.model_name
+            )
         )
-        assert winequality_copy["alcohol"].mean() == pytest.approx(
-            80.3934257349546, abs=1e-6
+        prediction = current_cursor().fetchone()
+        assert prediction == pytest.approx(model.to_python()([[3.0, 11.0, 93.0]])[0])
+
+    def test_to_sql(self, model):
+        current_cursor().execute(
+            "SELECT APPLY_SVD(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
+                model.model_name
+            )
+        )
+        prediction = [float(elem) for elem in current_cursor().fetchone()]
+        current_cursor().execute(
+            "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
+                ", ".join(model.to_sql())
+            )
         )
+        prediction2 = [float(elem) for elem in current_cursor().fetchone()]
+        assert prediction == pytest.approx(prediction2)
 
-        model_class.drop()
+    def test_to_memmodel(self, model):
+        current_cursor().execute(
+            "SELECT APPLY_SVD(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
+                model.model_name
+            )
+        )
+        prediction = [float(elem) for elem in current_cursor().fetchone()]
+        current_cursor().execute(
+            "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
+                ", ".join(
+                    model.to_memmodel().transform_sql(
+                        ["citric_acid", "residual_sugar", "alcohol"]
+                    )
+                )
+            )
+        )
+        prediction2 = [float(elem) for elem in current_cursor().fetchone()]
+        assert prediction == pytest.approx(prediction2)
+        prediction3 = model.to_memmodel().transform([[3.0, 11.0, 93.0]])
+        assert prediction == pytest.approx(list(prediction3[0]))
 
-    def test_model_from_vDF(self, winequality_vd):
-        model_test = Pipeline(
-            [
-                ("NormalizerWine", StandardScaler("std_model_test_vdf",),),
-                ("LinearRegressionWine", LinearRegression("linreg_model_test_vdf",),),
-            ]
+    def test_get_transform(self, winequality_vd, model):
+        winequality_trans = model.transform(
+            winequality_vd, X=["citric_acid", "residual_sugar", "alcohol"]
         )
-        model_test.drop()
-        model_test.fit(
-            winequality_vd, ["citric_acid", "residual_sugar", "alcohol"], "quality"
+        assert winequality_trans["col1"].mean() == pytest.approx(
+            0.0121807874344058, abs=1e-6
+        )
+        assert winequality_trans["col2"].mean() == pytest.approx(
+            -0.00200082024084619, abs=1e-6
+        )
+        assert winequality_trans["col3"].mean() == pytest.approx(
+            0.000194341623203586, abs=1e-6
+        )
+
+    def test_get_inverse_transform(self, winequality_vd, model):
+        winequality_trans = model.transform(
+            winequality_vd, X=["citric_acid", "residual_sugar", "alcohol"]
+        )
+        winequality_trans = model.inverse_transform(
+            winequality_trans, X=["col1", "col2", "col3"]
+        )
+        assert winequality_trans["citric_acid"].mean() == pytest.approx(
+            winequality_vd["citric_acid"].mean(), abs=1e-6
+        )
+        assert winequality_trans["residual_sugar"].mean() == pytest.approx(
+            winequality_vd["residual_sugar"].mean(), abs=1e-6
+        )
+        assert winequality_trans["alcohol"].mean() == pytest.approx(
+            winequality_vd["alcohol"].mean(), abs=1e-6
+        )
+
+    def test_svd_score(self, model):
+        result = model.score()
+        assert result["Score"][0] == pytest.approx(0.0, abs=1e-6)
+        assert result["Score"][1] == pytest.approx(0.0, abs=1e-6)
+        assert result["Score"][2] == pytest.approx(0.0, abs=1e-6)
+
+    def test_set_params(self, model):
+        model.set_params({"n_components": 3})
+        assert model.get_params()["n_components"] == 3
+
+    def test_model_from_vDF(self, winequality_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS SVD_vDF")
+        model_test = SVD(
+            "SVD_vDF",
         )
+        model_test.fit(winequality_vd, ["alcohol", "quality"])
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name IN ('std_model_test_vdf', 'linreg_model_test_vdf')"
+            "SELECT model_name FROM models WHERE model_name = 'SVD_vDF'"
         )
-        assert len(current_cursor().fetchall()) == 2
+        assert current_cursor().fetchone()[0] == "SVD_vDF"
         model_test.drop()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_randomforest_classifier.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_randomforest_classifier.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,33 +1,37 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import (
     vDataFrame,
     drop,
     set_option,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic, load_dataset_cl
 from verticapy.learn.ensemble import RandomForestClassifier
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
@@ -37,15 +41,17 @@
     drop(name="public.rfc_data", method="table")
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(rfc_data_vd):
     current_cursor().execute("DROP MODEL IF EXISTS rfc_model_test")
 
     current_cursor().execute(
@@ -64,35 +70,23 @@
         min_info_gain=0,
         nbins=40,
     )
     model_class.input_relation = "public.rfc_data"
     model_class.test_relation = model_class.input_relation
     model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
     model_class.y = "TransPortation"
-    current_cursor().execute(
-        "SELECT DISTINCT {} FROM {} WHERE {} IS NOT NULL ORDER BY 1".format(
-            model_class.y, model_class.input_relation, model_class.y
-        )
-    )
-    classes = current_cursor().fetchall()
-    model_class.classes_ = [item[0] for item in classes]
+    model_class._compute_attributes()
 
     yield model_class
     model_class.drop()
 
 
 class TestRFC:
     def test_repr(self, model):
-        assert (
-            "SELECT rf_classifier('public.rfc_model_test', 'public.rfc_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, TransPortation', ntree=3, mtry=4, sampling_size=1, max_depth=6, max_breadth=100, min_leaf_size=1, min_info_gain=0, nbins=40);"
-            in model.__repr__()
-        )
-        model_repr = RandomForestClassifier("RF_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<RandomForestClassifier>"
+        assert model.__repr__() == "<RandomForestClassifier>"
 
     def test_classification_report(self, model):
         cls_rep1 = model.classification_report().transpose()
 
         assert cls_rep1["auc"][0] == pytest.approx(1.0)
         assert cls_rep1["prc_auc"][0] == pytest.approx(1.0)
         assert cls_rep1["accuracy"][0] == pytest.approx(1.0)
@@ -100,52 +94,53 @@
         assert cls_rep1["precision"][0] == pytest.approx(1.0)
         assert cls_rep1["recall"][0] == pytest.approx(1.0)
         assert cls_rep1["f1_score"][0] == pytest.approx(1.0)
         assert cls_rep1["mcc"][0] == pytest.approx(1.0)
         assert cls_rep1["informedness"][0] == pytest.approx(1.0)
         assert cls_rep1["markedness"][0] == pytest.approx(1.0)
         assert cls_rep1["csi"][0] == pytest.approx(1.0)
-        assert cls_rep1["cutoff"][0] == pytest.approx(0.999)
-
-        cls_rep2 = model.classification_report(cutoff=0.999).transpose()
-
-        assert cls_rep2["cutoff"][0] == pytest.approx(0.999)
 
     def test_confusion_matrix(self, model):
         conf_mat1 = model.confusion_matrix()
 
-        assert conf_mat1["Bus"] == [4, 0, 0]
-        assert conf_mat1["Car"] == [0, 3, 0]
-        assert conf_mat1["Train"] == [0, 0, 3]
+        assert list(conf_mat1[:, 0]) == [4, 0, 0]
+        assert list(conf_mat1[:, 1]) == [0, 3, 0]
+        assert list(conf_mat1[:, 2]) == [0, 0, 3]
 
         conf_mat2 = model.confusion_matrix(cutoff=0.2)
 
-        assert conf_mat2["Bus"] == [4, 0, 0]
-        assert conf_mat2["Car"] == [0, 3, 0]
-        assert conf_mat2["Train"] == [0, 0, 3]
+        assert list(conf_mat2[:, 0]) == [4, 0, 0]
+        assert list(conf_mat2[:, 1]) == [0, 3, 0]
+        assert list(conf_mat2[:, 2]) == [0, 0, 3]
 
     def test_contour(self, titanic_vd):
-        model_test = RandomForestClassifier("model_contour",)
+        model_test = RandomForestClassifier(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) == 38
         model_test.drop()
 
     def test_deploySQL(self, model):
         expected_sql = 'PREDICT_RF_CLASSIFIER("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'rfc_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS rfc_model_test_drop")
-        model_test = RandomForestClassifier("rfc_model_test_drop",)
+        model_test = RandomForestClassifier(
+            "rfc_model_test_drop",
+        )
         model_test.fit(
             "public.rfc_data",
             ["Gender", '"owned cars"', "cost", "income"],
             "TransPortation",
         )
 
         current_cursor().execute(
@@ -156,23 +151,34 @@
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'rfc_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        f_imp = model.features_importance()
+        f_imp = model.features_importance(show=False)
 
         assert f_imp["index"] == ["cost", "owned cars", "gender", "income"]
         assert f_imp["importance"] == [75.76, 15.15, 9.09, 0.0]
         assert f_imp["sign"] == [1, 1, 1, 0]
         plt.close("all")
 
+    def test_get_score(self, model):
+        fim = model.get_score()
+
+        assert fim["predictor_name"] == ["gender", "owned cars", "cost", "income"]
+        assert fim["importance_value"] == [
+            pytest.approx(0.0909090909090909),
+            pytest.approx(0.151515151515152),
+            pytest.approx(0.757575757575758),
+            pytest.approx(0.0),
+        ]
+
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000)
+        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000, show=False)
 
         assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
         assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(1.0)
         assert lift_ch["lift"][300] == pytest.approx(2.5)
         assert lift_ch["decision_boundary"][900] == pytest.approx(0.9)
         assert lift_ch["positive_prediction_ratio"][900] == pytest.approx(1.0)
         assert lift_ch["lift"][900] == pytest.approx(2.5)
@@ -182,26 +188,20 @@
         model_test = RandomForestClassifier("rfc_python_test")
         model_test.drop()
         model_test.fit(titanic_vd, ["age", "fare", "sex"], "embarked")
         current_cursor().execute(
             "SELECT PREDICT_RF_CLASSIFIER(30.0, 45.0, 'male' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 45.0, "male"]])[0]
-        )
+        assert prediction == model_test.to_python()([[30.0, 45.0, "male"]])[0]
         current_cursor().execute(
             "SELECT PREDICT_RF_CLASSIFIER(30.0, 145.0, 'female' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 145.0, "female"]])[0]
-        )
+        assert prediction == model_test.to_python()([[30.0, 145.0, "female"]])[0]
 
     def test_to_sql(self, model, titanic_vd):
         model_test = RandomForestClassifier("rfc_sql_test")
         model_test.drop()
         model_test.fit(titanic_vd, ["age", "fare", "sex"], "survived")
         current_cursor().execute(
             "SELECT PREDICT_RF_CLASSIFIER(* USING PARAMETERS model_name = 'rfc_sql_test', match_by_pos=True)::int, {}::int FROM (SELECT 30.0 AS age, 45.0 AS fare, 'male' AS sex) x".format(
@@ -253,31 +253,31 @@
         )
         model.predict_proba(
             vdf, name="prediction_proba_vertica_sql_1", pos_label=model.classes_[1]
         )
         model.predict_proba(
             vdf, name="prediction_proba_vertica_sql_2", pos_label=model.classes_[2]
         )
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", "r2"
+            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", metric="r2"
         )
         assert score == pytest.approx(1.0)
 
-    def test_get_attr(self, model):
-        attr = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        attr = model.get_vertica_attributes()
         assert attr["attr_name"] == [
             "tree_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "details",
         ]
@@ -286,28 +286,34 @@
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "predictor, type",
         ]
         assert attr["#_of_rows"] == [1, 1, 1, 1, 4]
 
-        details = model.get_attr("details")
+        details = model.get_vertica_attributes("details")
         assert details["predictor"] == ["gender", "owned cars", "cost", "income"]
         assert details["type"] == [
             "char or varchar",
             "int",
             "char or varchar",
             "char or varchar",
         ]
 
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("tree_count")["tree_count"][0] == 3
         assert (
-            model.get_attr("call_string")["call_string"][0]
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 3
+        assert (
+            model.get_vertica_attributes("call_string")["call_string"][0]
             == "SELECT rf_classifier('public.rfc_model_test', 'public.rfc_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, TransPortation', ntree=3, mtry=4, sampling_size=1, max_depth=6, max_breadth=100, min_leaf_size=1, min_info_gain=0, nbins=40);"
         )
 
     def test_get_params(self, model):
         params = model.get_params()
 
         assert params == {
@@ -318,15 +324,15 @@
             "max_depth": 6,
             "min_samples_leaf": 1,
             "min_info_gain": 0,
             "nbins": 40,
         }
 
     def test_prc_curve(self, model):
-        prc = model.prc_curve(pos_label="Car", nbins=1000)
+        prc = model.prc_curve(pos_label="Car", nbins=1000, show=False)
 
         assert prc["threshold"][300] == pytest.approx(0.299)
         assert prc["recall"][300] == pytest.approx(1.0)
         assert prc["precision"][300] == pytest.approx(1.0)
         assert prc["threshold"][800] == pytest.approx(0.799)
         assert prc["recall"][800] == pytest.approx(1.0)
         assert prc["precision"][800] == pytest.approx(1.0)
@@ -352,121 +358,125 @@
         assert rfc_data_copy["prob_train"].avg() == 0.3
         assert rfc_data_copy["prob_car"].avg() == 0.3
 
         model.predict_proba(rfc_data_copy, name="prob_bus_2", pos_label="Bus")
         assert rfc_data_copy["prob_bus_2"].avg() == 0.4
 
     def test_roc_curve(self, model):
-        roc = model.roc_curve(pos_label="Train", nbins=1000)
+        roc = model.roc_curve(pos_label="Train", nbins=1000, show=False)
 
         assert roc["threshold"][100] == pytest.approx(0.1)
         assert roc["false_positive"][100] == pytest.approx(0.0)
         assert roc["true_positive"][100] == pytest.approx(1.0)
         assert roc["threshold"][700] == pytest.approx(0.7)
         assert roc["false_positive"][700] == pytest.approx(0.0)
         assert roc["true_positive"][700] == pytest.approx(1.0)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000)
+        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000, show=False)
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
         assert cutoff_curve["false_positive"][100] == pytest.approx(0.0)
         assert cutoff_curve["true_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
         assert cutoff_curve["false_positive"][700] == pytest.approx(0.0)
         assert cutoff_curve["true_positive"][700] == pytest.approx(1.0)
         plt.close("all")
 
     def test_score(self, model):
-        assert model.score(cutoff=0.9, method="accuracy") == pytest.approx(1.0)
-        assert model.score(cutoff=0.1, method="accuracy") == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="accuracy") == pytest.approx(1.0)
+        assert model.score(cutoff=0.1, metric="accuracy") == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="auc", pos_label="Train"
+            cutoff=0.9, metric="auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="auc", pos_label="Train"
+            cutoff=0.1, metric="auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="best_cutoff", pos_label="Train"
+            cutoff=0.9, metric="best_cutoff", pos_label="Train"
         ) == pytest.approx(0.999)
         assert model.score(
-            cutoff=0.1, method="best_cutoff", pos_label="Train"
+            cutoff=0.1, metric="best_cutoff", pos_label="Train"
         ) == pytest.approx(0.999)
-        assert model.score(cutoff=0.9, method="bm", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.9, metric="bm", pos_label="Train") == pytest.approx(
+            1.0
         )
-        assert model.score(cutoff=0.1, method="bm", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="bm", pos_label="Train") == pytest.approx(
+            1.0
         )
         assert model.score(
-            cutoff=0.9, method="csi", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="csi", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="csi", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="f1", pos_label="Train") == pytest.approx(
-            0.0
+            cutoff=0.1, metric="csi", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="f1", pos_label="Train") == pytest.approx(
+            1.0
         )
-        assert model.score(cutoff=0.1, method="f1", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="f1", pos_label="Train") == pytest.approx(
+            1.0
         )
         assert model.score(
-            cutoff=0.9, method="logloss", pos_label="Train"
+            cutoff=0.9, metric="logloss", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.1, method="logloss", pos_label="Train"
+            cutoff=0.1, metric="logloss", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.9, method="mcc", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="mcc", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="mcc", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="mk", pos_label="Train") == pytest.approx(
-            0.0
+            cutoff=0.1, metric="mcc", pos_label="Train"
+        ) == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="mk", pos_label="Train") == pytest.approx(
+            1.0
         )
-        assert model.score(cutoff=0.1, method="mk", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="mk", pos_label="Train") == pytest.approx(
+            1.0
         )
         assert model.score(
-            cutoff=0.9, method="npv", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="npv", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="npv", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.1, metric="npv", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="prc_auc", pos_label="Train"
+            cutoff=0.9, metric="prc_auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="prc_auc", pos_label="Train"
+            cutoff=0.1, metric="prc_auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="precision", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="precision", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="precision", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.1, metric="precision", pos_label="Train"
+        ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="specificity", pos_label="Train"
+            cutoff=0.9, metric="specificity", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="specificity", pos_label="Train"
+            cutoff=0.1, metric="specificity", pos_label="Train"
         ) == pytest.approx(1.0)
 
     def test_set_params(self, model):
         model.set_params({"nbins": 1000})
 
         assert model.get_params()["nbins"] == 1000
 
     def test_model_from_vDF(self, rfc_data_vd):
         current_cursor().execute("DROP MODEL IF EXISTS rfc_from_vDF")
-        model_test = RandomForestClassifier("rfc_from_vDF",)
+        model_test = RandomForestClassifier(
+            "rfc_from_vDF",
+        )
         model_test.fit(
-            rfc_data_vd, ["Gender", '"owned cars"', "cost", "income"], "TransPortation"
+            rfc_data_vd,
+            ["Gender", '"owned cars"', "cost", "income"],
+            "TransPortation",
         )
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'rfc_from_vDF'"
         )
         assert current_cursor().fetchone()[0] == "rfc_from_vDF"
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_randomforest_regressor.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_dummy_tree_regressor.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,142 +1,137 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
 from verticapy import (
     vDataFrame,
     drop,
     set_option,
 )
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_titanic, load_winequality, load_dataset_reg
-from verticapy.learn.ensemble import RandomForestRegressor
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_titanic, load_dataset_reg
+from verticapy.learn.tree import DummyTreeRegressor
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
-def winequality_vd():
-    winequality = load_winequality()
-    yield winequality
-    drop(name="public.winequality",)
+def tr_data_vd():
+    tr_data = load_dataset_reg(table_name="tr_data", schema="public")
+    yield tr_data
+    drop(name="public.tr_data", method="table")
 
 
 @pytest.fixture(scope="module")
-def titanic_vd():
-    titanic = load_titanic()
-    yield titanic
-    drop(name="public.titanic",)
-
-
-@pytest.fixture(scope="module")
-def rfr_data_vd():
-    rfr_data = load_dataset_reg(table_name="rfr_data", schema="public")
-    yield rfr_data
-    drop(name="public.rfr_data", method="table")
-
-
-@pytest.fixture(scope="module")
-def model(rfr_data_vd):
-    current_cursor().execute("DROP MODEL IF EXISTS rfr_model_test")
+def model(tr_data_vd):
+    current_cursor().execute("DROP MODEL IF EXISTS tr_model_test")
 
     current_cursor().execute(
-        "SELECT rf_regressor('rfr_model_test', 'public.rfr_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, transportation', mtry=4, ntree=3, max_breadth=100, sampling_size=1, max_depth=6, min_leaf_size=1, min_info_gain=0.0, nbins=40, seed=1, id_column='id')"
+        "SELECT rf_regressor('tr_model_test', 'public.tr_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, transportation', mtry=4, ntree=1, max_breadth=1e9, sampling_size=1, max_depth=100, min_leaf_size=1, min_info_gain=0.0, nbins=1000, seed=1, id_column='id')"
     )
 
     # I could use load_model but it is buggy
-    model_class = RandomForestRegressor(
-        "rfr_model_test",
-        n_estimators=3,
-        max_features=4,
-        max_leaf_nodes=100,
-        sample=1.0,
-        max_depth=6,
-        min_samples_leaf=1,
-        min_info_gain=0.0,
-        nbins=40,
+    model_class = DummyTreeRegressor(
+        "tr_model_test",
     )
-    model_class.input_relation = "public.rfr_data"
+    model_class.input_relation = "public.tr_data"
     model_class.test_relation = model_class.input_relation
     model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
     model_class.y = '"TransPortation"'
+    model_class._compute_attributes()
 
     yield model_class
     model_class.drop()
 
 
-class TestRFR:
+@pytest.fixture(scope="module")
+def titanic_vd():
+    titanic = load_titanic()
+    yield titanic
+    drop(
+        name="public.titanic",
+    )
+
+
+class TestDummyTreeRegressor:
     def test_repr(self, model):
-        assert "rf_regressor" in model.__repr__()
-        model_repr = RandomForestRegressor("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<RandomForestRegressor>"
+        assert model.__repr__() == "<RandomForestRegressor>"
 
     def test_contour(self, titanic_vd):
-        model_test = RandomForestRegressor("model_contour",)
+        model_test = DummyTreeRegressor(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
-        assert len(result.get_default_bbox_extra_artists()) == 38
+        assert len(result.get_default_bbox_extra_artists()) == 34
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_RF_REGRESSOR("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'rfr_model_test\', match_by_pos = \'true\')'
+        expected_sql = 'PREDICT_RF_REGRESSOR("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'tr_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS rfr_model_test_drop")
-        model_test = RandomForestRegressor("rfr_model_test_drop",)
+        current_cursor().execute("DROP MODEL IF EXISTS tr_model_test_drop")
+        model_test = DummyTreeRegressor(
+            "tr_model_test_drop",
+        )
         model_test.fit(
-            "public.rfr_data",
+            "public.tr_data",
             ["Gender", '"owned cars"', "cost", "income"],
             "TransPortation",
         )
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'rfr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "rfr_model_test_drop"
+        assert current_cursor().fetchone()[0] == "tr_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'rfr_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'tr_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
         assert fim["index"] == ["cost", "owned cars", "gender", "income"]
         assert fim["importance"] == [88.41, 7.25, 4.35, 0.0]
         assert fim["sign"] == [1, 1, 1, 0]
         plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "tree_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "details",
@@ -146,68 +141,61 @@
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "predictor, type",
         ]
         assert m_att["#_of_rows"] == [1, 1, 1, 1, 4]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
-        assert m_att_details["predictor"] == ["gender", "owned cars", "cost", "income"]
+        assert m_att_details["predictor"] == [
+            "gender",
+            "owned cars",
+            "cost",
+            "income",
+        ]
         assert m_att_details["type"] == [
             "char or varchar",
             "int",
             "char or varchar",
             "char or varchar",
         ]
 
-        assert model.get_attr("tree_count")["tree_count"][0] == 3
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 1
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
+        )
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "SELECT rf_regressor('public.rfr_model_test', 'public.rfr_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, transportation', ntree=3, mtry=4, sampling_size=1, max_depth=6, max_breadth=100, min_leaf_size=1, min_info_gain=0, nbins=40);"
+            model.get_vertica_attributes("call_string")["call_string"][0]
+            == "SELECT rf_regressor('public.tr_model_test', 'public.tr_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, transportation', ntree=1, mtry=4, sampling_size=1, max_depth=100, max_breadth=1000000000, min_leaf_size=1, min_info_gain=0, nbins=1000);"
         )
 
     def test_get_params(self, model):
-        assert model.get_params() == {
-            "n_estimators": 3,
-            "max_features": 4,
-            "max_leaf_nodes": 100,
-            "sample": 1,
-            "max_depth": 6,
-            "min_samples_leaf": 1,
-            "min_info_gain": 0,
-            "nbins": 40,
-        }
-
-    def test_get_plot(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = RandomForestRegressor("model_test_plot",)
-        model_test.fit(winequality_vd, ["alcohol"], "quality")
-        result = model_test.plot()
-        assert len(result.get_default_bbox_extra_artists()) == 9
-        plt.close("all")
-        model_test.drop()
+        assert model.get_params() == {}
 
     def test_to_python(self, model):
         current_cursor().execute(
             "SELECT PREDICT_RF_REGRESSOR('Male', 0, 'Cheap', 'Low' USING PARAMETERS model_name = '{}', match_by_pos=True)::float".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
-            model.to_python(return_str=False)([["Male", 0, "Cheap", "Low"]])[0]
+            model.to_python()([["Male", 0, "Cheap", "Low"]])[0]
         )
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT PREDICT_RF_REGRESSOR(* USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float FROM (SELECT 'Male' AS \"Gender\", 0 AS \"owned cars\", 'Cheap' AS \"cost\", 'Low' AS \"income\") x".format(
-                model.name, model.to_sql()
+                model.model_name, model.to_sql()
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model):
         mmodel = model.to_memmodel()
@@ -215,31 +203,31 @@
             [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
         )
         res_py = model.to_python()(
             [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
         )
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
-        vdf = vDataFrame("public.rfr_data")
+        vdf = vDataFrame("public.tr_data")
         vdf["prediction_sql"] = mmodel.predict_sql(
             ['"Gender"', '"owned cars"', '"cost"', '"income"']
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="r2")
         assert score == pytest.approx(1.0)
 
-    def test_get_predicts(self, rfr_data_vd, model):
-        rfr_data_copy = rfr_data_vd.copy()
+    def test_get_predicts(self, tr_data_vd, model):
+        tr_data_copy = tr_data_vd.copy()
         model.predict(
-            rfr_data_copy,
+            tr_data_copy,
             X=["Gender", '"owned cars"', "cost", "income"],
             name="predicted_quality",
         )
 
-        assert rfr_data_copy["predicted_quality"].mean() == pytest.approx(0.9, abs=1e-6)
+        assert tr_data_copy["predicted_quality"].mean() == pytest.approx(0.9, abs=1e-6)
 
     def test_regression_report(self, model):
         reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
             "max_error",
@@ -259,94 +247,97 @@
         assert reg_rep["value"][4] == pytest.approx(0.0, abs=1e-6)
         assert reg_rep["value"][5] == pytest.approx(0.0, abs=1e-6)
         assert reg_rep["value"][6] == pytest.approx(1.0, abs=1e-6)
         assert reg_rep["value"][7] == pytest.approx(1.0, abs=1e-6)
         assert reg_rep["value"][8] == pytest.approx(-float("inf"), abs=1e-6)
         assert reg_rep["value"][9] == pytest.approx(-float("inf"), abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
             10.0,
             4,
             pytest.approx(1.0),
             pytest.approx(1.0),
             float("inf"),
             pytest.approx(0.0),
             pytest.approx(-1.73372940858763),
             pytest.approx(0.223450528977454),
             pytest.approx(3.76564442746721),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
             pytest.approx(6.9),
             pytest.approx(0.0),
             pytest.approx(6.9),
         ]
-        assert reg_rep_anova["MS"][:-1] == [pytest.approx(1.725), pytest.approx(0.0)]
+        assert reg_rep_anova["MS"][:-1] == [
+            pytest.approx(1.725),
+            pytest.approx(0.0),
+        ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(0, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(0, abs=1e-6)
+        assert model.score(metric="median") == pytest.approx(0, abs=1e-6)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.0, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.0, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(0.0, abs=1e-6)
+        assert model.score(metric="msle") == pytest.approx(0.0, abs=1e-6)
         # method = "r2"
         assert model.score() == pytest.approx(1.0, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="r2a") == pytest.approx(1.0, abs=1e-6)
         # method = "var"
-        assert model.score(method="var") == pytest.approx(1.0, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(1.0, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-float("inf"), abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-float("inf"), abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-float("inf"), abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-float("inf"), abs=1e-6)
 
     def test_set_params(self, model):
-        model.set_params({"max_features": 1000})
-
-        assert model.get_params()["max_features"] == 1000
+        model.set_params({})
 
-    def test_model_from_vDF(self, rfr_data_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS rfr_from_vDF")
-        model_test = RandomForestRegressor("rfr_from_vDF",)
-        model_test.fit(rfr_data_vd, ["gender"], "transportation")
+    def test_model_from_vDF(self, tr_data_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS tr_from_vDF")
+        model_test = DummyTreeRegressor(
+            "tr_from_vDF",
+        )
+        model_test.fit(tr_data_vd, ["gender"], "transportation")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'rfr_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'tr_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "rfr_from_vDF"
+        assert current_cursor().fetchone()[0] == "tr_from_vDF"
 
         model_test.drop()
 
     def test_to_graphviz(self, model):
-        gvz_tree_1 = model.to_graphviz(
-            tree_id=1,
+        gvz_tree_0 = model.to_graphviz(
+            tree_id=0,
             classes_color=["red", "blue", "green"],
             round_pred=4,
             percent=True,
             vertical=False,
             node_style={"shape": "box", "style": "filled"},
             arrow_style={"color": "blue"},
             leaf_style={"shape": "circle", "style": "filled"},
         )
-        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_1
-        assert "0 -> 1" in gvz_tree_1
+        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_0
+        assert "0 -> 1" in gvz_tree_0
 
     def test_get_tree(self, model):
-        tree_1 = model.get_tree(tree_id=1)
+        tree_0 = model.get_tree()
 
-        assert tree_1["prediction"] == [
+        assert tree_0["prediction"] == [
             None,
             "2.000000",
             None,
             None,
             "1.000000",
             None,
             "0.000000",
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_ridge.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_randomforest_regressor.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,222 +1,281 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-from verticapy import drop, set_option
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_winequality
-from verticapy.learn.linear_model import Ridge
+from verticapy import (
+    vDataFrame,
+    drop,
+    set_option,
+)
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_titanic, load_winequality, load_dataset_reg
+from verticapy.learn.ensemble import RandomForestRegressor
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
     yield winequality
-    drop(name="public.winequality", method="table")
+    drop(
+        name="public.winequality",
+    )
+
+
+@pytest.fixture(scope="module")
+def titanic_vd():
+    titanic = load_titanic()
+    yield titanic
+    drop(
+        name="public.titanic",
+    )
+
+
+@pytest.fixture(scope="module")
+def rfr_data_vd():
+    rfr_data = load_dataset_reg(table_name="rfr_data", schema="public")
+    yield rfr_data
+    drop(name="public.rfr_data", method="table")
 
 
 @pytest.fixture(scope="module")
-def model(winequality_vd):
-    current_cursor().execute("DROP MODEL IF EXISTS ridge_model_test")
-    model_class = Ridge("ridge_model_test",)
-    model_class.fit(
-        "public.winequality", ["citric_acid", "residual_sugar", "alcohol"], "quality"
+def model(rfr_data_vd):
+    current_cursor().execute("DROP MODEL IF EXISTS rfr_model_test")
+
+    current_cursor().execute(
+        "SELECT rf_regressor('rfr_model_test', 'public.rfr_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, transportation', mtry=4, ntree=3, max_breadth=100, sampling_size=1, max_depth=6, min_leaf_size=1, min_info_gain=0.0, nbins=40, seed=1, id_column='id')"
     )
+
+    # I could use load_model but it is buggy
+    model_class = RandomForestRegressor(
+        "rfr_model_test",
+        n_estimators=3,
+        max_features=4,
+        max_leaf_nodes=100,
+        sample=1.0,
+        max_depth=6,
+        min_samples_leaf=1,
+        min_info_gain=0.0,
+        nbins=40,
+    )
+    model_class.input_relation = "public.rfr_data"
+    model_class.test_relation = model_class.input_relation
+    model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
+    model_class.y = '"TransPortation"'
+    model_class._compute_attributes()
+
     yield model_class
     model_class.drop()
 
 
-class TestRidge:
+class TestRFR:
     def test_repr(self, model):
-        assert "|coefficient|std_err |t_value |p_value" in model.__repr__()
-        model_repr = Ridge("lin_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<LinearRegression>"
+        assert model.__repr__() == "<RandomForestRegressor>"
 
-    def test_contour(self, winequality_vd):
-        model_test = Ridge("model_contour",)
+    def test_contour(self, titanic_vd):
+        model_test = RandomForestRegressor(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            winequality_vd, ["citric_acid", "residual_sugar"], "quality",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
-        assert len(result.get_default_bbox_extra_artists()) == 30
+        assert len(result.get_default_bbox_extra_artists()) == 38
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = 'PREDICT_LINEAR_REG("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'ridge_model_test\', match_by_pos = \'true\')'
+        expected_sql = 'PREDICT_RF_REGRESSOR("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'rfr_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS ridge_model_test_drop")
-        model_test = Ridge("ridge_model_test_drop",)
-        model_test.fit("public.winequality", ["alcohol"], "quality")
+        current_cursor().execute("DROP MODEL IF EXISTS rfr_model_test_drop")
+        model_test = RandomForestRegressor(
+            "rfr_model_test_drop",
+        )
+        model_test.fit(
+            "public.rfr_data",
+            ["Gender", '"owned cars"', "cost", "income"],
+            "TransPortation",
+        )
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'ridge_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'rfr_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "ridge_model_test_drop"
+        assert current_cursor().fetchone()[0] == "rfr_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'ridge_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'rfr_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
-        assert fim["index"] == ["alcohol", "residual_sugar", "citric_acid"]
-        assert fim["importance"] == [52.3, 32.63, 15.07]
-        assert fim["sign"] == [1, 1, 1]
+        assert fim["index"] == ["cost", "owned cars", "gender", "income"]
+        assert fim["importance"] == [88.41, 7.25, 4.35, 0.0]
+        assert fim["sign"] == [1, 1, 1, 0]
+        plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_score(self, model):
+        fim = model.get_score()
+
+        assert fim["predictor_name"] == ["gender", "owned cars", "cost", "income"]
+        assert fim["importance_value"] == [
+            pytest.approx(0.0434782608695652),
+            pytest.approx(0.072463768115942),
+            pytest.approx(0.884057971014493),
+            pytest.approx(0.0),
+        ]
+
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
-            "details",
-            "regularization",
-            "iteration_count",
+            "tree_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
+            "details",
         ]
         assert m_att["attr_fields"] == [
-            "predictor, coefficient, std_err, t_value, p_value",
-            "type, lambda",
-            "iteration_count",
+            "tree_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
+            "predictor, type",
         ]
-        assert m_att["#_of_rows"] == [4, 1, 1, 1, 1, 1]
+        assert m_att["#_of_rows"] == [1, 1, 1, 1, 4]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
         assert m_att_details["predictor"] == [
-            "Intercept",
-            "citric_acid",
-            "residual_sugar",
-            "alcohol",
+            "gender",
+            "owned cars",
+            "cost",
+            "income",
         ]
-        assert m_att_details["coefficient"][0] == pytest.approx(
-            1.77574980319025, abs=1e-6
-        )
-        assert m_att_details["coefficient"][1] == pytest.approx(
-            0.431005879933288, abs=1e-6
-        )
-        assert m_att_details["coefficient"][2] == pytest.approx(
-            0.0237636413018576, abs=1e-6
-        )
-        assert m_att_details["coefficient"][3] == pytest.approx(
-            0.359894749137091, abs=1e-6
+        assert m_att_details["type"] == [
+            "char or varchar",
+            "int",
+            "char or varchar",
+            "char or varchar",
+        ]
+
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 3
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
         )
-        assert m_att_details["std_err"][3] == pytest.approx(
-            0.00860813464286587, abs=1e-6
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
         )
-        assert m_att_details["t_value"][3] == pytest.approx(41.8086802853809, abs=1e-6)
-        assert m_att_details["p_value"][1] == pytest.approx(8.96677134128099e-11)
-
-        m_att_regularization = model.get_attr("regularization")
-
-        assert m_att_regularization["type"][0] == "l2"
-        assert m_att_regularization["lambda"][0] == 1
-
-        assert model.get_attr("iteration_count")["iteration_count"][0] == 1
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 6497
         assert (
-            model.get_attr("call_string")["call_string"][0]
-            == "linear_reg('public.ridge_model_test', 'public.winequality', '\"quality\"', '\"citric_acid\", \"residual_sugar\", \"alcohol\"'\nUSING PARAMETERS optimizer='newton', epsilon=1e-06, max_iterations=100, regularization='l2', lambda=1, alpha=0.5)"
+            model.get_vertica_attributes("call_string")["call_string"][0]
+            == "SELECT rf_regressor('public.rfr_model_test', 'public.rfr_data', 'transportation', '*' USING PARAMETERS exclude_columns='id, transportation', ntree=3, mtry=4, sampling_size=1, max_depth=6, max_breadth=100, min_leaf_size=1, min_info_gain=0, nbins=40);"
         )
 
     def test_get_params(self, model):
         assert model.get_params() == {
-            "solver": "newton",
-            "penalty": "l2",
-            "max_iter": 100,
-            "C": 1.0,
-            "tol": 1e-06,
+            "n_estimators": 3,
+            "max_features": 4,
+            "max_leaf_nodes": 100,
+            "sample": 1,
+            "max_depth": 6,
+            "min_samples_leaf": 1,
+            "min_info_gain": 0,
+            "nbins": 40,
         }
 
     def test_get_plot(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = Ridge("model_test_plot",)
+        model_test = RandomForestRegressor(
+            "model_test_plot",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "quality")
         result = model_test.plot()
         assert len(result.get_default_bbox_extra_artists()) == 9
         plt.close("all")
         model_test.drop()
 
     def test_to_python(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)".format(
-                model.name
+            "SELECT PREDICT_RF_REGRESSOR('Male', 0, 'Cheap', 'Low' USING PARAMETERS model_name = '{}', match_by_pos=True)::float".format(
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0]
+            model.to_python()([["Male", 0, "Cheap", "Low"]])[0]
         )
 
     def test_to_sql(self, model):
         current_cursor().execute(
-            "SELECT PREDICT_LINEAR_REG(3.0, 11.0, 93. USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float".format(
-                model.name, model.to_sql([3.0, 11.0, 93.0])
+            "SELECT PREDICT_RF_REGRESSOR(* USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float FROM (SELECT 'Male' AS \"Gender\", 0 AS \"owned cars\", 'Cheap' AS \"cost\", 'Low' AS \"income\") x".format(
+                model.model_name, model.to_sql()
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
-    def test_to_memmodel(self, model, winequality_vd):
+    def test_to_memmodel(self, model):
         mmodel = model.to_memmodel()
-        res = mmodel.predict([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
-        res_py = model.to_python()([[3.0, 11.0, 93.0], [11.0, 1.0, 99.0]])
+        res = mmodel.predict(
+            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
+        )
+        res_py = model.to_python()(
+            [["Male", 0, "Cheap", "Low"], ["Female", 1, "Expensive", "Low"]]
+        )
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
-        vdf = winequality_vd.copy()
+        vdf = vDataFrame("public.rfr_data")
         vdf["prediction_sql"] = mmodel.predict_sql(
-            ["citric_acid", "residual_sugar", "alcohol"]
+            ['"Gender"', '"owned cars"', '"cost"', '"income"']
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="r2")
         assert score == pytest.approx(1.0)
 
-    def test_get_predicts(self, winequality_vd, model):
-        winequality_copy = winequality_vd.copy()
+    def test_get_predicts(self, rfr_data_vd, model):
+        rfr_data_copy = rfr_data_vd.copy()
         model.predict(
-            winequality_copy,
-            X=["citric_acid", "residual_sugar", "alcohol"],
+            rfr_data_copy,
+            X=["Gender", '"owned cars"', "cost", "income"],
             name="predicted_quality",
         )
 
-        assert winequality_copy["predicted_quality"].mean() == pytest.approx(
-            5.8183779, abs=1e-6
-        )
+        assert rfr_data_copy["predicted_quality"].mean() == pytest.approx(0.9, abs=1e-6)
 
     def test_regression_report(self, model):
         reg_rep = model.regression_report()
 
         assert reg_rep["index"] == [
             "explained_variance",
             "max_error",
@@ -225,86 +284,117 @@
             "mean_squared_error",
             "root_mean_squared_error",
             "r2",
             "r2_adj",
             "aic",
             "bic",
         ]
-        assert reg_rep["value"][0] == pytest.approx(0.219816244842147, abs=1e-6)
-        assert reg_rep["value"][1] == pytest.approx(3.59213874427945, abs=1e-6)
-        assert reg_rep["value"][2] == pytest.approx(0.495516023908698, abs=1e-3)
-        assert reg_rep["value"][3] == pytest.approx(0.60908330928705, abs=1e-6)
-        assert reg_rep["value"][4] == pytest.approx(0.594856874272792, abs=1e-6)
-        assert reg_rep["value"][5] == pytest.approx(0.7712696508179172, abs=1e-6)
-        assert reg_rep["value"][6] == pytest.approx(0.219816244842152, abs=1e-6)
-        assert reg_rep["value"][7] == pytest.approx(0.21945577183037412, abs=1e-6)
-        assert reg_rep["value"][8] == pytest.approx(-3366.7594590080057, abs=1e-6)
-        assert reg_rep["value"][9] == pytest.approx(-3339.6492371939366, abs=1e-6)
+        assert reg_rep["value"][0] == pytest.approx(1.0, abs=1e-6)
+        assert reg_rep["value"][1] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][2] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][3] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][4] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][5] == pytest.approx(0.0, abs=1e-6)
+        assert reg_rep["value"][6] == pytest.approx(1.0, abs=1e-6)
+        assert reg_rep["value"][7] == pytest.approx(1.0, abs=1e-6)
+        assert reg_rep["value"][8] == pytest.approx(-float("inf"), abs=1e-6)
+        assert reg_rep["value"][9] == pytest.approx(-float("inf"), abs=1e-6)
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
-            6497.0,
-            3,
-            pytest.approx(0.219816244842152),
-            pytest.approx(0.21945577183037412),
-            pytest.approx(609.4456850593101),
+            10.0,
+            4,
+            pytest.approx(1.0),
+            pytest.approx(1.0),
+            float("inf"),
             pytest.approx(0.0),
-            pytest.approx(0.232322269343305),
-            pytest.approx(0.189622693372695),
-            pytest.approx(53.1115447611131),
+            pytest.approx(-1.73372940858763),
+            pytest.approx(0.223450528977454),
+            pytest.approx(3.76564442746721),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
-            pytest.approx(1088.2688789226),
-            pytest.approx(3864.78511215033),
-            pytest.approx(4953.68570109281),
+            pytest.approx(6.9),
+            pytest.approx(0.0),
+            pytest.approx(6.9),
         ]
         assert reg_rep_anova["MS"][:-1] == [
-            pytest.approx(362.7562929742),
-            pytest.approx(0.5952233346912568),
+            pytest.approx(1.725),
+            pytest.approx(0.0),
         ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") == pytest.approx(3.59213874427945, abs=1e-6)
+        assert model.score(metric="max") == pytest.approx(0, abs=1e-6)
         # method = "mae"
-        assert model.score(method="mae") == pytest.approx(0.60908330928705, abs=1e-6)
+        assert model.score(metric="mae") == pytest.approx(0, abs=1e-6)
         # method = "median"
-        assert model.score(method="median") == pytest.approx(
-            0.495516023908698, abs=1e-3
-        )
+        assert model.score(metric="median") == pytest.approx(0, abs=1e-6)
         # method = "mse"
-        assert model.score(method="mse") == pytest.approx(0.594856874272793, abs=1e-6)
+        assert model.score(metric="mse") == pytest.approx(0.0, abs=1e-6)
         # method = "rmse"
-        assert model.score(method="rmse") == pytest.approx(0.7712696508179179, abs=1e-6)
+        assert model.score(metric="rmse") == pytest.approx(0.0, abs=1e-6)
         # method = "msl"
-        assert model.score(method="msle") == pytest.approx(
-            0.00250970549028931, abs=1e-6
-        )
+        assert model.score(metric="msle") == pytest.approx(0.0, abs=1e-6)
         # method = "r2"
-        assert model.score(method="r2") == pytest.approx(0.219816244842152, abs=1e-6)
+        assert model.score() == pytest.approx(1.0, abs=1e-6)
         # method = "r2a"
-        assert model.score(method="r2a") == pytest.approx(0.21945577183037412, abs=1e-6)
+        assert model.score(metric="r2a") == pytest.approx(1.0, abs=1e-6)
         # method = "var"
-        assert model.score(method="var") == pytest.approx(0.219816244842147, abs=1e-6)
+        assert model.score(metric="var") == pytest.approx(1.0, abs=1e-6)
         # method = "aic"
-        assert model.score(method="aic") == pytest.approx(-3366.7594590080057, abs=1e-6)
+        assert model.score(metric="aic") == pytest.approx(-float("inf"), abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") == pytest.approx(-3339.6492371939366, abs=1e-6)
+        assert model.score(metric="bic") == pytest.approx(-float("inf"), abs=1e-6)
 
     def test_set_params(self, model):
-        model.set_params({"max_iter": 1000})
+        model.set_params({"max_features": 1000})
 
-        assert model.get_params()["max_iter"] == 1000
+        assert model.get_params()["max_features"] == 1000
 
-    def test_model_from_vDF(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS ridge_from_vDF")
-        model_test = Ridge("ridge_from_vDF",)
-        model_test.fit(winequality_vd, ["alcohol"], "quality")
+    def test_model_from_vDF(self, rfr_data_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS rfr_from_vDF")
+        model_test = RandomForestRegressor(
+            "rfr_from_vDF",
+        )
+        model_test.fit(rfr_data_vd, ["gender"], "transportation")
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'ridge_from_vDF'"
+            "SELECT model_name FROM models WHERE model_name = 'rfr_from_vDF'"
         )
-        assert current_cursor().fetchone()[0] == "ridge_from_vDF"
+        assert current_cursor().fetchone()[0] == "rfr_from_vDF"
 
         model_test.drop()
+
+    def test_to_graphviz(self, model):
+        gvz_tree_1 = model.to_graphviz(
+            tree_id=1,
+            classes_color=["red", "blue", "green"],
+            round_pred=4,
+            percent=True,
+            vertical=False,
+            node_style={"shape": "box", "style": "filled"},
+            arrow_style={"color": "blue"},
+            leaf_style={"shape": "circle", "style": "filled"},
+        )
+        assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_1
+        assert "0 -> 1" in gvz_tree_1
+
+    def test_get_tree(self, model):
+        tree_1 = model.get_tree(tree_id=1)
+
+        assert tree_1["prediction"] == [
+            None,
+            "2.000000",
+            None,
+            None,
+            "1.000000",
+            None,
+            "0.000000",
+            "0.000000",
+            "1.000000",
+        ]
+
+    def test_plot_tree(self, model):
+        result = model.plot_tree()
+        assert model.to_graphviz() == result.source.strip()
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_svd.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_kprototypes.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,217 +1,235 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
+# Other Modules
+import matplotlib.pyplot as plt
+
 # VerticaPy
+from verticapy.tests.conftest import get_version
 from verticapy import drop, set_option
-from verticapy.connect import current_cursor
-from verticapy.datasets import load_winequality
-from verticapy.learn.decomposition import SVD
+from verticapy.connection import current_cursor
+from verticapy.datasets import load_iris
+from verticapy.learn.cluster import KPrototypes
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
-def winequality_vd():
-    winequality = load_winequality()
-    yield winequality
-    drop(name="public.winequality",)
+def iris_vd():
+    iris = load_iris()
+    yield iris
+    drop(
+        name="public.iris",
+    )
 
 
 @pytest.fixture(scope="module")
-def model(winequality_vd):
-    model_class = SVD("SVD_model_test",)
+def model(iris_vd):
+    model_class = KPrototypes(
+        "kprototypes_model_test",
+        n_cluster=3,
+        max_iter=10,
+        init=[
+            [7.2, 3.0, 5.8, 1.6, "Iris-setosa"],
+            [6.9, 3.1, 4.9, 1.5, "Iris-versicolor"],
+            [5.7, 4.4, 1.5, 0.4, "Iris-virginica"],
+        ],
+    )
     model_class.drop()
-    model_class.fit("public.winequality", ["citric_acid", "residual_sugar", "alcohol"])
+    model_class.fit(iris_vd)
     yield model_class
     model_class.drop()
 
 
-class TestSVD:
+version = get_version()
+
+
+@pytest.mark.skipif(
+    version[0] < 12 or (version[0] == 12 and version[1] == 0 and version[2] < 3),
+    reason="requires vertica 12.0.3 or higher",
+)
+class TestKPrototypes:
     def test_repr(self, model):
-        assert "SVD" in model.__repr__()
-        model_repr = SVD("model_repr")
-        model_repr.drop()
-        assert model_repr.__repr__() == "<SVD>"
+        assert model.__repr__() == "<KPrototypes>"
 
     def test_deploySQL(self, model):
-        expected_sql = 'APPLY_SVD("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'SVD_model_test\', match_by_pos = \'true\', cutoff = 1)'
+        expected_sql = 'APPLY_KPROTOTYPES("SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species" USING PARAMETERS model_name = \'kprototypes_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
-    def test_deployInverseSQL(self, model):
-        expected_sql = 'APPLY_INVERSE_SVD("citric_acid", "residual_sugar", "alcohol" USING PARAMETERS model_name = \'SVD_model_test\', match_by_pos = \'true\')'
-        result_sql = model.deployInverseSQL()
-
-        assert result_sql == expected_sql
-
-    def test_plot(self, model):
-        result = model.plot()
-        assert len(result.get_default_bbox_extra_artists()) == 8
-        result = model.plot(dimensions=(2, 3))
-        assert len(result.get_default_bbox_extra_artists()) == 8
-
-    def test_plot_scree(self, model):
-        result = model.plot_scree()
-        assert len(result.get_default_bbox_extra_artists()) == 14
-
-    def test_plot_circle(self, model):
-        result = model.plot_circle()
-        assert len(result.get_default_bbox_extra_artists()) == 16
-        result = model.plot_circle(dimensions=(2, 3))
-        assert len(result.get_default_bbox_extra_artists()) == 16
-
     def test_drop(self):
-        current_cursor().execute("DROP MODEL IF EXISTS SVD_model_test_drop")
-        model_test = SVD("SVD_model_test_drop",)
-        model_test.fit("public.winequality", ["alcohol", "quality"])
+        current_cursor().execute("DROP MODEL IF EXISTS kprototypes_model_test_drop")
+        model_test = KPrototypes("kprototypes_model_test_drop", n_cluster=3)
+        model_test.fit("public.iris", ["SepalLengthCm", "SepalWidthCm", "Species"])
 
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'SVD_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'kprototypes_model_test_drop'"
         )
-        assert current_cursor().fetchone()[0] == "SVD_model_test_drop"
+        assert current_cursor().fetchone()[0] == "kprototypes_model_test_drop"
 
         model_test.drop()
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'SVD_model_test_drop'"
+            "SELECT model_name FROM models WHERE model_name = 'kprototypes_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
-        assert m_att["attr_name"] == [
-            "columns",
-            "singular_values",
-            "right_singular_vectors",
-            "counters",
-            "call_string",
-        ]
+        assert m_att["attr_name"] == ["centers", "metrics"]
         assert m_att["attr_fields"] == [
-            "index, name",
-            "index, value, explained_variance, accumulated_explained_variance",
-            "index, vector1, vector2, vector3",
-            "counter_name, counter_value",
-            "call_string",
+            "sepallengthcm, sepalwidthcm, petallengthcm, petalwidthcm, species",
+            "metrics",
         ]
-        assert m_att["#_of_rows"] == [3, 3, 3, 3, 1]
+        assert m_att["#_of_rows"] == [3, 1]
 
-        m_att_details = model.get_attr(attr_name="singular_values")
+        m_att_centers = model.get_vertica_attributes(attr_name="centers")
 
-        assert m_att_details["value"][0] == pytest.approx(968.964362586858, abs=1e-6)
-        assert m_att_details["value"][1] == pytest.approx(354.585184720344, abs=1e-6)
-        assert m_att_details["value"][2] == pytest.approx(11.7281921567471, abs=1e-6)
+        assert m_att_centers["sepallengthcm"] == [
+            pytest.approx(5.006),
+            pytest.approx(5.9156862745098),
+            pytest.approx(6.62244897959184),
+        ]
 
     def test_get_params(self, model):
-        assert model.get_params() == {"method": "lapack", "n_components": 0}
-
-    def test_to_python(self, model):
-        current_cursor().execute(
-            "SELECT APPLY_SVD(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
-            )
-        )
-        prediction = current_cursor().fetchone()
-        assert prediction == pytest.approx(
-            model.to_python(return_str=False)([[3.0, 11.0, 93.0]])[0]
+        assert model.get_params() == {
+            "max_iter": 10,
+            "tol": 0.0001,
+            "n_cluster": 3,
+            "init": [
+                [7.2, 3.0, 5.8, 1.6, "Iris-setosa"],
+                [6.9, 3.1, 4.9, 1.5, "Iris-versicolor"],
+                [5.7, 4.4, 1.5, 0.4, "Iris-virginica"],
+            ],
+            "gamma": 1.0,
+        }
+
+    def test_get_predict(self, iris_vd, model):
+        iris_copy = iris_vd.copy()
+
+        model.predict(
+            iris_copy,
+            X=[
+                "SepalLengthCm",
+                "SepalWidthCm",
+                "PetalLengthCm",
+                "PetalWidthCm",
+                "Species",
+            ],
+            name="pred",
         )
 
-    def test_to_sql(self, model):
-        current_cursor().execute(
-            "SELECT APPLY_SVD(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
-            )
-        )
-        prediction = [float(elem) for elem in current_cursor().fetchone()]
-        current_cursor().execute(
-            "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                ", ".join(model.to_sql())
-            )
-        )
-        prediction2 = [float(elem) for elem in current_cursor().fetchone()]
-        assert prediction == pytest.approx(prediction2)
+        assert iris_copy["pred"].mode() == 1
+        assert iris_copy["pred"].distinct() == [0, 1, 2]
 
-    def test_to_memmodel(self, model):
-        current_cursor().execute(
-            "SELECT APPLY_SVD(citric_acid, residual_sugar, alcohol USING PARAMETERS model_name = '{}', match_by_pos=True) FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                model.name
-            )
-        )
-        prediction = [float(elem) for elem in current_cursor().fetchone()]
+    def test_set_params(self, model):
+        model.set_params({"max_iter": 20})
+        assert model.get_params()["max_iter"] == 20
+
+    def test_model_from_vDF(self, iris_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS kprototypes_vDF")
+        model_test = KPrototypes("kprototypes_vDF", n_cluster=3, init="random")
+        model_test.fit(iris_vd, ["SepalLengthCm", "SepalWidthCm", "Species"])
         current_cursor().execute(
-            "SELECT {} FROM (SELECT 3.0 AS citric_acid, 11.0 AS residual_sugar, 93. AS alcohol) x".format(
-                ", ".join(
-                    model.to_memmodel().transform_sql(
-                        ["citric_acid", "residual_sugar", "alcohol"]
-                    )
-                )
-            )
+            "SELECT model_name FROM models WHERE model_name = 'kprototypes_vDF'"
         )
-        prediction2 = [float(elem) for elem in current_cursor().fetchone()]
-        assert prediction == pytest.approx(prediction2)
-        prediction3 = model.to_memmodel().transform([[3.0, 11.0, 93.0]])
-        assert prediction == pytest.approx(list(prediction3[0]))
+        assert current_cursor().fetchone()[0] == "kprototypes_vDF"
+        model_test.drop()
 
-    def test_get_transform(self, winequality_vd, model):
-        winequality_trans = model.transform(
-            winequality_vd, X=["citric_acid", "residual_sugar", "alcohol"]
-        )
-        assert winequality_trans["col1"].mean() == pytest.approx(
-            0.0121807874344058, abs=1e-6
-        )
-        assert winequality_trans["col2"].mean() == pytest.approx(
-            -0.00200082024084619, abs=1e-6
-        )
-        assert winequality_trans["col3"].mean() == pytest.approx(
-            0.000194341623203586, abs=1e-6
-        )
+    def test_init_method(self):
+        model_test_random = KPrototypes("random_test", n_cluster=3, init="random")
+        model_test_random.drop()
+        model_test_random.fit("public.iris")
+
+        current_cursor().execute(
+            "SELECT model_name FROM models WHERE model_name = 'random_test'"
+        )
+        assert current_cursor().fetchone()[0] == "random_test"
+        model_test_random.drop()
+
+    def test_get_plot(self, iris_vd):
+        current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
+        model_test = KPrototypes(
+            "model_test_plot",
+            n_cluster=3,
+        )
+        model_test.fit(iris_vd, ["SepalLengthCm", "PetalWidthCm"])
+        result = model_test.plot(color="b")
+        assert len(result.get_default_bbox_extra_artists()) > 8
+        plt.close("all")
+        model_test.drop()
+        # TODO: test for categorical inputs.
 
-    def test_get_inverse_transform(self, winequality_vd, model):
-        winequality_trans = model.transform(
-            winequality_vd, X=["citric_acid", "residual_sugar", "alcohol"]
-        )
-        winequality_trans = model.inverse_transform(
-            winequality_trans, X=["col1", "col2", "col3"]
-        )
-        assert winequality_trans["citric_acid"].mean() == pytest.approx(
-            winequality_vd["citric_acid"].mean(), abs=1e-6
+    def test_to_python(self, model):
+        current_cursor().execute(
+            "SELECT APPLY_KPROTOTYPES(5.006, 3.418, 1.464, 0.244, 'Iris-setosa' USING PARAMETERS model_name = '{0}', match_by_pos=True)".format(
+                model.model_name
+            )
         )
-        assert winequality_trans["residual_sugar"].mean() == pytest.approx(
-            winequality_vd["residual_sugar"].mean(), abs=1e-6
+        prediction = current_cursor().fetchone()[0]
+        assert prediction == pytest.approx(
+            model.to_python()([[5.006, 3.418, 1.464, 0.244, "Iris-setosa"]])[0]
         )
-        assert winequality_trans["alcohol"].mean() == pytest.approx(
-            winequality_vd["alcohol"].mean(), abs=1e-6
+        assert 0.0 == pytest.approx(
+            model.to_python(return_distance_clusters=True)(
+                [[5.006, 3.418, 1.464, 0.244]]
+            )[0][0]
         )
 
-    def test_svd_score(self, model):
-        result = model.score()
-        assert result["Score"][0] == pytest.approx(0.0, abs=1e-6)
-        assert result["Score"][1] == pytest.approx(0.0, abs=1e-6)
-        assert result["Score"][2] == pytest.approx(0.0, abs=1e-6)
-
-    def test_set_params(self, model):
-        model.set_params({"n_components": 3})
-        assert model.get_params()["n_components"] == 3
-
-    def test_model_from_vDF(self, winequality_vd):
-        current_cursor().execute("DROP MODEL IF EXISTS SVD_vDF")
-        model_test = SVD("SVD_vDF",)
-        model_test.fit(winequality_vd, ["alcohol", "quality"])
+    def test_to_sql(self, model):
         current_cursor().execute(
-            "SELECT model_name FROM models WHERE model_name = 'SVD_vDF'"
+            "SELECT APPLY_KPROTOTYPES(5.006, 3.418, 1.464, 0.244, 'Iris-setosa' USING PARAMETERS model_name = '{0}', match_by_pos=True)::float, {1}::float".format(
+                model.model_name,
+                model.to_sql([5.006, 3.418, 1.464, 0.244, "'Iris-setosa'"]),
+            )
         )
-        assert current_cursor().fetchone()[0] == "SVD_vDF"
-        model_test.drop()
+        prediction = current_cursor().fetchone()
+        assert prediction[0] == pytest.approx(prediction[1])
+
+    def test_to_memmodel(self, model, iris_vd):
+        mmodel = model.to_memmodel()
+        res = mmodel.predict(
+            [
+                [5.006, 3.418, 1.464, 0.244, "Iris-setosa"],
+                [3.0, 11.0, 1993.0, 0.0, "Iris-setosa"],
+            ]
+        )
+        res_py = model.to_python()(
+            [
+                [5.006, 3.418, 1.464, 0.244, "Iris-setosa"],
+                [3.0, 11.0, 1993.0, 0.0, "Iris-setosa"],
+            ]
+        )
+        assert res[0] == res_py[0]
+        assert res[1] == res_py[1]
+        vdf = iris_vd.copy()
+        vdf["prediction_sql"] = mmodel.predict_sql(
+            [
+                "SepalLengthCm",
+                "SepalWidthCm",
+                "PetalLengthCm",
+                "PetalWidthCm",
+                "Species",
+            ]
+        )
+        model.predict(vdf, name="prediction_vertica_sql")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
+        assert score == pytest.approx(0.993333333333333)  # can we do better?
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_tools.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_tools.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,339 +1,265 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Other Modules
 import matplotlib.pyplot as plt
 
 # VerticaPy
-from verticapy import set_option
-from verticapy.connect import current_cursor
+from verticapy import set_option, drop
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic
 from verticapy.learn.model_selection import *
 from verticapy.learn.linear_model import *
 from verticapy.learn.naive_bayes import *
 from verticapy.learn.ensemble import *
 from verticapy.learn.tree import *
 from verticapy.learn.svm import *
 from verticapy.learn.cluster import *
 from verticapy.learn.neighbors import *
 from verticapy.learn.decomposition import *
 from verticapy.learn.preprocessing import *
 from verticapy.learn.tsa import *
-from verticapy.learn.tools import *
+from verticapy.learn.tools import load_model
+import verticapy.machine_learning.memmodel.decomposition as mm
 
 set_option("print_info", False)
 set_option("random_state", 0)
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
     drop(name="public.titanic")
 
 
 class TestTools:
-    def test_does_model_exist(self, titanic_vd):
+    def test__is_already_stored(self, titanic_vd):
         current_cursor().execute("CREATE SCHEMA IF NOT EXISTS load_model_test")
         model = LinearRegression("load_model_test.model_test")
         model.drop()
-        assert does_model_exist("load_model_test.model_test") == False
+        assert not model._is_already_stored()
         model.fit(titanic_vd, ["age", "fare"], "survived")
-        assert does_model_exist("load_model_test.model_test") == True
+        assert model._is_already_stored()
         assert (
-            does_model_exist(
-                "load_model_test.model_test", return_model_type=True
-            ).lower()
+            model._is_already_stored(return_model_type=True).lower()
             == "linear_regression"
         )
         model.drop()
         current_cursor().execute("DROP SCHEMA load_model_test CASCADE")
 
     def test_load_model(self, titanic_vd):
-        create_verticapy_schema()
         current_cursor().execute("CREATE SCHEMA IF NOT EXISTS load_model_test")
-        # VAR
-        model = VAR("load_model_test.model_test", max_iter=100)
-        model.drop()
-        model.fit(titanic_vd, ["fare"], "age")
-        result = load_model("load_model_test.model_test")
-        assert isinstance(result, VAR) and result.get_params()["max_iter"] == 100
-        model.drop()
-        # SARIMAX
-        model = SARIMAX("load_model_test.model_test", max_iter=100)
-        model.drop()
-        model.fit(titanic_vd, "fare", "age")
-        result = load_model("load_model_test.model_test")
-        assert isinstance(result, SARIMAX) and result.get_params()["max_iter"] == 100
+        # Scaler
+        model = Scaler("load_model_test.model_test", method="minmax")
         model.drop()
-        # Normalizer
-        model = Normalizer("load_model_test.model_test", method="minmax")
-        model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
+        model.fit("public.titanic", ["age", "fare"])
         result = load_model("load_model_test.model_test")
-        assert (
-            isinstance(result, Normalizer) and result.get_params()["method"] == "minmax"
-        )
-        model.drop()
-        # Normalizer
-        model = Normalizer("load_model_test.model_test", method="minmax")
-        model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
-        result = load_model("load_model_test.model_test")
-        assert (
-            isinstance(result, Normalizer) and result.get_params()["method"] == "minmax"
-        )
+        assert isinstance(result, Scaler) and result.get_params()["method"] == "minmax"
         model.drop()
         # OneHotEncoder
         model = OneHotEncoder("load_model_test.model_test")
         model.drop()
-        model.fit(titanic_vd, ["sex", "embarked"])
+        model.fit("public.titanic", ["sex", "embarked"])
         result = load_model("load_model_test.model_test")
         assert isinstance(result, OneHotEncoder)
         model.drop()
-        # LOF
-        model = LocalOutlierFactor("load_model_test.model_test", p=3)
-        model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
-        result = load_model("load_model_test.model_test")
-        assert isinstance(result, LocalOutlierFactor) and result.get_params()["p"] == 3
-        model.drop()
-        # DBSCAN
-        model = DBSCAN("load_model_test.model_test", p=3)
-        model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
-        result = load_model("load_model_test.model_test")
-        assert isinstance(result, DBSCAN) and result.get_params()["p"] == 3
-        model.drop()
         # PCA
         model = PCA("load_model_test.model_test")
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
+        model.fit("public.titanic", ["age", "fare"])
         result = load_model("load_model_test.model_test")
         assert isinstance(result, PCA)
         model.drop()
         # SVD
         model = SVD("load_model_test.model_test")
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
+        model.fit("public.titanic", ["age", "fare"])
         result = load_model("load_model_test.model_test")
         assert isinstance(result, SVD)
         model.drop()
         # LinearRegression
         model = LinearRegression("load_model_test.model_test", tol=1e-88)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert result.get_params()["tol"] == 1e-88
-        assert (
-            isinstance(result, LinearRegression)
-            and result.get_params()["penalty"] == "none"
-        )
+        assert isinstance(result, LinearRegression)
         model.drop()
         # ElasticNet
         model = ElasticNet("load_model_test.model_test", tol=1e-88)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert result.get_params()["tol"] == 1e-88
-        assert (
-            isinstance(result, ElasticNet) and result.get_params()["penalty"] == "enet"
-        )
+        assert isinstance(result, ElasticNet)
         model.drop()
         # Lasso
         model = Lasso("load_model_test.model_test", tol=1e-88)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert result.get_params()["tol"] == 1e-88
-        assert isinstance(result, Lasso) and result.get_params()["penalty"] == "l1"
+        assert isinstance(result, Lasso)
         model.drop()
         # Ridge
         model = Ridge("load_model_test.model_test", tol=1e-88)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert result.get_params()["tol"] == 1e-88
-        assert isinstance(result, Ridge) and result.get_params()["penalty"] == "l2"
+        assert isinstance(result, Ridge)
         model.drop()
         # LogisticRegression
         model = LogisticRegression(
             "load_model_test.model_test", tol=1e-88, penalty="enet", solver="cgd"
         )
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert result.get_params()["tol"] == 1e-88
         assert result.get_params()["penalty"] == "enet"
         assert (
             isinstance(result, LogisticRegression)
             and result.get_params()["solver"] == "cgd"
         )
         model.drop()
         # DummyTreeClassifier
         model = DummyTreeClassifier("load_model_test.model_test")
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert isinstance(result, RandomForestClassifier)
         model.drop()
         # DummyTreeRegressor
         model = DummyTreeRegressor("load_model_test.model_test")
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert isinstance(result, RandomForestRegressor)
         model.drop()
         # DecisionTreeClassifier
         model = DecisionTreeClassifier("load_model_test.model_test", max_depth=3)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert (
             isinstance(result, RandomForestClassifier)
             and result.get_params()["max_depth"] == 3
         )
         model.drop()
         # DecisionTreeRegressor
         model = DecisionTreeRegressor("load_model_test.model_test", max_depth=3)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert (
             isinstance(result, RandomForestRegressor)
             and result.get_params()["max_depth"] == 3
         )
         model.drop()
         # RandomForestClassifier
         model = RandomForestClassifier("load_model_test.model_test", n_estimators=33)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert (
             isinstance(result, RandomForestClassifier)
             and result.get_params()["n_estimators"] == 33
         )
         model.drop()
         # RandomForestRegressor
         model = RandomForestRegressor("load_model_test.model_test", n_estimators=33)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert (
             isinstance(result, RandomForestRegressor)
             and result.get_params()["n_estimators"] == 33
         )
         model.drop()
-        # XGBoostClassifier
-        model = XGBoostClassifier("load_model_test.model_test", max_ntree=12)
+        # XGBClassifier
+        model = XGBClassifier("load_model_test.model_test", max_ntree=12)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert (
-            isinstance(result, XGBoostClassifier)
-            and result.get_params()["max_ntree"] == 12
+            isinstance(result, XGBClassifier) and result.get_params()["max_ntree"] == 12
         )
         model.drop()
-        # XGBoostRegressor
-        model = XGBoostRegressor("load_model_test.model_test", max_ntree=12)
+        # XGBRegressor
+        model = XGBRegressor("load_model_test.model_test", max_ntree=12)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert (
-            isinstance(result, XGBoostRegressor)
-            and result.get_params()["max_ntree"] == 12
+            isinstance(result, XGBRegressor) and result.get_params()["max_ntree"] == 12
         )
         model.drop()
         # NaiveBayes
         model = NaiveBayes("load_model_test.model_test", alpha=0.5)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert isinstance(result, NaiveBayes) and result.get_params()["alpha"] == 0.5
         model.drop()
         # LinearSVC
         model = LinearSVC("load_model_test.model_test", tol=1e-4)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert isinstance(result, LinearSVC) and result.get_params()["tol"] == 1e-4
         model.drop()
         # LinearSVR
         model = LinearSVR("load_model_test.model_test", tol=1e-4)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
+        model.fit("public.titanic", ["age", "fare"], "survived")
         result = load_model("load_model_test.model_test")
         assert isinstance(result, LinearSVR) and result.get_params()["tol"] == 1e-4
         model.drop()
         # KMeans
         model = KMeans("load_model_test.model_test", tol=1e-4)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
+        model.fit("public.titanic", ["age", "fare"])
         result = load_model("load_model_test.model_test")
         assert isinstance(result, KMeans) and result.get_params()["tol"] == 1e-4
         model.drop()
         # BisectingKMeans
         model = BisectingKMeans("load_model_test.model_test", tol=1e-4)
         model.drop()
-        model.fit(titanic_vd, ["age", "fare"])
+        model.fit("public.titanic", ["age", "fare"])
         result = load_model("load_model_test.model_test")
         assert (
             isinstance(result, BisectingKMeans) and result.get_params()["tol"] == 1e-4
         )
         model.drop()
-        # KNeighborsClassifier
-        model = KNeighborsClassifier("load_model_test.model_test", p=4)
-        model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
-        result = load_model("load_model_test.model_test")
-        assert (
-            isinstance(result, KNeighborsClassifier) and result.get_params()["p"] == 4
-        )
-        model.drop()
-        # KNeighborsRegressor
-        model = KNeighborsRegressor("load_model_test.model_test", p=4)
-        model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
-        result = load_model("load_model_test.model_test")
-        assert isinstance(result, KNeighborsRegressor) and result.get_params()["p"] == 4
-        model.drop()
-        # NearestCentroid
-        model = NearestCentroid("load_model_test.model_test", p=4)
-        model.drop()
-        model.fit(titanic_vd, ["age", "fare"], "survived")
-        result = load_model("load_model_test.model_test")
-        assert isinstance(result, NearestCentroid) and result.get_params()["p"] == 4
-        model.drop()
-        # KernelDensity - BUG
-        # model = KernelDensity("model_test", p=2)
-        # model.drop()
-        # model.fit(titanic_vd, ["age"])
-        # result = load_model("model_test")
-        # assert isinstance(result, KernelDensity) and result.get_params()["p"] == 2
-        # model.drop()
         current_cursor().execute("DROP SCHEMA load_model_test CASCADE")
 
     def test_matrix_rotation(self):
-        result = matrix_rotation([[0.5, 0.6], [0.1, 0.2]])
+        result = mm.PCA([], []).matrix_rotation([[0.5, 0.6], [0.1, 0.2]])
         assert result[0][0] == pytest.approx(0.01539405)
         assert result[0][1] == pytest.approx(0.78087324)
         assert result[1][0] == pytest.approx(0.05549495)
         assert result[1][1] == pytest.approx(0.21661097)
-        result = matrix_rotation([[0.5, 0.6], [0.1, 0.2]], gamma=0.0)
+        result = mm.PCA([], []).matrix_rotation([[0.5, 0.6], [0.1, 0.2]], gamma=0.0)
         assert result[0][0] == pytest.approx(0.0010429389547800816)
         assert result[0][1] == pytest.approx(0.78102427)
         assert result[1][0] == pytest.approx(-0.05092405)
         assert result[1][1] == pytest.approx(0.21773089)
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_xgb_classifier.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_xgb_classifier.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,41 +1,44 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import os
 
 # Other Modules
 import matplotlib.pyplot as plt
 import xgboost as xgb
 
 # VerticaPy
 import verticapy
 from verticapy.tests.conftest import get_version
-from verticapy import (
-    vDataFrame,
-    drop,
-    set_option,
-)
-from verticapy.connect import current_cursor
+from verticapy.core.vdataframe.base import vDataFrame
+from verticapy.utilities import drop
+from verticapy._config.config import set_option
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_titanic, load_dataset_cl
-from verticapy.learn.ensemble import XGBoostClassifier
+from verticapy.learn.ensemble import XGBClassifier
+from verticapy._utils._sql._format import clean_query
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def xgbc_data_vd():
     xgbc_data = load_dataset_cl(table_name="xgbc_data", schema="public")
@@ -43,15 +46,17 @@
     drop(name="public.xgbc_data", method="table")
 
 
 @pytest.fixture(scope="module")
 def titanic_vd():
     titanic = load_titanic()
     yield titanic
-    drop(name="public.titanic",)
+    drop(
+        name="public.titanic",
+    )
 
 
 @pytest.fixture(scope="module")
 def model(xgbc_data_vd):
     current_cursor().execute("DROP MODEL IF EXISTS xgbc_model_test")
 
     current_cursor().execute(
@@ -69,35 +74,28 @@
                     max_depth=6, 
                     nbins=40, 
                     seed=1, 
                     id_column='id')"""
     )
 
     # I could use load_model but it is buggy
-    model_class = XGBoostClassifier(
+    model_class = XGBClassifier(
         "xgbc_model_test",
         max_ntree=3,
         min_split_loss=0.1,
         learning_rate=0.2,
         sample=1.0,
         max_depth=6,
         nbins=40,
     )
     model_class.input_relation = "public.xgbc_data"
     model_class.test_relation = model_class.input_relation
     model_class.X = ["Gender", '"owned cars"', "cost", "income"]
     model_class.y = "TransPortation"
-    current_cursor().execute(
-        "SELECT DISTINCT {} FROM {} WHERE {} IS NOT NULL ORDER BY 1".format(
-            model_class.y, model_class.input_relation, model_class.y
-        )
-    )
-    classes = current_cursor().fetchall()
-    model_class.classes_ = [item[0] for item in classes]
-    model_class.prior_ = model_class.get_prior()
+    model_class._compute_attributes()
 
     yield model_class
     model_class.drop()
 
 
 @pytest.mark.skipif(
     get_version()[0] < 10 or (get_version()[0] == 10 and get_version()[1] == 0),
@@ -117,58 +115,56 @@
         assert cls_rep1["precision"][0] == pytest.approx(1.0)
         assert cls_rep1["recall"][0] == pytest.approx(1.0)
         assert cls_rep1["f1_score"][0] == pytest.approx(1.0)
         assert cls_rep1["mcc"][0] == pytest.approx(1.0)
         assert cls_rep1["informedness"][0] == pytest.approx(1.0)
         assert cls_rep1["markedness"][0] == pytest.approx(1.0)
         assert cls_rep1["csi"][0] == pytest.approx(1.0)
-        assert cls_rep1["cutoff"][0] in (
-            pytest.approx(0.6811, 1e-2),
-            pytest.approx(0.3863, 1e-2),
-        )
-
-        cls_rep2 = model.classification_report(cutoff=0.681).transpose()
-
-        assert cls_rep2["cutoff"][0] == pytest.approx(0.681)
 
     def test_confusion_matrix(self, model):
         conf_mat1 = model.confusion_matrix()
 
-        assert conf_mat1["Bus"] == [4, 0, 0]
-        assert conf_mat1["Car"] == [0, 3, 0]
-        assert conf_mat1["Train"] == [0, 0, 3]
+        assert list(conf_mat1[:, 0]) == [4, 0, 0]
+        assert list(conf_mat1[:, 1]) == [0, 3, 0]
+        assert list(conf_mat1[:, 2]) == [0, 0, 3]
 
         conf_mat2 = model.confusion_matrix(cutoff=0.2)
 
-        assert conf_mat2["Bus"] == [4, 0, 0]
-        assert conf_mat2["Car"] == [0, 3, 0]
-        assert conf_mat2["Train"] == [0, 0, 3]
+        assert list(conf_mat2[:, 0]) == [4, 0, 0]
+        assert list(conf_mat2[:, 1]) == [0, 3, 0]
+        assert list(conf_mat2[:, 2]) == [0, 0, 3]
 
     def test_contour(self, titanic_vd):
-        model_test = XGBoostClassifier("model_contour",)
+        model_test = XGBClassifier(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) in (38, 40, 43)
         model_test.drop()
 
     def test_deploySQL(self, model):
-        expected_sql = (
-            'PREDICT_XGB_CLASSIFIER(Gender, "owned cars", cost, income '
-            "USING PARAMETERS model_name = 'xgbc_model_test', match_by_pos = 'true')"
-        )
+        expected_sql = """PREDICT_XGB_CLASSIFIER("Gender", "owned cars", "cost", "income" 
+                              USING PARAMETERS 
+                              model_name = 'xgbc_model_test', 
+                              match_by_pos = 'true')"""
         result_sql = model.deploySQL()
 
-        assert result_sql == expected_sql
+        assert result_sql == clean_query(expected_sql)
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS xgbc_model_test_drop")
-        model_test = XGBoostClassifier("xgbc_model_test_drop",)
+        model_test = XGBClassifier(
+            "xgbc_model_test_drop",
+        )
         model_test.fit(
             "public.xgbc_data",
             ["Gender", '"owned cars"', "cost", "income"],
             "TransPortation",
         )
 
         current_cursor().execute(
@@ -178,54 +174,61 @@
 
         model_test.drop()
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'xgbc_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
-    @pytest.mark.skip(reason="not yet available.")
+    @pytest.mark.skip(reason="needs Vertica 12.0.3")
     def test_features_importance(self, model):
-        f_imp = model.features_importance()
+        fimp = model.features_importance(show=False)
 
-        assert f_imp["index"] == ["cost", "owned cars", "gender", "income"]
-        assert f_imp["importance"] == [75.76, 15.15, 9.09, 0.0]
-        assert f_imp["sign"] == [1, 1, 1, 0]
+        assert fimp["index"] == ["cost", "owned cars", "gender", "income"]
+        assert fimp["importance"] == [85.53, 9.61, 4.86, 0.0]
+        assert fimp["sign"] == [1, 1, -1, 0]
         plt.close("all")
 
+    @pytest.mark.skip(reason="needs Vertica 12.0.3")
+    def test_get_score(self, model):
+        fim = model.get_score()
+
+        assert fim["predictor_name"] == ["gender", "owned cars", "cost", "income"]
+        assert fim["frequency"] == [0.25, 0.25, 0.5, 0.0]
+        assert fim["total_gain"] == [
+            pytest.approx(-0.0276367140130583),
+            pytest.approx(0.0546732664706745),
+            pytest.approx(0.972963447542384),
+            pytest.approx(0.0),
+        ]
+
     def test_lift_chart(self, model):
-        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000)
+        lift_ch = model.lift_chart(pos_label="Bus", nbins=1000, show=False)
 
         assert lift_ch["decision_boundary"][300] == pytest.approx(0.3)
         assert lift_ch["positive_prediction_ratio"][300] == pytest.approx(0.0)
         assert lift_ch["lift"][300] == pytest.approx(2.5)
         plt.close("all")
 
     def test_to_python(self, model, titanic_vd):
-        model_test = XGBoostClassifier("rfc_python_test")
+        model_test = XGBClassifier("rfc_python_test")
         model_test.drop()
         model_test.fit(titanic_vd, ["age", "fare", "sex"], "embarked")
         current_cursor().execute(
             "SELECT PREDICT_XGB_CLASSIFIER(30.0, 45.0, 'male' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 45.0, "male"]])[0]
-        )
+        assert prediction == model_test.to_python()([[30.0, 45.0, "male"]])[0]
         current_cursor().execute(
             "SELECT PREDICT_XGB_CLASSIFIER(30.0, 145.0, 'female' USING PARAMETERS model_name = 'rfc_python_test', match_by_pos=True)"
         )
         prediction = current_cursor().fetchone()[0]
-        assert (
-            prediction
-            == model_test.to_python(return_str=False)([[30.0, 145.0, "female"]])[0]
-        )
+        assert prediction == model_test.to_python()([[30.0, 145.0, "female"]])[0]
 
     def test_to_sql(self, model, titanic_vd):
-        model_test = XGBoostClassifier("xgb_sql_test")
+        model_test = XGBClassifier("xgb_sql_test")
         model_test.drop()
         model_test.fit(titanic_vd, ["age", "fare", "sex"], "survived")
         current_cursor().execute(
             "SELECT PREDICT_XGB_CLASSIFIER(* USING PARAMETERS model_name = 'xgb_sql_test', match_by_pos=True)::int, {}::int FROM (SELECT 30.0 AS age, 45.0 AS fare, 'male' AS sex) x".format(
                 model_test.to_sql()
             )
         )
@@ -274,31 +277,31 @@
         )
         model.predict_proba(
             vdf, name="prediction_proba_vertica_sql_1", pos_label=model.classes_[1]
         )
         model.predict_proba(
             vdf, name="prediction_proba_vertica_sql_2", pos_label=model.classes_[2]
         )
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "accuracy")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="accuracy")
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", "r2"
+            "prediction_proba_sql_0", "prediction_proba_vertica_sql_0", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", "r2"
+            "prediction_proba_sql_1", "prediction_proba_vertica_sql_1", metric="r2"
         )
         assert score == pytest.approx(1.0)
         score = vdf.score(
-            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", "r2"
+            "prediction_proba_sql_2", "prediction_proba_vertica_sql_2", metric="r2"
         )
         assert score == pytest.approx(1.0)
 
-    def test_get_attr(self, model):
-        attr = model.get_attr()
+    def test_get_vertica_attributes(self, model):
+        attr = model.get_vertica_attributes()
         assert attr["attr_name"] == [
             "tree_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "details",
             "initial_prediction",
@@ -309,29 +312,35 @@
             "accepted_row_count",
             "call_string",
             "predictor, type",
             "response_label, value",
         ]
         assert attr["#_of_rows"] == [1, 1, 1, 1, 4, 3]
 
-        details = model.get_attr("details")
+        details = model.get_vertica_attributes("details")
         assert details["predictor"] == ["gender", "owned cars", "cost", "income"]
         assert details["type"] == [
             "char or varchar",
             "int",
             "char or varchar",
             "char or varchar",
         ]
 
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("tree_count")["tree_count"][0] == 3
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
+        )
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 3
         assert (
             "xgb_classifier('public.xgbc_model_test', 'public.xgbc_data', '\"transportation\"', '*' USING PARAMETERS"
-            in model.get_attr("call_string")["call_string"][0]
+            in model.get_vertica_attributes("call_string")["call_string"][0]
         )
 
     def test_get_params(self, model):
         assert model.get_params() == {
             "max_ntree": 3,
             "min_split_loss": 0.1,
             "learning_rate": 0.2,
@@ -352,15 +361,15 @@
             "tol": 0.001,
             "weight_reg": 0.0,
             "col_sample_by_tree": 1.0,
             "col_sample_by_node": 1.0,
         }
 
     def test_prc_curve(self, model):
-        prc = model.prc_curve(pos_label="Car", nbins=1000)
+        prc = model.prc_curve(pos_label="Car", nbins=1000, show=False)
 
         assert prc["threshold"][300] == pytest.approx(0.299)
         assert prc["recall"][300] == pytest.approx(1.0)
         assert prc["precision"][300] in (pytest.approx(1.0), pytest.approx(0.6))
         plt.close("all")
 
     def test_predict(self, xgbc_data_vd, model):
@@ -383,123 +392,125 @@
         assert xgbc_data_copy["prob_train"].avg() == 0.3199195
         assert xgbc_data_copy["prob_car"].avg() == 0.3360605
 
         model.predict_proba(xgbc_data_copy, name="prob_bus_2", pos_label="Bus")
         assert xgbc_data_copy["prob_bus_2"].avg() == 0.3440198
 
     def test_roc_curve(self, model):
-        roc = model.roc_curve(pos_label="Train", nbins=1000)
+        roc = model.roc_curve(pos_label="Train", nbins=1000, show=False)
 
         assert roc["threshold"][100] == pytest.approx(0.1)
         assert roc["false_positive"][100] == pytest.approx(1.0)
         assert roc["true_positive"][100] == pytest.approx(1.0)
         assert roc["threshold"][700] == pytest.approx(0.7)
         assert roc["false_positive"][700] == pytest.approx(0.0)
         assert roc["true_positive"][700] == pytest.approx(0.0)
         plt.close("all")
 
     def test_cutoff_curve(self, model):
-        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000)
+        cutoff_curve = model.cutoff_curve(pos_label="Train", nbins=1000, show=False)
 
         assert cutoff_curve["threshold"][100] == pytest.approx(0.1)
         assert cutoff_curve["false_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["true_positive"][100] == pytest.approx(1.0)
         assert cutoff_curve["threshold"][700] == pytest.approx(0.7)
         assert cutoff_curve["false_positive"][700] == pytest.approx(0.0)
         assert cutoff_curve["true_positive"][700] == pytest.approx(0.0)
         plt.close("all")
 
     def test_score(self, model):
-        assert model.score(cutoff=0.9, method="accuracy") == pytest.approx(1.0)
-        assert model.score(cutoff=0.1, method="accuracy") == pytest.approx(1.0)
+        assert model.score(cutoff=0.9, metric="accuracy") == pytest.approx(1.0)
+        assert model.score(cutoff=0.1, metric="accuracy") == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="auc", pos_label="Train"
+            cutoff=0.9, metric="auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="auc", pos_label="Train"
+            cutoff=0.1, metric="auc", pos_label="Train"
         ) == pytest.approx(1.0)
-        assert model.score(cutoff=0.9, method="best_cutoff", pos_label="Train") in (
+        assert model.score(cutoff=0.9, metric="best_cutoff", pos_label="Train") in (
             pytest.approx(0.6338, 1e-2),
             pytest.approx(0.3863, 1e-2),
         )
-        assert model.score(cutoff=0.1, method="best_cutoff", pos_label="Train") in (
+        assert model.score(cutoff=0.1, metric="best_cutoff", pos_label="Train") in (
             pytest.approx(0.6338, 1e-2),
             pytest.approx(0.3863, 1e-2),
         )
         assert model.score(
-            cutoff=0.633, method="bm", pos_label="Train"
+            cutoff=0.633, metric="bm", pos_label="Train"
         ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.1, method="bm", pos_label="Train") == pytest.approx(
+        assert model.score(cutoff=0.1, metric="bm", pos_label="Train") == pytest.approx(
             0.0
         )
         assert model.score(
-            cutoff=0.9, method="csi", pos_label="Train"
+            cutoff=0.9, metric="csi", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.1, method="csi", pos_label="Train"
-        ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="f1", pos_label="Train") == pytest.approx(
+            cutoff=0.1, metric="csi", pos_label="Train"
+        ) == pytest.approx(0.3)
+        assert model.score(cutoff=0.9, metric="f1", pos_label="Train") == pytest.approx(
             0.0
         )
-        assert model.score(cutoff=0.1, method="f1", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="f1", pos_label="Train") == pytest.approx(
+            0.4615384615384615
         )
-        assert model.score(cutoff=0.9, method="logloss", pos_label="Train") in (
+        assert model.score(cutoff=0.9, metric="logloss", pos_label="Train") in (
             pytest.approx(0.111961142833969),
             pytest.approx(0.21696238336042),
         )
-        assert model.score(cutoff=0.1, method="logloss", pos_label="Train") in (
+        assert model.score(cutoff=0.1, metric="logloss", pos_label="Train") in (
             pytest.approx(0.111961142833969),
             pytest.approx(0.21696238336042),
         )
         assert model.score(
-            cutoff=0.9, method="mcc", pos_label="Train"
+            cutoff=0.9, metric="mcc", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.1, method="mcc", pos_label="Train"
+            cutoff=0.1, metric="mcc", pos_label="Train"
         ) == pytest.approx(0.0)
-        assert model.score(cutoff=0.9, method="mk", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.9, metric="mk", pos_label="Train") == pytest.approx(
+            -0.30000000000000004
         )
-        assert model.score(cutoff=0.1, method="mk", pos_label="Train") == pytest.approx(
-            0.0
+        assert model.score(cutoff=0.1, metric="mk", pos_label="Train") == pytest.approx(
+            -0.7
         )
         assert model.score(
-            cutoff=0.9, method="npv", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.9, metric="npv", pos_label="Train"
+        ) == pytest.approx(0.7)
         assert model.score(
-            cutoff=0.1, method="npv", pos_label="Train"
+            cutoff=0.1, metric="npv", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.9, method="prc_auc", pos_label="Train"
+            cutoff=0.9, metric="prc_auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="prc_auc", pos_label="Train"
+            cutoff=0.1, metric="prc_auc", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.9, method="precision", pos_label="Train"
+            cutoff=0.9, metric="precision", pos_label="Train"
         ) == pytest.approx(0.0)
         assert model.score(
-            cutoff=0.1, method="precision", pos_label="Train"
-        ) == pytest.approx(0.0)
+            cutoff=0.1, metric="precision", pos_label="Train"
+        ) == pytest.approx(0.3)
         assert model.score(
-            cutoff=0.9, method="specificity", pos_label="Train"
+            cutoff=0.9, metric="specificity", pos_label="Train"
         ) == pytest.approx(1.0)
         assert model.score(
-            cutoff=0.1, method="specificity", pos_label="Train"
-        ) == pytest.approx(1.0)
+            cutoff=0.1, metric="specificity", pos_label="Train"
+        ) == pytest.approx(0.0)
 
     def test_set_params(self, model):
         model.set_params({"nbins": 1000})
 
         assert model.get_params()["nbins"] == 1000
 
     def test_model_from_vDF(self, xgbc_data_vd):
         current_cursor().execute("DROP MODEL IF EXISTS xgbc_from_vDF")
-        model_test = XGBoostClassifier("xgbc_from_vDF",)
+        model_test = XGBClassifier(
+            "xgbc_from_vDF",
+        )
         model_test.fit(
             xgbc_data_vd,
             ["Gender", '"owned cars"', "cost", "income"],
             "TransPortation",
         )
 
         current_cursor().execute(
@@ -536,15 +547,15 @@
         import xgboost as xgb
 
         titanic = titanic_vd.copy()
         titanic.fillna()
         path = "verticapy_test_xgbr.json"
         X = ["pclass", "age", "fare"]
         y = "survived"
-        model = XGBoostClassifier(
+        model = XGBClassifier(
             "verticapy_xgb_binaryclassifier_test", max_ntree=10, max_depth=5
         )
         model.drop()
         model.fit(titanic, X, y)
         X_test = titanic[X].to_numpy()
         y_test_vertica = model.to_python(return_proba=True)(X_test)
         if os.path.exists(path):
@@ -566,15 +577,15 @@
 
     def test_to_json_multiclass(self, titanic_vd):
         titanic = titanic_vd.copy()
         titanic.fillna()
         path = "verticapy_test_xgbr.json"
         X = ["survived", "age", "fare"]
         y = "pclass"
-        model = XGBoostClassifier(
+        model = XGBClassifier(
             "verticapy_xgb_multiclass_classifier_test", max_ntree=10, max_depth=5
         )
         model.drop()
         model.fit(titanic, X, y)
         X_test = titanic[X].to_numpy()
         y_test_vertica = model.to_python(return_proba=True)(X_test).argsort()
         if os.path.exists(path):
```

### Comparing `verticapy-0.9.0/verticapy/tests/vModel/test_xgb_regressor.py` & `verticapy-1.0.0b1/verticapy/tests/vModel/test_xgb_regressor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,40 +1,45 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 # Pytest
 import pytest
 
 # Standard Python Modules
 import os
 
 # Other Modules
 import matplotlib.pyplot as plt
 import xgboost as xgb
 
 # VerticaPy
+from verticapy._typing import NoneType
 from verticapy.tests.conftest import get_version
 from verticapy import (
     vDataFrame,
     drop,
     set_option,
 )
-from verticapy.connect import current_cursor
+from verticapy.connection import current_cursor
 from verticapy.datasets import load_winequality, load_titanic, load_dataset_reg
-from verticapy.learn.ensemble import XGBoostRegressor
+from verticapy.learn.ensemble import XGBRegressor
 
 set_option("print_info", False)
 
 
 @pytest.fixture(scope="module")
 def winequality_vd():
     winequality = load_winequality()
@@ -61,57 +66,63 @@
     current_cursor().execute("DROP MODEL IF EXISTS xgbr_model_test")
 
     current_cursor().execute(
         "SELECT xgb_regressor('xgbr_model_test', 'public.xgbr_data', 'TransPortation', '*' USING PARAMETERS exclude_columns='id, transportation', min_split_loss=0.1, max_ntree=3, learning_rate=0.2, sampling_size=1, max_depth=6, nbins=40, seed=1, id_column='id')"
     )
 
     # I could use load_model but it is buggy
-    model_class = XGBoostRegressor(
+    model_class = XGBRegressor(
         "xgbr_model_test",
         max_ntree=3,
         min_split_loss=0.1,
         learning_rate=0.2,
         sample=1.0,
         max_depth=6,
         nbins=40,
     )
     model_class.input_relation = "public.xgbr_data"
     model_class.test_relation = model_class.input_relation
     model_class.X = ['"Gender"', '"owned cars"', '"cost"', '"income"']
     model_class.y = '"TransPortation"'
-    model_class.prior_ = model_class.get_prior()
+    model_class._compute_attributes()
 
     yield model_class
     model_class.drop()
 
 
 @pytest.mark.skipif(
     get_version()[0] < 10 or (get_version()[0] == 10 and get_version()[1] == 0),
     reason="requires vertica 10.1 or higher",
 )
 class TestXGBR:
     def test_contour(self, titanic_vd):
-        model_test = XGBoostRegressor("model_contour",)
+        model_test = XGBRegressor(
+            "model_contour",
+        )
         model_test.drop()
         model_test.fit(
-            titanic_vd, ["age", "fare"], "survived",
+            titanic_vd,
+            ["age", "fare"],
+            "survived",
         )
         result = model_test.contour()
         assert len(result.get_default_bbox_extra_artists()) in (37, 34)
         model_test.drop()
 
     def test_deploySQL(self, model):
         expected_sql = 'PREDICT_XGB_REGRESSOR("Gender", "owned cars", "cost", "income" USING PARAMETERS model_name = \'xgbr_model_test\', match_by_pos = \'true\')'
         result_sql = model.deploySQL()
 
         assert result_sql == expected_sql
 
     def test_drop(self):
         current_cursor().execute("DROP MODEL IF EXISTS xgbr_model_test_drop")
-        model_test = XGBoostRegressor("xgbr_model_test_drop",)
+        model_test = XGBRegressor(
+            "xgbr_model_test_drop",
+        )
         model_test.fit(
             "public.xgbr_data",
             ['"Gender"', '"owned cars"', '"cost"', '"income"'],
             "TransPortation",
         )
 
         current_cursor().execute(
@@ -124,26 +135,26 @@
             "SELECT model_name FROM models WHERE model_name = 'xgbr_model_test_drop'"
         )
         assert current_cursor().fetchone() is None
 
     def test_to_python(self, model, titanic_vd):
         current_cursor().execute(
             "SELECT PREDICT_XGB_REGRESSOR('Male', 0, 'Cheap', 'Low' USING PARAMETERS model_name = '{}', match_by_pos=True)::float".format(
-                model.name
+                model.model_name
             )
         )
         prediction = current_cursor().fetchone()[0]
         assert prediction == pytest.approx(
             float(model.to_python()([["Male", 0, "Cheap", "Low"]])[0])
         )
 
     def test_to_sql(self, model):
         current_cursor().execute(
             "SELECT PREDICT_XGB_REGRESSOR(* USING PARAMETERS model_name = '{}', match_by_pos=True)::float, {}::float FROM (SELECT 'Male' AS \"Gender\", 0 AS \"owned cars\", 'Cheap' AS \"cost\", 'Low' AS \"income\") x".format(
-                model.name, model.to_sql()
+                model.model_name, model.to_sql()
             )
         )
         prediction = current_cursor().fetchone()
         assert prediction[0] == pytest.approx(prediction[1])
 
     def test_to_memmodel(self, model):
         mmodel = model.to_memmodel()
@@ -156,28 +167,41 @@
         assert res[0] == res_py[0]
         assert res[1] == res_py[1]
         vdf = vDataFrame("public.xgbr_data")
         vdf["prediction_sql"] = mmodel.predict_sql(
             ['"Gender"', '"owned cars"', '"cost"', '"income"']
         )
         model.predict(vdf, name="prediction_vertica_sql")
-        score = vdf.score("prediction_sql", "prediction_vertica_sql", "r2")
+        score = vdf.score("prediction_sql", "prediction_vertica_sql", metric="r2")
         assert score == pytest.approx(1.0)
 
-    @pytest.mark.skip(reason="not yet available")
+    @pytest.mark.skip(reason="needs Vertica 12.0.3")
     def test_features_importance(self, model):
-        fim = model.features_importance()
+        fim = model.features_importance(show=False)
 
         assert fim["index"] == ["cost", "owned cars", "gender", "income"]
-        assert fim["importance"] == [88.41, 7.25, 4.35, 0.0]
-        assert fim["sign"] == [1, 1, 1, 0]
+        assert fim["importance"] == [91.85, 7.18, 0.97, 0.0]
+        assert fim["sign"] == [1, 1, -1, 0]
         plt.close("all")
 
-    def test_get_attr(self, model):
-        m_att = model.get_attr()
+    @pytest.mark.skip(reason="needs Vertica 12.0.3")
+    def test_get_score(self, model):
+        fim = model.get_score()
+
+        assert fim["predictor_name"] == ["gender", "owned cars", "cost", "income"]
+        assert fim["frequency"] == [0.25, 0.25, 0.5, 0.0]
+        assert fim["total_gain"] == [
+            pytest.approx(-0.00510863674902467),
+            pytest.approx(0.037784719385885),
+            pytest.approx(0.96732391736314),
+            pytest.approx(0.0),
+        ]
+
+    def test_get_vertica_attributes(self, model):
+        m_att = model.get_vertica_attributes()
 
         assert m_att["attr_name"] == [
             "tree_count",
             "rejected_row_count",
             "accepted_row_count",
             "call_string",
             "details",
@@ -189,35 +213,41 @@
             "accepted_row_count",
             "call_string",
             "predictor, type",
             "initial_prediction",
         ]
         assert m_att["#_of_rows"] == [1, 1, 1, 1, 4, 1]
 
-        m_att_details = model.get_attr(attr_name="details")
+        m_att_details = model.get_vertica_attributes(attr_name="details")
 
         assert m_att_details["predictor"] == [
             "gender",
             "owned cars",
             "cost",
             "income",
         ]
         assert m_att_details["type"] == [
             "char or varchar",
             "int",
             "char or varchar",
             "char or varchar",
         ]
 
-        assert model.get_attr("tree_count")["tree_count"][0] == 3
-        assert model.get_attr("rejected_row_count")["rejected_row_count"][0] == 0
-        assert model.get_attr("accepted_row_count")["accepted_row_count"][0] == 10
+        assert model.get_vertica_attributes("tree_count")["tree_count"][0] == 3
+        assert (
+            model.get_vertica_attributes("rejected_row_count")["rejected_row_count"][0]
+            == 0
+        )
+        assert (
+            model.get_vertica_attributes("accepted_row_count")["accepted_row_count"][0]
+            == 10
+        )
         assert (
             "xgb_regressor('public.xgbr_model_test', 'public.xgbr_data', '\"transportation\"', '*' USING PARAMETERS"
-            in model.get_attr("call_string")["call_string"][0]
+            in model.get_vertica_attributes("call_string")["call_string"][0]
         )
 
     def test_get_params(self, model):
         assert model.get_params() == {
             "max_ntree": 3,
             "min_split_loss": 0.1,
             "learning_rate": 0.2,
@@ -297,24 +327,21 @@
             pytest.approx(0.737856, abs=1e-6),
             pytest.approx(0.604379313004277, abs=1e-6),
         )
         assert reg_rep["value"][7] in (
             pytest.approx(0.5281407999999999, abs=1e-6),
             pytest.approx(0.2878827634076987, abs=1e-6),
         )
-        assert reg_rep["value"][8] in (
-            pytest.approx(7.900750107239094, abs=1e-6),
-            pytest.approx(12.016369307174802, abs=1e-6),
-        )
+        assert reg_rep["value"][8] == pytest.approx(24.5163693071748, abs=1e-6)
         assert reg_rep["value"][9] in (
             pytest.approx(-5.586324427790675, abs=1e-6),
             pytest.approx(-1.4707052278549675, abs=1e-6),
         )
 
-        reg_rep_details = model.regression_report("details")
+        reg_rep_details = model.regression_report(metrics="details")
         assert reg_rep_details["value"][2:] == [
             10.0,
             4,
             pytest.approx(0.737856),
             pytest.approx(0.5281407999999999),
             pytest.approx(1.13555908203125),
             pytest.approx(0.3938936106224664),
@@ -329,15 +356,15 @@
             pytest.approx(0.4418800475932531),
             pytest.approx(0.7760066793800073),
             pytest.approx(-1.73372940858763),
             pytest.approx(0.223450528977454),
             pytest.approx(3.76564442746721),
         ]
 
-        reg_rep_anova = model.regression_report("anova")
+        reg_rep_anova = model.regression_report(metrics="anova")
         assert reg_rep_anova["SS"] == [
             pytest.approx(1.6431936),
             pytest.approx(1.8087936),
             pytest.approx(6.9),
         ] or reg_rep_anova["SS"] == [
             pytest.approx(0.964989221751972),
             pytest.approx(2.72978274027049),
@@ -349,77 +376,76 @@
         ] or reg_rep_anova["MS"][:-1] == [
             pytest.approx(0.241247305437993),
             pytest.approx(0.545956548054098),
         ]
 
     def test_score(self, model):
         # method = "max"
-        assert model.score(method="max") in (
+        assert model.score(metric="max") in (
             pytest.approx(0.5632, abs=1e-6),
             pytest.approx(0.6755375, abs=1e-6),
         )
         # method = "mae"
-        assert model.score(method="mae") in (
-            pytest.approx(0.36864, abs=1e-6),
+        assert model.score(metric="mae") in (
+            pytest.approx(0.5527125, abs=1e-6),
             pytest.approx(0.454394259259259, abs=1e-6),
         )
         # method = "median"
-        assert model.score(method="median") in (
+        assert model.score(metric="median") in (
             pytest.approx(0.4608, abs=1e-6),
             pytest.approx(0.5527125, abs=1e-6),
         )
         # method = "mse"
-        assert model.score(method="mse") in (
+        assert model.score(metric="mse") in (
             pytest.approx(0.18087936, abs=1e-6),
             pytest.approx(0.272978274027049, abs=1e-6),
         )
         # method = "rmse"
-        assert model.score(method="rmse") in (
+        assert model.score(metric="rmse") in (
             pytest.approx(0.42529914178140543, abs=1e-6),
             pytest.approx(0.5224732280481451, abs=1e-6),
         )
         # method = "msl"
-        assert model.score(method="msle") in (
+        assert model.score(metric="msle") in (
             pytest.approx(0.0133204031846029, abs=1e-6),
             pytest.approx(0.0195048419826687, abs=1e-6),
         )
         # method = "r2"
         assert model.score() in (
             pytest.approx(0.737856, abs=1e-6),
             pytest.approx(0.604379313004277, abs=1e-6),
         )
         # method = "r2a"
-        assert model.score(method="r2a") in (
+        assert model.score(metric="r2a") in (
             pytest.approx(0.5281407999999999, abs=1e-6),
             pytest.approx(0.2878827634076987, abs=1e-6),
         )
         # method = "var"
-        assert model.score(method="var") in (
+        assert model.score(metric="var") in (
             pytest.approx(0.737856, abs=1e-6),
             pytest.approx(0.60448287427822, abs=1e-6),
         )
         # method = "aic"
-        assert model.score(method="aic") in (
-            pytest.approx(7.900750107239094, abs=1e-6),
-            pytest.approx(12.016369307174802, abs=1e-6),
-        )
+        assert model.score(metric="aic") == pytest.approx(24.5163693071748, abs=1e-6)
         # method = "bic"
-        assert model.score(method="bic") in (
+        assert model.score(metric="bic") in (
             pytest.approx(-5.586324427790675, abs=1e-6),
             pytest.approx(-1.4707052278549675, abs=1e-6),
         )
 
     def test_set_params(self, model):
         model.set_params({"max_ntree": 5})
 
         assert model.get_params()["max_ntree"] == 5
 
     def test_model_from_vDF(self, xgbr_data_vd):
         current_cursor().execute("DROP MODEL IF EXISTS xgbr_from_vDF")
-        model_test = XGBoostRegressor("xgbr_from_vDF",)
+        model_test = XGBRegressor(
+            "xgbr_from_vDF",
+        )
         model_test.fit(xgbr_data_vd, ["gender"], "transportation")
 
         current_cursor().execute(
             "SELECT model_name FROM models WHERE model_name = 'xgbr_from_vDF'"
         )
         assert current_cursor().fetchone()[0] == "xgbr_from_vDF"
 
@@ -437,27 +463,29 @@
             leaf_style={"shape": "circle", "style": "filled"},
         )
         assert 'digraph Tree{\ngraph [rankdir = "LR"];\n0' in gvz_tree_1
         assert "0 -> 1" in gvz_tree_1
 
     def test_get_tree(self, model):
         tree_1 = model.get_tree(tree_id=1)
-        assert tree_1["prediction"][0] == None
+        assert isinstance(tree_1["prediction"][0], NoneType)
         assert tree_1["prediction"][1] in ("0.880000", "0.701250")
-        assert tree_1["prediction"][2] == None
-        assert tree_1["prediction"][3] == None
+        assert isinstance(tree_1["prediction"][2], NoneType)
+        assert isinstance(tree_1["prediction"][3], NoneType)
         assert tree_1["prediction"][4] in ("0.080000", "0.057778")
-        assert tree_1["prediction"][5] == None
+        assert isinstance(tree_1["prediction"][5], NoneType)
         assert tree_1["prediction"][6] in ("-0.720000", "-0.573750")
         assert tree_1["prediction"][7] in ("-0.720000", "-0.405000")
         assert tree_1["prediction"][8] in ("0.080000", "0.045000")
 
     def test_get_plot(self, winequality_vd):
         current_cursor().execute("DROP MODEL IF EXISTS model_test_plot")
-        model_test = XGBoostRegressor("model_test_plot",)
+        model_test = XGBRegressor(
+            "model_test_plot",
+        )
         model_test.fit(winequality_vd, ["alcohol"], "quality")
         result = model_test.plot()
         assert len(result.get_default_bbox_extra_artists()) in (9, 12)
         plt.close("all")
         model_test.drop()
 
     def test_plot_tree(self, model):
@@ -466,17 +494,15 @@
 
     def test_to_json(self, titanic_vd):
         titanic = titanic_vd.copy()
         titanic.fillna()
         path = "verticapy_test_xgbr.json"
         X = ["pclass", "age", "survived"]
         y = "fare"
-        model = XGBoostRegressor(
-            "verticapy_xgb_regressor_test", max_ntree=10, max_depth=5
-        )
+        model = XGBRegressor("verticapy_xgb_regressor_test", max_ntree=10, max_depth=5)
         model.drop()
         model.fit(titanic, X, y)
         X_test = titanic[X].to_numpy()
         y_test_vertica = model.to_python()(X_test)
         if os.path.exists(path):
             os.remove(path)
         model.to_json(path)
```

### Comparing `verticapy-0.9.0/verticapy/udf.py` & `verticapy-1.0.0b1/verticapy/sdk/vertica/udf/gen.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,283 +1,205 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# Standard Python Modules
-import datetime, decimal, inspect, os
-from typing import Union
-
-# VerticaPy Modules
-import verticapy
-from verticapy.utilities import *
-from verticapy.toolbox import *
-
-#
-# ---#
-def import_lib_udf(udf_list: list, library_name: str, include_dependencies: list = []):
-    """
----------------------------------------------------------------------------
-Install a library of Python functions in Vertica. This function will work only
-when it is executed directly in the server.
-
-Parameters
-----------
-udf_list: list
-	List of tuples including the different functions.
-		function     : [function] Python Function.
-	    arg_types    : [dict/list] List or dictionary of the function input types.
-	    			   Example: {"input1": int, "input2": float} or [int, float]
-	    return_type  : [type/dict] Function output type. In case of many outputs, 
-	    			   it must be a dictionary including all the outputs types and 
-	    			   names. Example: {"result1": int, "result2": float}
-	    parameters   : [dict] Dictionary of the function input optional parameters.
-	    			   Example: {"param1": int, "param2": str}
-	    new_name     : [str] New function name when installed in Vertica.
-library_name: str
-	Library Name.
-include_dependencies: list, optional
-	Library files dependencies. The function will copy paste the different files
-	in the UDF definition.
-	"""
-    directory = os.path.dirname(verticapy.__file__)
-    session_name = get_session()
-    file_name = f"{library_name}_{session_name}.py"
-    try:
-        os.remove(directory + "/" + file_name)
-    except:
-        pass
-    udx_str, sql = create_lib_udf(
-        udf_list, library_name, include_dependencies, directory + "/" + file_name
-    )
-    f = open(directory + "/" + file_name, "w")
-    f.write(udx_str)
-    f.close()
-    try:
-        for idx, query in enumerate(sql):
-            executeSQL(query, title="UDF installation. [step {}]".format(idx))
-        os.remove(directory + "/" + file_name)
-        return True
-    except Exception as e:
-        os.remove(directory + "/" + file_name)
-        if conn:
-            conn.close()
-        raise e
-    if conn:
-        conn.close()
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import os
+from typing import Optional, Union
+
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import format_type
+
+from verticapy.sdk.vertica.udf.utils import get_set_add_function
 
 
-# ---#
-def create_lib_udf(
+@save_verticapy_logs
+def generate_lib_udf(
     udf_list: list,
     library_name: str,
-    include_dependencies: list = [],
-    file_path: str = "",
+    include_dependencies: Union[None, str, list[str]] = None,
+    file_path: Optional[str] = None,
     create_file: bool = False,
-):
+) -> tuple[str, str]:
     """
----------------------------------------------------------------------------
-Generates the code needed to install a library of Python functions. It will
-use the Vertica SDK to create UDF of the input functions.
-
-Parameters
-----------
-udf_list: list
-	List of tuples including the different functions.
-		function     : [function] Python Function.
-	    arg_types    : [dict/list] List or dictionary of the function input types.
-	    			   Example: {"input1": int, "input2": float} or [int, float]
-	    return_type  : [type/dict] Function output type. In case of many outputs, 
-	    			   it must be a dictionary including all the outputs types and 
-	    			   names. Example: {"result1": int, "result2": float}
-	    parameters   : [dict] Dictionary of the function input optional parameters.
-	    			   Example: {"param1": int, "param2": str}
-	    new_name     : [str] New function name when installed in Vertica.
-library_name: str
-	Library Name.
-include_dependencies: list, optional
-	Library files dependencies. The function will copy paste the different files
-	in the UDF definition.
-file_path: str, optional
-	Path to the UDF file.
-create_file: bool, optional
-	If set to True, instead of returning the str of the UDx, the function will
-	create two files: one UDF py file and one SQL file to install it.
-
-Returns
--------
-udx_str, sql
-    UDF py file, str needed to install the library.
-	"""
-    if isinstance(include_dependencies, (str)):
-        include_dependencies = [include_dependencies]
-    if not (isinstance(include_dependencies, (list))):
+    Generates the code needed to install a library of
+    Python functions.  It uses the Vertica SDK to
+    create UDFs of the input functions.
+
+    Parameters
+    ----------
+    udf_list: list
+        List of tuples that includes the different functions.
+                function     : [function]   Python    Function.
+            arg_types    : [dict/list] List  or  dictionary
+                           of  the  function  input  types.
+                                   Example: {"input1": int,
+                                     "input2": float}    or
+                           [int, float]
+            return_type  : [type/dict] Function output type.
+                           In the case of many outputs, it
+                           must be a dictionary including
+                           all the outputs types and names,
+                           for example:
+                           {"result1": int, "result2": float}
+            parameters   : [dict] Dictionary of the function
+                           input optional parameters.
+                                   Example: {"param1": int,
+                                     "param2": str}
+            new_name     : [str] New function name when
+                           installed in Vertica.
+    library_name: str
+        Library Name.
+    include_dependencies: str / list, optional
+        Library files dependencies. The function copies and
+        pastes the different files in the UDF definition.
+    file_path: str, optional
+        Path to the UDF file.
+    create_file: bool, optional
+        If set to True,  instead of returning the str of the UDx,
+        the function creates two files:  a UDF py file and
+        a SQL file to install it.
+
+    Returns
+    -------
+    udx_str, sql
+        UDF py file, str needed to install the library.
+    """
+    include_dependencies = format_type(include_dependencies, dtype=list)
+    if not isinstance(include_dependencies, (list)):
         raise ValueError(
             "The parameter include_dependencies type must be <list>. "
             f"Found {type(include_dependencies)}."
         )
-    if not (isinstance(library_name, str)):
+    if not isinstance(library_name, str):
         raise ValueError(
             f"The parameter library_name type must be <str>. Found {type(library_name)}."
         )
-    if not (isinstance(file_path, str)):
+    if not isinstance(file_path, str):
         raise ValueError(
             f"The parameter file_path type must be <str>. Found {type(file_path)}."
         )
-    if not (isinstance(create_file, bool)):
+    if not isinstance(create_file, bool):
         raise ValueError(
             f"The parameter create_file type must be <bool>. Found {type(create_file)}."
         )
     udx_str = "import vertica_sdk\n"
     all_libraries = [udf[0].__module__ for udf in udf_list]
     all_libraries = list(dict.fromkeys(all_libraries))
     if "__main__" in all_libraries:
         all_libraries.remove("__main__")
     for udf in all_libraries:
         if udf:
             udx_str += f"import {udf}\n"
     if include_dependencies:
         for dep_file_path in include_dependencies:
-            if not (isinstance(dep_file_path, str)):
+            if not isinstance(dep_file_path, str):
                 raise ValueError(
                     "The parameter include_dependencies type must be <list> of <str>. "
                     f"Found {type(dep_file_path)} inside."
                 )
-            f = open(dep_file_path)
-            file_str = f.read()
-            exec(file_str)
-            udx_str += "\n" + file_str + "\n"
-            f.close()
+            with open(dep_file_path, "r", encoding="utf-8") as f:
+                file_str = f.read()
+                exec(file_str)
+                udx_str += "\n" + file_str + "\n"
     udx_str += "\n"
     sql = []
     for udf in udf_list:
-        tmp = create_udf(*udf, **{"library_name": library_name})
+        tmp = generate_udf(*udf, **{"library_name": library_name})
         udx_str += tmp[0]
         sql += [tmp[1]]
-    sql_path = (
-        os.path.dirname(file_path) + "/" + library_name + ".sql"
-        if (file_path)
-        else library_name + ".sql"
-    )
-    if not (file_path):
+    sql_path = f"{library_name}.sql"
+    if file_path:
+        sql_path = f"{os.path.dirname(file_path)}/{sql_path}"
+    if not file_path:
         file_path = f"verticapy_{library_name}.py"
     sql = [
         f"CREATE OR REPLACE LIBRARY {library_name} AS '{file_path}' LANGUAGE 'Python';"
     ] + sql
     if create_file:
-        f = open(file_path, "w")
-        f.write(udx_str)
-        f.close()
-        f = open(sql_path, "w")
-        f.write("\n".join(sql))
-        f.close()
+        with open(file_path, "w", encoding="utf-8") as f:
+            f.write(udx_str)
+        with open(sql_path, "w", encoding="utf-8") as f:
+            f.write("\n".join(sql))
     else:
         return udx_str, sql
 
 
-# Functions used to create the 2 main ones.
-
-# ---#
-def create_udf(
+def generate_udf(
     function,
     arg_types: Union[list, dict],
     return_type: Union[type, dict],
-    parameters: dict = {},
-    new_name: str = "",
-    library_name: str = "",
-):
-    if not (hasattr(function, "__call__")):
+    parameters: Optional[dict] = None,
+    new_name: Optional[str] = None,
+    library_name: Optional[str] = None,
+) -> tuple[str, str]:
+    """
+    Generates the UDx Python code and the SQL
+    statements needed to install it.
+    """
+    parameters = format_type(parameters, dtype=dict)
+    if not hasattr(function, "__call__"):
         raise ValueError(
             f"The function parameter must be a Python function. Found {type(function)}."
         )
-    if not (new_name):
+    if not new_name:
         new_name = function.__name__
-    elif not (isinstance(new_name, str)):
+    elif not isinstance(new_name, str):
         raise ValueError(
             f"The parameter new_name type must be <str>. Found {type(new_name)}."
         )
     module = function.__module__
     if module == "__main__":
         module = ""
-    if not (library_name):
+    if not library_name:
         library_name = module
     if module:
         module += "."
-    elif not (isinstance(library_name, str)):
+    elif not isinstance(library_name, str):
         raise ValueError(
             f"The parameter library_name type must be <str>. Found {type(library_name)}."
         )
     if isinstance(arg_types, dict):
         arg_types = [arg_types[var] for var in arg_types]
-    elif not (isinstance(arg_types, list)):
+    elif not isinstance(arg_types, list):
         raise ValueError(
             f"The arg_types parameter must be a <list> of <types>. Found {type(arg_types)}."
         )
     for idx, dtype in enumerate(arg_types):
-        if not (isinstance(dtype, type)):
+        if not isinstance(dtype, type):
             raise ValueError(
                 "Each element of arg_types parameter must be a <type>. "
                 f"Found {type(dtype)} at index {idx}."
             )
     if isinstance(return_type, dict):
         if len(return_type) == 0:
             raise ValueError(
                 "return_type is empty. The returned type must have at least one element."
             )
         elif len(return_type) == 1:
             return_type = [return_type[dtype] for dtype in return_type][0]
 
     # Main Function
     if isinstance(return_type, dict):
-        is_udtf, process_function, ftype = True, "processPartition", "TransformFunction"
+        is_udtf, process_function, ftype = (
+            True,
+            "processPartition",
+            "TransformFunction",
+        )
     elif isinstance(return_type, type):
         is_udtf, process_function, ftype = False, "processBlock", "ScalarFunction"
     else:
         raise ValueError(
             "return_type must be the dictionary of the returned types in case of "
             "Transform Function and only a type in case of Scalar Function. Can not"
             f" be of type {type(return_type)}."
@@ -366,85 +288,7 @@
     transform = "TRANSFORM " if is_udtf else ""
     sql = (
         f"CREATE OR REPLACE {transform}FUNCTION {new_name} AS NAME "
         f"'verticapy_{new_name}_factory' LIBRARY {library_name};"
     )
 
     return udx_str, sql
-
-
-# ---#
-def get_func_info(func):
-    # TO COMPLETE - GUESS FUNCTIONS TYPES
-    name = func.__name__
-    argspec = inspect.getfullargspec(func)[6]
-    if "return" in argspec:
-        return_type = argspec["return"]
-        del argspec["return"]
-    else:
-        return_type = None
-    arg_types = {}
-    parameters = {}
-    for param in argspec:
-        if inspect.signature(func).parameters[param].default == inspect._empty:
-            arg_types[param] = argspec[param]
-        else:
-            parameters[param] = argspec[param]
-    return (func, arg_types, return_type, parameters)
-
-
-# ---#
-def get_module_func_info(module):
-    # ---#
-    def get_list(module):
-        func_list = []
-        for func in dir(module):
-            if func[0] != "_":
-                func_list += [func]
-        return func_list
-
-    # TO COMPLETE - TRY AND RAISE THE APPROPRIATE ERROR
-    func_list = get_list(module)
-    func_info = []
-    for func in func_list:
-        ldic = locals()
-        exec(f"info = get_func_info(module.{func})", globals(), ldic)
-        func_info += [ldic["info"]]
-    return func_info
-
-
-# ---#
-def get_set_add_function(ftype, func: str = "get"):
-    # func = get / set / add
-    func = func.lower()
-    if ftype == bytes:
-        return f"{func}Binary"
-    elif ftype == bool:
-        return f"{func}Bool"
-    elif ftype == datetime.date:
-        return f"{func}Date"
-    elif ftype == float:
-        return f"{func}Float"
-    elif ftype == int:
-        return f"{func}Int"
-    elif ftype == datetime.timedelta:
-        return f"{func}Interval"
-    elif ftype == decimal.Decimal:
-        return f"{func}Numeric"
-    elif ftype == str:
-        if func == "add":
-            return f"{func}Varchar"
-        else:
-            return f"{func}String"
-    elif ftype == datetime.time:
-        return f"{func}Time"
-    elif ftype == (datetime.time, datetime.tzinfo):
-        return f"{func}TimeTz"
-    elif ftype == datetime.datetime:
-        return f"{func}Timestamp"
-    elif ftype == (datetime.datetime, datetime.tzinfo):
-        return f"{func}TimestampTz"
-    else:
-        raise (
-            "The input type is not managed by get_set_add_function. "
-            "Check the Vertica Python SDK for more information."
-        )
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `verticapy-0.9.0/verticapy/util/__init__.py` & `verticapy-1.0.0b1/verticapy/learn/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
```

### Comparing `verticapy-0.9.0/verticapy/util/log.py` & `verticapy-1.0.0b1/verticapy/tests/utils/log.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,19 +1,23 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
 
 from __future__ import print_function, division, absolute_import
 
 import logging
 from .os_utils import ensure_dir_exists
```

### Comparing `verticapy-0.9.0/verticapy/util/os_utils.py` & `verticapy-1.0.0b1/verticapy/tests/utils/os_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -41,15 +41,15 @@
         raise OSError("{} is not a file".format(filename))
     elif not os.access(filename, os.R_OK):
         raise OSError("{} is not readable".format(filename))
 
 
 def check_file_writable(filename):
     """Ensure this is a writable file. If the file doesn't exist,
-       ensure its directory is writable.
+    ensure its directory is writable.
     """
     if os.path.exists(filename):
         if not os.path.isfile(filename):
             raise OSError("{} is not a file".format(filename))
         if not os.access(filename, os.W_OK):
             raise OSError("{} is not writable".format(filename))
     # If target does not exist, check permission on parent dir
```

### Comparing `verticapy-0.9.0/verticapy/utilities.py` & `verticapy-1.0.0b1/verticapy/core/vdataframe/_corr.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,2500 +1,1791 @@
-# (c) Copyright [2018-2022] Micro Focus or one of its affiliates.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# You may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# |_     |~) _  _| _  /~\    _ |.
-# |_)\/  |_)(_|(_||   \_/|_|(_|||
-#    /
-#              ____________       ______
-#             / __        `\     /     /
-#            |  \/         /    /     /
-#            |______      /    /     /
-#                   |____/    /     /
-#          _____________     /     /
-#          \           /    /     /
-#           \         /    /     /
-#            \_______/    /     /
-#             ______     /     /
-#             \    /    /     /
-#              \  /    /     /
-#               \/    /     /
-#                    /     /
-#                   /     /
-#                   \    /
-#                    \  /
-#                     \/
-#                    _
-# \  / _  __|_. _ _ |_)
-#  \/ (/_|  | |(_(_|| \/
-#                     /
-# VerticaPy is a Python library with scikit-like functionality for conducting
-# data science projects on data stored in Vertica, taking advantage Verticas
-# speed and built-in analytics and machine learning features. It supports the
-# entire data science life cycle, uses a pipeline mechanism to sequentialize
-# data transformation operations, and offers beautiful graphical options.
-#
-# VerticaPy aims to do all of the above. The idea is simple: instead of moving
-# data around for processing, VerticaPy brings the logic to the data.
-#
-#
-# Modules
-#
-# Standard Python Modules
-import os, math, shutil, re, time, decimal, warnings, datetime
-from typing import Union
-
-# VerticaPy Modules
-import vertica_python
-import verticapy
-from verticapy.toolbox import *
-from verticapy.errors import *
-
-# Other Modules
-try:
-    from IPython.core.display import display
-except:
-    pass
-
-#
-# ---#
-def create_schema(
-    schema: str, raise_error: bool = False,
-):
-    """
----------------------------------------------------------------------------
-Creates a new schema.
-
-Parameters
-----------
-schema: str
-    Schema name.
-raise_error: bool, optional
-    If the schema couldn't be created, the function raises an error.
-
-Returns
--------
-bool
-    True if the schema was successfully created, False otherwise.
-    """
-    try:
-        executeSQL(f"CREATE SCHEMA {schema};", title="Creating the new schema.")
-        return True
-    except:
-        if raise_error:
-            raise
-        return False
-
-
-# ---#
-def create_table(
-    table_name: str,
-    dtype: dict,
-    schema: str = "",
-    temporary_table: bool = False,
-    temporary_local_table: bool = True,
-    genSQL: bool = False,
-    raise_error: bool = False,
-):
-    """
----------------------------------------------------------------------------
-Creates a new table using the input columns' names and data types.
-
-Parameters
-----------
-table_name: str, optional
-    The final table name.
-dtype: dict
-    Dictionary of the user types. Each key represents a column name and each
-    value represents its data type. 
-    Example: {"age": "int", "name": "varchar"}
-schema: str, optional
-    Schema name.
-temporary_table: bool, optional
-    If set to True, a temporary table will be created.
-temporary_local_table: bool, optional
-    If set to True, a temporary local table will be created. The parameter 
-    'schema' must be empty, otherwise this parameter is ignored.
-genSQL: bool, optional
-    If set to True, the SQL code for creating the final table will be 
-    generated but not executed.
-raise_error: bool, optional
-    If the relation couldn't be created, raises the entire error.
-
-Returns
--------
-bool
-    True if the table was successfully created, False otherwise.
-    """
-    check_types(
-        [
-            ("table_name", table_name, [str]),
-            ("schema", schema, [str]),
-            ("dtype", dtype, [dict]),
-            ("genSQL", genSQL, [bool]),
-            ("temporary_table", temporary_table, [bool]),
-            ("temporary_local_table", temporary_local_table, [bool]),
-            ("raise_error", raise_error, [bool]),
-        ]
-    )
-    if schema.lower() == "v_temp_schema":
-        schema = ""
-        temporary_local_table = True
-    input_relation = (
-        quote_ident(schema) + "." + quote_ident(table_name)
-        if schema
-        else quote_ident(table_name)
-    )
-    temp = "TEMPORARY " if temporary_table else ""
-    if not (schema):
-        temp = "LOCAL TEMPORARY " if temporary_local_table else ""
-    query = "CREATE {}TABLE {}({}){};".format(
-        temp,
-        input_relation,
-        ", ".join(
-            ["{} {}".format(quote_ident(column), dtype[column]) for column in dtype]
-        ),
-        " ON COMMIT PRESERVE ROWS" if temp else "",
-    )
-    if genSQL:
-        return query
-    try:
-        executeSQL(query, title="Creating the new table.")
-        return True
-    except:
-        if raise_error:
-            raise
-        return False
-
-
-# ---#
-def create_verticapy_schema():
-    """
----------------------------------------------------------------------------
-Creates a schema named 'verticapy' used to store VerticaPy extended models.
-    """
-    sql = "CREATE SCHEMA IF NOT EXISTS verticapy;"
-    executeSQL(sql, title="Creating VerticaPy schema.")
-    sql = """CREATE TABLE IF NOT EXISTS verticapy.models (model_name VARCHAR(128), 
-                                                          category VARCHAR(128), 
-                                                          model_type VARCHAR(128), 
-                                                          create_time TIMESTAMP, 
-                                                          size INT);"""
-    executeSQL(sql, title="Creating the models table.")
-    sql = """CREATE TABLE IF NOT EXISTS verticapy.attr (model_name VARCHAR(128), 
-                                                        attr_name VARCHAR(128), 
-                                                        value VARCHAR(65000));"""
-    executeSQL(sql, title="Creating the attr table.")
-
-
-# ---#
-def drop(name: str = "", method: str = "auto", raise_error: bool = False, **kwds):
-    """
----------------------------------------------------------------------------
-Drops the input relation. This can be a model, view, table, text index,
-schema, or geo index.
-
-Parameters
-----------
-name: str, optional
-    Relation name. If empty, it will drop all VerticaPy temporary 
-    elements.
-method / relation_type: str, optional
-    Method used to drop.
-        auto   : identifies the table/view/index/model to drop. 
-                 It will never drop an entire schema unless the 
-                 method is set to 'schema'.
-        model  : drops the input model.
-        table  : drops the input table.
-        view   : drops the input view.        
-        geo    : drops the input geo index.
-        text   : drops the input text index.
-        schema : drops the input schema.
-raise_error: bool, optional
-    If the object couldn't be dropped, this function raises an error.
-
-Returns
--------
-bool
-    True if the relation was dropped, False otherwise.
-    """
-    if "relation_type" in kwds and method == "auto":
-        method = kwds["relation_type"]
-    if isinstance(method, str):
-        method = method.lower()
-    check_types(
-        [
-            ("name", name, [str]),
-            (
-                "method",
-                method,
-                ["table", "view", "model", "geo", "text", "auto", "schema"],
-            ),
-            ("raise_error", raise_error, [bool]),
-        ]
-    )
-    schema, relation = schema_relation(name)
-    schema, relation = schema[1:-1], relation[1:-1]
-    if not (name):
-        method = "temp"
-    if method == "auto":
-        fail, end_conditions = False, False
-        query = (
-            f"SELECT * FROM columns WHERE table_schema = '{schema}'"
-            f" AND table_name = '{relation}'"
-        )
-        result = executeSQL(query, print_time_sql=False, method="fetchrow")
-        if not (result):
-            query = (
-                f"SELECT * FROM view_columns WHERE table_schema = '{schema}'"
-                f" AND table_name = '{relation}'"
-            )
-            result = executeSQL(query, print_time_sql=False, method="fetchrow")
-        elif not (end_conditions):
-            method = "table"
-            end_conditions = True
-        if not (result):
-            try:
-                query = (
-                    "SELECT model_type FROM verticapy.models WHERE "
-                    "LOWER(model_name) = '{0}'"
-                ).format(quote_ident(name).lower())
-                result = executeSQL(query, print_time_sql=False, method="fetchrow")
-            except:
-                result = []
-        elif not (end_conditions):
-            method = "view"
-            end_conditions = True
-        if not (result):
-            query = f"SELECT * FROM models WHERE schema_name = '{schema}' AND model_name = '{relation}'"
-            result = executeSQL(query, print_time_sql=False, method="fetchrow")
-        elif not (end_conditions):
-            method = "model"
-            end_conditions = True
-        if not (result):
-            query = (
-                "SELECT * FROM (SELECT STV_Describe_Index () OVER ()) x  WHERE name IN "
-                f"('{schema}.{relation}', '{relation}', '\"{schema}\".\"{relation}\"', "
-                f"'\"{relation}\"', '{schema}.\"{relation}\"', '\"{schema}\".{relation}')"
-            )
-            result = executeSQL(query, print_time_sql=False, method="fetchrow")
-        elif not (end_conditions):
-            method = "model"
-            end_conditions = True
-        if not (result):
-            try:
-                query = f'SELECT * FROM "{schema}"."{relation}" LIMIT 0;'
-                executeSQL(query, print_time_sql=False)
-                method = "text"
-            except:
-                fail = True
-        elif not (end_conditions):
-            method = "geo"
-            end_conditions = True
-        if fail:
-            if raise_error:
-                raise MissingRelation(
-                    f"No relation / index / view / model named '{name}' was detected."
-                )
-            return False
-    query = ""
-    if method == "model":
-        model_type = kwds["model_type"] if "model_type" in kwds else None
-        try:
-            query = "SELECT model_type FROM verticapy.models WHERE LOWER(model_name) = '{}'".format(
-                quote_ident(name).lower()
-            )
-            result = executeSQL(query, print_time_sql=False, method="fetchfirstelem")
-            is_in_verticapy_schema = True
-            if not (model_type):
-                model_type = result
-        except:
-            is_in_verticapy_schema = False
-        if (
-            model_type
-            in (
-                "DBSCAN",
-                "LocalOutlierFactor",
-                "CountVectorizer",
-                "KernelDensity",
-                "AutoDataPrep",
-                "KNeighborsRegressor",
-                "KNeighborsClassifier",
-                "NearestCentroid",
-            )
-            or is_in_verticapy_schema
-        ):
-            if model_type in ("DBSCAN", "LocalOutlierFactor"):
-                drop(name, method="table")
-            elif model_type == "CountVectorizer":
-                drop(name, method="text")
-                query = (
-                    "SELECT value FROM verticapy.attr WHERE LOWER(model_name) = '{0}' "
-                    "AND attr_name = 'countvectorizer_table'"
-                ).format(quote_ident(name).lower())
-                res = executeSQL(query, print_time_sql=False, method="fetchrow")
-                if res and res[0]:
-                    drop(res[0], method="table")
-            elif model_type == "KernelDensity":
-                drop(name.replace('"', "") + "_KernelDensity_Map", method="table")
-                drop(
-                    "{}_KernelDensity_Tree".format(name.replace('"', "")),
-                    method="model",
-                )
-            elif model_type == "AutoDataPrep":
-                drop(name, method="table")
-            if is_in_verticapy_schema:
-                sql = "DELETE FROM verticapy.models WHERE LOWER(model_name) = '{}';".format(
-                    quote_ident(name).lower()
-                )
-                executeSQL(sql, title="Deleting vModel.")
-                executeSQL("COMMIT;", title="Commit.")
-                sql = "DELETE FROM verticapy.attr WHERE LOWER(model_name) = '{}';".format(
-                    quote_ident(name).lower()
-                )
-                executeSQL(sql, title="Deleting vModel attributes.")
-                executeSQL("COMMIT;", title="Commit.")
-        else:
-            query = f"DROP MODEL {name};"
-    elif method == "table":
-        query = f"DROP TABLE {name};"
-    elif method == "view":
-        query = f"DROP VIEW {name};"
-    elif method == "geo":
-        query = f"SELECT STV_Drop_Index(USING PARAMETERS index ='{name}') OVER ();"
-    elif method == "text":
-        query = f"DROP TEXT INDEX {name};"
-    elif method == "schema":
-        query = f"DROP SCHEMA {name} CASCADE;"
-    if query:
-        try:
-            executeSQL(query, title="Deleting the relation.")
-            result = True
-        except:
-            if raise_error:
-                raise
-            result = False
-    elif method == "temp":
-        sql = """SELECT 
-                    table_schema, table_name 
-                 FROM columns 
-                 WHERE LOWER(table_name) LIKE '%_verticapy_tmp_%' 
-                 GROUP BY 1, 2;"""
-        all_tables = result = executeSQL(sql, print_time_sql=False, method="fetchall")
-        for elem in all_tables:
-            table = '"{}"."{}"'.format(
-                elem[0].replace('"', '""'), elem[1].replace('"', '""')
-            )
-            drop(table, method="table")
-        sql = """SELECT 
-                    table_schema, table_name 
-                 FROM view_columns 
-                 WHERE LOWER(table_name) LIKE '%_verticapy_tmp_%' 
-                 GROUP BY 1, 2;"""
-        all_views = executeSQL(sql, print_time_sql=False, method="fetchall")
-        for elem in all_views:
-            view = '"{}"."{}"'.format(
-                elem[0].replace('"', '""'), elem[1].replace('"', '""')
-            )
-            drop(view, method="view")
-        result = True
-    else:
-        result = True
-    return result
-
-
-# ---#
-def readSQL(query: str, time_on: bool = False, limit: int = 100):
-    """
-	---------------------------------------------------------------------------
-	Returns the result of a SQL query as a tablesample object.
-
-	Parameters
-	----------
-	query: str, optional
-		SQL Query.
-	time_on: bool, optional
-		If set to True, displays the query elapsed time.
-	limit: int, optional
-		Number maximum of elements to display.
-
- 	Returns
- 	-------
- 	tablesample
- 		Result of the query.
-	"""
-    check_types(
-        [
-            ("query", query, [str]),
-            ("time_on", time_on, [bool]),
-            ("limit", limit, [int, float]),
-        ]
-    )
-    while len(query) > 0 and query[-1] in (";", " "):
-        query = query[:-1]
-    count = executeSQL(
-        "SELECT COUNT(*) FROM ({}) VERTICAPY_SUBTABLE".format(query),
-        method="fetchfirstelem",
-        print_time_sql=False,
-    )
-    sql_on_init = verticapy.options["sql_on"]
-    time_on_init = verticapy.options["time_on"]
-    try:
-        verticapy.options["time_on"] = time_on
-        verticapy.options["sql_on"] = False
-        try:
-            result = to_tablesample("{} LIMIT {}".format(query, limit))
-        except:
-            result = to_tablesample(query)
-    except:
-        verticapy.options["time_on"] = time_on_init
-        verticapy.options["sql_on"] = sql_on_init
-        raise
-    verticapy.options["time_on"] = time_on_init
-    verticapy.options["sql_on"] = sql_on_init
-    result.count = count
-    if verticapy.options["percent_bar"]:
-        vdf = vDataFrameSQL("({}) VERTICAPY_SUBTABLE".format(query))
-        percent = vdf.agg(["percent"]).transpose().values
-        for column in result.values:
-            result.dtype[column] = vdf[column].ctype()
-            result.percent[column] = percent[vdf.format_colnames(column)][0]
-    return result
-
-
-# ---#
-def get_data_types(expr: str, column_name: str = ""):
-    """
----------------------------------------------------------------------------
-Returns customized relation columns and the respective data types.
-This process creates a temporary table.
-
-Parameters
-----------
-expr: str
-	An expression in pure SQL.
-column_name: str, optional
-	If not empty, it will return only the data type of the input column if it
-	is in the relation.
-
-Returns
--------
-list of tuples
-	The list of the different columns and their respective type.
-	"""
-    from verticapy.connect import current_cursor
-
-    if isinstance(current_cursor(), vertica_python.vertica.cursor.Cursor):
-        try:
-            if column_name:
-                executeSQL(expr, print_time_sql=False)
-                description = current_cursor().description[0]
-                return type_code_to_dtype(
-                    type_code=description[1],
-                    display_size=description[2],
-                    precision=description[4],
-                    scale=description[5],
-                )
-            else:
-                executeSQL(expr, print_time_sql=False)
-                description, ctype = current_cursor().description, []
-                for elem in description:
-                    ctype += [
-                        [
-                            elem[0],
-                            type_code_to_dtype(
-                                type_code=elem[1],
-                                display_size=elem[2],
-                                precision=elem[4],
-                                scale=elem[5],
-                            ),
-                        ]
-                    ]
-                return ctype
-        except:
-            pass
-    tmp_name, schema = gen_tmp_name(name="table"), "v_temp_schema"
-    drop("{}.{}".format(schema, tmp_name), method="table")
-    try:
-        if schema == "v_temp_schema":
-            executeSQL(
-                "CREATE LOCAL TEMPORARY TABLE {} ON COMMIT PRESERVE ROWS AS {}".format(
-                    tmp_name, expr
-                ),
-                print_time_sql=False,
-            )
-        else:
-            executeSQL(
-                "CREATE TEMPORARY TABLE {}.{} ON COMMIT PRESERVE ROWS AS {}".format(
-                    schema, tmp_name, expr
-                ),
-                print_time_sql=False,
-            )
-    except:
-        drop("{}.{}".format(schema, tmp_name), method="table")
-        raise
-    query = (
-        "SELECT column_name, data_type FROM columns WHERE {0}table_name = '{1}'"
-        " AND table_schema = '{2}' ORDER BY ordinal_position"
-    ).format(
-        f"column_name = '{column_name}' AND " if (column_name) else "",
-        tmp_name,
-        schema,
-    )
-    cursor = executeSQL(query, title="Getting the data types.")
-    if column_name:
-        ctype = cursor.fetchone()[1]
-    else:
-        ctype = cursor.fetchall()
-    drop("{}.{}".format(schema, tmp_name), method="table")
-    return ctype
-
-
-# ---#
-def insert_into(
-    table_name: str,
-    data: list,
-    schema: str = "",
-    column_names: list = [],
-    copy: bool = True,
-    genSQL: bool = False,
-):
-    """
----------------------------------------------------------------------------
-Inserts the dataset into an existing Vertica table.
-
-Parameters
-----------
-table_name: str
-    Name of the table to insert into.
-data: list
-    The data to ingest.
-schema: str, optional
-    Schema name.
-column_names: list, optional
-    Name of the column(s) to insert into.
-copy: bool, optional
-    If set to True, the batch insert is converted to a COPY statement 
-    with prepared statements. Otherwise, the INSERTs are performed
-    sequentially.
-genSQL: bool, optional
-    If set to True, the SQL code that would be used to insert the data 
-    is generated, but not executed.
-
-Returns
--------
-int
-    The number of rows ingested.
-
-See Also
---------
-pandas_to_vertica : Ingests a pandas DataFrame into the Vertica database.
-    """
-    check_types(
-        [
-            ("table_name", table_name, [str]),
-            ("column_names", column_names, [list]),
-            ("data", data, [list]),
-            ("schema", schema, [str]),
-            ("copy", copy, [bool]),
-            ("genSQL", genSQL, [bool]),
-        ]
-    )
-    if not (schema):
-        schema = verticapy.options["temp_schema"]
-    input_relation = "{}.{}".format(quote_ident(schema), quote_ident(table_name))
-    if not (column_names):
-        query = f"""SELECT 
-                        column_name
-                    FROM columns 
-                    WHERE table_name = '{table_name}' 
-                        AND table_schema = '{schema}' 
-                    ORDER BY ordinal_position"""
-        result = executeSQL(
-            query,
-            title=f"Getting the table {input_relation} column names.",
-            method="fetchall",
-        )
-        column_names = [elem[0] for elem in result]
-        assert column_names, MissingRelation(
-            f"The table {input_relation} does not exist."
-        )
-    cols = [quote_ident(col) for col in column_names]
-    if copy and not (genSQL):
-        sql = "INSERT INTO {} ({}) VALUES ({})".format(
-            input_relation,
-            ", ".join(cols),
-            ", ".join(["%s" for i in range(len(cols))]),
-        )
-        executeSQL(
-            sql,
-            title=(
-                f"Insert new lines in the {table_name} table. The batch insert is "
-                "converted into a COPY statement by using prepared statements."
-            ),
-            data=list(map(tuple, data)),
-        )
-        executeSQL("COMMIT;", title="Commit.")
-        return len(data)
-    else:
-        if genSQL:
-            sql = []
-        i, n, total_rows = 0, len(data), 0
-        header = "INSERT INTO {} ({}) VALUES ".format(input_relation, ", ".join(cols))
-        for i in range(n):
-            sql_tmp = "("
-            for elem in data[i]:
-                if isinstance(elem, str):
-                    sql_tmp += "'{}'".format(elem.replace("'", "''"))
-                elif elem is None or elem != elem:
-                    sql_tmp += "NULL"
-                else:
-                    sql_tmp += "'{}'".format(elem)
-                sql_tmp += ","
-            sql_tmp = sql_tmp[:-1] + ");"
-            query = header + sql_tmp
-            if genSQL:
-                sql += [query]
-            else:
-                try:
-                    executeSQL(
-                        query,
-                        title="Insert a new line in the relation: {}.".format(
-                            input_relation
-                        ),
-                    )
-                    executeSQL("COMMIT;", title="Commit.")
-                    total_rows += 1
-                except Exception as e:
-                    warning_message = "Line {} was skipped.\n{}".format(i, e)
-                    warnings.warn(warning_message, Warning)
-        if genSQL:
-            return sql
-        else:
-            return total_rows
+"""
+(c)  Copyright  [2018-2023]  OpenText  or one of its
+affiliates.  Licensed  under  the   Apache  License,
+Version 2.0 (the  "License"); You  may  not use this
+file except in compliance with the License.
+
+You may obtain a copy of the License at:
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless  required  by applicable  law or  agreed to in
+writing, software  distributed  under the  License is
+distributed on an  "AS IS" BASIS,  WITHOUT WARRANTIES
+OR CONDITIONS OF ANY KIND, either express or implied.
+See the  License for the specific  language governing
+permissions and limitations under the License.
+"""
+import copy
+import decimal
+import math
+from collections.abc import Iterable
+from typing import Literal, Optional, Union
+from tqdm.auto import tqdm
+import numpy as np
+import scipy.stats as scipy_st
+import scipy.special as scipy_special
+
+from vertica_python.errors import QueryError
+
+import verticapy._config.config as conf
+from verticapy._typing import NoneType, PlottingObject, SQLColumns
+from verticapy._utils._gen import gen_name, gen_tmp_name
+from verticapy._utils._object import get_vertica_mllib, create_new_vdf
+from verticapy._utils._sql._collect import save_verticapy_logs
+from verticapy._utils._sql._format import format_type, quote_ident
+from verticapy._utils._sql._sys import _executeSQL
+from verticapy._utils._sql._vertica_version import vertica_version
+from verticapy.errors import EmptyParameter, VersionError
+
+from verticapy.core.tablesample.base import TableSample
+
+from verticapy.core.vdataframe._encoding import vDFEncode, vDCEncode
 
+from verticapy.sql.drop import drop
 
-# ---#
-def pandas_to_vertica(
-    df,
-    name: str = "",
-    schema: str = "",
-    dtype: dict = {},
-    parse_nrows: int = 10000,
-    temp_path: str = "",
-    insert: bool = False,
-):
-    """
----------------------------------------------------------------------------
-Ingests a pandas DataFrame into the Vertica database by creating a 
-CSV file and then using flex tables to load the data.
-
-Parameters
-----------
-df: pandas.DataFrame
-    The pandas.DataFrame to ingest.
-name: str, optional
-    Name of the new relation or the relation in which to insert the 
-    data. If unspecified, a temporary local table is created. This 
-    temporary table is dropped at the end of the local session.
-schema: str, optional
-    Schema of the new relation. If empty, a temporary schema is used. 
-    To modify the temporary schema, use the 'set_option' function.
-dtype: dict, optional
-    Dictionary of input types. Providing a dictionary can increase 
-    ingestion speed and precision. If specified, rather than parsing 
-    the intermediate CSV and guessing the input types, VerticaPy uses 
-    the specified input types instead.
-parse_nrows: int, optional
-    If this parameter is greater than 0, VerticaPy creates and 
-    ingests a temporary file containing 'parse_nrows' number 
-    of rows to determine the input data types before ingesting 
-    the intermediate CSV file containing the rest of the data. 
-    This method of data type identification is less accurate, 
-    but is much faster for large datasets.
-temp_path: str, optional
-    The path to which to write the intermediate CSV file. This 
-    is useful in cases where the user does not have write 
-    permissions on the current directory.
-insert: bool, optional
-    If set to True, the data are ingested into the input relation. 
-    The column names of your table and the pandas.DataFrame must 
-    match.
-    
-Returns
--------
-vDataFrame
-    vDataFrame of the new relation.
-
-See Also
---------
-read_csv  : Ingests a  CSV file into the Vertica database.
-read_json : Ingests a JSON file into the Vertica database.
-    """
-    check_types(
-        [
-            ("name", name, [str]),
-            ("schema", schema, [str]),
-            ("parse_nrows", parse_nrows, [int]),
-            ("dtype", dtype, [dict]),
-            ("temp_path", temp_path, [str]),
-            ("insert", insert, [bool]),
-        ]
-    )
-    if not (schema):
-        schema = verticapy.options["temp_schema"]
-    assert name or not (insert), ParameterError(
-        "Parameter 'name' can not be empty when parameter 'insert' is set to True."
-    )
-    if not (name):
-        tmp_name = gen_tmp_name(name="df")[1:-1]
-    else:
-        tmp_name = ""
-    path = "{0}{1}{2}.csv".format(
-        temp_path, "/" if (len(temp_path) > 1 and temp_path[-1] != "/") else "", name
-    )
-    try:
-        # Adding the quotes to STR pandas columns in order to simplify the ingestion.
-        # Not putting them can lead to wrong data ingestion.
-        str_cols = []
-        for c in df.columns:
-            if df[c].dtype == object and isinstance(
-                df[c].loc[df[c].first_valid_index()], str
-            ):
-                str_cols += [c]
-        if str_cols:
-            tmp_df = df.copy()
-            for c in str_cols:
-                tmp_df[c] = '"' + tmp_df[c].str.replace('"', '""') + '"'
-            clear = True
-        else:
-            tmp_df = df
-            clear = False
-        import csv
-
-        tmp_df.to_csv(
-            path, index=False, quoting=csv.QUOTE_NONE, quotechar="", escapechar="\027"
-        )
-        if insert:
-            input_relation = "{}.{}".format(quote_ident(schema), quote_ident(name))
-            query = """COPY {0}({1}) 
-                       FROM LOCAL '{2}' 
-                       DELIMITER ',' 
-                       NULL ''
-                       ENCLOSED BY '\"' 
-                       ESCAPE AS '\\' 
-                       SKIP 1;""".format(
-                input_relation,
-                ", ".join(
-                    ['"' + col.replace('"', '""') + '"' for col in tmp_df.columns]
-                ),
-                path,
-            )
-            executeSQL(query, title="Inserting the pandas.DataFrame.")
-            from verticapy import vDataFrame
 
-            vdf = vDataFrame(name, schema=schema)
-        elif tmp_name:
-            vdf = read_csv(
-                path,
-                table_name=tmp_name,
-                dtype=dtype,
-                temporary_local_table=True,
-                parse_nrows=parse_nrows,
-                escape="\027",
-            )
-        else:
-            vdf = read_csv(
-                path,
-                table_name=name,
-                dtype=dtype,
-                schema=schema,
-                temporary_local_table=False,
-                parse_nrows=parse_nrows,
-                escape="\027",
-            )
-        os.remove(path)
-    except:
-        os.remove(path)
-        if clear:
-            del tmp_df
-        raise
-    if clear:
-        del tmp_df
-    return vdf
-
-
-# ---#
-def pcsv(
-    path: str,
-    sep: str = ",",
-    header: bool = True,
-    header_names: list = [],
-    na_rep: str = "",
-    quotechar: str = '"',
-    escape: str = "\027",
-    ingest_local: bool = True,
-):
-    """
----------------------------------------------------------------------------
-Parses a CSV file using flex tables. It will identify the columns and their
-respective types.
-
-Parameters
-----------
-path: str
-	Absolute path where the CSV file is located.
-sep: str, optional
-	Column separator.
-header: bool, optional
-	If set to False, the parameter 'header_names' will be to use to name the 
-	different columns.
-header_names: list, optional
-	List of the columns names.
-na_rep: str, optional
-	Missing values representation.
-quotechar: str, optional
-	Char which is enclosing the str values.
-escape: str, optional
-	Separator between each record.
-ingest_local: bool, optional
-    If set to True, the file will be ingested from the local machine.
-
-Returns
--------
-dict
-	dictionary containing for each column its type.
-
-See Also
---------
-read_csv  : Ingests a CSV file into the Vertica database.
-read_json : Ingests a JSON file into the Vertica database.
-	"""
-    flex_name = gen_tmp_name(name="flex")[1:-1]
-    executeSQL(
-        """CREATE FLEX LOCAL TEMP TABLE {0}(x int) 
-           ON COMMIT PRESERVE ROWS;""".format(
-            flex_name
-        ),
-        title="Creating flex table to identify the data types.",
-    )
-    header_names = (
-        ""
-        if not (header_names)
-        else "header_names = '{0}',".format(sep.join(header_names))
-    )
-    executeSQL(
-        """COPY {0} 
-           FROM{1} '{2}' 
-           PARSER FCSVPARSER(
-                type = 'traditional', 
-                delimiter = '{3}', 
-                header = {4}, {5} 
-                enclosed_by = '{6}', 
-                escape = '{7}') 
-           NULL '{8}';""".format(
-            flex_name,
-            " LOCAL" if ingest_local else "",
-            path,
-            sep,
-            header,
-            header_names,
-            quotechar,
-            escape,
-            na_rep,
-        ),
-        title="Parsing the data.",
-    )
-    executeSQL(
-        f"SELECT compute_flextable_keys('{flex_name}');",
-        title="Guessing flex tables keys.",
-    )
-    result = executeSQL(
-        f"SELECT key_name, data_type_guess FROM {flex_name}_keys",
-        title="Guessing the data types.",
-        method="fetchall",
-    )
-    dtype = {}
-    for column_dtype in result:
-        try:
-            query = """SELECT 
-                        (CASE 
-                            WHEN "{0}"=\'{1}\' THEN NULL 
-                            ELSE "{0}" 
-                         END)::{2} AS "{0}" 
-                       FROM {3} 
-                       WHERE "{0}" IS NOT NULL 
-                       LIMIT 1000""".format(
-                column_dtype[0], na_rep, column_dtype[1], flex_name,
-            )
-            executeSQL(query, print_time_sql=False)
-            dtype[column_dtype[0]] = column_dtype[1]
-        except:
-            dtype[column_dtype[0]] = "Varchar(100)"
-    drop(flex_name, method="table")
-    return dtype
-
-
-# ---#
-def help_start():
-    """
----------------------------------------------------------------------------
-VERTICAPY Interactive Help (FAQ).
-    """
-    try:
-        from IPython.core.display import HTML, display, Markdown
-    except:
-        pass
-    path = os.path.dirname(verticapy.__file__)
-    img1 = verticapy.gen_verticapy_logo_html(size="10%")
-    img2 = verticapy.gen_verticapy_logo_str()
-    message = img1 if (isnotebook()) else img2
-    message += (
-        "\n\n&#128226; Welcome to the <b>VerticaPy</b> help module."
-        "\n\nFrom here, you can learn how to connect to Vertica, "
-        "create a Virtual DataFrame, load your data, and more.\n "
-        "- <b>[Enter  0]</b> Overview of the library\n "
-        "- <b>[Enter  1]</b> Load an example dataset\n "
-        "- <b>[Enter  2]</b> View an example of data analysis with VerticaPy\n "
-        "- <b>[Enter  3]</b> Contribute on GitHub\n "
-        "- <b>[Enter  4]</b> View the SQL code generated by a vDataFrame and "
-        "the time elapsed for the query\n "
-        "- <b>[Enter  5]</b> Load your own dataset into Vertica \n "
-        "- <b>[Enter  6]</b> Write SQL queries in Jupyter\n "
-        "- <b>[Enter -1]</b> Exit"
-    )
-    if not (isnotebook()):
-        message = message.replace("<b>", "").replace("</b>", "")
-    display(Markdown(message)) if (isnotebook()) else print(message)
-    try:
-        response = int(input())
-    except:
-        print("Invalid choice.\nPlease enter a number between 0 and 11.")
-        try:
-            response = int(input())
-        except:
-            print("Invalid choice.\nRerun the help_start function when you need help.")
-            return
-    if response == 0:
-        link = "https://www.vertica.com/python/quick-start.php"
-    elif response == 1:
-        link = "https://www.vertica.com/python/documentation_last/datasets/"
-    elif response == 2:
-        link = "https://www.vertica.com/python/examples/"
-    elif response == 3:
-        link = "https://github.com/vertica/VerticaPy/"
-    elif response == 4:
-        link = "https://www.vertica.com/python/documentation_last/utilities/set_option/"
-    elif response == 5:
-        link = "https://www.vertica.com/python/documentation_last/datasets/"
-    elif response == 6:
-        link = "https://www.vertica.com/python/documentation_last/extensions/sql/"
-    elif response == -1:
-        message = "Thank you for using the VerticaPy help module."
-    elif response == 666:
-        message = (
-            "Thank you so much for using this library. My only purpose is to solve "
-            "real Big Data problems in the context of Data Science. I worked years "
-            "to be able to create this API and give you a real way to analyze your "
-            "data.\n\nYour devoted Data Scientist: <i>Badr Ouali</i>"
-        )
-    else:
-        message = "Invalid choice.\nPlease enter a number between -1 and 6."
-    if 0 <= response <= 6:
-        if not (isnotebook()):
-            message = f"Please go to {link}"
-        else:
-            message = f"Please go to <a href='{link}'>{link}</a>"
-    display(Markdown(message)) if (isnotebook()) else print(message)
+class vDFCorr(vDFEncode):
+    # System Methods.
 
-vHelp = help_start
-# ---#
-def pjson(path: str, ingest_local: bool = True):
-    """
----------------------------------------------------------------------------
-Parses a JSON file using flex tables. It will identify the columns and their
-respective types.
-
-Parameters
-----------
-path: str
-	Absolute path where the JSON file is located.
-ingest_local: bool, optional
-    If set to True, the file will be ingested from the local machine.
-
-Returns
--------
-dict
-	dictionary containing for each column its type.
-
-See Also
---------
-read_csv  : Ingests a CSV file into the Vertica database.
-read_json : Ingests a JSON file into the Vertica database.
-	"""
-    flex_name = gen_tmp_name(name="flex")[1:-1]
-    executeSQL(
-        "CREATE FLEX LOCAL TEMP TABLE {}(x int) ON COMMIT PRESERVE ROWS;".format(
-            flex_name
-        ),
-        title="Creating a flex table.",
-    )
-    executeSQL(
-        "COPY {} FROM{} '{}' PARSER FJSONPARSER();".format(
-            flex_name, " LOCAL" if ingest_local else "", path.replace("'", "''")
-        ),
-        title="Ingesting the data.",
-    )
-    executeSQL(
-        "SELECT compute_flextable_keys('{}');".format(flex_name),
-        title="Computing flex table keys.",
-    )
-    result = executeSQL(
-        "SELECT key_name, data_type_guess FROM {}_keys".format(flex_name),
-        title="Guessing data types.",
-        method="fetchall",
-    )
-    dtype = {}
-    for column_dtype in result:
-        dtype[column_dtype[0]] = column_dtype[1]
-    drop(name=flex_name, method="table")
-    return dtype
-
-
-# ---#
-def read_csv(
-    path: str,
-    schema: str = "",
-    table_name: str = "",
-    sep: str = ",",
-    header: bool = True,
-    header_names: list = [],
-    dtype: dict = {},
-    na_rep: str = "",
-    quotechar: str = '"',
-    escape: str = "\027",
-    genSQL: bool = False,
-    parse_nrows: int = -1,
-    insert: bool = False,
-    temporary_table: bool = False,
-    temporary_local_table: bool = True,
-    gen_tmp_table_name: bool = True,
-    ingest_local: bool = True,
-):
-    """
----------------------------------------------------------------------------
-Ingests a CSV file using flex tables.
-
-Parameters
-----------
-path: str
-	Absolute path where the CSV file is located.
-schema: str, optional
-	Schema where the CSV file will be ingested.
-table_name: str, optional
-	The final relation/table name. If unspecified, the the name is set to the 
-    name of the file or parent directory.
-sep: str, optional
-	Column separator.
-header: bool, optional
-	If set to False, the parameter 'header_names' will be to use to name the 
-	different columns.
-header_names: list, optional
-	List of the columns names.
-dtype: dict, optional
-    Dictionary of the user types. Providing a dictionary can increase 
-    ingestion speed and precision; instead of parsing the file to guess 
-    the different types, VerticaPy will use the input types.
-na_rep: str, optional
-	Missing values representation.
-quotechar: str, optional
-	Char which is enclosing the str values.
-escape: str, optional
-	Separator between each record.
-genSQL: bool, optional
-	If set to True, the SQL code for creating the final table will be 
-	generated but not executed. It is a good way to change the final
-	relation types or to customize the data ingestion.
-parse_nrows: int, optional
-	If this parameter is greater than 0. A new file of 'parse_nrows' rows
-	will be created and ingested first to identify the data types. It will be
-	then dropped and the entire file will be ingested. The data types identification
-	will be less precise but this parameter can make the process faster if the
-	file is heavy.
-insert: bool, optional
-	If set to True, the data will be ingested to the input relation. Be sure
-	that your file has a header corresponding to the name of the relation
-	columns, otherwise ingestion will fail.
-temporary_table: bool, optional
-    If set to True, a temporary table will be created.
-temporary_local_table: bool, optional
-    If set to True, a temporary local table will be created. The parameter 'schema'
-    must be empty, otherwise this parameter is ignored.
-gen_tmp_table_name: bool, optional
-    Sets the name of the temporary table. This parameter is only used when the 
-    parameter 'temporary_local_table' is set to True and if the parameters 
-    "table_name" and "schema" are unspecified.
-ingest_local: bool, optional
-    If set to True, the file will be ingested from the local machine.
-
-Returns
--------
-vDataFrame
-	The vDataFrame of the relation.
-
-See Also
---------
-read_json : Ingests a JSON file into the Vertica database.
-	"""
-    check_types(
-        [
-            ("path", path, [str]),
-            ("schema", schema, [str]),
-            ("table_name", table_name, [str]),
-            ("sep", sep, [str]),
-            ("header", header, [bool]),
-            ("header_names", header_names, [list]),
-            ("na_rep", na_rep, [str]),
-            ("dtype", dtype, [dict]),
-            ("quotechar", quotechar, [str]),
-            ("escape", escape, [str]),
-            ("genSQL", genSQL, [bool]),
-            ("parse_nrows", parse_nrows, [int, float]),
-            ("insert", insert, [bool]),
-            ("temporary_table", temporary_table, [bool]),
-            ("temporary_local_table", temporary_local_table, [bool]),
-            ("gen_tmp_table_name", gen_tmp_table_name, [bool]),
-            ("ingest_local", ingest_local, [bool]),
-        ]
-    )
-    if schema:
-        temporary_local_table = False
-    elif temporary_local_table:
-        schema = "v_temp_schema"
-    else:
-        schema = "public"
-    if header_names and dtype:
-        warning_message = (
-            "Parameters 'header_names' and 'dtype' are both defined. "
-            "Only 'dtype' will be used."
-        )
-        warnings.warn(warning_message, Warning)
-    if gen_tmp_table_name and temporary_local_table and not (table_name):
-        table_name = gen_tmp_name(name=path.split("/")[-1].split(".csv")[0])
-    assert not (temporary_table) or not (temporary_local_table), ParameterError(
-        "Parameters 'temporary_table' and 'temporary_local_table' can not be both "
-        "set to True."
-    )
-    path, sep, header_names, na_rep, quotechar, escape = (
-        path.replace("'", "''"),
-        sep.replace("'", "''"),
-        [str(elem).replace("'", "''") for elem in header_names],
-        na_rep.replace("'", "''"),
-        quotechar.replace("'", "''"),
-        escape.replace("'", "''"),
-    )
-    file = path.split("/")[-1]
-    file_extension = file[-3 : len(file)]
-    if file_extension != "csv":
-        raise ExtensionError("The file extension is incorrect !")
-    if not (table_name):
-        table_name = path.split("/")[-1].split(".csv")[0]
-    if table_name == "*":
-        assert dtype, ParameterError(
-            "Parameter 'dtype' must include the types of all columns in "
-            "the table when ingesting multiple files."
-        )
-        table_name = path.split("/")[-2]
-    query = """SELECT 
-                    column_name 
-               FROM columns 
-               WHERE table_name = '{0}' 
-                 AND table_schema = '{1}' 
-               ORDER BY ordinal_position""".format(
-        table_name.replace("'", "''"), schema.replace("'", "''")
-    )
-    result = executeSQL(
-        query, title="Looking if the relation exists.", method="fetchall"
-    )
-    if (result != []) and not (insert) and not (genSQL):
-        raise NameError(
-            'The table "{}"."{}" already exists !'.format(schema, table_name)
-        )
-    elif (result == []) and (insert):
-        raise MissingRelation(
-            'The table "{}"."{}" doesn\'t exist !'.format(schema, table_name)
-        )
-    else:
-        if not (temporary_local_table):
-            input_relation = "{}.{}".format(
-                quote_ident(schema), quote_ident(table_name)
-            )
-        else:
-            input_relation = "v_temp_schema.{}".format(quote_ident(table_name))
-        f = open(path, "r")
-        file_header = f.readline().replace("\n", "").replace('"', "").split(sep)
-        f.close()
-        if not (header_names) and not (dtype):
-            for idx, col in enumerate(file_header):
-                if col == "":
-                    if idx == 0:
-                        position = "beginning"
-                    elif idx == len(file_header) - 1:
-                        position = "end"
-                    else:
-                        position = "middle"
-                    file_header[idx] = "col{}".format(idx)
-                    warning_message = (
-                        "An inconsistent name was found in the {0} of the "
-                        "file header (isolated separator). It will be replaced "
-                        "by col{1}."
-                    ).format(position, idx)
-                    if idx == 0:
-                        warning_message += (
-                            "\nThis can happen when exporting a pandas DataFrame "
-                            "to CSV while retaining its indexes.\nTip: Use "
-                            "index=False when exporting with pandas.DataFrame.to_csv."
-                        )
-                    warnings.warn(warning_message, Warning)
-        if (header_names == []) and (header):
-            if not (dtype):
-                header_names = file_header
-            else:
-                header_names = [elem for elem in dtype]
-            for idx in range(len(header_names)):
-                h = header_names[idx]
-                n = len(h)
-                while n > 0 and h[0] == " ":
-                    h = h[1:]
-                    n -= 1
-                while n > 0 and h[-1] == " ":
-                    h = h[:-1]
-                    n -= 1
-                header_names[idx] = h
-        elif len(file_header) > len(header_names):
-            header_names += [
-                "ucol{}".format(i + len(header_names))
-                for i in range(len(file_header) - len(header_names))
-            ]
-        if (parse_nrows > 0) and not (insert):
-            f = open(path, "r")
-            f2 = open(path[0:-4] + "verticapy_copy.csv", "w")
-            for i in range(parse_nrows + int(header)):
-                line = f.readline()
-                f2.write(line)
-            f.close()
-            f2.close()
-            path_test = path[0:-4] + "verticapy_copy.csv"
-        else:
-            path_test = path
-        query1 = ""
-        if not (insert):
-            if not (dtype):
-                dtype = pcsv(
-                    path_test,
-                    sep,
-                    header,
-                    header_names,
-                    na_rep,
-                    quotechar,
-                    escape,
-                    ingest_local=ingest_local,
-                )
-            if parse_nrows > 0:
-                os.remove(path[0:-4] + "verticapy_copy.csv")
-            dtype_sorted = {}
-            for elem in header_names:
-                dtype_sorted[elem] = dtype[elem]
-            query1 = create_table(
-                table_name,
-                dtype_sorted,
-                schema,
-                temporary_table,
-                temporary_local_table,
-                genSQL=True,
-            )
-        skip = " SKIP 1" if (header) else ""
-        query2 = """COPY {0}({1}) 
-                    FROM {2} 
-                    DELIMITER '{3}' 
-                    NULL '{4}' 
-                    ENCLOSED BY '{5}' 
-                    ESCAPE AS '{6}'{7};""".format(
-            input_relation,
-            ", ".join(['"' + column + '"' for column in header_names]),
-            "{}",
-            sep,
-            na_rep,
-            quotechar,
-            escape,
-            skip,
-        )
-        if genSQL:
-            return [clean_query(query1), clean_query(query2)]
-        else:
-            if query1:
-                executeSQL(query1, "Creating the table.")
-            executeSQL(
-                query2.format("{}'{}'".format("LOCAL " if ingest_local else "", path)),
-                "Ingesting the data.",
-            )
-            if (
-                query1
-                and not (temporary_local_table)
-                and verticapy.options["print_info"]
+    def _aggregate_matrix(
+        self,
+        method: str = "pearson",
+        columns: Optional[SQLColumns] = None,
+        mround: int = 3,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Global method used to compute the Correlation/Cov/Regr Matrix.
+        """
+        method_name = "Correlation"
+        method_type = f" using the method = '{method}'"
+        if method == "cov":
+            method_name = "Covariance"
+            method_type = ""
+        columns = self.format_colnames(columns)
+        if method != "cramer":
+            for column in columns:
+                assert self[column].isnum(), TypeError(
+                    f"vDataColumn {column} must be numerical to "
+                    f"compute the {method_name} Matrix{method_type}."
+                )
+        if len(columns) == 1:
+            if method in (
+                "pearson",
+                "spearman",
+                "spearmand",
+                "kendall",
+                "biserial",
+                "cramer",
             ):
-                print(
-                    "The table {} has been successfully created.".format(input_relation)
-                )
-            from verticapy import vDataFrame
-
-            return vDataFrame(table_name, schema=schema)
-
-
-# ---#
-def read_json(
-    path: str,
-    schema: str = "",
-    table_name: str = "",
-    usecols: list = [],
-    new_name: dict = {},
-    insert: bool = False,
-    temporary_table: bool = False,
-    temporary_local_table: bool = True,
-    gen_tmp_table_name: bool = True,
-    ingest_local: bool = True,
-):
-    """
----------------------------------------------------------------------------
-Ingests a JSON file using flex tables.
-
-Parameters
-----------
-path: str
-	Absolute path where the JSON file is located.
-schema: str, optional
-	Schema where the JSON file will be ingested.
-table_name: str, optional
-	Final relation name.
-usecols: list, optional
-	List of the JSON parameters to ingest. The other ones will be ignored. If
-	empty all the JSON parameters will be ingested.
-new_name: dict, optional
-	Dictionary of the new columns name. If the JSON file is nested, it is advised
-	to change the final names as special characters will be included.
-	For example, {"param": {"age": 3, "name": Badr}, "date": 1993-03-11} will 
-	create 3 columns: "param.age", "param.name" and "date". You can rename these 
-	columns using the 'new_name' parameter with the following dictionary:
-	{"param.age": "age", "param.name": "name"}
-insert: bool, optional
-	If set to True, the data will be ingested to the input relation. The JSON
-	parameters must be the same than the input relation otherwise they will
-	not be ingested.
-temporary_table: bool, optional
-    If set to True, a temporary table will be created.
-temporary_local_table: bool, optional
-    If set to True, a temporary local table will be created. The parameter 'schema'
-    must be empty, otherwise this parameter is ignored.
-gen_tmp_table_name: bool, optional
-    Sets the name of the temporary table. This parameter is only used when the 
-    parameter 'temporary_local_table' is set to True and if the parameters 
-    "table_name" and "schema" are unspecified.
-ingest_local: bool, optional
-    If set to True, the file will be ingested from the local machine.
-
-Returns
--------
-vDataFrame
-	The vDataFrame of the relation.
-
-See Also
---------
-read_csv : Ingests a CSV file into the Vertica database.
-	"""
-    check_types(
-        [
-            ("schema", schema, [str]),
-            ("table_name", table_name, [str]),
-            ("usecols", usecols, [list]),
-            ("new_name", new_name, [dict]),
-            ("insert", insert, [bool]),
-            ("temporary_table", temporary_table, [bool]),
-            ("temporary_local_table", temporary_local_table, [bool]),
-            ("gen_tmp_table_name", gen_tmp_table_name, [bool]),
-            ("ingest_local", ingest_local, [bool]),
-        ]
-    )
-    if schema:
-        temporary_local_table = False
-    elif temporary_local_table:
-        schema = "v_temp_schema"
-    else:
-        schema = "public"
-    assert not (temporary_table) or not (temporary_local_table), ParameterError(
-        "Parameters 'temporary_table' and 'temporary_local_table' can not be both set to True."
-    )
-    file = path.split("/")[-1]
-    file_extension = file[-4 : len(file)]
-    if file_extension != "json":
-        raise ExtensionError("The file extension is incorrect !")
-    if gen_tmp_table_name and temporary_local_table and not (table_name):
-        table_name = gen_tmp_name(name=path.split("/")[-1].split(".json")[0])
-    if not (table_name):
-        table_name = path.split("/")[-1].split(".json")[0]
-    query = (
-        "SELECT column_name, data_type FROM columns WHERE table_name = '{0}' "
-        "AND table_schema = '{1}' ORDER BY ordinal_position"
-    ).format(table_name.replace("'", "''"), schema.replace("'", "''"))
-    column_name = executeSQL(
-        query, title="Looking if the relation exists.", method="fetchall"
-    )
-    if (column_name != []) and not (insert):
-        raise NameError(
-            'The table "{}"."{}" already exists !'.format(schema, table_name)
-        )
-    elif (column_name == []) and (insert):
-        raise MissingRelation(
-            'The table "{}"."{}" doesn\'t exist !'.format(schema, table_name)
-        )
-    else:
-        if not (temporary_local_table):
-            input_relation = '"{}"."{}"'.format(schema, table_name)
-        else:
-            input_relation = '"{}"'.format(table_name)
-        flex_name = gen_tmp_name(name="flex")[1:-1]
-        executeSQL(
-            "CREATE FLEX LOCAL TEMP TABLE {0}(x int) ON COMMIT PRESERVE ROWS;".format(
-                flex_name
-            ),
-            title="Creating flex table.",
-        )
-        executeSQL(
-            "COPY {} FROM{} '{}' PARSER FJSONPARSER();".format(
-                flex_name, " LOCAL" if ingest_local else "", path.replace("'", "''")
-            ),
-            title="Ingesting the data in the flex table.",
-        )
-        executeSQL(
-            "SELECT compute_flextable_keys('{}');".format(flex_name),
-            title="Computing flex table keys.",
-        )
-        result = executeSQL(
-            "SELECT key_name, data_type_guess FROM {}_keys".format(flex_name),
-            title="Guessing data types.",
-            method="fetchall",
-        )
-        dtype = {}
-        for column_dtype in result:
-            try:
-                executeSQL(
-                    'SELECT "{}"::{} FROM {} LIMIT 1000'.format(
-                        column_dtype[0], column_dtype[1], flex_name
+                return 1.0
+            elif method == "cov":
+                return self[columns[0]].var()
+        elif len(columns) == 2:
+            pre_comp_val = self._get_catalog_value(method=method, columns=columns)
+            if pre_comp_val != "VERTICAPY_NOT_PRECOMPUTED":
+                return pre_comp_val
+            cast_0 = "::int" if (self[columns[0]].isbool()) else ""
+            cast_1 = "::int" if (self[columns[1]].isbool()) else ""
+            if method in (
+                "pearson",
+                "spearman",
+                "spearmand",
+            ):
+                if columns[1] == columns[0]:
+                    return 1
+                if method == "pearson":
+                    table = self._genSQL()
+                else:
+                    table = f"""
+                        (SELECT 
+                            RANK() OVER (ORDER BY {columns[0]}) AS {columns[0]}, 
+                            RANK() OVER (ORDER BY {columns[1]}) AS {columns[1]} 
+                        FROM {self}) rank_spearman_table
+                    """
+                query = f"""
+                    SELECT 
+                        /*+LABEL('vDataframe._aggregate_matrix')*/ 
+                        CORR({columns[0]}{cast_0}, {columns[1]}{cast_1}) 
+                    FROM {table}"""
+                title = (
+                    f"Computes the {method} correlation between "
+                    f"{columns[0]} and {columns[1]}."
+                )
+            elif method == "biserial":
+                if columns[1] == columns[0]:
+                    return 1
+                elif (self[columns[1]].category() != "int") and (
+                    self[columns[0]].category() != "int"
+                ):
+                    return np.nan
+                elif self[columns[1]].category() == "int":
+                    if not self[columns[1]].isbool():
+                        agg = (
+                            self[columns[1]]
+                            .aggregate(["approx_unique", "min", "max"])
+                            .values[columns[1]]
+                        )
+                        if (agg[0] != 2) or (agg[1] != 0) or (agg[2] != 1):
+                            return np.nan
+                    column_b, column_n = columns[1], columns[0]
+                    cast_b, cast_n = cast_1, cast_0
+                elif self[columns[0]].category() == "int":
+                    if not self[columns[0]].isbool():
+                        agg = (
+                            self[columns[0]]
+                            .aggregate(["approx_unique", "min", "max"])
+                            .values[columns[0]]
+                        )
+                        if (agg[0] != 2) or (agg[1] != 0) or (agg[2] != 1):
+                            return np.nan
+                    column_b, column_n = columns[0], columns[1]
+                    cast_b, cast_n = cast_0, cast_1
+                else:
+                    return np.nan
+                query = f"""
+                    SELECT 
+                        /*+LABEL('vDataframe._aggregate_matrix')*/
+                        (AVG(DECODE({column_b}{cast_b}, 1, 
+                                    {column_n}{cast_n}, NULL)) 
+                       - AVG(DECODE({column_b}{cast_b}, 0, 
+                                    {column_n}{cast_n}, NULL))) 
+                       / STDDEV({column_n}{cast_n}) 
+                       * SQRT(SUM({column_b}{cast_b}) 
+                       * SUM(1 - {column_b}{cast_b}) 
+                       / COUNT(*) / COUNT(*)) 
+                    FROM {self} 
+                    WHERE {column_b} IS NOT NULL 
+                      AND {column_n} IS NOT NULL;"""
+                title = (
+                    "Computes the biserial correlation "
+                    f"between {column_b} and {column_n}."
+                )
+            elif method == "cramer":
+                if columns[1] == columns[0]:
+                    return 1
+                table_0_1 = f"""
+                    SELECT 
+                        {columns[0]}, 
+                        {columns[1]}, 
+                        COUNT(*) AS nij 
+                    FROM {self} 
+                    WHERE {columns[0]} IS NOT NULL 
+                      AND {columns[1]} IS NOT NULL 
+                    GROUP BY 1, 2"""
+                table_0 = f"""
+                    SELECT 
+                        {columns[0]}, 
+                        COUNT(*) AS ni 
+                    FROM {self} 
+                    WHERE {columns[0]} IS NOT NULL 
+                      AND {columns[1]} IS NOT NULL 
+                    GROUP BY 1"""
+                table_1 = f"""
+                    SELECT 
+                        {columns[1]}, 
+                        COUNT(*) AS nj 
+                    FROM {self} 
+                    WHERE {columns[0]} IS NOT NULL 
+                      AND {columns[1]} IS NOT NULL 
+                    GROUP BY 1"""
+                n, k, r = _executeSQL(
+                    query=f"""
+                        SELECT /*+LABEL('vDataframe._aggregate_matrix')*/
+                            COUNT(*) AS n, 
+                            APPROXIMATE_COUNT_DISTINCT({columns[0]}) AS k, 
+                            APPROXIMATE_COUNT_DISTINCT({columns[1]}) AS r 
+                         FROM {self} 
+                         WHERE {columns[0]} IS NOT NULL 
+                           AND {columns[1]} IS NOT NULL""",
+                    title="Computing the columns cardinalities.",
+                    method="fetchrow",
+                    sql_push_ext=self._vars["sql_push_ext"],
+                    symbol=self._vars["symbol"],
+                )
+                chi2 = f"""
+                    SELECT /*+LABEL('vDataframe._aggregate_matrix')*/
+                        SUM((nij - ni * nj / {n}) * (nij - ni * nj / {n}) 
+                            / ((ni * nj) / {n})) AS chi2 
+                    FROM 
+                        (SELECT 
+                            * 
+                         FROM ({table_0_1}) table_0_1 
+                         LEFT JOIN ({table_0}) table_0 
+                         ON table_0_1.{columns[0]} = table_0.{columns[0]}) x 
+                         LEFT JOIN ({table_1}) table_1 
+                         ON x.{columns[1]} = table_1.{columns[1]}"""
+                result = _executeSQL(
+                    chi2,
+                    title=(
+                        f"Computing the CramerV correlation between {columns[0]} "
+                        f"and {columns[1]} (Chi2 Statistic)."
                     ),
-                    print_time_sql=False,
+                    method="fetchfirstelem",
+                    sql_push_ext=self._vars["sql_push_ext"],
+                    symbol=self._vars["symbol"],
                 )
-                dtype[column_dtype[0]] = column_dtype[1]
-            except:
-                dtype[column_dtype[0]] = "Varchar(100)"
-        if not (insert):
-            cols = (
-                [column for column in dtype]
-                if not (usecols)
-                else [column for column in usecols]
-            )
-            for i, column in enumerate(cols):
-                cols[i] = (
-                    '"{}"::{} AS "{}"'.format(
-                        column.replace('"', ""), dtype[column], new_name[column]
-                    )
-                    if (column in new_name)
-                    else '"{}"::{}'.format(column.replace('"', ""), dtype[column])
+                if min(k - 1, r - 1) == 0:
+                    result = np.nan
+                else:
+                    result = float(math.sqrt(result / n / min(k - 1, r - 1)))
+                    if result > 1 or result < 0:
+                        result = np.nan
+                return result
+            elif method == "kendall":
+                if columns[1] == columns[0]:
+                    return 1
+                n_ = "SQRT(COUNT(*))"
+                n_c = f"""
+                    (SUM(((x.{columns[0]}{cast_0} < y.{columns[0]}{cast_0} 
+                       AND x.{columns[1]}{cast_1} < y.{columns[1]}{cast_1}) 
+                       OR (x.{columns[0]}{cast_0} > y.{columns[0]}{cast_0} 
+                       AND x.{columns[1]}{cast_1} > y.{columns[1]}{cast_1}))::int))/2"""
+                n_d = f"""
+                    (SUM(((x.{columns[0]}{cast_0} > y.{columns[0]}{cast_0} 
+                       AND x.{columns[1]}{cast_1} < y.{columns[1]}{cast_1}) 
+                       OR (x.{columns[0]}{cast_0} < y.{columns[0]}{cast_0}
+                       AND x.{columns[1]}{cast_1} > y.{columns[1]}{cast_1}))::int))/2"""
+                n_1 = f"(SUM((x.{columns[0]}{cast_0} = y.{columns[0]}{cast_0})::int)-{n_})/2"
+                n_2 = f"(SUM((x.{columns[1]}{cast_1} = y.{columns[1]}{cast_1})::int)-{n_})/2"
+                n_0 = f"{n_} * ({n_} - 1)/2"
+                tau_b = f"({n_c} - {n_d}) / sqrt(({n_0} - {n_1}) * ({n_0} - {n_2}))"
+                query = f"""
+                    SELECT /*+LABEL('vDataframe._aggregate_matrix')*/
+                        {tau_b} 
+                    FROM 
+                        (SELECT 
+                            {columns[0]}, 
+                            {columns[1]} 
+                         FROM {self}) x 
+                        CROSS JOIN 
+                        (SELECT 
+                            {columns[0]}, 
+                            {columns[1]} 
+                         FROM {self}) y"""
+                title = f"Computing the kendall correlation between {columns[0]} and {columns[1]}."
+            elif method == "cov":
+                query = f"""
+                    SELECT /*+LABEL('vDataframe._aggregate_matrix')*/ 
+                        COVAR_POP({columns[0]}{cast_0}, {columns[1]}{cast_1}) 
+                    FROM {self}"""
+                title = (
+                    f"Computing the covariance between {columns[0]} and {columns[1]}."
                 )
-            temp = "TEMPORARY " if temporary_table else ""
-            temp = "LOCAL TEMPORARY " if temporary_local_table else ""
-            executeSQL(
-                "CREATE {}TABLE {}{} AS SELECT {} FROM {}".format(
-                    temp,
-                    input_relation,
-                    " ON COMMIT PRESERVE ROWS" if temp else "",
-                    ", ".join(cols),
-                    flex_name,
-                ),
-                title="Creating table.",
-            )
-            if not (temporary_local_table) and verticapy.options["print_info"]:
-                print(
-                    "The table {} has been successfully created.".format(input_relation)
+            try:
+                result = _executeSQL(
+                    query=query,
+                    title=title,
+                    method="fetchfirstelem",
+                    sql_push_ext=self._vars["sql_push_ext"],
+                    symbol=self._vars["symbol"],
+                )
+            except QueryError:
+                result = np.nan
+            self._update_catalog(
+                values={columns[1]: result}, matrix=method, column=columns[0]
+            )
+            self._update_catalog(
+                values={columns[0]: result}, matrix=method, column=columns[1]
+            )
+            if isinstance(result, decimal.Decimal):
+                result = float(result)
+            return result
+        elif len(columns) > 2:
+            nb_precomputed, n = 0, len(columns)
+            for column1 in columns:
+                for column2 in columns:
+                    pre_comp_val = self._get_catalog_value(
+                        method=method, columns=[column1, column2]
+                    )
+                    if pre_comp_val != "VERTICAPY_NOT_PRECOMPUTED":
+                        nb_precomputed += 1
+            try:
+                vertica_version(condition=[9, 2, 1])
+                assert (nb_precomputed <= n * n / 3) and (
+                    method
+                    in (
+                        "pearson",
+                        "spearman",
+                        "spearmand",
+                    )
                 )
-        else:
-            column_name_dtype = {}
-            for elem in column_name:
-                column_name_dtype[elem[0]] = elem[1]
-            final_cols = {}
-            for column in column_name_dtype:
-                final_cols[column] = None
-            for column in column_name_dtype:
-                if column in dtype:
-                    final_cols[column] = column
+                fun = "DENSE_RANK" if method == "spearmand" else "RANK"
+                if method == "pearson":
+                    table = self._genSQL()
                 else:
-                    for col in new_name:
-                        if new_name[col] == column:
-                            final_cols[column] = col
-            final_transformation = []
-            for column in final_cols:
-                final_transformation += (
-                    ['NULL AS "{}"'.format(column)]
-                    if (final_cols[column] == None)
-                    else [
-                        '"{}"::{} AS "{}"'.format(
-                            final_cols[column], column_name_dtype[column], column
+                    columns_str = ", ".join(
+                        [
+                            f"{fun}() OVER (ORDER BY {column}) AS {column}"
+                            for column in columns
+                        ]
+                    )
+                    table = f"(SELECT {columns_str} FROM {self}) spearman_table"
+                result = _executeSQL(
+                    query=f"""SELECT /*+LABEL('vDataframe._aggregate_matrix')*/ 
+                                CORR_MATRIX({', '.join(columns)}) 
+                                OVER () 
+                             FROM {table}""",
+                    title=f"Computing the {method} Corr Matrix.",
+                    method="fetchall",
+                )
+                corr_dict = {}
+                for idx, column in enumerate(columns):
+                    corr_dict[column] = idx
+                n = len(columns)
+                matrix = np.array([[1.0 for i in range(0, n)] for i in range(0, n)])
+                for x in result:
+                    i = corr_dict[quote_ident(x[0])]
+                    j = corr_dict[quote_ident(x[1])]
+                    if not isinstance(x[2], NoneType):
+                        matrix[i][j] = x[2]
+                    else:
+                        matrix[i][j] = np.nan
+            except (AssertionError, QueryError, VersionError):
+                if method in (
+                    "pearson",
+                    "spearman",
+                    "spearmand",
+                    "kendall",
+                    "biserial",
+                    "cramer",
+                ):
+                    title = "Computing all Correlations in a single query"
+                    if method == "biserial":
+                        i0, step = 0, 1
+                    else:
+                        i0, step = 1, 0
+                elif method == "cov":
+                    title = "Computing all covariances in a single query"
+                    i0, step = 0, 1
+                n = len(columns)
+                loop = tqdm(range(i0, n)) if conf.get_option("tqdm") else range(i0, n)
+                try:
+                    all_list = []
+                    nb_precomputed = 0
+                    nb_loop = 0
+                    for i in loop:
+                        for j in range(0, i + step):
+                            nb_loop += 1
+                            cast_i = "::int" if (self[columns[i]].isbool()) else ""
+                            cast_j = "::int" if (self[columns[j]].isbool()) else ""
+                            pre_comp_val = self._get_catalog_value(
+                                method=method, columns=[columns[i], columns[j]]
+                            )
+                            if (
+                                isinstance(pre_comp_val, NoneType)
+                                or pre_comp_val != pre_comp_val
+                            ):
+                                pre_comp_val = "NULL"
+                            if pre_comp_val != "VERTICAPY_NOT_PRECOMPUTED":
+                                all_list += [str(pre_comp_val)]
+                                nb_precomputed += 1
+                            elif method in ("pearson", "spearman", "spearmand"):
+                                all_list += [
+                                    f"""CORR({columns[i]}{cast_i}, {columns[j]}{cast_j})"""
+                                ]
+                            elif method == "kendall":
+                                n_ = "SQRT(COUNT(*))"
+                                n_c = f"""
+                                    (SUM(((x.{columns[i]}{cast_i} 
+                                         < y.{columns[i]}{cast_i} 
+                                       AND x.{columns[j]}{cast_j} 
+                                         < y.{columns[j]}{cast_j})
+                                       OR (x.{columns[i]}{cast_i} 
+                                         > y.{columns[i]}{cast_i} 
+                                       AND x.{columns[j]}{cast_j} 
+                                         > y.{columns[j]}{cast_j}))::int))/2"""
+                                n_d = f"""
+                                    (SUM(((x.{columns[i]}{cast_i} 
+                                         > y.{columns[i]}{cast_i} 
+                                       AND x.{columns[j]}{cast_j} 
+                                         < y.{columns[j]}{cast_j}) 
+                                       OR (x.{columns[i]}{cast_i} 
+                                         < y.{columns[i]}{cast_i} 
+                                       AND x.{columns[j]}{cast_j} 
+                                         > y.{columns[j]}{cast_j}))::int))/2"""
+                                n_1 = f"""
+                                    (SUM((x.{columns[i]}{cast_i} = 
+                                          y.{columns[i]}{cast_i})::int)-{n_})/2"""
+                                n_2 = f"""(SUM((x.{columns[j]}{cast_j} = 
+                                          y.{columns[j]}{cast_j})::int)-{n_})/2"""
+                                n_0 = f"{n_} * ({n_} - 1)/2"
+                                tau_b = f"({n_c} - {n_d}) / sqrt(({n_0} - {n_1}) * ({n_0} - {n_2}))"
+                                all_list += [tau_b]
+                            elif method == "cov":
+                                all_list += [
+                                    f"COVAR_POP({columns[i]}{cast_i}, {columns[j]}{cast_j})"
+                                ]
+                            else:
+                                assert False
+                    if method in ("spearman", "spearmand"):
+                        fun = "DENSE_RANK" if method == "spearmand" else "RANK"
+                        rank = [
+                            f"{fun}() OVER (ORDER BY {column}) AS {column}"
+                            for column in columns
+                        ]
+                        table = f"(SELECT {', '.join(rank)} FROM {self}) rank_spearman_table"
+                    elif method == "kendall":
+                        table = f"""
+                                           (SELECT {", ".join(columns)} FROM {self}) x 
+                                CROSS JOIN (SELECT {", ".join(columns)} FROM {self}) y"""
+                    else:
+                        table = self._genSQL()
+                    if nb_precomputed == nb_loop:
+                        result = _executeSQL(
+                            query=f"""
+                                SELECT 
+                                    /*+LABEL('vDataframe._aggregate_matrix')*/ 
+                                    {', '.join(all_list)}""",
+                            print_time_sql=False,
+                            method="fetchrow",
                         )
-                    ]
-                )
-            executeSQL(
-                "INSERT INTO {} SELECT {} FROM {}".format(
-                    input_relation, ", ".join(final_transformation), flex_name
-                ),
-                title="Inserting data into table.",
-            )
-        drop(name=flex_name, method="table")
-        from verticapy import vDataFrame
-
-        return vDataFrame(table_name, schema=schema)
-
-
-# ---#
-def read_shp(
-    path: str, schema: str = "public", table_name: str = "",
-):
-    """
----------------------------------------------------------------------------
-Ingests a SHP file. For the moment, only files located in the Vertica server 
-can be ingested.
-
-Parameters
-----------
-path: str
-    Absolute path where the SHP file is located.
-schema: str, optional
-    Schema where the SHP file will be ingested.
-table_name: str, optional
-    Final relation name.
-
-Returns
--------
-vDataFrame
-    The vDataFrame of the relation.
-    """
-    check_types(
-        [
-            ("path", path, [str]),
-            ("schema", schema, [str]),
-            ("table_name", table_name, [str]),
-        ]
-    )
-    file = path.split("/")[-1]
-    file_extension = file[-3 : len(file)]
-    if file_extension != "shp":
-        raise ExtensionError("The file extension is incorrect !")
-    query = (
-        f"SELECT STV_ShpCreateTable(USING PARAMETERS file='{path}')"
-        " OVER() AS create_shp_table;"
-    )
-    result = executeSQL(query, title="Getting SHP definition.", method="fetchall")
-    if not (table_name):
-        table_name = file[:-4]
-    result[0] = [f'CREATE TABLE "{schema}"."{table_name}"(']
-    result = [elem[0] for elem in result]
-    result = "".join(result)
-    executeSQL(result, title="Creating the relation.")
-    query = (
-        f'COPY "{schema}"."{table_name}" WITH SOURCE STV_ShpSource(file=\'{path}\')'
-        " PARSER STV_ShpParser();"
-    )
-    executeSQL(query, title="Ingesting the data.")
-    print(f'The table "{schema}"."{table_name}" has been successfully created.')
-    from verticapy import vDataFrame
-
-    return vDataFrame(table_name, schema=schema)
-
-
-# ---#
-def set_option(option: str, value: Union[bool, int, str] = None):
-    """
-    ---------------------------------------------------------------------------
-    Sets VerticaPy options.
-
-    Parameters
-    ----------
-    option: str
-        Option to use.
-        cache          : bool
-            If set to True, the vDataFrame will save in memory the computed
-            aggregations.
-        colors         : list
-            List of the colors used to draw the graphics.
-        color_style    : str
-            Style used to color the graphics, one of the following:
-            "rgb", "sunset", "retro", "shimbg", "swamp", "med", "orchid", 
-            "magenta", "orange", "vintage", "vivid", "berries", "refreshing", 
-            "summer", "tropical", "india", "default".
-        max_columns    : int
-            Maximum number of columns to display. If the parameter is incorrect, 
-            nothing is changed.
-        max_rows       : int
-            Maximum number of rows to display. If the parameter is incorrect, 
-            nothing is changed.
-        mode           : str
-            How to display VerticaPy outputs.
-                full  : VerticaPy regular display mode.
-                light : Minimalist display mode.
-        overwrite_model: bool
-            If set to True and you try to train a model with an existing name. 
-            It will be automatically overwritten.
-        percent_bar    : bool
-            If set to True, it displays the percent of non-missing values.
-        print_info     : bool
-            If set to True, information will be printed each time the vDataFrame 
-            is modified.
-        random_state   : int
-            Integer used to seed the random number generation in VerticaPy.
-        sql_on         : bool
-            If set to True, displays all the SQL queries.
-        temp_schema    : str
-            Specifies the temporary schema that certain methods/functions use to 
-            create intermediate objects, if needed. 
-        time_on        : bool
-            If set to True, displays all the SQL queries elapsed time.
-        tqdm           : bool
-            If set to True, a loading bar is displayed when using iterative 
-            functions.
-    value: object, optional
-        New value of option.
-    """
-    if isinstance(option, str):
-        option = option.lower()
-    check_types(
-        [
-            (
-                "option",
-                option,
-                [
-                    "cache",
-                    "colors",
-                    "color_style",
-                    "max_columns",
-                    "max_rows",
-                    "mode",
-                    "overwrite_model",
-                    "percent_bar",
-                    "print_info",
-                    "random_state",
-                    "sql_on",
-                    "temp_schema",
-                    "time_on",
-                    "tqdm",
-                ],
-            ),
-        ]
-    )
-    if option == "cache":
-        check_types([("value", value, [bool])])
-        if isinstance(value, bool):
-            verticapy.options["cache"] = value
-    elif option == "colors":
-        check_types([("value", value, [list])])
-        if isinstance(value, list):
-            verticapy.options["colors"] = [str(elem) for elem in value]
-    elif option == "color_style":
-        check_types(
-            [
-                (
-                    "value",
-                    value,
-                    [
-                        "rgb",
-                        "sunset",
-                        "retro",
-                        "shimbg",
-                        "swamp",
-                        "med",
-                        "orchid",
-                        "magenta",
-                        "orange",
-                        "vintage",
-                        "vivid",
-                        "berries",
-                        "refreshing",
-                        "summer",
-                        "tropical",
-                        "india",
-                        "default",
-                    ],
+                    else:
+                        result = _executeSQL(
+                            query=f"""
+                                SELECT 
+                                    /*+LABEL('vDataframe._aggregate_matrix')*/ 
+                                    {', '.join(all_list)} 
+                                FROM {table}""",
+                            title=title,
+                            method="fetchrow",
+                            sql_push_ext=self._vars["sql_push_ext"],
+                            symbol=self._vars["symbol"],
+                        )
+                except (AssertionError, QueryError):
+                    n = len(columns)
+                    result = []
+                    for i in loop:
+                        for j in range(0, i + step):
+                            result += [
+                                self._aggregate_matrix(method, [columns[i], columns[j]])
+                            ]
+                matrix = np.array([[1.0 for i in range(0, n)] for i in range(0, n)])
+                k = 0
+                for i in range(i0, n):
+                    for j in range(0, i + step):
+                        current = result[k]
+                        k += 1
+                        if isinstance(current, NoneType):
+                            current = np.nan
+                        matrix[i][j] = current
+                        matrix[j][i] = current
+            if show:
+                vmin = 0 if (method == "cramer") else -1
+                if method == "cov":
+                    vmin = None
+                vmax = (
+                    1
+                    if (
+                        method
+                        in (
+                            "pearson",
+                            "spearman",
+                            "spearmand",
+                            "kendall",
+                            "biserial",
+                            "cramer",
+                        )
+                    )
+                    else None
                 )
-            ]
-        )
-        if isinstance(value, str):
-            verticapy.options["color_style"] = value
-    elif option == "max_columns":
-        check_types([("value", value, [int, float])])
-        if value > 0:
-            verticapy.options["max_columns"] = int(value)
-    elif option == "max_rows":
-        check_types([("value", value, [int, float])])
-        if value >= 0:
-            verticapy.options["max_rows"] = int(value)
-    elif option == "mode":
-        check_types([("value", value, ["light", "full"])])
-        if value.lower() in ["light", "full", None]:
-            verticapy.options["mode"] = value.lower()
-    elif option == "percent_bar":
-        check_types([("value", value, [bool])])
-        if value in (True, False, None):
-            verticapy.options["percent_bar"] = value
-    elif option == "print_info":
-        check_types([("value", value, [bool])])
-        if isinstance(value, bool):
-            verticapy.options["print_info"] = value
-    elif option == "overwrite_model":
-        check_types([("value", value, [bool])])
-        if value in (True, False, None):
-            verticapy.options["overwrite_model"] = value
-    elif option == "random_state":
-        check_types([("value", value, [int])])
-        if value < 0:
-            raise ParameterError("Random State Value must be positive.")
-        if isinstance(value, int):
-            verticapy.options["random_state"] = int(value)
-        elif value == None:
-            verticapy.options["random_state"] = None
-    elif option == "sql_on":
-        check_types([("value", value, [bool])])
-        if value in (True, False, None):
-            verticapy.options["sql_on"] = value
-    elif option == "temp_schema":
-        check_types([("value", value, [str])])
-        if isinstance(value, str):
-            query = """SELECT 
-                          schema_name 
-                       FROM v_catalog.schemata 
-                       WHERE schema_name = '{}' LIMIT 1;""".format(
-                value.replace("'", "''")
-            )
-            res = executeSQL(
-                query, title="Checking if the schema exists.", method="fetchrow"
-            )
-            if res:
-                verticapy.options["temp_schema"] = str(value)
-            else:
-                raise ParameterError(f"The schema '{value}' could not be found.")
-    elif option == "time_on":
-        check_types([("value", value, [bool])])
-        if value in (True, False, None):
-            verticapy.options["time_on"] = value
-    elif option == "tqdm":
-        check_types([("value", value, [bool])])
-        if value in (True, False, None):
-            verticapy.options["tqdm"] = value
-    else:
-        raise ParameterError(f"Option '{option}' does not exist.")
-
-
-# ---#
-class tablesample:
-    """
----------------------------------------------------------------------------
-The tablesample is the transition from 'Big Data' to 'Small Data'. 
-This object allows you to conveniently display your results without any  
-dependencies on any other module. It stores the aggregated result in memory
-which can then be transformed into a pandas.DataFrame or vDataFrame.
-
-Parameters
-----------
-values: dict, optional
-	Dictionary of columns (keys) and their values. The dictionary must be
-	similar to the following one:
-	{"column1": [val1, ..., valm], ... "columnk": [val1, ..., valm]}
-dtype: dict, optional
-	Columns data types.
-count: int, optional
-	Number of elements if we had to load the entire dataset. It is used 
-	only for rendering purposes.
-offset: int, optional
-	Number of elements that were skipped if we had to load the entire
-	dataset. It is used only for rendering purposes.
-percent: dict, optional
-    Dictionary of missing values (Used to display the percent bars)
-max_columns: int, optional
-    Maximum number of columns to display.
-
-Attributes
-----------
-The tablesample attributes are the same than the parameters.
-	"""
-
-    #
-    # Special Methods
-    #
-    # ---#
-    def __init__(
-        self,
-        values: dict = {},
-        dtype: dict = {},
-        count: int = 0,
-        offset: int = 0,
-        percent: dict = {},
-        max_columns: int = -1,
-    ):
-        check_types(
-            [
-                ("values", values, [dict]),
-                ("dtype", dtype, [dict]),
-                ("count", count, [int]),
-                ("offset", offset, [int]),
-                ("percent", percent, [dict]),
-                ("max_columns", max_columns, [int]),
-            ]
-        )
-        self.values = values
-        self.dtype = dtype
-        self.count = count
-        self.offset = offset
-        self.percent = percent
-        self.max_columns = max_columns
-        for column in values:
-            if column not in dtype:
-                self.dtype[column] = "undefined"
-
-    # ---#
-    def __iter__(self):
-        columns = self.values
-        return (elem for elem in columns)
-
-    # ---#
-    def __getitem__(self, key):
-        all_cols = [elem for elem in self.values]
-        for elem in all_cols:
-            if quote_ident(str(elem).lower()) == quote_ident(str(key).lower()):
-                key = elem
-                break
-        return self.values[key]
-
-    # ---#
-    def _repr_html_(self):
-        if len(self.values) == 0:
-            return ""
-        n = len(self.values)
-        dtype = self.dtype
-        max_columns = self.max_columns if self.max_columns > 0 else verticapy.options["max_columns"]
-        if n < max_columns:
-            data_columns = [[column] + self.values[column] for column in self.values]
-        else:
-            k = int(max_columns / 2)
-            columns = [elem for elem in self.values]
-            values0 = [[columns[i]] + self.values[columns[i]] for i in range(k)]
-            values1 = [["..." for i in range(len(self.values[columns[0]]) + 1)]]
-            values2 = [
-                [columns[i]] + self.values[columns[i]]
-                for i in range(n - max_columns + k, n)
-            ]
-            data_columns = values0 + values1 + values2
-            dtype["..."] = "undefined"
-        percent = self.percent
-        for elem in self.values:
-            if elem not in percent and (elem != "index"):
-                percent = {}
-                break
-        formatted_text = print_table(
-            data_columns,
-            is_finished=(self.count <= len(data_columns[0]) + self.offset),
-            offset=self.offset,
-            repeat_first_column=("index" in self.values),
-            return_html=True,
-            dtype=dtype,
-            percent=percent,
-        )
-        start, end = self.offset + 1, len(data_columns[0]) - 1 + self.offset
-        formatted_text += '<div style="margin-top:6px; font-size:1.02em">'
-        if (self.offset == 0) and (len(data_columns[0]) - 1 == self.count):
-            rows = self.count
+                vpy_plt, kwargs = self.get_plotting_lib(
+                    class_name="HeatMap",
+                    chart=chart,
+                    style_kwargs=style_kwargs,
+                )
+                data = {"X": matrix}
+                layout = {
+                    "columns": [None, None],
+                    "method": method,
+                    "x_labels": columns,
+                    "y_labels": columns,
+                    "vmax": vmax,
+                    "vmin": vmin,
+                    "mround": mround,
+                    "with_numbers": True,
+                }
+                return vpy_plt.HeatMap(data=data, layout=layout).draw(**kwargs)
+            values = {"index": columns}
+            for idx in range(len(matrix)):
+                values[columns[idx]] = list(matrix[:, idx])
+            for column1 in values:
+                if column1 != "index":
+                    val = {}
+                    for idx, column2 in enumerate(values["index"]):
+                        val[column2] = values[column1][idx]
+                    self._update_catalog(values=val, matrix=method, column=column1)
+            return TableSample(values=values).decimal_to_float()
         else:
-            if start > end:
-                rows = "0{}".format(
-                    " of {}".format(self.count) if (self.count > 0) else ""
+            if method == "cramer":
+                cols = self.catcol()
+                assert len(cols) != 0, EmptyParameter(
+                    "No categorical column found in the vDataFrame."
                 )
             else:
-                rows = "{}-{}{}".format(
-                    start, end, " of {}".format(self.count) if (self.count > 0) else "",
-                )
-        if len(self.values) == 1:
-            column = list(self.values.keys())[0]
-            if self.offset > self.count:
-                formatted_text += "<b>Column:</b> {} | <b>Type:</b> {}".format(
-                    column, self.dtype[column]
+                cols = self.numcol()
+                assert len(cols) != 0, EmptyParameter(
+                    "No numerical column found in the vDataFrame."
+                )
+            return self._aggregate_matrix(
+                method=method,
+                columns=cols,
+                mround=mround,
+                show=show,
+                **style_kwargs,
+            )
+
+    def _aggregate_vector(
+        self,
+        focus: str,
+        method: str = "pearson",
+        columns: Optional[SQLColumns] = None,
+        mround: int = 3,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
+        """
+        Global method used to compute the Correlation/Cov/Beta Vector.
+        """
+        if not columns:
+            if method == "cramer":
+                cols = self.catcol()
+                assert cols, EmptyParameter(
+                    "No categorical column found in the vDataFrame."
                 )
             else:
-                formatted_text += "<b>Rows:</b> {} | <b>Column:</b> {} | <b>Type:</b> {}".format(
-                    rows, column, self.dtype[column]
+                cols = self.numcol()
+                assert cols, EmptyParameter(
+                    "No numerical column found in the vDataFrame."
                 )
         else:
-            if self.offset > self.count:
-                formatted_text += "<b>Columns:</b> {}".format(n)
-            else:
-                formatted_text += "<b>Rows:</b> {} | <b>Columns:</b> {}".format(rows, n)
-        formatted_text += "</div>"
-        return formatted_text
-
-    # ---#
-    def __repr__(self):
-        if len(self.values) == 0:
-            return ""
-        n = len(self.values)
-        dtype = self.dtype
-        max_columns = self.max_columns if self.max_columns > 0 else verticapy.options["max_columns"]
-        if n < max_columns:
-            data_columns = [[column] + self.values[column] for column in self.values]
-        else:
-            k = int(max_columns / 2)
-            columns = [elem for elem in self.values]
-            values0 = [[columns[i]] + self.values[columns[i]] for i in range(k)]
-            values1 = [["..." for i in range(len(self.values[columns[0]]) + 1)]]
-            values2 = [
-                [columns[i]] + self.values[columns[i]]
-                for i in range(n - max_columns + k, n)
-            ]
-            data_columns = values0 + values1 + values2
-            dtype["..."] = "undefined"
-        formatted_text = print_table(
-            data_columns,
-            is_finished=(self.count <= len(data_columns[0]) + self.offset),
-            offset=self.offset,
-            repeat_first_column=("index" in self.values),
-            return_html=False,
-            dtype=dtype,
-            percent=self.percent,
-        )
-        start, end = self.offset + 1, len(data_columns[0]) - 1 + self.offset
-        if (self.offset == 0) and (len(data_columns[0]) - 1 == self.count):
-            rows = self.count
-        else:
-            if start > end:
-                rows = "0{}".format(
-                    " of {}".format(self.count) if (self.count > 0) else ""
-                )
-            else:
-                rows = "{}-{}{}".format(
-                    start, end, " of {}".format(self.count) if (self.count > 0) else "",
-                )
-        if len(self.values) == 1:
-            column = list(self.values.keys())[0]
-            if self.offset > self.count:
-                formatted_text += "Column: {} | Type: {}".format(
-                    column, self.dtype[column]
+            cols = self.format_colnames(columns)
+        if method != "cramer":
+            method_name = "Correlation"
+            method_type = f" using the method = '{method}'"
+            if method == "cov":
+                method_name = "Covariance"
+                method_type = ""
+            for column in cols:
+                assert self[column].isnum(), TypeError(
+                    f"vDataColumn '{column}' must be numerical to "
+                    f"compute the {method_name} Vector{method_type}."
                 )
-            else:
-                formatted_text += "Rows: {} | Column: {} | Type: {}".format(
-                    rows, column, self.dtype[column]
+        if method in ("spearman", "spearmand", "pearson", "kendall", "cov") and (
+            len(cols) >= 1
+        ):
+            try:
+                fail = 0
+                cast_i = "::int" if (self[focus].isbool()) else ""
+                all_list, all_cols = [], [focus]
+                nb_precomputed = 0
+                for column in cols:
+                    if (
+                        column.replace('"', "").lower()
+                        != focus.replace('"', "").lower()
+                    ):
+                        all_cols += [column]
+                    cast_j = "::int" if (self[column].isbool()) else ""
+                    pre_comp_val = self._get_catalog_value(
+                        method=method, columns=[focus, column]
+                    )
+                    if (
+                        isinstance(pre_comp_val, NoneType)
+                        or pre_comp_val != pre_comp_val
+                    ):
+                        pre_comp_val = "NULL"
+                    if pre_comp_val != "VERTICAPY_NOT_PRECOMPUTED":
+                        all_list += [str(pre_comp_val)]
+                        nb_precomputed += 1
+                    elif method in ("pearson", "spearman", "spearmand"):
+                        all_list += [f"CORR({focus}{cast_i}, {column}{cast_j})"]
+                    elif method == "kendall":
+                        n = "SQRT(COUNT(*))"
+                        n_c = f"""
+                            (SUM(((x.{focus}{cast_i} 
+                                 < y.{focus}{cast_i} 
+                               AND x.{column}{cast_j}
+                                 < y.{column}{cast_j})
+                               OR (x.{focus}{cast_i} 
+                                 > y.{focus}{cast_i} 
+                               AND x.{column}{cast_j} 
+                                 > y.{column}{cast_j}))::int))/2"""
+                        n_d = f"""
+                            (SUM(((x.{focus}{cast_i} 
+                                 > y.{focus}{cast_i} 
+                               AND x.{column}{cast_j} 
+                                 < y.{column}{cast_j})
+                               OR (x.{focus}{cast_i}
+                                 < y.{focus}{cast_i}
+                               AND x.{column}{cast_j} 
+                                 > y.{column}{cast_j}))::int))/2"""
+                        n_1 = (
+                            f"(SUM((x.{focus}{cast_i} = y.{focus}{cast_i})::int)-{n})/2"
+                        )
+                        n_2 = f"(SUM((x.{column}{cast_j} = y.{column}{cast_j})::int)-{n})/2"
+                        n_0 = f"{n} * ({n} - 1)/2"
+                        tau_b = (
+                            f"({n_c} - {n_d}) / sqrt(({n_0} - {n_1}) * ({n_0} - {n_2}))"
+                        )
+                        all_list += [tau_b]
+                    elif method == "cov":
+                        all_list += [f"COVAR_POP({focus}{cast_i}, {column}{cast_j})"]
+                if method in ("spearman", "spearmand"):
+                    fun = "DENSE_RANK" if method == "spearmand" else "RANK"
+                    rank = [
+                        f"{fun}() OVER (ORDER BY {column}) AS {column}"
+                        for column in all_cols
+                    ]
+                    table = (
+                        f"(SELECT {', '.join(rank)} FROM {self}) rank_spearman_table"
+                    )
+                elif method == "kendall":
+                    table = f"""
+                        (SELECT {", ".join(all_cols)} FROM {self}) x 
+             CROSS JOIN (SELECT {", ".join(all_cols)} FROM {self}) y"""
+                else:
+                    table = self._genSQL()
+                if nb_precomputed == len(cols):
+                    result = _executeSQL(
+                        query=f"""
+                            SELECT 
+                                /*+LABEL('vDataframe._aggregate_vector')*/ 
+                                {', '.join(all_list)}""",
+                        method="fetchrow",
+                        print_time_sql=False,
+                    )
+                else:
+                    result = _executeSQL(
+                        query=f"""
+                            SELECT 
+                                /*+LABEL('vDataframe._aggregate_vector')*/ 
+                                {', '.join(all_list)} 
+                            FROM {table} 
+                            LIMIT 1""",
+                        title=f"Computing the Correlation Vector ({method})",
+                        method="fetchrow",
+                        sql_push_ext=self._vars["sql_push_ext"],
+                        symbol=self._vars["symbol"],
+                    )
+                matrix = copy.deepcopy(result)
+            except QueryError:
+                fail = 1
+        if not (
+            method in ("spearman", "spearmand", "pearson", "kendall", "cov")
+            and (len(cols) >= 1)
+        ) or (fail):
+            matrix = []
+            for column in cols:
+                if column.replace('"', "").lower() == focus.replace(
+                    '"', ""
+                ).lower() and method in ("spearman", "spearmand", "pearson", "kendall"):
+                    matrix += [1.0]
+                else:
+                    matrix += [
+                        self._aggregate_matrix(method=method, columns=[column, focus])
+                    ]
+        matrix = [np.nan if isinstance(x, NoneType) else x for x in matrix]
+        data = [(cols[i], float(matrix[i])) for i in range(len(matrix))]
+        data.sort(key=lambda tup: abs(tup[1]), reverse=True)
+        cols = [x[0] for x in data]
+        matrix = np.array([[x[1] for x in data]])
+        if show:
+            vmin = 0 if (method == "cramer") else -1
+            if method == "cov":
+                vmin = None
+            vmax = (
+                1
+                if (
+                    method
+                    in (
+                        "pearson",
+                        "spearman",
+                        "spearmand",
+                        "kendall",
+                        "biserial",
+                        "cramer",
+                    )
                 )
-        else:
-            if self.offset > self.count:
-                formatted_text += "Columns: {}".format(n)
-            else:
-                formatted_text += "Rows: {} | Columns: {}".format(rows, n)
-        return formatted_text
+                else None
+            )
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="HeatMap",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            data = {"X": matrix}
+            layout = {
+                "columns": [None, None],
+                "method": method,
+                "x_labels": [focus],
+                "y_labels": cols,
+                "vmax": vmax,
+                "vmin": vmin,
+                "mround": mround,
+                "with_numbers": True,
+            }
+            return vpy_plt.HeatMap(data=data, layout=layout).draw(**kwargs)
+        for idx, column in enumerate(cols):
+            self._update_catalog(
+                values={focus: matrix[0][idx]}, matrix=method, column=column
+            )
+            self._update_catalog(
+                values={column: matrix[0][idx]}, matrix=method, column=focus
+            )
+        return TableSample(
+            values={"index": cols, focus: list(matrix[0])}
+        ).decimal_to_float()
 
-    #
-    # Methods
-    #
-    # ---#
-    def append(self, tbs):
+    # Correlation.
+
+    @save_verticapy_logs
+    def corr(
+        self,
+        columns: Optional[SQLColumns] = None,
+        method: Literal[
+            "pearson", "kendall", "spearman", "spearmand", "biserial", "cramer"
+        ] = "pearson",
+        mround: int = 3,
+        focus: Optional[str] = None,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
         """
-        ---------------------------------------------------------------------------
-        Appends the input tablesample to a target tablesample.
+        Computes the Correlation Matrix of the vDataFrame.
 
         Parameters
         ----------
-        tbs: tablesample
-            Tablesample to append.
+        columns: SQLColumns, optional
+            List of the vDataColumns names. If empty, all
+            numerical vDataColumns are used.
+        method: str, optional
+            Method to use to compute the correlation.
+                pearson   : Pearson's  correlation coefficient
+                            (linear).
+                spearman  : Spearman's correlation coefficient
+                            (monotonic - rank based).
+                spearmanD : Spearman's correlation coefficient
+                            using  the   DENSE  RANK  function
+                            instead of the RANK function.
+                kendall   : Kendall's  correlation coefficient
+                            (similar trends).  The method
+                            computes the Tau-B coefficient.
+                            \u26A0 Warning : This method  uses a CROSS
+                                             JOIN  during  computation
+                                             and      is     therefore
+                                             computationally expensive
+                                             at  O(n * n),  where n is
+                                             the  total  count of  the
+                                             vDataFrame.
+                cramer    : Cramer's V
+                            (correlation between categories).
+                biserial  : Biserial Point
+                            (correlation between binaries and a
+                            numericals).
+        mround: int, optional
+            Rounds  the coefficient using  the input number of
+            digits. This is only used to display the correlation
+            matrix.
+        focus: str, optional
+            Focus  the  computation  on  one  vDataColumn.
+        show: bool, optional
+            If  set  to  True,  the  Plotting  object  is
+            returned.
+        chart: PlottingObject, optional
+            The chart object used to plot.
+        **style_kwargs
+            Any  optional  parameter  to pass to the  plotting
+            functions.
 
         Returns
         -------
-        tablesample
-            self
+        obj
+            Plotting Object.
         """
-        check_types([("tbs", tbs, [tablesample])])
-        n1, n2 = self.shape()[0], tbs.shape()[0]
-        assert n1 == n2, ParameterError(
-            "The input and target tablesamples must have the same number of columns."
-            f" Expected {n1}, Found {n2}."
-        )
-        cols1, cols2 = [col for col in self.values], [col for col in tbs.values]
-        for idx in range(n1):
-            self.values[cols1[idx]] += tbs.values[cols2[idx]]
-        return self
+        method = str(method).lower()
+        columns = format_type(columns, dtype=list, na_out=self.numcol())
+        columns, focus = self.format_colnames(columns, focus)
+        fun = self._aggregate_matrix
+        args = []
+        kwargs = {
+            "method": method,
+            "columns": columns,
+            "mround": mround,
+            "show": show,
+            "chart": chart,
+            **style_kwargs,
+        }
+        if focus:
+            args += [focus]
+            fun = self._aggregate_vector
+        return fun(*args, **kwargs)
 
-    # ---#
-    def decimal_to_float(self):
+    @save_verticapy_logs
+    def corr_pvalue(
+        self,
+        column1: str,
+        column2: str,
+        method: Literal[
+            "pearson",
+            "kendall",
+            "kendalla",
+            "kendallb",
+            "kendallc",
+            "spearman",
+            "spearmand",
+            "biserial",
+            "cramer",
+        ] = "pearson",
+    ) -> tuple[float, float]:
         """
-    ---------------------------------------------------------------------------
-    Converts all the tablesample's decimals to floats.
+        Computes  the Correlation Coefficient of the two input
+        vDataColumns and its pvalue.
+
+        Parameters
+        ----------
+        column1: str
+            Input vDataColumn.
+        column2: str
+            Input vDataColumn.
+        method: str, optional
+            Method to use to compute the correlation.
+                pearson   : Pearson's  correlation coefficient
+                            (linear).
+                spearman  : Spearman's correlation coefficient
+                            (monotonic - rank based).
+                spearmanD : Spearman's correlation coefficient
+                            using  the   DENSE  RANK  function
+                            instead of the RANK function.
+                kendall   : Kendall's  correlation coefficient
+                            (similar trends).  The method
+                            computes the Tau-B coefficient.
+                            \u26A0 Warning : This method  uses a CROSS
+                                             JOIN  during  computation
+                                             and      is     therefore
+                                             computationally expensive
+                                             at  O(n * n),  where n is
+                                             the  total  count of  the
+                                             vDataFrame.
+                cramer    : Cramer's V
+                            (correlation between categories).
+                biserial  : Biserial Point
+                            (correlation between binaries and a
+                            numericals).
 
-    Returns
-    -------
-    tablesample
-        self
+        Returns
+        -------
+        tuple
+            (Correlation Coefficient, pvalue)
         """
-        for elem in self.values:
-            if elem != "index":
-                for i in range(len(self.values[elem])):
-                    if isinstance(self.values[elem][i], decimal.Decimal):
-                        self.values[elem][i] = float(self.values[elem][i])
-        return self
+        method = str(method).lower()
+        column1, column2 = self.format_colnames(column1, column2)
+        if method.startswith("kendall"):
+            if method == "kendall":
+                kendall_type = "b"
+            else:
+                kendall_type = method[-1]
+            method = "kendall"
+        else:
+            kendall_type = None
+        if (method == "kendall" and kendall_type == "b") or (method != "kendall"):
+            val = self.corr(columns=[column1, column2], method=method)
+        sql = f"""
+            SELECT 
+                /*+LABEL('vDataframe.corr_pvalue')*/ COUNT(*) 
+            FROM {self} 
+            WHERE {column1} IS NOT NULL AND {column2} IS NOT NULL;"""
+        n = _executeSQL(
+            sql,
+            title="Computing the number of elements.",
+            method="fetchfirstelem",
+            sql_push_ext=self._vars["sql_push_ext"],
+            symbol=self._vars["symbol"],
+        )
+        if method in ("pearson", "biserial"):
+            x = val * math.sqrt((n - 2) / (1 - val * val))
+            pvalue = 2 * scipy_st.t.sf(abs(x), n - 2)
+        elif method in ("spearman", "spearmand"):
+            z = math.sqrt((n - 3) / 1.06) * 0.5 * np.log((1 + val) / (1 - val))
+            pvalue = 2 * scipy_st.norm.sf(abs(z))
+        elif method == "kendall":
+            cast_i = "::int" if (self[column1].isbool()) else ""
+            cast_j = "::int" if (self[column2].isbool()) else ""
+            n_c = f"""
+                (SUM(((x.{column1}{cast_i} 
+                     < y.{column1}{cast_i} 
+                   AND x.{column2}{cast_j} 
+                     < y.{column2}{cast_j})
+                   OR (x.{column1}{cast_i} 
+                     > y.{column1}{cast_i}
+                   AND x.{column2}{cast_j} 
+                     > y.{column2}{cast_j}))::int))/2"""
+            n_d = f"""
+                (SUM(((x.{column1}{cast_i} 
+                     > y.{column1}{cast_i}
+                   AND x.{column2}{cast_j} 
+                     < y.{column2}{cast_j})
+                   OR (x.{column1}{cast_i} 
+                     < y.{column1}{cast_i} 
+                   AND x.{column2}{cast_j} 
+                     > y.{column2}{cast_j}))::int))/2"""
+            table = f"""
+                (SELECT 
+                    {", ".join([column1, column2])} 
+                 FROM {self}) x 
+                CROSS JOIN 
+                (SELECT 
+                    {", ".join([column1, column2])} 
+                 FROM {self}) y"""
+            nc, nd = _executeSQL(
+                query=f"""
+                    SELECT 
+                        /*+LABEL('vDataframe.corr_pvalue')*/ 
+                        {n_c}::float, 
+                        {n_d}::float 
+                    FROM {table};""",
+                title="Computing nc and nd.",
+                method="fetchrow",
+                sql_push_ext=self._vars["sql_push_ext"],
+                symbol=self._vars["symbol"],
+            )
+            if kendall_type == "a":
+                val = (nc - nd) / (n * (n - 1) / 2)
+                Z = 3 * (nc - nd) / math.sqrt(n * (n - 1) * (2 * n + 5) / 2)
+            elif kendall_type in ("b", "c"):
+                vt, v1_0, v2_0 = _executeSQL(
+                    query=f"""
+                        SELECT 
+                            /*+LABEL('vDataframe.corr_pvalue')*/
+                            SUM(ni * (ni - 1) * (2 * ni + 5)), 
+                            SUM(ni * (ni - 1)), 
+                            SUM(ni * (ni - 1) * (ni - 2)) 
+                        FROM 
+                            (SELECT 
+                                {column1}, 
+                                COUNT(*) AS ni 
+                             FROM {self} 
+                             GROUP BY 1) VERTICAPY_SUBTABLE""",
+                    title="Computing vti.",
+                    method="fetchrow",
+                    sql_push_ext=self._vars["sql_push_ext"],
+                    symbol=self._vars["symbol"],
+                )
+                vu, v1_1, v2_1 = _executeSQL(
+                    query=f"""
+                        SELECT 
+                            /*+LABEL('vDataframe.corr_pvalue')*/
+                            SUM(ni * (ni - 1) * (2 * ni + 5)), 
+                            SUM(ni * (ni - 1)), 
+                            SUM(ni * (ni - 1) * (ni - 2)) 
+                       FROM 
+                            (SELECT 
+                                {column2}, 
+                                COUNT(*) AS ni 
+                             FROM {self} 
+                             GROUP BY 1) VERTICAPY_SUBTABLE""",
+                    title="Computing vui.",
+                    method="fetchrow",
+                    sql_push_ext=self._vars["sql_push_ext"],
+                    symbol=self._vars["symbol"],
+                )
+                v0 = n * (n - 1) * (2 * n + 5)
+                v1 = v1_0 * v1_1 / (2 * n * (n - 1))
+                v2 = v2_0 * v2_1 / (9 * n * (n - 1) * (n - 2))
+                Z = (nc - nd) / math.sqrt((v0 - vt - vu) / 18 + v1 + v2)
+                if kendall_type == "c":
+                    k, r = _executeSQL(
+                        query=f"""
+                            SELECT /*+LABEL('vDataframe.corr_pvalue')*/
+                                APPROXIMATE_COUNT_DISTINCT({column1}) AS k, 
+                                APPROXIMATE_COUNT_DISTINCT({column2}) AS r 
+                            FROM {self} 
+                            WHERE {column1} IS NOT NULL 
+                              AND {column2} IS NOT NULL""",
+                        title="Computing the columns categories in the pivot table.",
+                        method="fetchrow",
+                        sql_push_ext=self._vars["sql_push_ext"],
+                        symbol=self._vars["symbol"],
+                    )
+                    m = min(k, r)
+                    val = 2 * (nc - nd) / (n * n * (m - 1) / m)
+            pvalue = 2 * scipy_st.norm.sf(abs(Z))
+        elif method == "cramer":
+            k, r = _executeSQL(
+                query=f"""
+                    SELECT /*+LABEL('vDataframe.corr_pvalue')*/
+                        APPROXIMATE_COUNT_DISTINCT({column1}) AS k, 
+                        APPROXIMATE_COUNT_DISTINCT({column2}) AS r 
+                    FROM {self} 
+                    WHERE {column1} IS NOT NULL 
+                      AND {column2} IS NOT NULL""",
+                title="Computing the columns categories in the pivot table.",
+                method="fetchrow",
+                sql_push_ext=self._vars["sql_push_ext"],
+                symbol=self._vars["symbol"],
+            )
+            x = val * val * n * min(k, r)
+            pvalue = scipy_st.chi2.sf(x, (k - 1) * (r - 1))
+        return (val, pvalue)
 
-    # ---#
-    def merge(self, tbs):
+    # Covariance.
+
+    @save_verticapy_logs
+    def cov(
+        self,
+        columns: Optional[SQLColumns] = None,
+        focus: Optional[str] = None,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
         """
-        ---------------------------------------------------------------------------
-        Merges the input tablesample to a target tablesample.
+        Computes the covariance matrix of the vDataFrame.
 
         Parameters
         ----------
-        tbs: tablesample
-            Tablesample to merge.
+        columns: SQLColumns, optional
+            List of the vDataColumns names. If empty, all numerical
+            vDataColumns are used.
+        focus: str, optional
+            Focus the computation on one vDataColumn.
+        show: bool, optional
+            If   set  to   True,  the   Plotting   object  is
+            returned.
+        chart: PlottingObject, optional
+            The chart object used to plot.
+        **style_kwargs
+            Any   optional  parameter  to  pass  to  the   plotting
+            functions.
 
         Returns
         -------
-        tablesample
-            self
+        obj
+            Plotting Object.
         """
-        check_types([("tbs", tbs, [tablesample])])
-        n1, n2 = self.shape()[1], tbs.shape()[1]
-        assert n1 == n2, ParameterError(
-            "The input and target tablesamples must have the same number of rows."
-            f" Expected {n1}, Found {n2}."
-        )
-        for col in tbs.values:
-            if col != "index":
-                if col not in self.values:
-                    self.values[col] = []
-                self.values[col] += tbs.values[col]
-        return self
+        columns = format_type(columns, dtype=list)
+        columns, focus = self.format_colnames(columns, focus)
+        fun = self._aggregate_matrix
+        args = []
+        kwargs = {
+            "method": "cov",
+            "columns": columns,
+            "show": show,
+            "chart": chart,
+            **style_kwargs,
+        }
+        if focus:
+            args += [focus]
+            fun = self._aggregate_vector
 
-    # ---#
-    def shape(self):
-        """
-    ---------------------------------------------------------------------------
-    Computes the tablesample shape.
+        return fun(*args, **kwargs)
 
-    Returns
-    -------
-    tuple
-        (number of columns, number of rows)
-        """
-        cols = [col for col in self.values]
-        n, m = len(cols), len(self.values[cols[0]])
-        return (n, m)
+    # Regression Metrics.
 
-    # ---#
-    def sort(self, column: str, desc: bool = False):
+    @save_verticapy_logs
+    def regr(
+        self,
+        columns: Optional[SQLColumns] = None,
+        method: Literal[
+            "avgx",
+            "avgy",
+            "count",
+            "intercept",
+            "r2",
+            "slope",
+            "sxx",
+            "sxy",
+            "syy",
+            "beta",
+            "alpha",
+        ] = "r2",
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
         """
-        ---------------------------------------------------------------------------
-        Sorts the tablesample using the input column.
+        Computes the regression matrix of the vDataFrame.
 
         Parameters
         ----------
-        column: str, optional
-            Column used to sort the data.
-        desc: bool, optional
-            If set to True, the result is sorted in descending order.
+        columns: SQLColumns, optional
+            List  of the  vDataColumns names. If empty, all numerical
+            vDataColumns are used.
+        method: str, optional
+            Method to use to compute the regression matrix.
+                avgx  : Average  of  the  independent  expression  in
+                        an expression pair.
+                avgy  : Average  of  the dependent  expression in  an
+                        expression pair.
+                count : Count  of  all  rows  in  an expression  pair.
+                alpha : Intercept  of the regression line  determined
+                        by a set of expression pairs.
+                r2    : Square  of  the correlation  coefficient of a
+                        set of expression pairs.
+                beta  : Slope of  the regression  line, determined by
+                        a set of expression pairs.
+                sxx   : Sum of squares of  the independent expression
+                        in an expression pair.
+                sxy   : Sum of products of the independent expression
+                        multiplied by the  dependent expression in an
+                        expression pair.
+                syy   : Returns  the sum of squares of the  dependent
+                        expression in an expression pair.
+        show: bool, optional
+            If set to True, the Plotting object is returned.
+        chart: PlottingObject, optional
+            The chart object used to plot.
+        **style_kwargs
+            Any optional parameter to pass to the plotting functions.
 
         Returns
         -------
-        tablesample
-            self
+        obj
+            Plotting Object.
         """
-        check_types([("column", column, [str]), ("desc", desc, [bool])])
-        column = column.replace('"', "").lower()
-        columns = [col for col in self.values]
-        idx = None
-        for i, col in enumerate(columns):
-            col_tmp = col.replace('"', "").lower()
-            if column == col_tmp:
-                idx = i
-                column = col
-        if idx is None:
-            raise MissingColumn(
-                "The Column '{}' doesn't exist.".format(column.lower().replace('"', ""))
-            )
-        n, sort = len(self[column]), []
-        for i in range(n):
-            tmp_list = []
-            for col in columns:
-                tmp_list += [self[col][i]]
-            sort += [tmp_list]
-        sort.sort(key=lambda tup: tup[idx], reverse=desc)
-        for i, col in enumerate(columns):
-            self.values[col] = [sort[j][i] for j in range(n)]
-        return self
+        columns = format_type(columns, dtype=list)
+        if method == "beta":
+            method = "slope"
+        elif method == "alpha":
+            method = "intercept"
+        method = f"regr_{method}"
+        if not columns:
+            columns = self.numcol()
+            assert columns, EmptyParameter(
+                "No numerical column found in the vDataFrame."
+            )
+        columns = self.format_colnames(columns)
+        for column in columns:
+            assert self[column].isnum(), TypeError(
+                f"vDataColumn {column} must be numerical to compute the Regression Matrix."
+            )
+        n = len(columns)
+        all_list, nb_precomputed = [], 0
+        for i in range(0, n):
+            for j in range(0, n):
+                cast_i = "::int" if (self[columns[i]].isbool()) else ""
+                cast_j = "::int" if (self[columns[j]].isbool()) else ""
+                pre_comp_val = self._get_catalog_value(
+                    method=method, columns=[columns[i], columns[j]]
+                )
+                if isinstance(pre_comp_val, NoneType) or pre_comp_val != pre_comp_val:
+                    pre_comp_val = "NULL"
+                if pre_comp_val != "VERTICAPY_NOT_PRECOMPUTED":
+                    all_list += [str(pre_comp_val)]
+                    nb_precomputed += 1
+                else:
+                    all_list += [
+                        f"{method.upper()}({columns[i]}{cast_i}, {columns[j]}{cast_j})"
+                    ]
+        try:
+            if nb_precomputed == n * n:
+                result = _executeSQL(
+                    query=f"""
+                        SELECT 
+                            /*+LABEL('vDataframe.regr')*/ 
+                            {", ".join(all_list)}""",
+                    print_time_sql=False,
+                    method="fetchrow",
+                )
+            else:
+                result = _executeSQL(
+                    query=f"""
+                        SELECT 
+                            /*+LABEL('vDataframe.regr')*/
+                            {", ".join(all_list)} 
+                        FROM {self}""",
+                    title=f"Computing the {method.upper()} Matrix.",
+                    method="fetchrow",
+                    sql_push_ext=self._vars["sql_push_ext"],
+                    symbol=self._vars["symbol"],
+                )
+            if n == 1:
+                return result[0]
+        except QueryError:
+            n = len(columns)
+            result = []
+            for i in range(0, n):
+                for j in range(0, n):
+                    result += [
+                        _executeSQL(
+                            query=f"""
+                                SELECT 
+                                    /*+LABEL('vDataframe.regr')*/ 
+                                    {method.upper()}({columns[i]}{cast_i}, 
+                                                     {columns[j]}{cast_j}) 
+                                FROM {self}""",
+                            title=f"Computing the {method.upper()} aggregation, one at a time.",
+                            method="fetchfirstelem",
+                            sql_push_ext=self._vars["sql_push_ext"],
+                            symbol=self._vars["symbol"],
+                        )
+                    ]
+        matrix = np.array([[1.0 for i in range(0, n)] for i in range(0, n)])
+        k = 0
+        for i in range(0, n):
+            for j in range(0, n):
+                current = result[k]
+                k += 1
+                if isinstance(current, NoneType):
+                    current = np.nan
+                matrix[i][j] = current
+        if show:
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="HeatMap",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            data = {"X": matrix}
+            layout = {
+                "columns": [None, None],
+                "x_labels": columns,
+                "y_labels": columns,
+                "with_numbers": True,
+            }
+            return vpy_plt.HeatMap(data=data, layout=layout).draw(**kwargs)
+        values = {"index": columns}
+        for idx in range(len(matrix)):
+            values[columns[idx]] = list(matrix[:, idx])
+        for column1 in values:
+            if column1 != "index":
+                val = {}
+                for idx, column2 in enumerate(values["index"]):
+                    val[column2] = values[column1][idx]
+                self._update_catalog(values=val, matrix=method, column=column1)
+        return TableSample(values=values).decimal_to_float()
 
-    # ---#
-    def transpose(self):
+    # Time Series.
+
+    @save_verticapy_logs
+    def acf(
+        self,
+        column: str,
+        ts: str,
+        by: Optional[SQLColumns] = None,
+        p: Union[int, list] = 12,
+        unit: str = "rows",
+        method: Literal[
+            "pearson", "kendall", "spearman", "spearmand", "biserial", "cramer"
+        ] = "pearson",
+        confidence: bool = True,
+        alpha: float = 0.95,
+        show: bool = True,
+        kind: Literal["line", "heatmap", "bar"] = "bar",
+        mround: int = 3,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
         """
-	---------------------------------------------------------------------------
-	Transposes the tablesample.
+        Computes the correlations of the input vDataColumn
+        and its lags.
 
- 	Returns
- 	-------
- 	tablesample
- 		transposed tablesample
-		"""
-        index = [column for column in self.values]
-        first_item = list(self.values.keys())[0]
-        columns = [[] for i in range(len(self.values[first_item]))]
-        for column in self.values:
-            for idx, item in enumerate(self.values[column]):
-                try:
-                    columns[idx] += [item]
-                except:
-                    pass
-        columns = [index] + columns
-        values = {}
-        for item in columns:
-            values[item[0]] = item[1 : len(item)]
-        return tablesample(values, self.dtype, self.count, self.offset, self.percent)
+        Parameters
+        ----------
+        column: str
+            Input vDataColumn used to compute the Auto
+            Correlation Plot.
+        ts: str
+            TS (Time Series)  vDataColumn used to order
+            the  data.  It  can  be  of type  date  or  a
+            numerical vDataColumn.
+        by: SQLColumns, optional
+            vDataColumns used in the partition.
+        p: int | list, optional
+            Int  equal  to  the  maximum  number  of lag  to
+            consider  during the  computation or  List of the
+            different  lags to include during the computation.
+            p must be positive or a list of positive integers.
+        unit: str, optional
+            Unit used to compute the lags.
+                rows : Natural lags
+                else : Any time unit.  For example,  you  can
+                       write 'hour' to compute the hours lags
+                       or 'day' to compute the days lags.
+        method: str, optional
+            Method used to compute the correlation.
+                pearson   : Pearson's  correlation coefficient
+                            (linear).
+                spearman  : Spearman's correlation coefficient
+                            (monotonic - rank based).
+                spearmanD : Spearman's correlation coefficient
+                            using  the   DENSE  RANK  function
+                            instead of the RANK function.
+                kendall   : Kendall's  correlation coefficient
+                            (similar trends).  The method
+                            computes the Tau-B coefficient.
+                            \u26A0 Warning : This method  uses a CROSS
+                                             JOIN  during  computation
+                                             and      is     therefore
+                                             computationally expensive
+                                             at  O(n * n),  where n is
+                                             the  total  count of  the
+                                             vDataFrame.
+                cramer    : Cramer's V
+                            (correlation between categories).
+                biserial  : Biserial Point
+                            (correlation between binaries and a
+                            numericals).
+        confidence: bool, optional
+            If set to True, the confidence band width is drawn.
+        alpha: float, optional
+            Significance Level. Probability to accept H0. Only
+            used   to  compute   the  confidence  band   width.
+        show: bool, optional
+                If  set  to True,  the Plotting object is
+                returned.
+        kind: str, optional
+            ACF Type.
+                bar     : Classical Autocorrelation Plot using
+                          bars.
+                heatmap : Draws the ACF heatmap.
+                line    : Draws the ACF using a Line Plot.
+        mround: int, optional
+            Round  the  coefficient using the input number  of
+            digits. It is used only to display the ACF  Matrix
+            (kind must be set to 'heatmap').
+        chart: PlottingObject, optional
+            The chart object used to plot.
+        **style_kwargs
+            Any optional parameter  to  pass  to  the plotting
+            functions.
 
-    # ---#
-    def to_list(self):
+        Returns
+        -------
+        obj
+            Plotting Object.
         """
-    ---------------------------------------------------------------------------
-    Converts the tablesample to a list.
+        method = str(method).lower()
+        by = format_type(by, dtype=list)
+        by, column, ts = self.format_colnames(by, column, ts)
+        if unit == "rows":
+            table = self._genSQL()
+        else:
+            table = self.interpolate(
+                ts=ts, rule=f"1 {unit}", method={column: "linear"}, by=by
+            )._genSQL()
+        if isinstance(p, (int, float)):
+            p = range(1, p + 1)
+        by = f"PARTITION BY {', '.join(by)} " if by else ""
+        columns = [
+            f"LAG({column}, {i}) OVER ({by}ORDER BY {ts}) AS lag_{i}_{gen_name([column])}"
+            for i in p
+        ]
+        query = f"SELECT {', '.join([column] + columns)} FROM {table}"
+        if len(p) == 1:
+            return create_new_vdf(query).corr([], method=method)
+        elif kind == "heatmap":
+            return create_new_vdf(query).corr(
+                [],
+                method=method,
+                mround=mround,
+                focus=column,
+                show=show,
+                **style_kwargs,
+            )
+        else:
+            result = create_new_vdf(query).corr(
+                [], method=method, focus=column, show=False
+            )
+            columns = copy.deepcopy(result.values["index"])
+            acf = copy.deepcopy(result.values[column])
+            acf_band = []
+            if confidence:
+                for k in range(1, len(acf) + 1):
+                    acf_band += [
+                        math.sqrt(2)
+                        * scipy_special.erfinv(alpha)
+                        / math.sqrt(self[column].count() - k + 1)
+                        * math.sqrt((1 + 2 * sum(acf[i] ** 2 for i in range(1, k))))
+                    ]
+            if columns[0] == column:
+                columns[0] = 0
+            for i in range(1, len(columns)):
+                columns[i] = int(columns[i].split("_")[1])
+            data = [(columns[i], acf[i]) for i in range(len(columns))]
+            data.sort(key=lambda tup: tup[0])
+            del result.values[column]
+            result.values["index"] = [elem[0] for elem in data]
+            result.values["value"] = [elem[1] for elem in data]
+            if acf_band:
+                result.values["confidence"] = acf_band
+            if show:
+                vpy_plt, kwargs = self.get_plotting_lib(
+                    class_name="ACFPlot",
+                    chart=chart,
+                    style_kwargs=style_kwargs,
+                )
+                data = {
+                    "x": np.array(result.values["index"]),
+                    "y": np.array(result.values["value"]),
+                    "z": np.array(acf_band),
+                }
+                layout = {}
+                return vpy_plt.ACFPlot(
+                    data=data, layout=layout, misc_layout={"kind": kind, "pacf": False}
+                ).draw(**kwargs)
+            return result
 
-    Returns
-    -------
-    list
-        Python list.
+    @save_verticapy_logs
+    def pacf(
+        self,
+        column: str,
+        ts: str,
+        by: Optional[SQLColumns] = None,
+        p: Union[int, list] = 5,
+        unit: str = "rows",
+        method: Literal[
+            "pearson", "kendall", "spearman", "spearmand", "biserial", "cramer"
+        ] = "pearson",
+        confidence: bool = True,
+        alpha: float = 0.95,
+        show: bool = True,
+        kind: Literal["line", "bar"] = "bar",
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ):
         """
-        result = []
-        all_cols = [elem for elem in self.values]
-        if all_cols == []:
-            return []
-        for i in range(len(self.values[all_cols[0]])):
-            result_tmp = []
-            for elem in self.values:
-                if elem != "index":
-                    result_tmp += [self.values[elem][i]]
-            result += [result_tmp]
-        return result
+        Computes the partial autocorrelations of the input
+        vDataColumn.
 
-    # ---#
-    def to_numpy(self):
-        """
-    ---------------------------------------------------------------------------
-    Converts the tablesample to a numpy array.
+        Parameters
+        ----------
+        column: str
+            Input vDataColumn  used to compute the Auto
+            Correlation Plot.
+        ts: str
+            TS (Time Series)  vDataColumn used to order
+            the  data.  It  can  be  of type  date  or  a
+            numerical vDataColumn.
+        by: SQLColumns, optional
+            vDataColumns used in the partition.
+        p: int | list, optional
+            Int  equal  to  the  maximum  number  of lag  to
+            consider  during the  computation or  List of the
+            different  lags to include during the computation.
+            p must be positive or a list of positive integers.
+        unit: str, optional
+            Unit to use to compute the lags.
+                rows : Natural lags.
+                else : Any time unit.  For  example, you  can
+                       write 'hour' to compute the hours lags
+                       or 'day' to compute the days lags.
+        method: str, optional
+            Method to use to compute the correlation.
+                pearson   : Pearson's  correlation coefficient
+                            (linear).
+                spearman  : Spearman's correlation coefficient
+                            (monotonic - rank based).
+                spearmanD : Spearman's correlation coefficient
+                            using  the   DENSE  RANK  function
+                            instead of the RANK function.
+                kendall   : Kendall's  correlation coefficient
+                            (similar trends).  The method
+                            computes the Tau-B coefficient.
+                            \u26A0 Warning : This method  uses a CROSS
+                                             JOIN  during  computation
+                                             and      is     therefore
+                                             computationally expensive
+                                             at  O(n * n),  where n is
+                                             the  total  count of  the
+                                             vDataFrame.
+                cramer    : Cramer's V
+                            (correlation between categories).
+                biserial  : Biserial Point
+                            (correlation between binaries and a
+                            numericals).
+        confidence: bool, optional
+            If set to True, the confidence band width is drawn.
+        alpha: float, optional
+            Significance Level. Probability to accept H0. Only
+            used   to  compute   the  confidence  band   width.
+        show: bool, optional
+                If  set  to True,  the Plotting object is
+                returned.
+        kind: str, optional
+            ACF Type.
+                bar  : Classical  Partial Autocorrelation Plot
+                       using bars.
+                line : Draws  the   PACF  using  a  Line  Plot.
+        chart: PlottingObject, optional
+            The chart object used to plot.
+        **style_kwargs
+            Any optional parameter  to  pass  to  the plotting
+            functions.
 
-    Returns
-    -------
-    numpy.array
-        Numpy Array.
+        Returns
+        -------
+        obj
+            Plotting Object.
         """
-        import numpy as np
+        vml = get_vertica_mllib()
+        if isinstance(by, str):
+            by = [by]
+        if isinstance(p, Iterable) and (len(p) == 1):
+            p = p[0]
+            if p == 0:
+                return 1.0
+            elif p == 1:
+                return self.acf(
+                    ts=ts, column=column, by=by, p=[1], unit=unit, method=method
+                )
+            by, column, ts = self.format_colnames(by, column, ts)
+            if unit == "rows":
+                table = self._genSQL()
+            else:
+                table = self.interpolate(
+                    ts=ts, rule=f"1 {unit}", method={column: "linear"}, by=by
+                )._genSQL()
+            by = f"PARTITION BY {', '.join(by)} " if by else ""
+            columns = [
+                f"LAG({column}, {i}) OVER ({by}ORDER BY {ts}) AS lag_{i}_{gen_name([column])}"
+                for i in range(1, p + 1)
+            ]
+            relation = f"(SELECT {', '.join([column] + columns)} FROM {table}) pacf"
+            tmp_view_name = gen_tmp_name(
+                schema=conf.get_option("temp_schema"), name="linear_reg_view"
+            )
+            tmp_lr0_name = gen_tmp_name(
+                schema=conf.get_option("temp_schema"), name="linear_reg0"
+            )
+            tmp_lr1_name = gen_tmp_name(
+                schema=conf.get_option("temp_schema"), name="linear_reg1"
+            )
+            try:
+                drop(tmp_view_name, method="view")
+                query = f"""
+                    CREATE VIEW {tmp_view_name} 
+                        AS SELECT /*+LABEL('vDataframe.pacf')*/ * FROM {relation}"""
+                _executeSQL(query, print_time_sql=False)
+                vdf = create_new_vdf(tmp_view_name)
+                drop(tmp_lr0_name, method="model")
+                model = vml.LinearRegression(name=tmp_lr0_name, solver="Newton")
+                model.fit(
+                    input_relation=tmp_view_name,
+                    X=[f"lag_{i}_{gen_name([column])}" for i in range(1, p)],
+                    y=column,
+                )
+                model.predict(vdf, name="prediction_0")
+                drop(tmp_lr1_name, method="model")
+                model = vml.LinearRegression(name=tmp_lr1_name, solver="Newton")
+                model.fit(
+                    input_relation=tmp_view_name,
+                    X=[f"lag_{i}_{gen_name([column])}" for i in range(1, p)],
+                    y=f"lag_{p}_{gen_name([column])}",
+                )
+                model.predict(vdf, name="prediction_p")
+                vdf.eval(expr=f"{column} - prediction_0", name="eps_0")
+                vdf.eval(
+                    expr=f"lag_{p}_{gen_name([column])} - prediction_p",
+                    name="eps_p",
+                )
+                result = vdf.corr(["eps_0", "eps_p"], method=method)
+            finally:
+                drop(tmp_view_name, method="view")
+                drop(tmp_lr0_name, method="model")
+                drop(tmp_lr1_name, method="model")
+            return result
+        else:
+            if isinstance(p, (float, int)):
+                p = range(0, p + 1)
+            loop = tqdm(p) if conf.get_option("tqdm") else p
+            pacf = []
+            for i in loop:
+                pacf += [self.pacf(ts=ts, column=column, by=by, p=[i], unit=unit)]
+            columns = list(p)
+            pacf_band = []
+            if confidence:
+                for k in range(1, len(pacf) + 1):
+                    pacf_band += [
+                        math.sqrt(2)
+                        * scipy_special.erfinv(alpha)
+                        / math.sqrt(self[column].count() - k + 1)
+                        * math.sqrt((1 + 2 * sum(pacf[i] ** 2 for i in range(1, k))))
+                    ]
+            result = TableSample({"index": columns, "value": pacf})
+            if pacf_band:
+                result.values["confidence"] = pacf_band
+            if show:
+                vpy_plt, kwargs = self.get_plotting_lib(
+                    class_name="ACFPlot",
+                    chart=chart,
+                    style_kwargs=style_kwargs,
+                )
+                data = {
+                    "x": np.array(result.values["index"]),
+                    "y": np.array(result.values["value"]),
+                    "z": np.array(pacf_band),
+                }
+                layout = {}
+                return vpy_plt.ACFPlot(
+                    data=data, layout=layout, misc_layout={"kind": kind, "pacf": True}
+                ).draw(**kwargs)
+            return result
 
-        return np.array(self.to_list())
+    # Weight of Evidence.
 
-    # ---#
-    def to_pandas(self):
+    @save_verticapy_logs
+    def iv_woe(
+        self,
+        y: str,
+        columns: Optional[SQLColumns] = None,
+        nbins: int = 10,
+        show: bool = True,
+        chart: Optional[PlottingObject] = None,
+        **style_kwargs,
+    ) -> PlottingObject:
         """
-	---------------------------------------------------------------------------
-	Converts the tablesample to a pandas DataFrame.
+        Computes the Information Value (IV) Table. This shows
+        the predictive power  of an independent variable in
+        relation to the dependent variable.
 
- 	Returns
- 	-------
- 	pandas.DataFrame
- 		pandas DataFrame of the tablesample.
-
-	See Also
-	--------
-	tablesample.to_sql : Generates the SQL query associated to the tablesample.
-	tablesample.to_vdf : Converts the tablesample to vDataFrame.
-		"""
-        import pandas as pd
-
-        if "index" in self.values:
-            df = pd.DataFrame(data=self.values, index=self.values["index"])
-            return df.drop(columns=["index"])
-        else:
-            return pd.DataFrame(data=self.values)
+        Parameters
+        ----------
+        y: str
+            Response vDataColumn.
+        columns: SQLColumns, optional
+            List  of  the  vDataColumns names. If  empty,  all
+            vDataColumns  except  the response  are used.
+        nbins: int, optional
+            Maximum number of bins used for the discretization
+            (must be > 1).
+        show: bool, optional
+            If  set  to True,  the  Plotting  object  is
+            returned.
+        chart: PlottingObject, optional
+            The chart object used to plot.
+        **style_kwargs
+            Any  optional  parameter to  pass to the  plotting
+            functions.
 
-    # ---#
-    def to_sql(self):
+        Returns
+        -------
+        obj
+            Plotting Object.
         """
-	---------------------------------------------------------------------------
-	Generates the SQL query associated to the tablesample.
+        columns = format_type(columns, dtype=list)
+        columns, y = self.format_colnames(columns, y)
+        if not columns:
+            columns = self.get_columns(exclude_columns=[y])
+        importance = np.array(
+            [self[col].iv_woe(y=y, nbins=nbins)["iv"][-1] for col in columns]
+        )
+        if show:
+            data = {
+                "importance": importance,
+            }
+            layout = {"columns": copy.deepcopy(columns), "x_label": "IV"}
+            vpy_plt, kwargs = self.get_plotting_lib(
+                class_name="ImportanceBarChart",
+                chart=chart,
+                style_kwargs=style_kwargs,
+            )
+            return vpy_plt.ImportanceBarChart(data=data, layout=layout).draw(**kwargs)
+        return TableSample(
+            {
+                "index": copy.deepcopy(columns),
+                "iv": importance,
+            }
+        ).sort(
+            column="iv",
+            desc=True,
+        )
 
- 	Returns
- 	-------
- 	str
- 		SQL query associated to the tablesample.
-
-	See Also
-	--------
-	tablesample.to_pandas : Converts the tablesample to a pandas DataFrame.
-	tablesample.to_sql    : Generates the SQL query associated to the tablesample.
-		"""
-        sql = []
-        n = len(self.values[list(self.values.keys())[0]])
-        for i in range(n):
-            row = []
-            for column in self.values:
-                val = self.values[column][i]
-                if isinstance(val, str):
-                    val = "'" + val.replace("'", "''") + "'"
-                elif val == None:
-                    val = "NULL"
-                elif isinstance(val, bytes):
-                    val = str(val)[2:-1]
-                    val = "'{}'::binary({})".format(val, len(val))
-                elif isinstance(val, datetime.datetime):
-                    val = "'{}'::datetime".format(val)
-                elif isinstance(val, datetime.date):
-                    val = "'{}'::date".format(val)
-                try:
-                    if math.isnan(val):
-                        val = "NULL"
-                except:
-                    pass
-                row += ["{} AS {}".format(val, '"' + column.replace('"', "") + '"')]
-            sql += ["(SELECT {})".format(", ".join(row))]
-        sql = " UNION ALL ".join(sql)
-        return sql
 
-    # ---#
-    def to_vdf(self):
-        """
-	---------------------------------------------------------------------------
-	Converts the tablesample to a vDataFrame.
+class vDCCorr(vDCEncode):
+    # Weight of Evidence.
 
- 	Returns
- 	-------
- 	vDataFrame
- 		vDataFrame of the tablesample.
-
-	See Also
-	--------
-	tablesample.to_pandas : Converts the tablesample to a pandas DataFrame.
-	tablesample.to_sql    : Generates the SQL query associated to the tablesample.
-		"""
-        relation = "({}) sql_relation".format(self.to_sql())
-        return vDataFrameSQL(relation)
-
-
-# ---#
-def to_tablesample(query: str, title: str = "", max_columns: int = -1,):
-    """
-	---------------------------------------------------------------------------
-	Returns the result of a SQL query as a tablesample object.
-
-	Parameters
-	----------
-	query: str, optional
-		SQL Query.
-	title: str, optional
-		Query title when the query is displayed.
-    max_columns: int, optional
-        Maximum number of columns to display.
-
- 	Returns
- 	-------
- 	tablesample
- 		Result of the query.
-
-	See Also
-	--------
-	tablesample : Object in memory created for rendering purposes.
-	"""
-    check_types([("query", query, [str]), ("max_columns", max_columns, [int]),])
-    if verticapy.options["sql_on"]:
-        print_query(query, title)
-    start_time = time.time()
-    cursor = executeSQL(query, print_time_sql=False)
-    description, dtype = cursor.description, {}
-    for elem in description:
-        dtype[elem[0]] = type_code_to_dtype(
-            type_code=elem[1], display_size=elem[2], precision=elem[4], scale=elem[5]
-        )
-    elapsed_time = time.time() - start_time
-    if verticapy.options["time_on"]:
-        print_time(elapsed_time)
-    result = cursor.fetchall()
-    columns = [column[0] for column in cursor.description]
-    data_columns = [[item] for item in columns]
-    data = [item for item in result]
-    for row in data:
-        for idx, val in enumerate(row):
-            data_columns[idx] += [val]
-    values = {}
-    for column in data_columns:
-        values[column[0]] = column[1 : len(column)]
-    return tablesample(values=values, dtype=dtype, max_columns=max_columns,).decimal_to_float()
-
-
-# ---#
-def vDataFrameSQL(
-    relation: str,
-    name: str = "VDF",
-    schema: str = "public",
-    history: list = [],
-    saving: list = [],
-    vdf=None,
-):
-    """
----------------------------------------------------------------------------
-Creates a vDataFrame based on a customized relation.
-
-Parameters
-----------
-relation: str
-	Relation. You can also specify a customized relation, 
-    but you must enclose it with an alias. For example "(SELECT 1) x" is 
-    correct whereas "(SELECT 1)" and "SELECT 1" are incorrect.
-name: str, optional
-	Name of the vDataFrame. It is used only when displaying the vDataFrame.
-schema: str, optional
-	Relation schema. It can be to use to be less ambiguous and allow to 
-    create schema and relation name with dots '.' inside.
-history: list, optional
-	vDataFrame history (user modifications). To use to keep the previous 
-    vDataFrame history.
-saving: list, optional
-	List to use to reconstruct the vDataFrame from previous transformations.
-
-Returns
--------
-vDataFrame
-	The vDataFrame associated to the input relation.
-	"""
-    # Initialization
-    from verticapy import vDataFrame
-
-    check_types(
-        [
-            ("relation", relation, [str]),
-            ("name", name, [str]),
-            ("schema", schema, [str]),
-            ("history", history, [list]),
-            ("saving", saving, [list]),
-        ]
-    )
-    if isinstance(vdf, vDataFrame):
-        vdf.__init__("", empty=True)
-    else:
-        vdf = vDataFrame("", empty=True)
-    vdf._VERTICAPY_VARIABLES_["input_relation"] = name
-    vdf._VERTICAPY_VARIABLES_["main_relation"] = relation
-    vdf._VERTICAPY_VARIABLES_["schema"] = schema
-    vdf._VERTICAPY_VARIABLES_["where"] = []
-    vdf._VERTICAPY_VARIABLES_["order_by"] = {}
-    vdf._VERTICAPY_VARIABLES_["exclude_columns"] = []
-    vdf._VERTICAPY_VARIABLES_["history"] = history
-    vdf._VERTICAPY_VARIABLES_["saving"] = saving
-    dtypes = get_data_types(f"SELECT * FROM {relation} LIMIT 0")
-    vdf._VERTICAPY_VARIABLES_["columns"] = ['"' + item[0] + '"' for item in dtypes]
-
-    # Creating the vColumns
-    for column, ctype in dtypes:
-        if '"' in column:
-            warning_message = (
-                'A double quote " was found in the column {0}, its '
-                "alias was changed using underscores '_' to {1}"
-            ).format(column, column.replace('"', "_"))
-            warnings.warn(warning_message, Warning)
-        from verticapy.vcolumn import vColumn
-
-        new_vColumn = vColumn(
-            '"{}"'.format(column.replace('"', "_")),
-            parent=vdf,
-            transformations=[
-                (
-                    '"{}"'.format(column.replace('"', '""')),
-                    ctype,
-                    get_category_from_vertica_type(ctype),
-                )
-            ],
-        )
-        setattr(vdf, '"{}"'.format(column.replace('"', "_")), new_vColumn)
-        setattr(vdf, column.replace('"', "_"), new_vColumn)
+    @save_verticapy_logs
+    def iv_woe(self, y: str, nbins: int = 10) -> TableSample:
+        """
+        Computes the Information Value (IV) / Weight Of
+        Evidence  (WOE) Table. This shows the predictive
+        power of an independent variable in relation to
+        the dependent variable.
 
-    return vdf
+        Parameters
+        ----------
+        y: str
+            Response vDataColumn.
+        nbins: int, optional
+            Maximum  number  of   nbins  used  for  the
+            discretization (must be > 1)
 
-vdf_from_relation = vDataFrameSQL
-# ---#
-def version(condition: list = []):
-    """
----------------------------------------------------------------------------
-Returns the Vertica Version.
-
-Parameters
-----------
-condition: list, optional
-    List of the minimal version information. If the current version is not
-    greater or equal to this one, it will raise an error.
-
-Returns
--------
-list
-    List containing the version information.
-    [MAJOR, MINOR, PATCH, POST]
-    """
-    check_types([("condition", condition, [list])])
-    if condition:
-        condition = condition + [0 for elem in range(4 - len(condition))]
-    if not (verticapy.options["vertica_version"]):
-        version = executeSQL(
-            "SELECT version();", title="Getting the version.", method="fetchfirstelem"
-        ).split("Vertica Analytic Database v")[1]
-        version = version.split(".")
-        result = []
-        try:
-            result += [int(version[0])]
-            result += [int(version[1])]
-            result += [int(version[2].split("-")[0])]
-            result += [int(version[2].split("-")[1])]
-        except:
-            pass
-        verticapy.options["vertica_version"] = result
-    else:
-        result = verticapy.options["vertica_version"]
-    if condition:
-        if condition[0] < result[0]:
-            test = True
-        elif condition[0] == result[0]:
-            if condition[1] < result[1]:
-                test = True
-            elif condition[1] == result[1]:
-                if condition[2] <= result[2]:
-                    test = True
-                else:
-                    test = False
-            else:
-                test = False
-        else:
-            test = False
-        if not (test):
-            raise VersionError(
-                (
-                    "This Function is not available for Vertica version {0}.\n"
-                    "Please upgrade your Vertica version to at least {1} to "
-                    "get this functionality."
-                ).format(
-                    version[0] + "." + version[1] + "." + version[2].split("-")[0],
-                    ".".join([str(elem) for elem in condition[:3]]),
-                )
-            )
-    return result
+        Returns
+        -------
+        obj
+            Tablesample.
+        """
+        y = self._parent.format_colnames(y)
+        assert self._parent[y].nunique() == 2, TypeError(
+            f"vDataColumn {y} must be binary to use iv_woe."
+        )
+        response_cat = self._parent[y].distinct()
+        response_cat.sort()
+        assert response_cat == [0, 1], TypeError(
+            f"vDataColumn {y} must be binary to use iv_woe."
+        )
+        self._parent[y].distinct()
+        trans = self.discretize(
+            method="same_width" if self.isnum() else "topk",
+            nbins=nbins,
+            k=nbins,
+            new_category="Others",
+            return_enum_trans=True,
+        )[0].replace("{}", self._alias)
+        query = f"""
+            SELECT 
+                {trans} AS {self}, 
+                {self} AS ord, 
+                {y}::int AS {y} 
+            FROM {self._parent}"""
+        query = f"""
+            SELECT 
+                {self}, 
+                MIN(ord) AS ord, 
+                SUM(1 - {y}) AS non_events, 
+                SUM({y}) AS events 
+            FROM ({query}) x GROUP BY 1"""
+        query = f"""
+            SELECT 
+                {self}, 
+                ord, 
+                non_events, 
+                events, 
+                non_events / NULLIFZERO(SUM(non_events) OVER ()) AS pt_non_events, 
+                events / NULLIFZERO(SUM(events) OVER ()) AS pt_events 
+            FROM ({query}) x"""
+        query = f"""
+            SELECT 
+                {self} AS index, 
+                non_events, 
+                events, 
+                pt_non_events, 
+                pt_events, 
+                CASE 
+                    WHEN non_events = 0 OR events = 0 THEN 0 
+                    ELSE ZEROIFNULL(LN(pt_non_events / NULLIFZERO(pt_events))) 
+                END AS woe, 
+                CASE 
+                    WHEN non_events = 0 OR events = 0 THEN 0 
+                    ELSE (pt_non_events - pt_events) 
+                        * ZEROIFNULL(LN(pt_non_events 
+                        / NULLIFZERO(pt_events))) 
+                END AS iv 
+            FROM ({query}) x ORDER BY ord"""
+        title = f"Computing WOE & IV of {self} (response = {y})."
+        result = TableSample.read_sql(
+            query,
+            title=title,
+            sql_push_ext=self._parent._vars["sql_push_ext"],
+            symbol=self._parent._vars["symbol"],
+        )
+        result.values["index"] += ["total"]
+        result.values["non_events"] += [sum(result["non_events"])]
+        result.values["events"] += [sum(result["events"])]
+        result.values["pt_non_events"] += [""]
+        result.values["pt_events"] += [""]
+        result.values["woe"] += [""]
+        result.values["iv"] += [sum(result["iv"])]
+        return result
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

