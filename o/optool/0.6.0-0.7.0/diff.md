# Comparing `tmp/optool-0.6.0-py3-none-any.whl.zip` & `tmp/optool-0.7.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,39 +1,38 @@
-Zip file size: 65064 bytes, number of entries: 37
--rw-rw-rw-  2.0 unx        6 b- defN 23-Jun-13 17:28 optool/VERSION
--rw-r--r--  2.0 unx     1929 b- defN 23-Jun-13 17:28 optool/__init__.py
--rw-r--r--  2.0 unx     2191 b- defN 23-Jun-13 17:28 optool/conversions.py
--rw-r--r--  2.0 unx     6459 b- defN 23-Jun-13 17:28 optool/core.py
--rw-r--r--  2.0 unx     2670 b- defN 23-Jun-13 17:28 optool/languages.py
--rw-r--r--  2.0 unx     9554 b- defN 23-Jun-13 17:28 optool/logging.py
--rw-r--r--  2.0 unx    10277 b- defN 23-Jun-13 17:28 optool/math.py
--rw-r--r--  2.0 unx     4180 b- defN 23-Jun-13 17:28 optool/parallel.py
--rw-rw-rw-  2.0 unx        0 b- defN 23-Jun-13 17:28 optool/py.typed
--rw-r--r--  2.0 unx    32727 b- defN 23-Jun-13 17:28 optool/uom.py
--rw-r--r--  2.0 unx     1618 b- defN 23-Jun-13 17:28 optool/util.py
--rw-r--r--  2.0 unx      628 b- defN 23-Jun-13 17:28 optool/fields/__init__.py
--rw-r--r--  2.0 unx     6206 b- defN 23-Jun-13 17:28 optool/fields/callables.py
--rw-r--r--  2.0 unx     5786 b- defN 23-Jun-13 17:28 optool/fields/containers.py
--rw-r--r--  2.0 unx     3937 b- defN 23-Jun-13 17:28 optool/fields/dataframe.py
--rw-r--r--  2.0 unx     2456 b- defN 23-Jun-13 17:28 optool/fields/misc.py
--rw-r--r--  2.0 unx    12294 b- defN 23-Jun-13 17:28 optool/fields/numeric.py
--rw-r--r--  2.0 unx     9788 b- defN 23-Jun-13 17:28 optool/fields/quantities.py
--rw-r--r--  2.0 unx     9218 b- defN 23-Jun-13 17:28 optool/fields/series.py
--rw-r--r--  2.0 unx     3724 b- defN 23-Jun-13 17:28 optool/fields/symbolic.py
--rw-r--r--  2.0 unx    11127 b- defN 23-Jun-13 17:28 optool/fields/util.py
--rw-r--r--  2.0 unx      347 b- defN 23-Jun-13 17:28 optool/optimization/__init__.py
--rw-r--r--  2.0 unx     5772 b- defN 23-Jun-13 17:28 optool/optimization/constraints.py
--rw-r--r--  2.0 unx     5882 b- defN 23-Jun-13 17:28 optool/optimization/helpers.py
--rw-r--r--  2.0 unx     7125 b- defN 23-Jun-13 17:28 optool/optimization/ode.py
--rw-r--r--  2.0 unx    25679 b- defN 23-Jun-13 17:28 optool/optimization/problem.py
--rw-r--r--  2.0 unx     8812 b- defN 23-Jun-13 17:28 optool/optimization/variables.py
--rw-r--r--  2.0 unx     6382 b- defN 23-Jun-13 17:28 optool/serialization/__init__.py
--rw-r--r--  2.0 unx     1610 b- defN 23-Jun-13 17:28 optool/serialization/datetime_objects.py
--rw-r--r--  2.0 unx      832 b- defN 23-Jun-13 17:28 optool/serialization/numpy_objects.py
--rw-r--r--  2.0 unx     3396 b- defN 23-Jun-13 17:28 optool/serialization/pandas_objects.py
--rw-r--r--  2.0 unx     1643 b- defN 23-Jun-13 17:28 optool/serialization/pint_objects.py
--rw-rw-rw-  2.0 unx     1081 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     4190 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2992 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/RECORD
-37 files, 212617 bytes uncompressed, 60338 bytes compressed:  71.6%
+Zip file size: 66104 bytes, number of entries: 36
+-rw-rw-rw-  2.0 unx        6 b- defN 23-Jun-20 13:32 optool/VERSION
+-rw-r--r--  2.0 unx     1929 b- defN 23-Jun-20 13:32 optool/__init__.py
+-rw-r--r--  2.0 unx     4577 b- defN 23-Jun-20 13:32 optool/conversions.py
+-rw-r--r--  2.0 unx     6459 b- defN 23-Jun-20 13:32 optool/core.py
+-rw-r--r--  2.0 unx     2670 b- defN 23-Jun-20 13:32 optool/languages.py
+-rw-r--r--  2.0 unx     9585 b- defN 23-Jun-20 13:32 optool/logging.py
+-rw-r--r--  2.0 unx    10255 b- defN 23-Jun-20 13:32 optool/math.py
+-rw-r--r--  2.0 unx     4161 b- defN 23-Jun-20 13:32 optool/parallel.py
+-rw-rw-rw-  2.0 unx        0 b- defN 23-Jun-20 13:32 optool/py.typed
+-rw-r--r--  2.0 unx    32727 b- defN 23-Jun-20 13:32 optool/uom.py
+-rw-r--r--  2.0 unx     1618 b- defN 23-Jun-20 13:32 optool/util.py
+-rw-r--r--  2.0 unx      629 b- defN 23-Jun-20 13:32 optool/fields/__init__.py
+-rw-r--r--  2.0 unx     6206 b- defN 23-Jun-20 13:32 optool/fields/callables.py
+-rw-r--r--  2.0 unx     5789 b- defN 23-Jun-20 13:32 optool/fields/containers.py
+-rw-r--r--  2.0 unx     2221 b- defN 23-Jun-20 13:32 optool/fields/misc.py
+-rw-r--r--  2.0 unx    11571 b- defN 23-Jun-20 13:32 optool/fields/numeric.py
+-rw-r--r--  2.0 unx     9783 b- defN 23-Jun-20 13:32 optool/fields/quantities.py
+-rw-r--r--  2.0 unx     3537 b- defN 23-Jun-20 13:32 optool/fields/symbolic.py
+-rw-r--r--  2.0 unx    12687 b- defN 23-Jun-20 13:32 optool/fields/tabular.py
+-rw-r--r--  2.0 unx    11128 b- defN 23-Jun-20 13:32 optool/fields/util.py
+-rw-r--r--  2.0 unx      347 b- defN 23-Jun-20 13:32 optool/optimization/__init__.py
+-rw-r--r--  2.0 unx     5772 b- defN 23-Jun-20 13:32 optool/optimization/constraints.py
+-rw-r--r--  2.0 unx     5882 b- defN 23-Jun-20 13:32 optool/optimization/helpers.py
+-rw-r--r--  2.0 unx     9240 b- defN 23-Jun-20 13:32 optool/optimization/ode.py
+-rw-r--r--  2.0 unx    26264 b- defN 23-Jun-20 13:32 optool/optimization/problem.py
+-rw-r--r--  2.0 unx     8827 b- defN 23-Jun-20 13:32 optool/optimization/variables.py
+-rw-r--r--  2.0 unx     6382 b- defN 23-Jun-20 13:32 optool/serialization/__init__.py
+-rw-r--r--  2.0 unx     1610 b- defN 23-Jun-20 13:32 optool/serialization/datetime_objects.py
+-rw-r--r--  2.0 unx      832 b- defN 23-Jun-20 13:32 optool/serialization/numpy_objects.py
+-rw-r--r--  2.0 unx     3396 b- defN 23-Jun-20 13:32 optool/serialization/pandas_objects.py
+-rw-r--r--  2.0 unx     1643 b- defN 23-Jun-20 13:32 optool/serialization/pint_objects.py
+-rw-rw-rw-  2.0 unx     1081 b- defN 23-Jun-20 13:32 optool-0.7.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     7897 b- defN 23-Jun-20 13:32 optool-0.7.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-20 13:32 optool-0.7.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 23-Jun-20 13:32 optool-0.7.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2911 b- defN 23-Jun-20 13:32 optool-0.7.0.dist-info/RECORD
+36 files, 219721 bytes uncompressed, 61504 bytes compressed:  72.0%
```

## zipnote {}

```diff
@@ -36,30 +36,27 @@
 
 Filename: optool/fields/callables.py
 Comment: 
 
 Filename: optool/fields/containers.py
 Comment: 
 
-Filename: optool/fields/dataframe.py
-Comment: 
-
 Filename: optool/fields/misc.py
 Comment: 
 
 Filename: optool/fields/numeric.py
 Comment: 
 
 Filename: optool/fields/quantities.py
 Comment: 
 
-Filename: optool/fields/series.py
+Filename: optool/fields/symbolic.py
 Comment: 
 
-Filename: optool/fields/symbolic.py
+Filename: optool/fields/tabular.py
 Comment: 
 
 Filename: optool/fields/util.py
 Comment: 
 
 Filename: optool/optimization/__init__.py
 Comment: 
@@ -90,23 +87,23 @@
 
 Filename: optool/serialization/pandas_objects.py
 Comment: 
 
 Filename: optool/serialization/pint_objects.py
 Comment: 
 
-Filename: optool-0.6.0.dist-info/LICENSE.txt
+Filename: optool-0.7.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: optool-0.6.0.dist-info/METADATA
+Filename: optool-0.7.0.dist-info/METADATA
 Comment: 
 
-Filename: optool-0.6.0.dist-info/WHEEL
+Filename: optool-0.7.0.dist-info/WHEEL
 Comment: 
 
-Filename: optool-0.6.0.dist-info/top_level.txt
+Filename: optool-0.7.0.dist-info/top_level.txt
 Comment: 
 
-Filename: optool-0.6.0.dist-info/RECORD
+Filename: optool-0.7.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## optool/VERSION

```diff
@@ -1 +1 @@
-0.6.0
+0.7.0
```

## optool/conversions.py

```diff
@@ -3,53 +3,95 @@
 
 This module is dedicated to providing reliable conversions between common time-series data representations.
 It offers functions that transform {py:class}`pandas.DatetimeIndex` objects into sample times or intervals expressed in
 seconds and vice versa.
 """
 
 from datetime import datetime, timedelta
+from typing import Union
 
 import numpy as np
 import pandas as pd
 from pandas import DatetimeIndex
 
+from optool.math import is_monotonic
 from optool.uom import UNITS, Quantity
 
 
 def datetime_index_to_samples(timestamps: DatetimeIndex) -> Quantity:
     """
     Converts a {py:class}`~pandas.DatetimeIndex` to a {py:data}`~optool.uom.Quantity` object representing the sample
     times in seconds.
 
-    :param timestamps: The absolute timestamps.
+    :param timestamps: The absolute, monotonic increasing timestamps.
     :return: A quantity object with the sample times in seconds since the first timestamp.
+    :raises ValueError: If the timestamps are not {py:attr}`~pandas.Index.is_monotonic_increasing`.
     """
+    if not timestamps.is_monotonic_increasing:
+        raise ValueError("The timestamps must be monotonic increasing.")
     duration = (timestamps - timestamps[0]).to_pytimedelta()
     sample_times_seconds = np.array([val.total_seconds() for val in duration])
     return Quantity(sample_times_seconds, UNITS.second)
 
 
 def datetime_index_to_intervals(timestamps: DatetimeIndex) -> Quantity:
     """
     Converts a {py:class}`~pandas.DatetimeIndex` to a {py:data}`~optool.uom.Quantity` object representing the intervals
     between timestamps in seconds.
 
-    :param timestamps: The absolute timestamps.
+    :param timestamps: The absolute, monotonic increasing  timestamps.
     :return: A Quantity object with the intervals between the timestamps in seconds.
+    :raises ValueError: If the timestamps are not {py:attr}`~pandas.Index.is_monotonic_increasing`.
     """
+    if not timestamps.is_monotonic_increasing:
+        raise ValueError("The timestamps must be monotonic increasing.")
     intervals_seconds = np.diff(timestamps.astype(np.int64)) / 10**9
     return Quantity(intervals_seconds, UNITS.second)
 
 
 def samples_to_datetime_index(start: datetime, sample_times: Quantity) -> DatetimeIndex:
     """
     Converts a {py:data}`~optool.uom.Quantity` object representing sample times into a {py:class}`~pandas.DatetimeIndex`
     object.
 
     :param start: The starting date and time.
     :param sample_times: A quantity object with the sample times in seconds since the start time.
     :return: The absolute timestamps.
+    :raises ValueError: If the sample times are not monotonic increasing.
     """
+    if not is_monotonic(sample_times, 'ascending', strict=False):
+        raise ValueError("The sample times must be monotonic increasing.")
     sample_times_seconds = sample_times.m_as(UNITS.second)
     duration = [timedelta(0, float(second)) for second in sample_times_seconds]
     datetime_values = [start + val for val in duration]
     return pd.DatetimeIndex(datetime_values)
+
+
+def any_to_intervals(time: Union[Quantity, DatetimeIndex]) -> Quantity:
+    """
+    Converts a given time input to time intervals.
+
+    - If the input is a {py:class}`~pandas.DatetimeIndex`, time intervals are obtained via
+      {py:func}`~datetime_index_to_intervals`.
+
+    - If the input is a {py:class}`~optool.uom.Quantity` and is monotonically increasing, the intervals are simply the
+      discrete difference.
+
+    - Otherwise, the input is assumed to be already the time intervals and a copy of the input is returned.
+
+    :param time: The time input to be converted to intervals. It can either be absolute timestamps, time steps, or time
+        intervals.
+    :return: The intervals derived from the input time.
+    :raises ValueError: If the input is a {py:class}`~pandas.DatetimeIndex` but it is not monotonically increasing, or
+        the input is not compatible with time, or the magnitude is not a one-dimensional {py:class}`~numpy.ndarray`.
+    """
+    if isinstance(time, DatetimeIndex):
+        return datetime_index_to_intervals(time)
+    if not time.is_compatible_with('s'):
+        raise ValueError(f"The time array must be compatible with time, but has units {time.u!r}.")
+    if not isinstance(time.m, np.ndarray):
+        raise ValueError(f"Time must have a magnitude that is a Numpy array, but has type {type(time.m)}.")
+    if np.ndim(time.m) != 1:
+        raise ValueError(f"Time must have a magnitude that is a one-dimensional array, but have shape {time.m.shape}.")
+    if is_monotonic(time, 'ascending', strict=False):
+        return Quantity(np.diff(time.m), time.u)
+    return time.copy()
```

## optool/logging.py

```diff
@@ -28,15 +28,14 @@
 It can then be used everywhere in the code as follows:
 ```python
 from optool.logging import LOGGER
 
 LOGGER.info("The value {!r} is logged on level info.", 13.0)
 ```
 ::::
-
 """
 
 import time
 from fnmatch import fnmatch
 from functools import wraps
 from typing import Any, Callable, Dict, List, Literal, Optional, Protocol, TextIO, Union
 
@@ -178,16 +177,16 @@
 
 
 # noinspection PyShadowingBuiltins
 def setup_logger(sink: Union[TextIO, Writable, str], filter: Optional[Callable[[Any], bool]], level: LogLevels) -> None:
     """
     Sets up the logger with a given sink, filter, and level.
 
-    :param sink: Where the log messages should be sent. This can be either a {py:class}`~io.TextIOWrapper` object (e.g.
-        {py:data}`sys.stdout`) or a string specifying a filename.
+    :param sink: Where the log messages should be sent. This can be an object either of type {py:class}`~typing.TextIO`
+        or {py:class}`~Writable` (e.g. {py:data}`sys.stdout`), or a string specifying a filename.
     :param filter: The filter to use for the log messages.
     :param level: The minimum log level to record.
     :raises ValueError: If sink is not a {py:class}`~io.TextIOWrapper` object or a string.
     """
 
     logger.remove()
     if isinstance(sink, str):
```

## optool/math.py

```diff
@@ -1,41 +1,40 @@
 """
 Collection of classes and functions geared towards facilitating computations involving both numeric and symbolic values.
 
 This module offers various functions for performing a series of checks and operations on numerical and unit-based data.
 For instance, the module contains functions to verify if a value or unit is dimensionless, to determine if a numeric
-value has an offset, or to check if two values or units are compatible.
-A set of utility functions provides detailed inspection capabilities on numerical data, allowing to ascertain whether a
-value is zero, non-zero, or not a number (NaN), among others.
-Moreover, the module includes functions to confirm the data structure type of the provided data, identifying whether
-it's a scalar, an array, or a vector.
+value has an offset, or to check if two values or units are compatible. A set of utility functions provides detailed
+inspection capabilities on numerical data, allowing to ascertain whether a value is zero, non-zero, or not a number
+(NaN), among others. Moreover, the module includes functions to confirm the data structure type of the provided data,
+identifying whether it's a scalar, an array, or a vector.
 """
 
 from enum import Enum
 from numbers import Number
 from typing import Literal, Optional, Union, get_args
 
 import casadi
 import numpy as np
 
 from optool.uom import UNITS, Quantity, Unit
 
-NUMERIC_TYPES = Union[Number, np.ndarray, Quantity]  # type:ignore
-"""Numbers with or without units of measurement"""
+NUMERIC_TYPES = Union[Number, np.ndarray, Quantity]
+"""Numbers with or without units of measurement."""
 
 SYMBOLIC_TYPES = Union[casadi.SX]
-"""Symbols used by the supported modeling languages"""
+"""Symbols used by the supported modeling languages."""
 
 
 class VectorRepresentation(Enum):
     """Description of vector layouts."""
     COLUMN = 0
-    """Convenient shortcut to describe column vectors, where the entries are stacked vertically"""
+    """Convenient shortcut to describe column vectors, where the entries are stacked vertically."""
     ROW = 1
-    """Convenient shortcut to describe row vectors, where the entries are stacked horizontally"""
+    """Convenient shortcut to describe row vectors, where the entries are stacked horizontally."""
 
 
 def has_offset(value_or_unit: Union[NUMERIC_TYPES, Unit]) -> bool:
     """
     Checks whether a given numeric value or unit has an offset in its measurement, such as Celsius.
 
     :param value_or_unit: Numeric value or unit to check.
@@ -133,15 +132,15 @@
     """
     Checks if a given value is symbolic.
 
     :param value: The value to check.
     :return: {py:data}`True` if the given value is symbolic, {py:data}`False` otherwise.
     """
     return isinstance(value, SYMBOLIC_TYPES) or (  # type: ignore
-        isinstance(value, Quantity) and is_symbolic(value.magnitude))  # type: ignore
+        isinstance(value, Quantity) and is_symbolic(value.magnitude))
 
 
 def is_scalar(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> bool:
     """
     Checks if a given value is scalar.
 
     :param value: The value to check.
@@ -164,15 +163,16 @@
     if isinstance(value, Quantity):
         return is_array(value.magnitude)
     raise TypeError(f"Unsupported type {type(value)}")
 
 
 def is_vector(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES],
               representation: Optional[VectorRepresentation] = None) -> bool:
-    """Checks if a given value is a (column or row) vector.
+    """
+    Checks if a given value is a (column or row) vector.
 
     :param value: The value to check
     :param representation: The representation of the vector
     :returns: {py:data}`True` if the value has two dimensions and is either a row or a column vector, depending on the
         requested axis, or {py:data}`False` otherwise.
 
     :::{seealso}
```

## optool/parallel.py

```diff
@@ -1,10 +1,8 @@
-"""
-Utilities to execute code in parallel across multiple processes.
-"""
+"""Utilities to execute code in parallel across multiple processes."""
 
 import os
 from multiprocessing import Pool, current_process
 from typing import Any, Callable, Iterable
 
 from pydantic import DirectoryPath, StrictInt
 
@@ -65,15 +63,15 @@
     function: Callable
     """The function to execute."""
 
     log_sink: DirectoryPath
     """The directory path where to store the logs."""
 
     log_level: LogLevels = 'TRACE'
-    """The level of logging, default to 'TRACE'."""
+    """The level of logging, defaults to 'TRACE'."""
 
     processes: StrictInt = _get_system_cpu_counts()
     """The number of processes to spawn, defaults to the system CPU count."""
 
     @timeit(log_level='INFO')
     def run(self, *args: Any) -> Any:
         """
@@ -101,17 +99,15 @@
         """
         self.setup_subprocess_logger()
         out = timeit(self.function, log_level='INFO')(arg)
         self.tear_down_subprocess_logger()
         return out
 
     def setup_subprocess_logger(self) -> None:
-        """
-        Sets up the logger for a subprocess.
-        """
+        """Sets up the logger for a subprocess."""
         process = current_process()
         log_file_name = str(self.log_sink.absolute() / f"log_{process.name}.log")
         setup_logger(sink=log_file_name, filter=LogFilter(), level=self.log_level)
 
     @staticmethod
     def tear_down_subprocess_logger() -> None:
         """
```

## optool/fields/__init__.py

```diff
@@ -1,11 +1,11 @@
 """
 Custom Pydantic-compatible fields for data validation and parsing.
 
-This package is a comprehensive toolkit for advanced data validation based on the Python library
-[Pydantic](https://docs.pydantic.dev/latest/).
+This package is a comprehensive toolkit for advanced data validation based on the Python library [Pydantic](
+https://docs.pydantic.dev/latest/).
 It provides a suite of custom Pydantic-compatible fields designed to enforce specific constraints on various data types,
 including numbers, Numpy arrays, quantities, symbolic expressions, containers, callables, and Pandas Series and
 DataFrames.
 These custom fields ensure that data adheres to the specific requirements of optimization models, supporting robust and
 reliable model construction and execution.
 """
```

## optool/fields/callables.py

```diff
@@ -1,13 +1,13 @@
 """
 Pydantic-compatible field types for objects that are callable.
 
-This module provides utilities to validate and enforce constraints on Python callable objects.
-A callable in Python is any object that can be called like a function.
-The validation includes checks on the number of parameters and the types of both these parameters and the return value.
+This module provides utilities to validate and enforce constraints on Python callable objects. A callable in Python is
+any object that can be called like a function. The validation includes checks on the number of parameters and the types
+of both these parameters and the return value.
 """
 
 from __future__ import annotations
 
 import inspect
 from typing import Any, Callable, Optional, Type, Union, get_type_hints
```

## optool/fields/containers.py

```diff
@@ -1,13 +1,12 @@
 """
 Pydantic-compatible field types for data containers such as lists.
 
 This module offers functionalities to create and manage typed and constrained containers that allow type checking and
-validation.
-This ensures that data integrity is maintained as elements are added or manipulated within the container.
+validation. This ensures that data integrity is maintained as elements are added or manipulated within the container.
 """
 
 import types
 import typing
 from typing import TYPE_CHECKING, Callable, Iterable, List, Optional, Type, TypeVar, Union, get_args
 
 import pydantic
@@ -139,9 +138,9 @@
     :param min_items: The minimum number of items the list should contain. (Default: {py:data}`None`)
     :param max_items: The maximum number of items the list should contain. (Default: {py:data}`None`)
     :param custom: Custom validation functions. This could be a single function or an iterable of functions. (Default:
         {py:data}`None`)
     :return: A list type with the specified constraints.
     """
     namespace = dict(min_items=min_items, max_items=max_items, custom_validators=custom)
-    return types.new_class(  # type:ignore
-        'ConstrainedMutatingListValue', (ConstrainedMutatingList[item_type],), {}, lambda ns: ns.update(namespace))
+    return types.new_class('ConstrainedMutatingListValue', (ConstrainedMutatingList[item_type],), {},
+                           lambda ns: ns.update(namespace))
```

## optool/fields/misc.py

```diff
@@ -5,75 +5,58 @@
 on strings and numbers, including non-empty strings, number bounds within the [0, 1] interval, finiteness checks, etc.
 """
 
 from typing import TYPE_CHECKING
 
 from pydantic import ConstrainedFloat, ConstrainedStr
 
-
-class NonEmptyStr(ConstrainedStr):
-    """Pydantic-compatible field type for non-empty strings, with leading and trailing spaces removed."""
-    strict = True
-    strip_whitespace = True
-    min_length = 1
-
-
-class FractionFloat(ConstrainedFloat):
-    """
-    Pydantic-compatible field type for numbers, enforcing them to be greater than or equal to zero and smaller than
-    or equal to one.
-    """
-    strict = False
-    ge = 0.0
-    le = 1.0
-    allow_inf_nan = False
-
-
-class PositiveFiniteFloat(ConstrainedFloat):
-    """
-    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and greater
-    than zero.
-    """
-    strict = False
-    gt = 0
-    allow_inf_nan = False
-
-
-class NonNegativeFiniteFloat(ConstrainedFloat):
-    """
-    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and greater
-    than or equal to zero.
-    """
-    strict = False
-    ge = 0
-    allow_inf_nan = False
-
-
-class NegativeFiniteFloat(ConstrainedFloat):
-    """
-    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and smaller
-    than zero.
-    """
-    strict = False
-    lt = 0
-    allow_inf_nan = False
-
-
-class NonPositiveFiniteFloat(ConstrainedFloat):
-    """
-    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and smaller
-    than or equal to zero.
-    """
-    strict = False
-    le = 0
-    allow_inf_nan = False
-
-
 if TYPE_CHECKING:
-    from typing_extensions import TypeAlias
-
-    NonEmptyStr: TypeAlias = str  # type: ignore[no-redef] # noqa: F811
-    FractionFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
-    PositiveFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
-    NonNegativeFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
-    NegativeFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
-    NonPositiveFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
+    NonEmptyStr = str
+    FractionFloat = float
+    PositiveFiniteFloat = float
+    NonNegativeFiniteFloat = float
+    NegativeFiniteFloat = float
+    NonPositiveFiniteFloat = float
+
+if not TYPE_CHECKING:
+
+    class NonEmptyStr(ConstrainedStr):
+        """Pydantic-compatible field type for non-empty strings, with leading and trailing spaces removed."""
+        strict = True
+        strip_whitespace = True
+        min_length = 1
+
+    class FractionFloat(ConstrainedFloat):
+        """Pydantic-compatible field type for numbers, enforcing them to be greater than or equal to zero and smaller
+        than or equal to one."""
+        strict = False
+        ge = 0.0
+        le = 1.0
+        allow_inf_nan = False
+
+    class PositiveFiniteFloat(ConstrainedFloat):
+        """Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and
+        greater than zero."""
+        strict = False
+        gt = 0
+        allow_inf_nan = False
+
+    class NonNegativeFiniteFloat(ConstrainedFloat):
+        """Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and
+        greater than or equal to zero."""
+        strict = False
+        ge = 0
+        allow_inf_nan = False
+
+    class NegativeFiniteFloat(ConstrainedFloat):
+        """Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and
+        smaller than zero."""
+        strict = False
+        lt = 0
+        allow_inf_nan = False
+
+    class NonPositiveFiniteFloat(ConstrainedFloat):
+        """Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and
+        smaller than or equal to zero."""
+        strict = False
+        le = 0
+        allow_inf_nan = False
```

## optool/fields/numeric.py

```diff
@@ -7,39 +7,26 @@
 desired data shapes, dimensions, and immutability constraints.
 """
 
 from __future__ import annotations
 
 import itertools
 import numbers
-from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Literal, Optional, Tuple, Type, TypeVar, Union
+from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Literal, Optional, Tuple, TypeVar, Type
 
 import numpy as np
 import pydantic
 from pydantic.fields import ModelField
 
-from optool.fields.util import (WrongTypeError, check_only_one_specified, check_sub_fields_level, get_subtype_validator,
-                                get_type_validator, update_object_schema)
+from optool.fields.util import (WrongTypeError, check_sub_fields_level, get_subtype_validator, get_type_validator,
+                                update_object_schema, check_only_one_specified)
 
 T = TypeVar("T", bound=np.generic)
 """Type variable with an upper bound of {py:class}`numpy.generic`."""
 
-TypesParseableToNdArrays = Union[np.ndarray[Any, np.dtype[T]], numbers.Number, Iterable]
-"""
-Type alias for a union of types that can be parsed into objects of type {py:class}`numpy.ndarray`.
-
-:type: typing.Union[numpy.ndarray[typing.Any, numpy.dtype[numpy.generic]], numbers.Number, typing.Iterable]
-
-The union includes the following types:
-
-- N-dimensional arrays.
-- Any type of number, including integers, floating points, complex numbers, etc.
-- Any object capable of returning its members one at a time, including lists, tuples, strings, dictionaries, sets, etc.
-"""
-
 
 # Due to the generic class, Pydantic has to be tricked out such that the automatic creation of schemas is working.
 class ConstrainedNdArray(pydantic.BaseModel, Generic[T]):
     """Pydantic-compatible field type for {py:class}`numpy.ndarray` objects, which allows to specify the data-type.
 
     The approach is inspired by https://github.com/cheind/pydantic-numpy.
 
@@ -131,128 +118,28 @@
         shape: Optional[Tuple[int, ...]] = None,
         writeable: Literal['keep', 'make_true', 'make_false', 'check_true',
                            'check_false'] = 'keep') -> Type[np.ndarray]:
     """Creates a Pydantic-compatible field type for {py:class}`numpy.ndarray` objects, which allows specifying
     constraints on the accepted values.
 
     :param strict: If {py:data}`True` only values of type {py:class}`numpy.ndarray` are accepted. (Default:
-            {py:data}`False`)
+        {py:data}`False`)
     :param dimensions: The expected dimensions as in {py:attr}`~numpy.ndarray.ndim`.
     :param shape: The shape expected. One shape dimension can be {py:data}`None` indicating that this dimension is
         arbitrary.
     :param writeable: Specification on how to deal with the `writeable` flag of the {py:class}`numpy.ndarray` object.
     :returns: A new Pydantic-compatible field type.
 
     :::{seealso}
     {py:func}`pydantic.conint` or similar of {py:mod}`pydantic`.
     :::
     """
     check_only_one_specified(dimensions, shape, "Cannot specify both dimensions and shape.")
     namespace = dict(strict=strict, dimensions=dimensions, shape=shape, writeable=writeable)
-    return type('ConstrainedNdArrayValue', (ConstrainedNdArray,), namespace)  # type: ignore
-
-
-class NdArrayLike(ConstrainedNdArray[T]):
-    """
-    Pydantic-compatible field type for {py:class}`numpy.ndarray` objects.
-
-    Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
-    subtype specified using type hinting (e.g., ``NdArrayLike[np.int_]``) is also not enforced, but rather used to set
-    the data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
-    """
-    strict = False
-    strict_datatype = False
-
-
-class Array(ConstrainedNdArray[T]):
-    """
-    Pydantic-compatible field type for one-dimensional {py:class}`numpy.ndarray` objects.
-
-    Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
-    subtype specified using type hinting (e.g., ``Array[np.int_]``) is also not enforced, but rather used to set the
-    data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
-    """
-    strict = False
-    strict_datatype = False
-    dimensions = 1
-
-
-class ImmutableArray(ConstrainedNdArray[T]):
-    """
-    Pydantic-compatible field type for one-dimensional immutable {py:class}`numpy.ndarray` objects.
-
-    Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
-    subtype specified using type hinting (e.g., ``ImmutableArray[np.int_]``) is also not enforced, but rather used to
-    set the data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
-
-    Immutability is established by setting the flag ``writeable`` to {py:data}`False`.
-    """
-    strict = False
-    strict_datatype = False
-    dimensions = 1
-    writeable = 'make_false'
-
-
-class StrictNdArray(ConstrainedNdArray[T]):
-    """
-    Pydantic-compatible field type for {py:class}`numpy.ndarray` objects.
-
-    Assigned values must be of type {py:class}`~numpy.ndarray`. Furthermore, the subtype specified using type hinting
-    (e.g., ``StrictNdArray[np.int_]``) must also match the data-type of the numeric values.
-    """
-    strict = True
-    strict_datatype = True
-
-
-class Row(ConstrainedNdArray[T]):
-    """
-    Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing row vectors.
-
-    Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting (e.g.,
-    ``Row[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
-    """
-    strict = True
-    strict_datatype = False
-    shape = (1, None)
-
-
-class Column(ConstrainedNdArray[T]):
-    """
-    Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing column vectors.
-
-    Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting (e.g.,
-    ``Column[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
-    """
-    strict = True
-    strict_datatype = False
-    shape = (None, 1)
-
-
-class Matrix(ConstrainedNdArray[T]):
-    """
-    Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing matrices.
-
-    Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting (e.g.,
-    ``Matrix[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
-    """
-    strict = True
-    strict_datatype = False
-    dimensions = 2
-
-
-if TYPE_CHECKING:
-    from typing_extensions import TypeAlias
-
-    NdArrayLike: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
-    Array: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
-    ImmutableArray: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
-    StrictNdArray: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
-    Row: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
-    Column: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
-    Matrix: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+    return type('ConstrainedNdArrayValue', (ConstrainedNdArray,), namespace)
 
 
 class ShapeError(ValueError):
     """
     Raised when the shape of a numpy array does not meet the expectations.
 
     :param expected: The expected shape of the array, {py:data}`None` indicating arbitrary length of the corresponding
@@ -283,7 +170,101 @@
     :param expected: The expected state of the writeable flag.
     :param value: The array that causes the error due to its writeable flag state.
     """
 
     def __init__(self, *, expected: bool, value: np.ndarray) -> None:
         super().__init__(f"expected writeable is {expected}, "
                          f"but got a value with writeable flag set to {value.flags.writeable}")
+
+
+if TYPE_CHECKING:
+    NdArrayLike = np.ndarray[Any, np.dtype[T]]
+    Array = np.ndarray[Any, np.dtype[T]]
+    ImmutableArray = np.ndarray[Any, np.dtype[T]]
+    StrictNdArray = np.ndarray[Any, np.dtype[T]]
+    Row = np.ndarray[Any, np.dtype[T]]
+    Column = np.ndarray[Any, np.dtype[T]]
+    Matrix = np.ndarray[Any, np.dtype[T]]
+
+else:
+
+    class NdArrayLike(ConstrainedNdArray[T]):
+        """
+        Pydantic-compatible field type for {py:class}`numpy.ndarray` objects.
+
+        Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
+        subtype specified using type hinting (e.g., ``NdArrayLike[np.int_]``) is also not enforced, but rather used to
+        set the data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
+        """
+        strict = False
+        strict_datatype = False
+
+    class Array(ConstrainedNdArray[T]):
+        """
+        Pydantic-compatible field type for one-dimensional {py:class}`numpy.ndarray` objects.
+
+        Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
+        subtype specified using type hinting (e.g., ``Array[np.int_]``) is also not enforced, but rather used to set the
+        data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
+        """
+        strict = False
+        strict_datatype = False
+        dimensions = 1
+
+    class ImmutableArray(ConstrainedNdArray[T]):
+        """
+        Pydantic-compatible field type for one-dimensional immutable {py:class}`numpy.ndarray` objects.
+
+        Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
+        subtype specified using type hinting (e.g., ``ImmutableArray[np.int_]``) is also not enforced, but rather used
+        to set the data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
+
+        Immutability is established by setting the flag ``writeable`` to {py:data}`False`.
+        """
+        strict = False
+        strict_datatype = False
+        dimensions = 1
+        writeable = 'make_false'
+
+    class StrictNdArray(ConstrainedNdArray[T]):
+        """
+        Pydantic-compatible field type for {py:class}`numpy.ndarray` objects.
+
+        Assigned values must be of type {py:class}`~numpy.ndarray`. Furthermore, the subtype specified using type
+        hinting (e.g., ``StrictNdArray[np.int_]``) must also match the data-type of the numeric values.
+        """
+        strict = True
+        strict_datatype = True
+
+    class Row(ConstrainedNdArray[T]):
+        """
+        Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing row vectors.
+
+        Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting
+        (e.g., ``Row[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
+        """
+        strict = True
+        strict_datatype = False
+        shape = (1, None)
+
+    class Column(ConstrainedNdArray[T]):
+        """
+        Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing column
+        vectors.
+
+        Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting
+        (e.g., ``Column[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
+        """
+        strict = True
+        strict_datatype = False
+        shape = (None, 1)
+
+    class Matrix(ConstrainedNdArray[T]):
+        """
+        Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing matrices.
+
+        Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting
+        (e.g., ``Matrix[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
+        """
+        strict = True
+        strict_datatype = False
+        dimensions = 2
```

## optool/fields/quantities.py

```diff
@@ -131,104 +131,14 @@
         valid_value, error = magnitude_field.validate(val.m, {}, loc='magnitude')
         if error:
             raise ValidationError([error], cls)
 
         return Quantity(valid_value, val.u)
 
 
-class UnitLike(ConstrainedUnit[D], Unit):
-    """
-    Pydantic-compatible field type for {py:class}`~optool.uom.Unit` objects.
-
-    Assigned values not already of type {py:class}`~optool.uom.Unit` are parsed using `UNITS.parse_units(...)`. The
-    subtype specified using type hinting (e.g., `UnitLike[Length]`) is used to check if the unit has the correct
-    dimensionality.
-
-    ::::{admonition} Example
-    :class: example dropdown
-
-    ```python
-    from optool import BaseModel
-    from optool.fields.quantities import UnitLike
-    from optool.uom import Mass
-
-    class ExampleModel(BaseModel):
-        mass_unit: UnitLike[Mass]
-
-    mdl = BaseModel(mass_unit="kg")
-    ```
-    """
-    strict = False
-
-
-class StrictUnit(ConstrainedUnit[D], Unit):
-    """
-    Pydantic-compatible field type for {py:class}`~optool.uom.Unit` objects.
-
-    Assigned values must be of type {py:class}`~optool.uom.Unit`. The subtype specified using type hinting (e.g.,
-    `UnitLike[Length]`) is used to check if the unit has the correct dimensionality.
-    """
-    strict = True
-
-
-class QuantityLike(ConstrainedQuantity[D, T], Quantity):
-    """
-    Pydantic-compatible field type for {py:class}`~optool.uom.Quantity` objects.
-
-    Assigned values not already of type {py:class}`~optool.uom.Quantity` are parsed using the regular constructor
-    {py:class}`Quantity(val) <optool.uom.Quantity>`.
-
-    The two subtypes specified using type hinting (e.g., `QuantityLike[Length, PositiveInt]`) are used to check if the
-    unit has the expected dimensionality and the magnitude matches the expected specification. For the latter, the
-    validation and parsing is forwarded to the specific type, e.g., in the example above, the value is validated using
-    the implementation provided by {py:class}`pydantic.PositiveInt`.
-
-    ::::{admonition} Example
-    :class: example dropdown
-
-    ```python
-    from optool import BaseModel
-    from optool.fields.quantities import QuantityLike
-    from optool.fields.misc import PositiveFiniteFloat
-    from optool.uom import Mass
-
-    class ExampleModel(BaseModel):
-        mass: QuantityLike[Mass, PositiveFiniteFloat]
-
-    mdl = BaseModel(mass="5 kg")
-    ```
-    """
-    strict = False
-    strict_magnitude = False
-
-
-class StrictQuantity(ConstrainedQuantity[D, T], Quantity):
-    """
-    Pydantic-compatible field type for {py:class}`~optool.uom.Quantity` objects.
-
-    Assigned values must be of type {py:class}`~optool.uom.Quantity`.
-
-    The two subtypes specified using type hinting (e.g., `QuantityLike[Length, PositiveInt]`) are used to check if the
-    unit has the expected dimensionality and the magnitude matches the expected specification. For the latter, the
-    validation and parsing is forwarded to the specific type, e.g., in the example above, the value is validated using
-    the implementation provided by {py:class}`pydantic.PositiveInt`.
-    """
-    strict = True
-    strict_magnitude = False
-
-
-if TYPE_CHECKING:
-    from typing_extensions import TypeAlias
-
-    UnitLike: TypeAlias = Unit  # type: ignore[no-redef] # noqa: F811
-    StrictUnit: TypeAlias = Unit  # type: ignore[no-redef] # noqa: F811
-    QuantityLike: TypeAlias = Quantity  # type: ignore[no-redef] # noqa: F811
-    StrictQuantity: TypeAlias = Quantity  # type: ignore[no-redef] # noqa: F811
-
-
 class DimensionalityError(ValueError):
     """
     Raised when an incorrect dimensionality is encountered.
 
     :param expected: The expected dimensionality
     :param value: The value that causes the error due to its incorrect dimensionality.
     """
@@ -254,7 +164,94 @@
     Raised when a unit string cannot be parsed.
 
     :param unit: The unit string that causes the error due to parsing issues.
     """
 
     def __init__(self, *, unit: str) -> None:
         super().__init__(f"cannot parse the unit {unit}")
+
+
+if TYPE_CHECKING:
+
+    UnitLike = Unit
+    StrictUnit = Unit
+    QuantityLike = Quantity
+    StrictQuantity = Quantity
+
+else:
+
+    class UnitLike(ConstrainedUnit[D], Unit):
+        """
+        Pydantic-compatible field type for {py:class}`~optool.uom.Unit` objects.
+
+        Assigned values not already of type {py:class}`~optool.uom.Unit` are parsed using `UNITS.parse_units(...)`. The
+        subtype specified using type hinting (e.g., `UnitLike[Length]`) is used to check if the unit has the correct
+        dimensionality.
+
+        ::::{admonition} Example
+        :class: example dropdown
+
+        ```python
+        from optool import BaseModel
+        from optool.fields.quantities import UnitLike
+        from optool.uom import Mass
+
+        class ExampleModel(BaseModel):
+            mass_unit: UnitLike[Mass]
+
+        mdl = BaseModel(mass_unit="kg")
+        ```
+        """
+        strict = False
+
+    class StrictUnit(ConstrainedUnit[D], Unit):
+        """
+        Pydantic-compatible field type for {py:class}`~optool.uom.Unit` objects.
+
+        Assigned values must be of type {py:class}`~optool.uom.Unit`. The subtype specified using type hinting (e.g.,
+        `UnitLike[Length]`) is used to check if the unit has the correct dimensionality.
+        """
+        strict = True
+
+    class QuantityLike(ConstrainedQuantity[D, T], Quantity):
+        """
+        Pydantic-compatible field type for {py:class}`~optool.uom.Quantity` objects.
+
+        Assigned values not already of type {py:class}`~optool.uom.Quantity` are parsed using the regular constructor
+        {py:class}`Quantity(val) <optool.uom.Quantity>`.
+
+        The two subtypes specified using type hinting (e.g., `QuantityLike[Length, PositiveInt]`) are used to check if
+        the unit has the expected dimensionality and the magnitude matches the expected specification. For the latter,
+        the validation and parsing is forwarded to the specific type, e.g., in the example above, the value is validated
+        using the implementation provided by {py:class}`pydantic.PositiveInt`.
+
+        ::::{admonition} Example
+        :class: example dropdown
+
+        ```python
+        from optool import BaseModel
+        from optool.fields.quantities import QuantityLike
+        from optool.fields.misc import PositiveFiniteFloat
+        from optool.uom import Mass
+
+        class ExampleModel(BaseModel):
+            mass: QuantityLike[Mass, PositiveFiniteFloat]
+
+        mdl = BaseModel(mass="5 kg")
+        ```
+        """
+        strict = False
+        strict_magnitude = False
+
+    class StrictQuantity(ConstrainedQuantity[D, T], Quantity):
+        """
+        Pydantic-compatible field type for {py:class}`~optool.uom.Quantity` objects.
+
+        Assigned values must be of type {py:class}`~optool.uom.Quantity`.
+
+        The two subtypes specified using type hinting (e.g., `QuantityLike[Length, PositiveInt]`) are used to check if
+        the unit has the expected dimensionality and the magnitude matches the expected specification. For the latter,
+        the validation and parsing is forwarded to the specific type, e.g., in the example above, the value is validated
+        using the implementation provided by {py:class}`pydantic.PositiveInt`.
+        """
+        strict = True
+        strict_magnitude = False
```

## optool/fields/symbolic.py

```diff
@@ -49,48 +49,44 @@
         raise ShapeError(expected=cls.shape, value=val)
 
     @classmethod
     def _compare_dim(cls, expected: Optional[int], actual: Optional[int]) -> bool:
         return actual == expected or expected is None
 
 
-class CasadiScalar(ConstrainedCasadiSymbol):
-    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing scalars."""
-    shape = (1, 1)
-
-
-class CasadiRow(ConstrainedCasadiSymbol):
-    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing row vectors."""
-    shape = (1, None)
-
-
-class CasadiColumn(ConstrainedCasadiSymbol):
-    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing column vectors."""
-    shape = (None, 1)
-
-
-class CasadiMatrix(ConstrainedCasadiSymbol):
-    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing matrices."""
-    shape = (None, None)
-
-
-if TYPE_CHECKING:
-    from typing_extensions import TypeAlias
-
-    CasadiScalar: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
-    CasadiRow: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
-    CasadiColumn: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
-    CasadiMatrix: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
-
-
 class ShapeError(ValueError):
     """
     Raised when the shape of a CasADi SX variable does not meet the expectations.
 
     :param expected: The expected shape of the array, {py:data}`None` indicating arbitrary length of the corresponding
         dimension.
     :param value: The CasADi SX variable that causes the error due to its shape.
     """
 
     def __init__(self, *, expected: Tuple[Optional[int], ...], value: casadi.SX) -> None:
         super().__init__(f"expected the shape {expected}, "
                          f"but got a value with shape ('called size' in CasADi) {value.size()}")
+
+
+if TYPE_CHECKING:
+    CasadiScalar = casadi.SX
+    CasadiRow = casadi.SX
+    CasadiColumn = casadi.SX
+    CasadiMatrix = casadi.SX
+else:
+
+    class CasadiScalar(ConstrainedCasadiSymbol):
+        """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing scalars."""
+        shape = (1, 1)
+
+    class CasadiRow(ConstrainedCasadiSymbol):
+        """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing row vectors."""
+        shape = (1, None)
+
+    class CasadiColumn(ConstrainedCasadiSymbol):
+        """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing column
+        vectors."""
+        shape = (None, 1)
+
+    class CasadiMatrix(ConstrainedCasadiSymbol):
+        """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing matrices."""
+        shape = (None, None)
```

## optool/fields/util.py

```diff
@@ -1,13 +1,13 @@
 """
 Utility functions facilitating the implementation of the Pydantic-compatible fields in the other modules of this
 package.
 
-This module contains various utility functions designed to augment and simplify coding tasks related to the
-Pydantic-compatible fields implemented in the other modules of this package.
+This module contains various utility functions designed to augment and simplify coding tasks related to the Pydantic-
+compatible fields implemented in the other modules of this package.
 """
 
 from __future__ import annotations
 
 import re
 from typing import Any, Callable, Dict, Iterable, Optional, Tuple, Type, TypeVar, Union
```

## optool/optimization/__init__.py

```diff
@@ -1,7 +1,7 @@
 """
 Facilitating the formulation and solution of nonlinear programming (NLP) problems.
 
 It comes with a collection of modules that facilitate the representation of decision variables, constraints, ordinary
-differential equations (ODEs), and overall problem formulation.
-It also provides utilities for debugging and analyzing solver responses.
+differential equations (ODEs), and overall problem formulation. It also provides utilities for debugging and analyzing
+solver responses.
 """
```

## optool/optimization/helpers.py

```diff
@@ -1,12 +1,12 @@
 """
 Utilities that assist the formulation and analysis of optimization problems.
 
-This module provides ancillary features for the optimization process.
-It houses tools for organizing debug information, handling solver responses, and managing Ipopt options.
+This module provides ancillary features for the optimization process. It houses tools for organizing debug information,
+handling solver responses, and managing Ipopt options.
 """
 
 from datetime import timedelta
 from typing import TYPE_CHECKING, Any, Dict
 
 import humanize
 import pandas as pd
```

## optool/optimization/ode.py

```diff
@@ -1,32 +1,36 @@
 """
 Representation of ordinary differential equations (ODE)s.
 
 This module contains numerical integration methods that simplify the implementation of ODEs with respect to their use in
 an optimization context.
 """
 
-from typing import Any, Dict, Protocol, final
+from typing import Any, Dict, Protocol, final, overload, Union
 
 from casadi import casadi
 from pandas import DatetimeIndex
 from pydantic import root_validator
+from typing_extensions import TypeAlias
 
-from optool.conversions import datetime_index_to_intervals
+from optool.conversions import any_to_intervals
 from optool.core import BaseModel
 from optool.fields.callables import concallable
 from optool.fields.misc import NonEmptyStr
 from optool.fields.quantities import QuantityLike
 from optool.fields.symbolic import CasadiColumn
 from optool.fields.util import validate
 from optool.logging import LOGGER
 from optool.math import num_elements
 from optool.optimization.constraints import Equation
 from optool.uom import UNITS, Quantity
 
+OdeFunction: TypeAlias = concallable(num_params=2)  # type: ignore[valid-type]
+"""A callable with two input parameters."""
+
 
 class OrdinaryDifferentialEquation(BaseModel, frozen=True):
     r"""
     Representation of an ordinary differential equation (ODE).
 
     An ODE is described by the mathematical formula of the following form,
 
@@ -43,15 +47,15 @@
 
     name: NonEmptyStr
     """The name of the ordinary differential equation."""
     state_variable: QuantityLike[Any, CasadiColumn]
     """The array of state variables of the ordinary differential equation."""
     input_variable: QuantityLike[Any, CasadiColumn]
     """The array of input variables of the ordinary differential equation."""
-    function: concallable(num_params=2)  # type: ignore[valid-type]
+    function: OdeFunction
     """The function of the ordinary differential equation."""
 
     @final
     @root_validator
     def _is_consistent(cls, values: Dict) -> Dict:
         name = values['name']
         function = values['function']
@@ -84,16 +88,58 @@
                 f"elements than the vector of input variables, but have {time_derivative.numel()} and "
                 f"{num_input_variables}.")
 
         return values
 
 
 class IntegrationMethod(Protocol):
+    """
+    A protocol that defines the contract for methods that implement numerical integration.
+
+    Any class that provides an implementation for this method is considered as an integration method. It will be capable
+    of solving an ordinary differential equation (ODE) by numerical integration over a time horizon specified as
+    timestamps.
+
+    This protocol can be used as a type hint for functions or methods that require an integration method.
+    """
+
+    @overload
+    def integrate(self, ode: OrdinaryDifferentialEquation, time_intervals: Quantity) -> Equation:
+        """
+        Integrate the ordinary differential equation (ODE) over a specified horizon given as a sequence of timestamps.
+
+        :param ode: The ODE that represents the differential equation to be integrated.
+        :param time_intervals: The time intervals between the samples over which to perform the integration.
+        :return: The solution of the ODE after performing the numerical integration.
+        """
+        pass
+
+    @overload
+    def integrate(self, ode: OrdinaryDifferentialEquation, time_steps: Quantity) -> Equation:
+        """
+        Integrate the ordinary differential equation (ODE) over a specified horizon given as a sequence of timestamps.
+
+        :param ode: The ODE that represents the differential equation to be integrated.
+        :param time_steps: The time steps over which to perform the integration.
+        :return: The solution of the ODE after performing the numerical integration.
+        """
+        pass
 
+    @overload
     def integrate(self, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
+        """
+        Integrate the ordinary differential equation (ODE) over a specified horizon given as a sequence of timestamps.
+
+        :param ode: The ODE that represents the differential equation to be integrated.
+        :param timestamps: The timestamps over which to perform the integration.
+        :return: The solution of the ODE after performing the numerical integration.
+        """
+        pass
+
+    def integrate(self, ode, time):
         pass
 
 
 class ForwardEuler:
     r"""
     The Euler method for numerical integration.
 
@@ -108,18 +154,18 @@
 
     :::{seealso}
     [Wikipedia: Euler method](wiki:Euler_method)
     :::
     """
 
     @classmethod
-    def integrate(cls, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
+    def integrate(cls, ode: OrdinaryDifferentialEquation, time: Union[Quantity, DatetimeIndex]) -> Equation:
         LOGGER.debug("Integrating {} with {}.", ode.name, cls.__name__)
 
-        time_intervals = datetime_index_to_intervals(timestamps)
+        time_intervals = any_to_intervals(time)
         time_derivative = ode.function(ode.state_variable[1:], ode.input_variable)
         next_state = ode.state_variable[:-1] + time_intervals * time_derivative
         return Equation(name=ode.name, lhs=ode.state_variable[1:], rhs=next_state)
 
 
 class RungeKutta4:
     r"""
@@ -148,18 +194,18 @@
 
     :::{seealso}
     [Wikipedia: RungeKutta methods](wiki:RungeKutta_methods)
     :::
     """
 
     @classmethod
-    def integrate(cls, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
+    def integrate(cls, ode: OrdinaryDifferentialEquation, time: Union[Quantity, DatetimeIndex]) -> Equation:
         LOGGER.debug("Integrating {} with {}.", ode.name, cls.__name__)
 
-        time_intervals = datetime_index_to_intervals(timestamps)
+        time_intervals = any_to_intervals(time)
         k1 = ode.function(ode.state_variable[1:], ode.input_variable)
         k2 = ode.function(ode.state_variable[1:] + time_intervals / 2 * k1, ode.input_variable)
         k3 = ode.function(ode.state_variable[1:] + time_intervals / 2 * k2, ode.input_variable)
         k4 = ode.function(ode.state_variable[1:] + time_intervals * k3, ode.input_variable)
         next_state = ode.state_variable[1:] + time_intervals / 6 * (k1 + 2 * k2 + 2 * k3 + k4)
 
         return Equation(name=ode.name, lhs=ode.state_variable[1:], rhs=next_state)
```

## optool/optimization/problem.py

```diff
@@ -6,22 +6,22 @@
 complex problem structures, thereby simplifying the optimization process.
 """
 
 import io
 import re
 from abc import ABC, abstractmethod
 from contextlib import redirect_stdout
-from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union, final
+from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union, final, TYPE_CHECKING
 
 import casadi
 import numpy as np
 from pydantic import StrictFloat, StrictInt, StrictStr, validate_arguments, validator
 
 from optool.core import BaseModel
-from optool.fields.containers import conlist
+from optool.fields.containers import ConstrainedMutatingList
 from optool.fields.numeric import ImmutableArray
 from optool.fields.quantities import QuantityLike, UnitLike
 from optool.fields.symbolic import CasadiColumn, CasadiScalar
 from optool.fields.util import validate, validate_each
 from optool.logging import LOGGER
 from optool.math import is_symbolic, num_elements
 from optool.optimization.constraints import BoxConstraint, Equation, ExpressionConstraint, OptimizationConstraint
@@ -62,37 +62,56 @@
         return casadi.nlpsol(*inputs)
     elif formulation == "QP":
         return casadi.qpsol(*inputs)
     else:
         raise ValueError(f"Unknown type '{formulation}'. Use either 'NLP' or 'QP'.")
 
 
-class ProblemElements(BaseModel):
-    """
-    Container for storing and managing the variables and constraints of an optimization problem.
-
-    :param variables: A list of variables of the optimization problem.
-    :param constraints: A list of constraints of the optimization problem.
-    """
+if TYPE_CHECKING:
+    VariableList = list
+    ConstraintList = list
+else:
 
-    @staticmethod
-    def _is_distinct(values):
+    def _is_distinct(_, values):  # First parameter is the ConstrainedMutatingList itself
         """
         Validates if all elements in the list have distinct names.
 
         :param values: A list of elements.
         :return: The input list if validation is successful.
         """
         names = [element.name for element in values]
         validate(names, len(names) == len(set(names)), 'All names must be distinct, but have {value}.')
         return values
 
-    variables: conlist(OptimizationVariable, custom=_is_distinct) = []  # type:ignore[valid-type]
+    class VariableList(ConstrainedMutatingList[OptimizationVariable]):
+        """
+        List that accepts only elements of type {py:class}`~optool.optimization.variables.OptimizationVariable`, the
+        names of which must be distinct.
+        """
+        custom_validators = _is_distinct
+
+    class ConstraintList(ConstrainedMutatingList[OptimizationConstraint]):
+        """
+        List that accepts only elements of type {py:class}`~optool.optimization.constraints.OptimizationConstraint`, the
+        names of which must be distinct.
+        """
+        custom_validators = _is_distinct
+
+
+class ProblemElements(BaseModel):
+    """
+    Container for storing and managing the variables and constraints of an optimization problem.
+
+    :param variables: A list of variables of the optimization problem.
+    :param constraints: A list of constraints of the optimization problem.
+    """
+
+    variables: VariableList = []
     """A list of variables of the optimization problem."""
-    constraints: conlist(OptimizationConstraint, custom=_is_distinct) = []  # type:ignore[valid-type]
+    constraints: ConstraintList = []
     """A list of constraints of the optimization problem."""
 
     def has_variable(self, name: str) -> bool:
         """
         Checks if a variable with the specified name exists in the problem.
 
         :param name: Name of the variable.
@@ -427,20 +446,20 @@
 
     def _get_constraints(self, constraint_type: Type[_T]) -> List[_T]:
         return list(filter(lambda x: isinstance(x, constraint_type), self.constraints))
 
     def _get_normed_replicated_variable_values(self, field_name: str) -> np.ndarray:
         normed_values = [getattr(val, field_name) / val.nominal_values for val in self.variables]
         dimensionless_values = [val.m_as(UNITS.dimensionless) for val in normed_values]
-        return np.concatenate(dimensionless_values)  # type: ignore
+        return np.concatenate(dimensionless_values)
 
     def _get_normed_replicated_constraint_bounds(self, method_name: str):
         box_constraints = self._get_constraints(BoxConstraint)
         dimensionless_values = [getattr(val, method_name)() for val in box_constraints]
-        return np.concatenate(dimensionless_values)  # type: ignore
+        return np.concatenate(dimensionless_values)
 
     @validate_arguments
     def new_variable(self, name: StrictStr, n: StrictInt, unit: Optional[UnitLike] = None) -> CasadiVariable:
         variable = OptimizationVariable.casadi(name, n, unit)
         self.variables.append(variable)
         return variable
```

## optool/optimization/variables.py

```diff
@@ -1,14 +1,13 @@
 """
 Representation of decision variables for Nonlinear Programs (NLP)s.
 
-This module offers utilities for defining and manipulating decision variables in an NLP formulation.
-With functions to handle the initialization, updates, and querying of variable sets, this module streamlines the
-creation and management of decision variables, providing a flexible and intuitive interface for designing numerical
-optimization problems.
+This module offers utilities for defining and manipulating decision variables in an NLP formulation. With functions to
+handle the initialization, updates, and querying of variable sets, this module streamlines the creation and management
+of decision variables, providing a flexible and intuitive interface for designing numerical optimization problems.
 """
 
 import math
 from abc import ABC, abstractmethod
 from typing import Any, Dict, Optional, Union, final
 
 import casadi
@@ -25,15 +24,19 @@
 from optool.uom import UNITS, Quantity, Unit
 
 
 class OptimizationVariable(BaseModel, ABC):
     """Abstract representation of a decision variable for the formulation of a nonlinear program (NLP)."""
 
     _frozen_nominal_values: StrictBool = False
-    """Indicator flag to ensure that nominal values cannot be changed anymore. Shouldn't be set manually."""
+    """
+    Indicator flag to ensure that nominal values cannot be changed anymore.
+
+    Shouldn't be set manually.
+    """
 
     name: NonEmptyStr
     """The name of the decision variable."""
 
     unit: Optional[UnitLike] = None
     """The physical unit of the decision variable."""
```

## optool/serialization/__init__.py

```diff
@@ -140,16 +140,16 @@
         return _encode_obj
 
     @classmethod
     def json_loader(cls, raw: Union[str, bytes]) -> Any:
         """
         Loads a JSON object from a string or bytes and parses it into a Python object.
 
-        This method uses the custom object pair hook function defined in this class to parse the
-        JSON object, providing flexibility in how the JSON object is converted to a Python object.
+        This method uses the custom object pair hook function defined in this class to parse the JSON object, providing
+        flexibility in how the JSON object is converted to a Python object.
 
         :param raw: The JSON object to be loaded, represented as a string or bytes.
         :return: The Python object resulting from parsing the JSON object.
         """
         return json.loads(raw, object_pairs_hook=cls._parse_raw)
 
     @classmethod
```

## Comparing `optool/fields/series.py` & `optool/fields/tabular.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,60 +1,79 @@
 """
-Pydantic-compatible field types for [Pandas](https://pypi.org/project/pandas/) series objects.
+Pydantic-compatible field types for [Pandas](https://pypi.org/project/pandas/) series and dataframe objects.
 
 This module contains classes that provide Pydantic-compatible field types specifically tailored for Pandas series
-objects.
+and dataframe objects.
 These custom fields allow developers to enforce specific data types including their physical dimensionality and the
 index type.
 """
 
 from __future__ import annotations
 
+import inspect
 from numbers import Number
-from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Optional, Sequence, Type, TypeVar, cast
+from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Optional, Sequence, Type, TypeVar, cast, Dict, Union
 
 import numpy as np
 import pandas as pd
 import pydantic
-from pandas import DatetimeIndex, Index, TimedeltaIndex
+from pandas import Index
 from pint_pandas import PintArray
 from pydantic import ValidationError
 from pydantic.fields import ModelField
 
 from optool.fields.util import (WrongTypeError, check_sub_fields_level, get_subfield_schema, get_type_validator,
                                 update_object_schema)
 from optool.uom import PhysicalDimension, Quantity
 
 T = TypeVar("T")
 """Type variable without an upper bound, specifying the type of the data of {py:class}`pandas.Series`."""
 
+_PANDAS_INDEX_TYPES: Dict[str, Type[pd.Index]] = dict(
+    inspect.getmembers(pd, lambda member: inspect.isclass(member) and issubclass(member, pd.Index)))
+
+_R = TypeVar("_R", pd.Series, pd.DataFrame)
+
+
+def _validate_index_type(val: _R, expected_index_type_name: Optional[str]) -> _R:
+    if expected_index_type_name is None:
+        return val
+
+    if (expected_index_type := _PANDAS_INDEX_TYPES.get(expected_index_type_name, None)) is None:
+        raise ValueError(f"The index type specified ({expected_index_type_name}) is not a valid pandas index. "
+                         f"Valid index types are {list(_PANDAS_INDEX_TYPES.keys())}.")
+
+    if isinstance(val.index, expected_index_type):
+        return val
+    raise IndexTypeError(expected=expected_index_type, value=val)
+
 
 class ConstrainedSeries(pydantic.BaseModel, Generic[T]):
     """
     Pydantic-compatible field type for {py:class}`pandas.Series` objects, which allows to specify the data-type.
 
     :::{seealso}
     [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
     {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`
     :::
     """
 
     strict: ClassVar[bool] = True
-    index_type: ClassVar[Type[Index]] = pd.RangeIndex
+    index_type: ClassVar[Optional[str]] = 'RangeIndex'
 
     @classmethod
     def __get_validators__(cls):
         yield get_type_validator(pd.Series) if cls.strict else cls.validate_series
         yield cls.validate_index_type
         yield cls.validate_dimensionality
         yield cls.validate_data_type
 
     @classmethod
     def __modify_schema__(cls, field_schema, field: Optional[ModelField]):
-        update_object_schema(field_schema, index_type=cls.index_type.__name__, datatype=get_subfield_schema(field, 0))
+        update_object_schema(field_schema, index_type=cls.index_type, datatype=get_subfield_schema(field, 0))
 
     @classmethod
     def validate_series(cls, val: Any, field: ModelField) -> pd.Series:
         if isinstance(val, pd.Series):
             return val
         if not field.sub_fields:
             return pd.Series(val)
@@ -96,17 +115,15 @@
                 raise ValidationError([error], cls)
             valid_value.append(valid_element)
 
         return pd.Series(valid_value, index=index, dtype=data_type)
 
     @classmethod
     def validate_index_type(cls, val: pd.Series, field: ModelField) -> pd.Series:
-        if cls.index_type is None or isinstance(val.index, cls.index_type):
-            return val
-        raise IndexTypeError(expected=cls.index_type, value=val)
+        return _validate_index_type(val, cls.index_type)
 
     @classmethod
     def validate_dimensionality(cls, val: pd.Series, field: ModelField) -> pd.Series:
         if not field.sub_fields or not cls._is_physical_dimension(field.sub_fields[0]):
             return val
 
         if not isinstance(val.array, PintArray):
@@ -136,83 +153,152 @@
         return val
 
     @classmethod
     def _is_physical_dimension(cls, field: ModelField) -> bool:
         return field.type_ == Any or issubclass(field.type_, PhysicalDimension)
 
 
-class SeriesLike(ConstrainedSeries[T]):
-    """
-    Pydantic-compatible field type for {py:class}`pandas.Series` objects.
-
-    Assigned values not already of type {py:class}`~pandas.Series` are parsed using
-    {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic type
-    annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is verified. If the
-    generic type annotation is different, the validation and parsing of each data element is forwarded to the specific
-    type specified, e.g., in the example above, the value is validated using the implementation provided by
-    {py:class}`pydantic.PositiveInt`.
-    """
-    strict = False
-
-
-class DatetimeSeries(ConstrainedSeries[T]):
+class ConstrainedDataFrame:
     """
-    Pydantic-compatible field type for {py:class}`pandas.Series` objects, the index of which must be of type
-    {py:class}`~pandas.DatetimeIndex`.
+    Pydantic-compatible field type for {py:class}`pandas.DataFrame` objects, which allows to specify the index type.
 
-    Assigned values not already of type {py:class}`~pandas.Series` are parsed using
-    {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic type
-    annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is verified. If the
-    generic type annotation is different, the validation and parsing of each data element is forwarded to the specific
-    type specified, e.g., in the example above, the value is validated using the implementation provided by
-    {py:class}`pydantic.PositiveInt`.
+    :::{seealso}
+    [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
+    {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`
+    :::
     """
-    strict = False
-    index_type = DatetimeIndex
 
+    strict: bool = True
+    index_type: ClassVar[Optional[str]] = 'RangeIndex'
 
-class TimedeltaSeries(ConstrainedSeries[T]):
-    """
-    Pydantic-compatible field type for {py:class}`pandas.Series` objects, the index of which must be of type
-    {py:class}`~pandas.TimedeltaIndex`.
+    @classmethod
+    def __get_validators__(cls):
+        yield get_type_validator(pd.DataFrame) if cls.strict else cls.validate_dataframe
+        yield cls.validate_index_type
 
-    Assigned values not already of type {py:class}`~pandas.Series` are parsed using
-    {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic type
-    annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is verified. If the
-    generic type annotation is different, the validation and parsing of each data element is forwarded to the specific
-    type specified, e.g., in the example above, the value is validated using the implementation provided by
-    {py:class}`pydantic.PositiveInt`.
-    """
-    strict = False
-    index_type = TimedeltaIndex
+    @classmethod
+    def __modify_schema__(cls, field_schema, field: Optional[ModelField]):
+        update_object_schema(field_schema, index_type=cls.index_type)
 
+    @classmethod
+    def validate_dataframe(cls, val: Any, field: ModelField) -> pd.DataFrame:
+        if isinstance(val, pd.DataFrame):
+            return val
+        if field.sub_fields:
+            raise TypeError(f"A constrained DataFrame cannot by typed, but have sub-fields {field.sub_fields}")
 
-if TYPE_CHECKING:
-    from typing_extensions import TypeAlias
+        return pd.DataFrame(val)
 
-    SeriesLike: TypeAlias = pd.Series  # type: ignore[no-redef] # noqa: F811
-    DatetimeSeries: TypeAlias = pd.Series  # type: ignore[no-redef] # noqa: F811
-    TimedeltaSeries: TypeAlias = pd.Series  # type: ignore[no-redef] # noqa: F811
+    @classmethod
+    def validate_index_type(cls, val: pd.DataFrame, field: ModelField) -> pd.DataFrame:
+        return _validate_index_type(val, cls.index_type)
 
 
 class IndexTypeError(ValueError):
     """
-    Raised when the type of index of a {py:class}`pandas.Series` does not meet the expectations.
+    Raised when the type of index of a {py:class}`pandas.Series` or {py:class}`pandas.DataFrame` object does not meet
+    the expectations.
 
     :param expected: The expected type of the index.
-    :param value: The series that causes the error due to its index type.
+    :param value: The series or dataframe that causes the error due to its index type.
     """
 
-    def __init__(self, *, expected: Type[Index], value: pd.Series) -> None:
-        super().__init__(f"expected index type {expected}, but got a series with index type {type(value.index)}")
+    def __init__(self, *, expected: Type[Index], value: Union[pd.Series, pd.DataFrame]) -> None:
+        type_name = value.__class__.__name__
+        super().__init__(f"expected index type {expected}, but got a {type_name} with index type {type(value.index)}")
 
 
 class DimensionalityError(ValueError):
     """
     Raised when the dimensionality of a {py:class}`pandas.Series` does not meet the expectations.
 
     :param expected: The expected dimensionality.
     :param value: The series that causes the error due to its dimensionality.
     """
 
     def __init__(self, *, expected: str, value: pd.Series) -> None:
         super().__init__(f"expected the dimensionality {expected}, but got a series with data-type {value.dtype}")
+
+
+if TYPE_CHECKING:
+    SeriesLike = pd.Series
+    DatetimeSeries = pd.Series
+    TimedeltaSeries = pd.Series
+
+    DataFrameLike = pd.DataFrame
+    DatetimeDataFrame = pd.DataFrame
+    TimedeltaDataFrame = pd.DataFrame
+else:
+
+    class SeriesLike(ConstrainedSeries[T]):
+        """
+        Pydantic-compatible field type for {py:class}`pandas.Series` objects.
+
+        Assigned values not already of type {py:class}`~pandas.Series` are parsed using
+        {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic
+        type annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is
+        verified. If the generic type annotation is different, the validation and parsing of each data element is
+        forwarded to the specific type specified, e.g., in the example above, the value is validated using the
+        implementation provided by {py:class}`pydantic.PositiveInt`.
+        """
+        strict = False
+
+    class DatetimeSeries(ConstrainedSeries[T]):
+        """
+        Pydantic-compatible field type for {py:class}`pandas.Series` objects, the index of which must be of type
+        {py:class}`~pandas.DatetimeIndex`.
+
+        Assigned values not already of type {py:class}`~pandas.Series` are parsed using
+        {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic
+        type annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is
+        verified. If the generic type annotation is different, the validation and parsing of each data element is
+        forwarded to the specific type specified, e.g., in the example above, the value is validated using the
+        implementation provided by {py:class}`pydantic.PositiveInt`.
+        """
+        strict = False
+        index_type = 'DatetimeIndex'
+
+    class TimedeltaSeries(ConstrainedSeries[T]):
+        """
+        Pydantic-compatible field type for {py:class}`pandas.Series` objects, the index of which must be of type
+        {py:class}`~pandas.TimedeltaIndex`.
+
+        Assigned values not already of type {py:class}`~pandas.Series` are parsed using
+        {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic
+        type annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is
+        verified. If the generic type annotation is different, the validation and parsing of each data element is
+        forwarded to the specific type specified, e.g., in the example above, the value is validated using the
+        implementation provided by {py:class}`pydantic.PositiveInt`.
+        """
+        strict = False
+        index_type = 'TimedeltaIndex'
+
+    class DataFrameLike(ConstrainedDataFrame):
+        """
+        Pydantic-compatible field type for {py:class}`~pandas.DataFrame` objects.
+
+        Assigned values not already of type {py:class}`~pandas.DataFrame` are parsed using the regular constructor
+        {py:class}`DataFrame(val) <pandas.DataFrame>`.
+        """
+        strict = False
+
+    class DatetimeDataFrame(ConstrainedDataFrame):
+        """
+        Pydantic-compatible field type for {py:class}`~pandas.DataFrame` objects, the index of which must be of type
+        {py:class}`~pandas.DatetimeIndex`.
+
+        Assigned values not already of type {py:class}`~pandas.DataFrame` are parsed using the regular constructor
+        {py:class}`DataFrame(val) <pandas.DataFrame>`.
+        """
+        strict = False
+        index_type = 'DatetimeIndex'
+
+    class TimedeltaDataFrame(ConstrainedDataFrame):
+        """
+        Pydantic-compatible field type for {py:class}`~pandas.DataFrame` objects, the index of which must be of type
+        {py:class}`~pandas.TimedeltaIndex`.
+
+        Assigned values not already of type {py:class}`~pandas.DataFrame` are parsed using the regular constructor
+        {py:class}`DataFrame(val) <pandas.DataFrame>`.
+        """
+        strict = False
+        index_type = 'TimedeltaIndex'
```

## Comparing `optool-0.6.0.dist-info/LICENSE.txt` & `optool-0.7.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `optool-0.6.0.dist-info/RECORD` & `optool-0.7.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,36 @@
-optool/VERSION,sha256=l6XW5UCmEg0Jw53bZn4Ojiusf8wv_vgTuC4I_WA2W84,6
+optool/VERSION,sha256=ln2a-xATRmZxZvLnboGRC8GQSI19QdUMoAcunZLwDjI,6
 optool/__init__.py,sha256=su-p6dH_StDid9r8jiyHVOupEzhXhO0pD3VayuKZte8,1929
-optool/conversions.py,sha256=kryQ3PIEgIBqCBl3GgPrrcWMgVBR_3o-JOZYXlMsncQ,2191
+optool/conversions.py,sha256=7k5pQo1dITAvNRu1c8Q5An2mdpt4nI0UvKFFBTHsTBk,4577
 optool/core.py,sha256=OkLG3hOhEUBJZQ5bYX5sh0j3WOQ_8g3OyD8-5M1YXkA,6459
 optool/languages.py,sha256=h0IhjTnZ7L7bPFFbhVmaQN960phrXbOt5eon0IB10oY,2670
-optool/logging.py,sha256=FD6yqPAT2XdAK-ETl7w5aQweoVHBhaHrN77HcwPKcWY,9554
-optool/math.py,sha256=8hHg6BXNp6bbFeE5gIRb-__ZVTRmWQXUdhK2maZU-58,10277
-optool/parallel.py,sha256=S8PsUvSDPvUsrh8D8OI52HaRM00xDusB0B2cdBbfgWU,4180
+optool/logging.py,sha256=K0hGi3jQSgZXSspSKMiChwKCEQxZgChRzflReHaTWnQ,9585
+optool/math.py,sha256=5qO5vcJd1UWe_4yVCcXm2Xrp4A5E9Ty22b9hHm2E4rI,10255
+optool/parallel.py,sha256=CTXnhnG2PN80wPRaH9DEKjEjUf4ufb2tgtJJuVMer6Y,4161
 optool/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 optool/uom.py,sha256=CcqNS3Yq61SwYtGMKxwmeYP_cJv1ZLcLIRQR03tjLxc,32727
 optool/util.py,sha256=bgNco_mgCjfnC3ppoa1A61_YUC5yH6lvcXRC6XBLdkU,1618
-optool/fields/__init__.py,sha256=qBtAfZKOr4VE14R-ZMhHcek4uuEH7gWQ1Obtoa3gLxo,628
-optool/fields/callables.py,sha256=oaHE36UMU3dKz-1gudA76PKkweXBzMpODFxg3A-X9oQ,6206
-optool/fields/containers.py,sha256=c4JPgWnDy1Vgpxv4dCh8-tw-PbPgVKbcrDjmHNSttUA,5786
-optool/fields/dataframe.py,sha256=4VR89l-TT2HAs6n5ZOZeqFHFipPth9ptMF1Z9X3m8Zw,3937
-optool/fields/misc.py,sha256=-cowM9ZF6YE-O0TP4TVZwmujuF2FoMMAj4IhvGMIlnY,2456
-optool/fields/numeric.py,sha256=d8IUg3gsJ3q1hbu1JMsjD2UrfCorc7G0WqFJkvgbohY,12294
-optool/fields/quantities.py,sha256=z3NTFvTJovJkYCTmuk_LSsCf3QD1DqquzWoMxzazAxw,9788
-optool/fields/series.py,sha256=yiELcUahjiPbtGJp0Ls4pAv61K5G6JotAxNbo8iXxKY,9218
-optool/fields/symbolic.py,sha256=1f5faPx29FVQrN9ooM078d9d9Igm8h-zVzjLqqt6zQk,3724
-optool/fields/util.py,sha256=ORjvwyUmarfqoCac6rFpNlL_61AMbtZFJsotU6BjXCs,11127
-optool/optimization/__init__.py,sha256=EtHlu2Q0SOjHT_kjgZ9n70FvfOwR6f63-pIbqzpR8Jg,347
+optool/fields/__init__.py,sha256=tF5pP1L4VghtN37WmX2Na7vQ3DbmPB3htLlQ2_EbIx4,629
+optool/fields/callables.py,sha256=HL54luF8gCkr74-yq7PhlO3hTH2-wIaZ0T1p_q5oM8Q,6206
+optool/fields/containers.py,sha256=BNaE43dqcj8VsWNRJCgjVNQzdQ8tB_J4gtkJOocpTgg,5789
+optool/fields/misc.py,sha256=VbzXIteW_Qz6UI2_9JjaHJlIQZ0OpFY_TPRUSmVFusM,2221
+optool/fields/numeric.py,sha256=CTdQ03_jeQrifAWNdoNUg4xHt4wX2se9EKB9a3YJVXY,11571
+optool/fields/quantities.py,sha256=vbNrvk7unqFitF1kyzgzOw7jafUq_wdFa28euSjKLqg,9783
+optool/fields/symbolic.py,sha256=oiwEf3441OlxBBF9aYYUy-KYqQXB6T1hUnMg1YlO6Ws,3537
+optool/fields/tabular.py,sha256=Dd9bE67i3PBLFTJpX6h7R9nC3xPZdNHe9p5emLq88HQ,12687
+optool/fields/util.py,sha256=yBU4W9yw6hqTY2l9CeodQCQiTIMUypQRhSz_OQnowao,11128
+optool/optimization/__init__.py,sha256=WmYfWzk6JtB9aHuYCq7zvZfL74oVMPDjf6pMtNYaYXo,347
 optool/optimization/constraints.py,sha256=tJeiSBQWJEwcD5Xcmc_XVcNqDD6Yk9SGCStt6BETmCk,5772
-optool/optimization/helpers.py,sha256=KTWWUSjA2r8iVDc_7b67TKDVMgwUCDbjtBd7DXhIA8Q,5882
-optool/optimization/ode.py,sha256=OA2EK8RXmQ-69mOQy6Itg0nbAvI3mi05YDKlN5aDfKQ,7125
-optool/optimization/problem.py,sha256=K8pJi5wjdmR6UEzSe8ATstTKb0ct_SJw4lCPSZfsZ7c,25679
-optool/optimization/variables.py,sha256=6THq0l9nEbvMJXfqFZ30KWoZtLgQapaeIFZdzC7lHJk,8812
-optool/serialization/__init__.py,sha256=yUvmOZGj-_pTqyBzoQhIRp1PeN29hee-wYePW1r04x0,6382
+optool/optimization/helpers.py,sha256=e-kyv3joOMevVJYpwVqzTXcR3aFI91xsvFTd5EnIXHg,5882
+optool/optimization/ode.py,sha256=M_GgIt4x61qbKenvbK5Ohe40Q_J_F4d8xZxle5IWkh4,9240
+optool/optimization/problem.py,sha256=rNG6bynFvO6ln_42SoauDzfEty6lh_-dYL1WPk-dOkM,26264
+optool/optimization/variables.py,sha256=dYsqjI3gc93nDV3rhTIfsAY29IwbmE4GAmxJoCsFn-M,8827
+optool/serialization/__init__.py,sha256=4iqoSli3IBfqOZEbOy7shP8eFLa54FbwfYW2NnGpquc,6382
 optool/serialization/datetime_objects.py,sha256=60bxDqmJUoGFROPMirQbmFetqaL06nuAwGNsdvsW4_Q,1610
 optool/serialization/numpy_objects.py,sha256=e9P9Dg5Nkv_UDLwjG1KKjxNtPVE9ruzcQ6XZXozigw4,832
 optool/serialization/pandas_objects.py,sha256=5k3LGti3qtf3LdkZoMyRgFhARRqTJeWEz5mq7XUQ6AU,3396
 optool/serialization/pint_objects.py,sha256=gSKF3GwnUqSUVt8JK4VtxKAbH7DLepei_jCNXrpirek,1643
-optool-0.6.0.dist-info/LICENSE.txt,sha256=hrghB2ojre3BR1nxeXTWOsbhc7CzFIRaC2r1Ez1gV10,1081
-optool-0.6.0.dist-info/METADATA,sha256=_yRax25b6vNl9sWZ48V3eJAUBCpaE6QWEmfHPD_o0VU,4190
-optool-0.6.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-optool-0.6.0.dist-info/top_level.txt,sha256=rBfjZiBTokmEUSgWAvNw23rTh-MvuILbz6o2LeWprko,7
-optool-0.6.0.dist-info/RECORD,,
+optool-0.7.0.dist-info/LICENSE.txt,sha256=hrghB2ojre3BR1nxeXTWOsbhc7CzFIRaC2r1Ez1gV10,1081
+optool-0.7.0.dist-info/METADATA,sha256=d-5Q-RlZHAbG1mlpuze2V1NtYIMqqOnNhkeXd6JN7mY,7897
+optool-0.7.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+optool-0.7.0.dist-info/top_level.txt,sha256=rBfjZiBTokmEUSgWAvNw23rTh-MvuILbz6o2LeWprko,7
+optool-0.7.0.dist-info/RECORD,,
```

