# Comparing `tmp/span_marker-1.2.2.tar.gz` & `tmp/span_marker-1.2.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "span_marker-1.2.2.tar", last modified: Tue Jun 20 09:28:22 2023, max compression
+gzip compressed data, was "span_marker-1.2.3.tar", last modified: Tue Jun 20 10:23:50 2023, max compression
```

## Comparing `span_marker-1.2.2.tar` & `span_marker-1.2.3.tar`

### file list

```diff
@@ -1,32 +1,32 @@
-drwxrwxrwx   0        0        0        0 2023-06-20 09:28:22.535627 span_marker-1.2.2/
--rw-rw-rw-   0        0        0    11558 2023-05-04 23:33:40.000000 span_marker-1.2.2/LICENSE
--rw-rw-rw-   0        0        0    15023 2023-06-20 09:28:22.535627 span_marker-1.2.2/PKG-INFO
--rw-rw-rw-   0        0        0    14455 2023-06-20 09:23:03.000000 span_marker-1.2.2/README.md
--rw-rw-rw-   0        0        0     2521 2023-06-20 09:21:34.000000 span_marker-1.2.2/pyproject.toml
--rw-rw-rw-   0        0        0       42 2023-06-20 09:28:22.535627 span_marker-1.2.2/setup.cfg
--rw-rw-rw-   0        0        0      140 2023-06-20 09:21:47.000000 span_marker-1.2.2/setup.py
-drwxrwxrwx   0        0        0        0 2023-06-20 09:28:22.522716 span_marker-1.2.2/span_marker/
--rw-rw-rw-   0        0        0     1659 2023-06-20 09:27:28.000000 span_marker-1.2.2/span_marker/__init__.py
--rw-rw-rw-   0        0        0     7070 2023-06-15 09:34:01.000000 span_marker-1.2.2/span_marker/configuration.py
--rw-rw-rw-   0        0        0     6630 2023-06-13 22:30:33.000000 span_marker-1.2.2/span_marker/data_collator.py
--rw-rw-rw-   0        0        0     5276 2023-06-15 14:37:15.000000 span_marker-1.2.2/span_marker/evaluation.py
--rw-rw-rw-   0        0        0     5230 2023-05-31 13:34:45.000000 span_marker-1.2.2/span_marker/label_normalizer.py
--rw-rw-rw-   0        0        0     2472 2023-06-13 22:30:43.000000 span_marker-1.2.2/span_marker/model_card.py
--rw-rw-rw-   0        0        0    31577 2023-06-20 07:05:23.000000 span_marker-1.2.2/span_marker/modeling.py
--rw-rw-rw-   0        0        0     2221 2023-06-13 22:30:33.000000 span_marker-1.2.2/span_marker/output.py
--rw-rw-rw-   0        0        0     3902 2023-06-20 09:24:39.000000 span_marker-1.2.2/span_marker/spacy_integration.py
--rw-rw-rw-   0        0        0    11828 2023-06-19 13:54:52.000000 span_marker-1.2.2/span_marker/tokenizer.py
--rw-rw-rw-   0        0        0    21019 2023-06-15 10:02:14.000000 span_marker-1.2.2/span_marker/trainer.py
-drwxrwxrwx   0        0        0        0 2023-06-20 09:28:22.532121 span_marker-1.2.2/span_marker.egg-info/
--rw-rw-rw-   0        0        0    15023 2023-06-20 09:28:22.000000 span_marker-1.2.2/span_marker.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0      676 2023-06-20 09:28:22.000000 span_marker-1.2.2/span_marker.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-06-20 09:28:22.000000 span_marker-1.2.2/span_marker.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0       80 2023-06-20 09:28:22.000000 span_marker-1.2.2/span_marker.egg-info/entry_points.txt
--rw-rw-rw-   0        0        0      267 2023-06-20 09:28:22.000000 span_marker-1.2.2/span_marker.egg-info/requires.txt
--rw-rw-rw-   0        0        0       12 2023-06-20 09:28:22.000000 span_marker-1.2.2/span_marker.egg-info/top_level.txt
-drwxrwxrwx   0        0        0        0 2023-06-20 09:28:22.534624 span_marker-1.2.2/tests/
--rw-rw-rw-   0        0        0     1645 2023-06-10 13:15:43.000000 span_marker-1.2.2/tests/test_configuration.py
--rw-rw-rw-   0        0        0     1410 2023-06-01 07:33:08.000000 span_marker-1.2.2/tests/test_model_card.py
--rw-rw-rw-   0        0        0     9228 2023-06-15 09:34:01.000000 span_marker-1.2.2/tests/test_modeling.py
--rw-rw-rw-   0        0        0     1165 2023-06-20 09:23:58.000000 span_marker-1.2.2/tests/test_spacy_integration.py
--rw-rw-rw-   0        0        0    10372 2023-06-15 09:34:01.000000 span_marker-1.2.2/tests/test_trainer.py
+drwxrwxrwx   0        0        0        0 2023-06-20 10:23:50.975869 span_marker-1.2.3/
+-rw-rw-rw-   0        0        0    11558 2023-05-04 23:33:40.000000 span_marker-1.2.3/LICENSE
+-rw-rw-rw-   0        0        0    15586 2023-06-20 10:23:50.975869 span_marker-1.2.3/PKG-INFO
+-rw-rw-rw-   0        0        0    15018 2023-06-20 09:31:29.000000 span_marker-1.2.3/README.md
+-rw-rw-rw-   0        0        0     2521 2023-06-20 09:21:34.000000 span_marker-1.2.3/pyproject.toml
+-rw-rw-rw-   0        0        0       42 2023-06-20 10:23:50.975869 span_marker-1.2.3/setup.cfg
+-rw-rw-rw-   0        0        0      140 2023-06-20 09:21:47.000000 span_marker-1.2.3/setup.py
+drwxrwxrwx   0        0        0        0 2023-06-20 10:23:50.954844 span_marker-1.2.3/span_marker/
+-rw-rw-rw-   0        0        0     1659 2023-06-20 10:22:50.000000 span_marker-1.2.3/span_marker/__init__.py
+-rw-rw-rw-   0        0        0     7070 2023-06-15 09:34:01.000000 span_marker-1.2.3/span_marker/configuration.py
+-rw-rw-rw-   0        0        0     6630 2023-06-13 22:30:33.000000 span_marker-1.2.3/span_marker/data_collator.py
+-rw-rw-rw-   0        0        0     5276 2023-06-15 14:37:15.000000 span_marker-1.2.3/span_marker/evaluation.py
+-rw-rw-rw-   0        0        0     5230 2023-05-31 13:34:45.000000 span_marker-1.2.3/span_marker/label_normalizer.py
+-rw-rw-rw-   0        0        0     2472 2023-06-13 22:30:43.000000 span_marker-1.2.3/span_marker/model_card.py
+-rw-rw-rw-   0        0        0    31577 2023-06-20 07:05:23.000000 span_marker-1.2.3/span_marker/modeling.py
+-rw-rw-rw-   0        0        0     2221 2023-06-13 22:30:33.000000 span_marker-1.2.3/span_marker/output.py
+-rw-rw-rw-   0        0        0     3932 2023-06-20 10:16:09.000000 span_marker-1.2.3/span_marker/spacy_integration.py
+-rw-rw-rw-   0        0        0    11828 2023-06-19 13:54:52.000000 span_marker-1.2.3/span_marker/tokenizer.py
+-rw-rw-rw-   0        0        0    21019 2023-06-15 10:02:14.000000 span_marker-1.2.3/span_marker/trainer.py
+drwxrwxrwx   0        0        0        0 2023-06-20 10:23:50.971870 span_marker-1.2.3/span_marker.egg-info/
+-rw-rw-rw-   0        0        0    15586 2023-06-20 10:23:50.000000 span_marker-1.2.3/span_marker.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0      676 2023-06-20 10:23:50.000000 span_marker-1.2.3/span_marker.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-06-20 10:23:50.000000 span_marker-1.2.3/span_marker.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0       80 2023-06-20 10:23:50.000000 span_marker-1.2.3/span_marker.egg-info/entry_points.txt
+-rw-rw-rw-   0        0        0      267 2023-06-20 10:23:50.000000 span_marker-1.2.3/span_marker.egg-info/requires.txt
+-rw-rw-rw-   0        0        0       12 2023-06-20 10:23:50.000000 span_marker-1.2.3/span_marker.egg-info/top_level.txt
+drwxrwxrwx   0        0        0        0 2023-06-20 10:23:50.974870 span_marker-1.2.3/tests/
+-rw-rw-rw-   0        0        0     1645 2023-06-10 13:15:43.000000 span_marker-1.2.3/tests/test_configuration.py
+-rw-rw-rw-   0        0        0     1410 2023-06-01 07:33:08.000000 span_marker-1.2.3/tests/test_model_card.py
+-rw-rw-rw-   0        0        0     9228 2023-06-15 09:34:01.000000 span_marker-1.2.3/tests/test_modeling.py
+-rw-rw-rw-   0        0        0     1165 2023-06-20 09:23:58.000000 span_marker-1.2.3/tests/test_spacy_integration.py
+-rw-rw-rw-   0        0        0    10372 2023-06-15 09:34:01.000000 span_marker-1.2.3/tests/test_trainer.py
```

### Comparing `span_marker-1.2.2/LICENSE` & `span_marker-1.2.3/LICENSE`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/PKG-INFO` & `span_marker-1.2.3/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 00000000: 4d65 7461 6461 7461 2d56 6572 7369 6f6e  Metadata-Version
 00000010: 3a20 322e 310d 0a4e 616d 653a 2073 7061  : 2.1..Name: spa
 00000020: 6e5f 6d61 726b 6572 0d0a 5665 7273 696f  n_marker..Versio
-00000030: 6e3a 2031 2e32 2e32 0d0a 5375 6d6d 6172  n: 1.2.2..Summar
+00000030: 6e3a 2031 2e32 2e33 0d0a 5375 6d6d 6172  n: 1.2.3..Summar
 00000040: 793a 204e 616d 6564 2045 6e74 6974 7920  y: Named Entity 
 00000050: 5265 636f 676e 6974 696f 6e20 7573 696e  Recognition usin
 00000060: 6720 5370 616e 204d 6172 6b65 7273 0d0a  g Span Markers..
 00000070: 4175 7468 6f72 3a20 546f 6d20 4161 7273  Author: Tom Aars
 00000080: 656e 0d0a 4d61 696e 7461 696e 6572 3a20  en..Maintainer: 
 00000090: 546f 6d20 4161 7273 656e 0d0a 5072 6f6a  Tom Aarsen..Proj
 000000a0: 6563 742d 5552 4c3a 2044 6f63 756d 656e  ect-URL: Documen
@@ -29,911 +29,947 @@
 000001c0: 436f 6e74 656e 742d 5479 7065 3a20 7465  Content-Type: te
 000001d0: 7874 2f6d 6172 6b64 6f77 6e0d 0a50 726f  xt/markdown..Pro
 000001e0: 7669 6465 732d 4578 7472 613a 2064 6576  vides-Extra: dev
 000001f0: 0d0a 5072 6f76 6964 6573 2d45 7874 7261  ..Provides-Extra
 00000200: 3a20 646f 6373 0d0a 5072 6f76 6964 6573  : docs..Provides
 00000210: 2d45 7874 7261 3a20 7761 6e64 620d 0a4c  -Extra: wandb..L
 00000220: 6963 656e 7365 2d46 696c 653a 204c 4943  icense-File: LIC
-00000230: 454e 5345 0d0a 0d0a 3c68 3120 616c 6967  ENSE....<h1 alig
-00000240: 6e3d 2263 656e 7465 7222 3e0d 0a53 7061  n="center">..Spa
-00000250: 6e4d 6172 6b65 7220 666f 7220 4e61 6d65  nMarker for Name
-00000260: 6420 456e 7469 7479 2052 6563 6f67 6e69  d Entity Recogni
-00000270: 7469 6f6e 0d0a 3c2f 6831 3e0d 0a3c 6469  tion..</h1>..<di
-00000280: 7620 616c 6967 6e3d 2263 656e 7465 7222  v align="center"
-00000290: 3e0d 0a0d 0a5b f09f a497 204d 6f64 656c  >....[.... Model
-000002a0: 735d 2868 7474 7073 3a2f 2f68 7567 6769  s](https://huggi
-000002b0: 6e67 6661 6365 2e63 6f2f 6d6f 6465 6c73  ngface.co/models
-000002c0: 3f6f 7468 6572 3d73 7061 6e2d 6d61 726b  ?other=span-mark
-000002d0: 6572 2920 7c0d 0a5b f09f 9ba0 efb8 8f20  er) |..[....... 
-000002e0: 4765 7474 696e 6720 5374 6172 7465 6420  Getting Started 
-000002f0: 496e 2047 6f6f 676c 6520 436f 6c61 625d  In Google Colab]
-00000300: 2868 7474 7073 3a2f 2f63 6f6c 6162 2e72  (https://colab.r
-00000310: 6573 6561 7263 682e 676f 6f67 6c65 2e63  esearch.google.c
-00000320: 6f6d 2f67 6974 6875 622f 746f 6d61 6172  om/github/tomaar
-00000330: 7365 6e2f 5370 616e 4d61 726b 6572 4e45  sen/SpanMarkerNE
-00000340: 522f 626c 6f62 2f6d 6169 6e2f 6e6f 7465  R/blob/main/note
-00000350: 626f 6f6b 732f 6765 7474 696e 675f 7374  books/getting_st
-00000360: 6172 7465 642e 6970 796e 6229 207c 0d0a  arted.ipynb) |..
-00000370: 5bf0 9f93 8420 446f 6375 6d65 6e74 6174  [.... Documentat
-00000380: 696f 6e5d 2868 7474 7073 3a2f 2f74 6f6d  ion](https://tom
-00000390: 6161 7273 656e 2e67 6974 6875 622e 696f  aarsen.github.io
-000003a0: 2f53 7061 6e4d 6172 6b65 724e 4552 290d  /SpanMarkerNER).
-000003b0: 0a3c 2f64 6976 3e0d 0a0d 0a53 7061 6e4d  .</div>....SpanM
-000003c0: 6172 6b65 7220 6973 2061 2066 7261 6d65  arker is a frame
-000003d0: 776f 726b 2066 6f72 2074 7261 696e 696e  work for trainin
-000003e0: 6720 706f 7765 7266 756c 204e 616d 6564  g powerful Named
-000003f0: 2045 6e74 6974 7920 5265 636f 676e 6974   Entity Recognit
-00000400: 696f 6e20 6d6f 6465 6c73 2075 7369 6e67  ion models using
-00000410: 2066 616d 696c 6961 7220 656e 636f 6465   familiar encode
-00000420: 7273 2073 7563 6820 6173 2042 4552 542c  rs such as BERT,
-00000430: 2052 6f42 4552 5461 2061 6e64 2044 6542   RoBERTa and DeB
-00000440: 4552 5461 2e0d 0a54 6967 6874 6c79 2069  ERTa...Tightly i
-00000450: 6d70 6c65 6d65 6e74 6564 206f 6e20 746f  mplemented on to
-00000460: 7020 6f66 2074 6865 205b f09f a497 2054  p of the [.... T
-00000470: 7261 6e73 666f 726d 6572 735d 2868 7474  ransformers](htt
-00000480: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-00000490: 6875 6767 696e 6766 6163 652f 7472 616e  huggingface/tran
-000004a0: 7366 6f72 6d65 7273 2f29 206c 6962 7261  sformers/) libra
-000004b0: 7279 2c20 5370 616e 4d61 726b 6572 2063  ry, SpanMarker c
-000004c0: 616e 2074 616b 6520 6164 7661 6e74 6167  an take advantag
-000004d0: 6520 6f66 2069 7473 2076 616c 7561 626c  e of its valuabl
-000004e0: 6520 6675 6e63 7469 6f6e 616c 6974 792e  e functionality.
-000004f0: 0d0a 3c21 2d2d 206c 696b 6520 7065 7266  ..<!-- like perf
-00000500: 6f72 6d61 6e63 6520 6461 7368 626f 6172  ormance dashboar
-00000510: 6420 696e 7465 6772 6174 696f 6e2c 2061  d integration, a
-00000520: 7574 6f6d 6174 6963 206d 6978 6564 2070  utomatic mixed p
-00000530: 7265 6369 7369 6f6e 2c20 382d 6269 7420  recision, 8-bit 
-00000540: 696e 6665 7265 6e63 652d 2d3e 0d0a 0d0a  inference-->....
-00000550: 4261 7365 6420 6f6e 2074 6865 205b 504c  Based on the [PL
-00000560: 2d4d 6172 6b65 725d 2868 7474 7073 3a2f  -Marker](https:/
-00000570: 2f61 7278 6976 2e6f 7267 2f70 6466 2f32  /arxiv.org/pdf/2
-00000580: 3130 392e 3036 3036 372e 7064 6629 2070  109.06067.pdf) p
-00000590: 6170 6572 2c20 5370 616e 4d61 726b 6572  aper, SpanMarker
-000005a0: 2062 7265 616b 7320 7468 6520 6d6f 6c64   breaks the mold
-000005b0: 2074 6872 6f75 6768 2069 7473 2061 6363   through its acc
-000005c0: 6573 7369 6269 6c69 7479 2061 6e64 2065  essibility and e
-000005d0: 6173 6520 6f66 2075 7365 2e20 4372 7563  ase of use. Cruc
-000005e0: 6961 6c6c 792c 2053 7061 6e4d 6172 6b65  ially, SpanMarke
-000005f0: 7220 776f 726b 7320 6f75 7420 6f66 2074  r works out of t
-00000600: 6865 2062 6f78 2077 6974 6820 6d61 6e79  he box with many
-00000610: 2063 6f6d 6d6f 6e20 656e 636f 6465 7273   common encoders
-00000620: 2073 7563 6820 6173 2060 6265 7274 2d62   such as `bert-b
-00000630: 6173 652d 6361 7365 6460 2061 6e64 2060  ase-cased` and `
-00000640: 726f 6265 7274 612d 6c61 7267 6560 2c20  roberta-large`, 
-00000650: 616e 6420 6175 746f 6d61 7469 6361 6c6c  and automaticall
-00000660: 7920 776f 726b 7320 7769 7468 2064 6174  y works with dat
-00000670: 6173 6574 7320 7573 696e 6720 7468 6520  asets using the 
-00000680: 6049 4f42 602c 2060 494f 4232 602c 2060  `IOB`, `IOB2`, `
-00000690: 4249 4f45 5360 2c20 6042 494c 4f55 6020  BIOES`, `BILOU` 
-000006a0: 6f72 206e 6f20 6c61 6265 6c20 616e 6e6f  or no label anno
-000006b0: 7461 7469 6f6e 2073 6368 656d 652e 0d0a  tation scheme...
-000006c0: 0d0a 4164 6469 7469 6f6e 616c 6c79 2c20  ..Additionally, 
-000006d0: 7468 6520 5370 616e 4d61 726b 6572 206c  the SpanMarker l
-000006e0: 6962 7261 7279 2068 6173 2062 6565 6e20  ibrary has been 
-000006f0: 696e 7465 6772 6174 6564 2077 6974 6820  integrated with 
-00000700: 7468 6520 4875 6767 696e 6720 4661 6365  the Hugging Face
-00000710: 2048 7562 2061 6e64 2074 6865 2048 7567   Hub and the Hug
-00000720: 6769 6e67 2046 6163 6520 496e 6665 7265  ging Face Infere
-00000730: 6e63 6520 4150 492e 2053 6565 2074 6865  nce API. See the
-00000740: 2053 7061 6e4d 6172 6b65 7220 646f 6375   SpanMarker docu
-00000750: 6d65 6e74 6174 696f 6e20 6f6e 205b 4875  mentation on [Hu
-00000760: 6767 696e 6720 4661 6365 5d28 6874 7470  gging Face](http
-00000770: 733a 2f2f 6875 6767 696e 6766 6163 652e  s://huggingface.
-00000780: 636f 2f64 6f63 732f 6875 622f 7370 616e  co/docs/hub/span
-00000790: 5f6d 6172 6b65 7229 206f 7220 7365 6520  _marker) or see 
-000007a0: 5b61 6c6c 2053 7061 6e4d 6172 6b65 7220  [all SpanMarker 
-000007b0: 6d6f 6465 6c73 206f 6e20 7468 6520 4875  models on the Hu
-000007c0: 6767 696e 6720 4661 6365 2048 7562 5d28  gging Face Hub](
-000007d0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
-000007e0: 6163 652e 636f 2f6d 6f64 656c 733f 6c69  ace.co/models?li
-000007f0: 6272 6172 793d 7370 616e 2d6d 6172 6b65  brary=span-marke
-00000800: 7229 2e0d 0a0d 0a54 6872 6f75 6768 2074  r).....Through t
-00000810: 6865 2049 6e66 6572 656e 6365 2041 5049  he Inference API
-00000820: 2069 6e74 6567 7261 7469 6f6e 2c20 7573   integration, us
-00000830: 6572 7320 6361 6e20 7465 7374 2061 6e79  ers can test any
-00000840: 2053 7061 6e4d 6172 6b65 7220 6d6f 6465   SpanMarker mode
-00000850: 6c20 6f6e 2074 6865 2048 7567 6769 6e67  l on the Hugging
-00000860: 2046 6163 6520 4875 6220 666f 7220 6672   Face Hub for fr
-00000870: 6565 2075 7369 6e67 2061 2077 6964 6765  ee using a widge
-00000880: 7420 6f6e 2074 6865 205b 6d6f 6465 6c20  t on the [model 
-00000890: 7061 6765 5d28 6874 7470 733a 2f2f 6875  page](https://hu
-000008a0: 6767 696e 6766 6163 652e 636f 2f74 6f6d  ggingface.co/tom
-000008b0: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
-000008c0: 6572 2d62 6572 742d 6261 7365 2d66 6577  er-bert-base-few
-000008d0: 6e65 7264 2d66 696e 652d 7375 7065 7229  nerd-fine-super)
-000008e0: 2e20 4675 7274 6865 726d 6f72 652c 2065  . Furthermore, e
-000008f0: 6163 6820 7075 626c 6963 2053 7061 6e4d  ach public SpanM
-00000900: 6172 6b65 7220 6d6f 6465 6c20 6f66 6665  arker model offe
-00000910: 7273 2061 2066 7265 6520 4150 4920 666f  rs a free API fo
-00000920: 7220 6661 7374 2070 726f 746f 7479 7069  r fast prototypi
-00000930: 6e67 2061 6e64 2063 616e 2062 6520 6465  ng and can be de
-00000940: 706c 6f79 6564 2074 6f20 7072 6f64 7563  ployed to produc
-00000950: 7469 6f6e 2075 7369 6e67 2048 7567 6769  tion using Huggi
-00000960: 6e67 2046 6163 6520 496e 6665 7265 6e63  ng Face Inferenc
-00000970: 6520 456e 6470 6f69 6e74 732e 0d0a 0d0a  e Endpoints.....
-00000980: 7c20 496e 6665 7265 6e63 6520 4150 4920  | Inference API 
-00000990: 5769 6467 6574 2028 6f6e 2061 206d 6f64  Widget (on a mod
-000009a0: 656c 2070 6167 6529 207c 2046 7265 6520  el page) | Free 
-000009b0: 496e 6665 7265 6e63 6520 4150 4920 2860  Inference API (`
-000009c0: 4465 706c 6f79 6020 3e20 6049 6e66 6572  Deploy` > `Infer
-000009d0: 656e 6365 2041 5049 6020 6f6e 2061 206d  ence API` on a m
-000009e0: 6f64 656c 2070 6167 6529 207c 0d0a 7c20  odel page) |..| 
-000009f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d20 7c20  ------------- | 
-00000a00: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d20 7c0d  ------------- |.
-00000a10: 0a7c 2020 215b 696d 6167 655d 2868 7474  .|  ![image](htt
-00000a20: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-00000a30: 746f 6d61 6172 7365 6e2f 5370 616e 4d61  tomaarsen/SpanMa
-00000a40: 726b 6572 4e45 522f 6173 7365 7473 2f33  rkerNER/assets/3
-00000a50: 3736 3231 3439 312f 3233 3430 3738 6237  7621491/234078b7
-00000a60: 2d32 3263 382d 3439 3163 2d38 3638 362d  -22c8-491c-8686-
-00000a70: 6661 6363 6433 3934 6636 3833 2920 7c20  faccd394f683) | 
-00000a80: 2021 5b69 6d61 6765 5d28 6874 7470 733a   ![image](https:
-00000a90: 2f2f 6769 7468 7562 2e63 6f6d 2f74 6f6d  //github.com/tom
-00000aa0: 6161 7273 656e 2f53 7061 6e4d 6172 6b65  aarsen/SpanMarke
-00000ab0: 724e 4552 2f61 7373 6574 732f 3337 3632  rNER/assets/3762
-00000ac0: 3134 3931 2f34 3130 6535 3139 312d 3933  1491/410e5191-93
-00000ad0: 3534 2d34 6532 372d 6237 3138 2d32 6436  54-4e27-b718-2d6
-00000ae0: 3961 6636 3738 6562 3729 207c 0d0a 0d0a  9af678eb7) |....
-00000af0: 2323 2044 6f63 756d 656e 7461 7469 6f6e  ## Documentation
-00000b00: 0d0a 4665 656c 2066 7265 6520 746f 2068  ..Feel free to h
-00000b10: 6176 6520 6120 6c6f 6f6b 2061 7420 7468  ave a look at th
-00000b20: 6520 5b64 6f63 756d 656e 7461 7469 6f6e  e [documentation
-00000b30: 5d28 6874 7470 733a 2f2f 746f 6d61 6172  ](https://tomaar
-00000b40: 7365 6e2e 6769 7468 7562 2e69 6f2f 5370  sen.github.io/Sp
-00000b50: 616e 4d61 726b 6572 4e45 5229 2e0d 0a0d  anMarkerNER)....
-00000b60: 0a23 2320 496e 7374 616c 6c61 7469 6f6e  .## Installation
-00000b70: 0d0a 596f 7520 6d61 7920 696e 7374 616c  ..You may instal
-00000b80: 6c20 7468 6520 5b60 7370 616e 5f6d 6172  l the [`span_mar
-00000b90: 6b65 7260 5d28 6874 7470 733a 2f2f 7079  ker`](https://py
-00000ba0: 7069 2e6f 7267 2f70 726f 6a65 6374 2f73  pi.org/project/s
-00000bb0: 7061 6e2d 6d61 726b 6572 2920 5079 7468  pan-marker) Pyth
-00000bc0: 6f6e 206d 6f64 756c 6520 7669 6120 6070  on module via `p
-00000bd0: 6970 6020 6c69 6b65 2073 6f3a 0d0a 6060  ip` like so:..``
-00000be0: 600d 0a70 6970 2069 6e73 7461 6c6c 2073  `..pip install s
-00000bf0: 7061 6e5f 6d61 726b 6572 0d0a 6060 600d  pan_marker..```.
-00000c00: 0a0d 0a23 2320 5175 6963 6b20 5374 6172  ...## Quick Star
-00000c10: 740d 0a23 2323 2054 7261 696e 696e 670d  t..### Training.
-00000c20: 0a50 6c65 6173 6520 6861 7665 2061 206c  .Please have a l
-00000c30: 6f6f 6b20 6174 206f 7572 205b 4765 7474  ook at our [Gett
-00000c40: 696e 6720 5374 6172 7465 645d 286e 6f74  ing Started](not
-00000c50: 6562 6f6f 6b73 2f67 6574 7469 6e67 5f73  ebooks/getting_s
-00000c60: 7461 7274 6564 2e69 7079 6e62 2920 6e6f  tarted.ipynb) no
-00000c70: 7465 626f 6f6b 2066 6f72 2064 6574 6169  tebook for detai
-00000c80: 6c73 206f 6e20 686f 7720 5370 616e 4d61  ls on how SpanMa
-00000c90: 726b 6572 2069 7320 636f 6d6d 6f6e 6c79  rker is commonly
-00000ca0: 2075 7365 642e 2049 7420 6578 706c 6169   used. It explai
-00000cb0: 6e73 2074 6865 2066 6f6c 6c6f 7769 6e67  ns the following
-00000cc0: 2073 6e69 7070 6574 2069 6e20 6d6f 7265   snippet in more
-00000cd0: 2064 6574 6169 6c2e 2041 6c74 6572 6e61   detail. Alterna
-00000ce0: 7469 7665 6c79 2c20 6861 7665 2061 206c  tively, have a l
-00000cf0: 6f6f 6b20 6174 2074 6865 205b 7472 6169  ook at the [trai
-00000d00: 6e69 6e67 2073 6372 6970 7473 5d28 7472  ning scripts](tr
-00000d10: 6169 6e69 6e67 5f73 6372 6970 7473 2920  aining_scripts) 
-00000d20: 7468 6174 2068 6176 6520 6265 656e 2073  that have been s
-00000d30: 7563 6365 7373 6675 6c6c 7920 7573 6564  uccessfully used
-00000d40: 2069 6e20 7468 6520 7061 7374 2e0d 0a0d   in the past....
-00000d50: 0a7c 2043 6f6c 6162 2020 2020 2020 2020  .| Colab        
-00000d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e20: 207c 204b 6167 676c 6520 2020 2020 2020   | Kaggle       
-00000e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ef0: 2020 2020 2020 7c20 4772 6164 6965 6e74        | Gradient
-00000f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000230: 454e 5345 0d0a 0d0a 3c64 6976 2061 6c69  ENSE....<div ali
+00000240: 676e 3d22 6365 6e74 6572 223e 0d0a 3c68  gn="center">..<h
+00000250: 313e 0d0a 5370 616e 4d61 726b 6572 2066  1>..SpanMarker f
+00000260: 6f72 204e 616d 6564 2045 6e74 6974 7920  or Named Entity 
+00000270: 5265 636f 676e 6974 696f 6e0d 0a3c 2f68  Recognition..</h
+00000280: 313e 0d0a 3c61 2068 7265 663d 2268 7474  1>..<a href="htt
+00000290: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+000002a0: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
+000002b0: 616e 2d6d 6172 6b65 722d 726f 6265 7274  an-marker-robert
+000002c0: 612d 6c61 7267 652d 6f6e 746f 6e6f 7465  a-large-ontonote
+000002d0: 7335 2220 7461 7267 6574 3d22 5f62 6c61  s5" target="_bla
+000002e0: 6e6b 223e 0d0a 2020 2020 3c69 6d67 2073  nk">..    <img s
+000002f0: 7263 3d22 6874 7470 733a 2f2f 6769 7468  rc="https://gith
+00000300: 7562 2e63 6f6d 2f74 6f6d 6161 7273 656e  ub.com/tomaarsen
+00000310: 2f53 7061 6e4d 6172 6b65 724e 4552 2f61  /SpanMarkerNER/a
+00000320: 7373 6574 732f 3337 3632 3134 3931 2f63  ssets/37621491/c
+00000330: 3736 6436 3339 332d 6262 3062 2d34 3463  76d6393-bb0b-44c
+00000340: 332d 3934 3132 2d66 6439 6338 3331 3364  3-9412-fd9c8313d
+00000350: 6363 3122 3e0d 0a3c 2f61 3e0d 0a0d 0a5b  cc1">..</a>....[
+00000360: f09f a497 204d 6f64 656c 735d 2868 7474  .... Models](htt
+00000370: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+00000380: 2e63 6f2f 6d6f 6465 6c73 3f6c 6962 7261  .co/models?libra
+00000390: 7279 3d73 7061 6e2d 6d61 726b 6572 2920  ry=span-marker) 
+000003a0: 7c0d 0a5b f09f 9ba0 efb8 8f20 4765 7474  |..[....... Gett
+000003b0: 696e 6720 5374 6172 7465 6420 496e 2047  ing Started In G
+000003c0: 6f6f 676c 6520 436f 6c61 625d 2868 7474  oogle Colab](htt
+000003d0: 7073 3a2f 2f63 6f6c 6162 2e72 6573 6561  ps://colab.resea
+000003e0: 7263 682e 676f 6f67 6c65 2e63 6f6d 2f67  rch.google.com/g
+000003f0: 6974 6875 622f 746f 6d61 6172 7365 6e2f  ithub/tomaarsen/
+00000400: 5370 616e 4d61 726b 6572 4e45 522f 626c  SpanMarkerNER/bl
+00000410: 6f62 2f6d 6169 6e2f 6e6f 7465 626f 6f6b  ob/main/notebook
+00000420: 732f 6765 7474 696e 675f 7374 6172 7465  s/getting_starte
+00000430: 642e 6970 796e 6229 207c 0d0a 5bf0 9f93  d.ipynb) |..[...
+00000440: 8420 446f 6375 6d65 6e74 6174 696f 6e5d  . Documentation]
+00000450: 2868 7474 7073 3a2f 2f74 6f6d 6161 7273  (https://tomaars
+00000460: 656e 2e67 6974 6875 622e 696f 2f53 7061  en.github.io/Spa
+00000470: 6e4d 6172 6b65 724e 4552 290d 0a3c 2f64  nMarkerNER)..</d
+00000480: 6976 3e0d 0a0d 0a53 7061 6e4d 6172 6b65  iv>....SpanMarke
+00000490: 7220 6973 2061 2066 7261 6d65 776f 726b  r is a framework
+000004a0: 2066 6f72 2074 7261 696e 696e 6720 706f   for training po
+000004b0: 7765 7266 756c 204e 616d 6564 2045 6e74  werful Named Ent
+000004c0: 6974 7920 5265 636f 676e 6974 696f 6e20  ity Recognition 
+000004d0: 6d6f 6465 6c73 2075 7369 6e67 2066 616d  models using fam
+000004e0: 696c 6961 7220 656e 636f 6465 7273 2073  iliar encoders s
+000004f0: 7563 6820 6173 2042 4552 542c 2052 6f42  uch as BERT, RoB
+00000500: 4552 5461 2061 6e64 2044 6542 4552 5461  ERTa and DeBERTa
+00000510: 2e0d 0a42 7569 6c74 206f 6e20 746f 7020  ...Built on top 
+00000520: 6f66 2074 6865 2066 616d 696c 6961 7220  of the familiar 
+00000530: 5bf0 9fa4 9720 5472 616e 7366 6f72 6d65  [.... Transforme
+00000540: 7273 5d28 6874 7470 733a 2f2f 6769 7468  rs](https://gith
+00000550: 7562 2e63 6f6d 2f68 7567 6769 6e67 6661  ub.com/huggingfa
+00000560: 6365 2f74 7261 6e73 666f 726d 6572 7329  ce/transformers)
+00000570: 206c 6962 7261 7279 2c20 5370 616e 4d61   library, SpanMa
+00000580: 726b 6572 2069 6e68 6572 6974 7320 6120  rker inherits a 
+00000590: 7769 6465 2072 616e 6765 206f 6620 706f  wide range of po
+000005a0: 7765 7266 756c 2066 756e 6374 696f 6e61  werful functiona
+000005b0: 6c69 7469 6573 2c20 7375 6368 2061 7320  lities, such as 
+000005c0: 6561 7369 6c79 206c 6f61 6469 6e67 2061  easily loading a
+000005d0: 6e64 2073 6176 696e 6720 6d6f 6465 6c73  nd saving models
+000005e0: 2c20 6879 7065 7270 6172 616d 6574 6572  , hyperparameter
+000005f0: 206f 7074 696d 697a 6174 696f 6e2c 2061   optimization, a
+00000600: 7574 6f6d 6174 6963 206c 6f67 6769 6e67  utomatic logging
+00000610: 2069 6e20 7661 7269 6f75 7320 746f 6f6c   in various tool
+00000620: 732c 2063 6865 636b 706f 696e 7469 6e67  s, checkpointing
+00000630: 2c20 6361 6c6c 6261 636b 732c 206d 6978  , callbacks, mix
+00000640: 6564 2070 7265 6369 7369 6f6e 2074 7261  ed precision tra
+00000650: 696e 696e 672c 2038 2d62 6974 2069 6e66  ining, 8-bit inf
+00000660: 6572 656e 6365 2c20 616e 6420 6d6f 7265  erence, and more
+00000670: 2e0d 0a0d 0a3c 212d 2d54 6967 6874 6c79  .....<!--Tightly
+00000680: 2069 6d70 6c65 6d65 6e74 6564 206f 6e20   implemented on 
+00000690: 746f 7020 6f66 2074 6865 205b f09f a497  top of the [....
+000006a0: 2054 7261 6e73 666f 726d 6572 735d 2868   Transformers](h
+000006b0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
+000006c0: 6d2f 6875 6767 696e 6766 6163 652f 7472  m/huggingface/tr
+000006d0: 616e 7366 6f72 6d65 7273 2f29 206c 6962  ansformers/) lib
+000006e0: 7261 7279 2c20 5370 616e 4d61 726b 6572  rary, SpanMarker
+000006f0: 2063 616e 2074 616b 6520 6164 7661 6e74   can take advant
+00000700: 6167 6520 6f66 2069 7473 2076 616c 7561  age of its valua
+00000710: 626c 6520 6675 6e63 7469 6f6e 616c 6974  ble functionalit
+00000720: 792e 2d2d 3e0d 0a3c 212d 2d20 6c69 6b65  y.-->..<!-- like
+00000730: 2070 6572 666f 726d 616e 6365 2064 6173   performance das
+00000740: 6862 6f61 7264 2069 6e74 6567 7261 7469  hboard integrati
+00000750: 6f6e 2c20 6175 746f 6d61 7469 6320 6d69  on, automatic mi
+00000760: 7865 6420 7072 6563 6973 696f 6e2c 2038  xed precision, 8
+00000770: 2d62 6974 2069 6e66 6572 656e 6365 2d2d  -bit inference--
+00000780: 3e0d 0a0d 0a42 6173 6564 206f 6e20 7468  >....Based on th
+00000790: 6520 5b50 4c2d 4d61 726b 6572 5d28 6874  e [PL-Marker](ht
+000007a0: 7470 733a 2f2f 6172 7869 762e 6f72 672f  tps://arxiv.org/
+000007b0: 7064 662f 3231 3039 2e30 3630 3637 2e70  pdf/2109.06067.p
+000007c0: 6466 2920 7061 7065 722c 2053 7061 6e4d  df) paper, SpanM
+000007d0: 6172 6b65 7220 6272 6561 6b73 2074 6865  arker breaks the
+000007e0: 206d 6f6c 6420 7468 726f 7567 6820 6974   mold through it
+000007f0: 7320 6163 6365 7373 6962 696c 6974 7920  s accessibility 
+00000800: 616e 6420 6561 7365 206f 6620 7573 652e  and ease of use.
+00000810: 2043 7275 6369 616c 6c79 2c20 5370 616e   Crucially, Span
+00000820: 4d61 726b 6572 2077 6f72 6b73 206f 7574  Marker works out
+00000830: 206f 6620 7468 6520 626f 7820 7769 7468   of the box with
+00000840: 206d 616e 7920 636f 6d6d 6f6e 2065 6e63   many common enc
+00000850: 6f64 6572 7320 7375 6368 2061 7320 6062  oders such as `b
+00000860: 6572 742d 6261 7365 2d63 6173 6564 6020  ert-base-cased` 
+00000870: 616e 6420 6072 6f62 6572 7461 2d6c 6172  and `roberta-lar
+00000880: 6765 602c 2061 6e64 2061 7574 6f6d 6174  ge`, and automat
+00000890: 6963 616c 6c79 2077 6f72 6b73 2077 6974  ically works wit
+000008a0: 6820 6461 7461 7365 7473 2075 7369 6e67  h datasets using
+000008b0: 2074 6865 2060 494f 4260 2c20 6049 4f42   the `IOB`, `IOB
+000008c0: 3260 2c20 6042 494f 4553 602c 2060 4249  2`, `BIOES`, `BI
+000008d0: 4c4f 5560 206f 7220 6e6f 206c 6162 656c  LOU` or no label
+000008e0: 2061 6e6e 6f74 6174 696f 6e20 7363 6865   annotation sche
+000008f0: 6d65 2e0d 0a0d 0a41 6464 6974 696f 6e61  me.....Additiona
+00000900: 6c6c 792c 2074 6865 2053 7061 6e4d 6172  lly, the SpanMar
+00000910: 6b65 7220 6c69 6272 6172 7920 6861 7320  ker library has 
+00000920: 6265 656e 2069 6e74 6567 7261 7465 6420  been integrated 
+00000930: 7769 7468 2074 6865 2048 7567 6769 6e67  with the Hugging
+00000940: 2046 6163 6520 4875 6220 616e 6420 7468   Face Hub and th
+00000950: 6520 4875 6767 696e 6720 4661 6365 2049  e Hugging Face I
+00000960: 6e66 6572 656e 6365 2041 5049 2e20 5365  nference API. Se
+00000970: 6520 7468 6520 5370 616e 4d61 726b 6572  e the SpanMarker
+00000980: 2064 6f63 756d 656e 7461 7469 6f6e 206f   documentation o
+00000990: 6e20 5b48 7567 6769 6e67 2046 6163 655d  n [Hugging Face]
+000009a0: 2868 7474 7073 3a2f 2f68 7567 6769 6e67  (https://hugging
+000009b0: 6661 6365 2e63 6f2f 646f 6373 2f68 7562  face.co/docs/hub
+000009c0: 2f73 7061 6e5f 6d61 726b 6572 2920 6f72  /span_marker) or
+000009d0: 2073 6565 205b 616c 6c20 5370 616e 4d61   see [all SpanMa
+000009e0: 726b 6572 206d 6f64 656c 7320 6f6e 2074  rker models on t
+000009f0: 6865 2048 7567 6769 6e67 2046 6163 6520  he Hugging Face 
+00000a00: 4875 625d 2868 7474 7073 3a2f 2f68 7567  Hub](https://hug
+00000a10: 6769 6e67 6661 6365 2e63 6f2f 6d6f 6465  gingface.co/mode
+00000a20: 6c73 3f6c 6962 7261 7279 3d73 7061 6e2d  ls?library=span-
+00000a30: 6d61 726b 6572 292e 0d0a 5468 726f 7567  marker)...Throug
+00000a40: 6820 7468 6520 496e 6665 7265 6e63 6520  h the Inference 
+00000a50: 4150 4920 696e 7465 6772 6174 696f 6e2c  API integration,
+00000a60: 2075 7365 7273 2063 616e 2074 6573 7420   users can test 
+00000a70: 616e 7920 5370 616e 4d61 726b 6572 206d  any SpanMarker m
+00000a80: 6f64 656c 206f 6e20 7468 6520 4875 6767  odel on the Hugg
+00000a90: 696e 6720 4661 6365 2048 7562 2066 6f72  ing Face Hub for
+00000aa0: 2066 7265 6520 7573 696e 6720 6120 7769   free using a wi
+00000ab0: 6467 6574 206f 6e20 7468 6520 5b6d 6f64  dget on the [mod
+00000ac0: 656c 2070 6167 655d 2868 7474 7073 3a2f  el page](https:/
+00000ad0: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
+00000ae0: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
+00000af0: 6172 6b65 722d 6265 7274 2d62 6173 652d  arker-bert-base-
+00000b00: 6665 776e 6572 642d 6669 6e65 2d73 7570  fewnerd-fine-sup
+00000b10: 6572 292e 2046 7572 7468 6572 6d6f 7265  er). Furthermore
+00000b20: 2c20 6561 6368 2070 7562 6c69 6320 5370  , each public Sp
+00000b30: 616e 4d61 726b 6572 206d 6f64 656c 206f  anMarker model o
+00000b40: 6666 6572 7320 6120 6672 6565 2041 5049  ffers a free API
+00000b50: 2066 6f72 2066 6173 7420 7072 6f74 6f74   for fast protot
+00000b60: 7970 696e 6720 616e 6420 6361 6e20 6265  yping and can be
+00000b70: 2064 6570 6c6f 7965 6420 746f 2070 726f   deployed to pro
+00000b80: 6475 6374 696f 6e20 7573 696e 6720 4875  duction using Hu
+00000b90: 6767 696e 6720 4661 6365 2049 6e66 6572  gging Face Infer
+00000ba0: 656e 6365 2045 6e64 706f 696e 7473 2e0d  ence Endpoints..
+00000bb0: 0a0d 0a7c 2049 6e66 6572 656e 6365 2041  ...| Inference A
+00000bc0: 5049 2057 6964 6765 7420 286f 6e20 6120  PI Widget (on a 
+00000bd0: 6d6f 6465 6c20 7061 6765 2920 7c20 4672  model page) | Fr
+00000be0: 6565 2049 6e66 6572 656e 6365 2041 5049  ee Inference API
+00000bf0: 2028 6044 6570 6c6f 7960 203e 2060 496e   (`Deploy` > `In
+00000c00: 6665 7265 6e63 6520 4150 4960 206f 6e20  ference API` on 
+00000c10: 6120 6d6f 6465 6c20 7061 6765 2920 7c0d  a model page) |.
+00000c20: 0a7c 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  .| -------------
+00000c30: 207c 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d   | -------------
+00000c40: 207c 0d0a 7c20 2021 5b69 6d61 6765 5d28   |..|  ![image](
+00000c50: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
+00000c60: 6f6d 2f74 6f6d 6161 7273 656e 2f53 7061  om/tomaarsen/Spa
+00000c70: 6e4d 6172 6b65 724e 4552 2f61 7373 6574  nMarkerNER/asset
+00000c80: 732f 3337 3632 3134 3931 2f32 3334 3037  s/37621491/23407
+00000c90: 3862 372d 3232 6338 2d34 3931 632d 3836  8b7-22c8-491c-86
+00000ca0: 3836 2d66 6163 6364 3339 3466 3638 3329  86-faccd394f683)
+00000cb0: 207c 2020 215b 696d 6167 655d 2868 7474   |  ![image](htt
+00000cc0: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
+00000cd0: 746f 6d61 6172 7365 6e2f 5370 616e 4d61  tomaarsen/SpanMa
+00000ce0: 726b 6572 4e45 522f 6173 7365 7473 2f33  rkerNER/assets/3
+00000cf0: 3736 3231 3439 312f 3431 3065 3531 3931  7621491/410e5191
+00000d00: 2d39 3335 342d 3465 3237 2d62 3731 382d  -9354-4e27-b718-
+00000d10: 3264 3639 6166 3637 3865 6237 2920 7c0d  2d69af678eb7) |.
+00000d20: 0a0d 0a23 2320 446f 6375 6d65 6e74 6174  ...## Documentat
+00000d30: 696f 6e0d 0a46 6565 6c20 6672 6565 2074  ion..Feel free t
+00000d40: 6f20 6861 7665 2061 206c 6f6f 6b20 6174  o have a look at
+00000d50: 2074 6865 205b 646f 6375 6d65 6e74 6174   the [documentat
+00000d60: 696f 6e5d 2868 7474 7073 3a2f 2f74 6f6d  ion](https://tom
+00000d70: 6161 7273 656e 2e67 6974 6875 622e 696f  aarsen.github.io
+00000d80: 2f53 7061 6e4d 6172 6b65 724e 4552 292e  /SpanMarkerNER).
+00000d90: 0d0a 0d0a 2323 2049 6e73 7461 6c6c 6174  ....## Installat
+00000da0: 696f 6e0d 0a59 6f75 206d 6179 2069 6e73  ion..You may ins
+00000db0: 7461 6c6c 2074 6865 205b 6073 7061 6e5f  tall the [`span_
+00000dc0: 6d61 726b 6572 605d 2868 7474 7073 3a2f  marker`](https:/
+00000dd0: 2f70 7970 692e 6f72 672f 7072 6f6a 6563  /pypi.org/projec
+00000de0: 742f 7370 616e 2d6d 6172 6b65 7229 2050  t/span-marker) P
+00000df0: 7974 686f 6e20 6d6f 6475 6c65 2076 6961  ython module via
+00000e00: 2060 7069 7060 206c 696b 6520 736f 3a0d   `pip` like so:.
+00000e10: 0a60 6060 0d0a 7069 7020 696e 7374 616c  .```..pip instal
+00000e20: 6c20 7370 616e 5f6d 6172 6b65 720d 0a60  l span_marker..`
+00000e30: 6060 0d0a 0d0a 2323 2051 7569 636b 2053  ``....## Quick S
+00000e40: 7461 7274 0d0a 2323 2320 5472 6169 6e69  tart..### Traini
+00000e50: 6e67 0d0a 506c 6561 7365 2068 6176 6520  ng..Please have 
+00000e60: 6120 6c6f 6f6b 2061 7420 6f75 7220 5b47  a look at our [G
+00000e70: 6574 7469 6e67 2053 7461 7274 6564 5d28  etting Started](
+00000e80: 6e6f 7465 626f 6f6b 732f 6765 7474 696e  notebooks/gettin
+00000e90: 675f 7374 6172 7465 642e 6970 796e 6229  g_started.ipynb)
+00000ea0: 206e 6f74 6562 6f6f 6b20 666f 7220 6465   notebook for de
+00000eb0: 7461 696c 7320 6f6e 2068 6f77 2053 7061  tails on how Spa
+00000ec0: 6e4d 6172 6b65 7220 6973 2063 6f6d 6d6f  nMarker is commo
+00000ed0: 6e6c 7920 7573 6564 2e20 4974 2065 7870  nly used. It exp
+00000ee0: 6c61 696e 7320 7468 6520 666f 6c6c 6f77  lains the follow
+00000ef0: 696e 6720 736e 6970 7065 7420 696e 206d  ing snippet in m
+00000f00: 6f72 6520 6465 7461 696c 2e20 416c 7465  ore detail. Alte
+00000f10: 726e 6174 6976 656c 792c 2068 6176 6520  rnatively, have 
+00000f20: 6120 6c6f 6f6b 2061 7420 7468 6520 5b74  a look at the [t
+00000f30: 7261 696e 696e 6720 7363 7269 7074 735d  raining scripts]
+00000f40: 2874 7261 696e 696e 675f 7363 7269 7074  (training_script
+00000f50: 7329 2074 6861 7420 6861 7665 2062 6565  s) that have bee
+00000f60: 6e20 7375 6363 6573 7366 756c 6c79 2075  n successfully u
+00000f70: 7365 6420 696e 2074 6865 2070 6173 742e  sed in the past.
+00000f80: 0d0a 0d0a 7c20 436f 6c61 6220 2020 2020  ....| Colab     
 00000f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000fb0: 2020 2020 2020 2020 207c 2053 7475 6469           | Studi
-00000fc0: 6f20 4c61 6220 2020 2020 2020 2020 2020  o Lab           
+00000fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001000: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001010: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001020: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001030: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001050: 2020 2020 7c20 4b61 6767 6c65 2020 2020      | Kaggle    
 00001060: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001070: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001090: 2020 7c0d 0a7c 3a2d 2d2d 2d2d 2d2d 2d2d    |..|:---------
-000010a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001100: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001110: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001120: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001130: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001140: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001150: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001160: 2d2d 2d2d 2d7c 3a2d 2d2d 2d2d 2d2d 2d2d  -----|:---------
-00001170: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001180: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001190: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001200: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001210: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001220: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001230: 2d2d 2d2d 2d2d 2d2d 2d2d 7c3a 2d2d 2d2d  ----------|:----
-00001240: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001250: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001260: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001270: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001280: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001290: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001120: 2020 2020 2020 2020 207c 2047 7261 6469           | Gradi
+00001130: 656e 7420 2020 2020 2020 2020 2020 2020  ent             
+00001140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011e0: 2020 2020 2020 2020 2020 2020 7c20 5374              | St
+000011f0: 7564 696f 204c 6162 2020 2020 2020 2020  udio Lab        
+00001200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000012a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000012b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000012c0: 2020 2020 207c 0d0a 7c3a 2d2d 2d2d 2d2d       |..|:------
 000012d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000012e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d7c 3a2d  -------------|:-
+000012f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001300: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001310: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001320: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001330: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001340: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001350: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001360: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001370: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001380: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001390: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001390: 2d2d 2d2d 2d2d 2d2d 7c3a 2d2d 2d2d 2d2d  --------|:------
 000013a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000013b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000013c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000013d0: 2d2d 2d2d 2d2d 7c0d 0a7c 205b 215b 4f70  ------|..| [![Op
-000013e0: 656e 2049 6e20 436f 6c61 625d 2868 7474  en In Colab](htt
-000013f0: 7073 3a2f 2f63 6f6c 6162 2e72 6573 6561  ps://colab.resea
-00001400: 7263 682e 676f 6f67 6c65 2e63 6f6d 2f61  rch.google.com/a
-00001410: 7373 6574 732f 636f 6c61 622d 6261 6467  ssets/colab-badg
-00001420: 652e 7376 6729 5d28 6874 7470 733a 2f2f  e.svg)](https://
-00001430: 636f 6c61 622e 7265 7365 6172 6368 2e67  colab.research.g
-00001440: 6f6f 676c 652e 636f 6d2f 6769 7468 7562  oogle.com/github
-00001450: 2f74 6f6d 6161 7273 656e 2f53 7061 6e4d  /tomaarsen/SpanM
-00001460: 6172 6b65 724e 4552 2f62 6c6f 622f 6d61  arkerNER/blob/ma
-00001470: 696e 2f6e 6f74 6562 6f6f 6b73 2f67 6574  in/notebooks/get
-00001480: 7469 6e67 5f73 7461 7274 6564 2e69 7079  ting_started.ipy
-00001490: 6e62 2920 2020 2020 2020 2020 2020 2020  nb)             
-000014a0: 2020 2020 2020 2020 2020 7c20 5b21 5b4b            | [![K
-000014b0: 6167 676c 655d 2868 7474 7073 3a2f 2f6b  aggle](https://k
-000014c0: 6167 676c 652e 636f 6d2f 7374 6174 6963  aggle.com/static
-000014d0: 2f69 6d61 6765 732f 6f70 656e 2d69 6e2d  /images/open-in-
-000014e0: 6b61 6767 6c65 2e73 7667 295d 2868 7474  kaggle.svg)](htt
-000014f0: 7073 3a2f 2f6b 6167 676c 652e 636f 6d2f  ps://kaggle.com/
-00001500: 6b65 726e 656c 732f 7765 6c63 6f6d 653f  kernels/welcome?
-00001510: 7372 633d 6874 7470 733a 2f2f 6769 7468  src=https://gith
-00001520: 7562 2e63 6f6d 2f74 6f6d 6161 7273 656e  ub.com/tomaarsen
-00001530: 2f53 7061 6e4d 6172 6b65 724e 4552 2f62  /SpanMarkerNER/b
-00001540: 6c6f 622f 6d61 696e 2f6e 6f74 6562 6f6f  lob/main/noteboo
-00001550: 6b73 2f67 6574 7469 6e67 5f73 7461 7274  ks/getting_start
-00001560: 6564 2e69 7079 6e62 2920 2020 2020 2020  ed.ipynb)       
-00001570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001580: 7c20 5b21 5b47 7261 6469 656e 745d 2868  | [![Gradient](h
-00001590: 7474 7073 3a2f 2f61 7373 6574 732e 7061  ttps://assets.pa
-000015a0: 7065 7273 7061 6365 2e69 6f2f 696d 672f  perspace.io/img/
-000015b0: 6772 6164 6965 6e74 2d62 6164 6765 2e73  gradient-badge.s
-000015c0: 7667 295d 2868 7474 7073 3a2f 2f63 6f6e  vg)](https://con
-000015d0: 736f 6c65 2e70 6170 6572 7370 6163 652e  sole.paperspace.
-000015e0: 636f 6d2f 6769 7468 7562 2f74 6f6d 6161  com/github/tomaa
-000015f0: 7273 656e 2f53 7061 6e4d 6172 6b65 724e  rsen/SpanMarkerN
-00001600: 4552 2f62 6c6f 622f 6d61 696e 2f6e 6f74  ER/blob/main/not
-00001610: 6562 6f6f 6b73 2f67 6574 7469 6e67 5f73  ebooks/getting_s
-00001620: 7461 7274 6564 2e69 7079 6e62 2920 2020  tarted.ipynb)   
-00001630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001640: 2020 2020 7c20 5b21 5b4f 7065 6e20 496e      | [![Open In
-00001650: 2053 6167 654d 616b 6572 2053 7475 6469   SageMaker Studi
-00001660: 6f20 4c61 625d 2868 7474 7073 3a2f 2f73  o Lab](https://s
-00001670: 7475 6469 6f6c 6162 2e73 6167 656d 616b  tudiolab.sagemak
-00001680: 6572 2e61 7773 2f73 7475 6469 6f6c 6162  er.aws/studiolab
-00001690: 2e73 7667 295d 2868 7474 7073 3a2f 2f73  .svg)](https://s
-000016a0: 7475 6469 6f6c 6162 2e73 6167 656d 616b  tudiolab.sagemak
-000016b0: 6572 2e61 7773 2f69 6d70 6f72 742f 6769  er.aws/import/gi
-000016c0: 7468 7562 2f74 6f6d 6161 7273 656e 2f53  thub/tomaarsen/S
-000016d0: 7061 6e4d 6172 6b65 724e 4552 2f62 6c6f  panMarkerNER/blo
-000016e0: 622f 6d61 696e 2f6e 6f74 6562 6f6f 6b73  b/main/notebooks
-000016f0: 2f67 6574 7469 6e67 5f73 7461 7274 6564  /getting_started
-00001700: 2e69 7079 6e62 2920 2020 2020 2020 2020  .ipynb)         
-00001710: 2020 2020 2020 2020 2020 2020 2020 7c0d                |.
-00001720: 0a0d 0a60 6060 7079 7468 6f6e 0d0a 6672  ...```python..fr
-00001730: 6f6d 2064 6174 6173 6574 7320 696d 706f  om datasets impo
-00001740: 7274 206c 6f61 645f 6461 7461 7365 740d  rt load_dataset.
-00001750: 0a66 726f 6d20 7472 616e 7366 6f72 6d65  .from transforme
-00001760: 7273 2069 6d70 6f72 7420 5472 6169 6e69  rs import Traini
-00001770: 6e67 4172 6775 6d65 6e74 730d 0a66 726f  ngArguments..fro
-00001780: 6d20 7370 616e 5f6d 6172 6b65 7220 696d  m span_marker im
-00001790: 706f 7274 2053 7061 6e4d 6172 6b65 724d  port SpanMarkerM
-000017a0: 6f64 656c 2c20 5472 6169 6e65 720d 0a0d  odel, Trainer...
-000017b0: 0a0d 0a64 6566 206d 6169 6e28 2920 2d3e  ...def main() ->
-000017c0: 204e 6f6e 653a 0d0a 2020 2020 2320 4c6f   None:..    # Lo
-000017d0: 6164 2074 6865 2064 6174 6173 6574 2c20  ad the dataset, 
-000017e0: 656e 7375 7265 2022 746f 6b65 6e73 2220  ensure "tokens" 
-000017f0: 616e 6420 226e 6572 5f74 6167 7322 2063  and "ner_tags" c
-00001800: 6f6c 756d 6e73 2c20 616e 6420 6765 7420  olumns, and get 
-00001810: 6120 6c69 7374 206f 6620 6c61 6265 6c73  a list of labels
-00001820: 0d0a 2020 2020 6461 7461 7365 7420 3d20  ..    dataset = 
-00001830: 6c6f 6164 5f64 6174 6173 6574 2822 4446  load_dataset("DF
-00001840: 4b49 2d53 4c54 2f66 6577 2d6e 6572 6422  KI-SLT/few-nerd"
-00001850: 2c20 2273 7570 6572 7669 7365 6422 290d  , "supervised").
-00001860: 0a20 2020 2064 6174 6173 6574 203d 2064  .    dataset = d
-00001870: 6174 6173 6574 2e72 656d 6f76 655f 636f  ataset.remove_co
-00001880: 6c75 6d6e 7328 226e 6572 5f74 6167 7322  lumns("ner_tags"
-00001890: 290d 0a20 2020 2064 6174 6173 6574 203d  )..    dataset =
-000018a0: 2064 6174 6173 6574 2e72 656e 616d 655f   dataset.rename_
-000018b0: 636f 6c75 6d6e 2822 6669 6e65 5f6e 6572  column("fine_ner
-000018c0: 5f74 6167 7322 2c20 226e 6572 5f74 6167  _tags", "ner_tag
-000018d0: 7322 290d 0a20 2020 206c 6162 656c 7320  s")..    labels 
-000018e0: 3d20 6461 7461 7365 745b 2274 7261 696e  = dataset["train
-000018f0: 225d 2e66 6561 7475 7265 735b 226e 6572  "].features["ner
-00001900: 5f74 6167 7322 5d2e 6665 6174 7572 652e  _tags"].feature.
-00001910: 6e61 6d65 730d 0a0d 0a20 2020 2023 2049  names....    # I
-00001920: 6e69 7469 616c 697a 6520 6120 5370 616e  nitialize a Span
-00001930: 4d61 726b 6572 206d 6f64 656c 2075 7369  Marker model usi
-00001940: 6e67 2061 2070 7265 7472 6169 6e65 6420  ng a pretrained 
-00001950: 4245 5254 2d73 7479 6c65 2065 6e63 6f64  BERT-style encod
-00001960: 6572 0d0a 2020 2020 6d6f 6465 6c5f 6e61  er..    model_na
-00001970: 6d65 203d 2022 6265 7274 2d62 6173 652d  me = "bert-base-
-00001980: 6361 7365 6422 0d0a 2020 2020 6d6f 6465  cased"..    mode
-00001990: 6c20 3d20 5370 616e 4d61 726b 6572 4d6f  l = SpanMarkerMo
-000019a0: 6465 6c2e 6672 6f6d 5f70 7265 7472 6169  del.from_pretrai
-000019b0: 6e65 6428 0d0a 2020 2020 2020 2020 6d6f  ned(..        mo
-000019c0: 6465 6c5f 6e61 6d65 2c0d 0a20 2020 2020  del_name,..     
-000019d0: 2020 206c 6162 656c 733d 6c61 6265 6c73     labels=labels
-000019e0: 2c0d 0a20 2020 2020 2020 2023 2053 7061  ,..        # Spa
-000019f0: 6e4d 6172 6b65 7220 6879 7065 7270 6172  nMarker hyperpar
-00001a00: 616d 6574 6572 733a 0d0a 2020 2020 2020  ameters:..      
-00001a10: 2020 6d6f 6465 6c5f 6d61 785f 6c65 6e67    model_max_leng
-00001a20: 7468 3d32 3536 2c0d 0a20 2020 2020 2020  th=256,..       
-00001a30: 206d 6172 6b65 725f 6d61 785f 6c65 6e67   marker_max_leng
-00001a40: 7468 3d31 3238 2c0d 0a20 2020 2020 2020  th=128,..       
-00001a50: 2065 6e74 6974 795f 6d61 785f 6c65 6e67   entity_max_leng
-00001a60: 7468 3d38 2c0d 0a20 2020 2029 0d0a 0d0a  th=8,..    )....
-00001a70: 2020 2020 2320 5072 6570 6172 6520 7468      # Prepare th
-00001a80: 6520 f09f a497 2074 7261 6e73 666f 726d  e .... transform
-00001a90: 6572 7320 7472 6169 6e69 6e67 2061 7267  ers training arg
-00001aa0: 756d 656e 7473 0d0a 2020 2020 6172 6773  uments..    args
-00001ab0: 203d 2054 7261 696e 696e 6741 7267 756d   = TrainingArgum
-00001ac0: 656e 7473 280d 0a20 2020 2020 2020 206f  ents(..        o
-00001ad0: 7574 7075 745f 6469 723d 226d 6f64 656c  utput_dir="model
-00001ae0: 732f 7370 616e 5f6d 6172 6b65 725f 6265  s/span_marker_be
-00001af0: 7274 5f62 6173 655f 6361 7365 645f 6665  rt_base_cased_fe
-00001b00: 776e 6572 645f 6669 6e65 5f73 7570 6572  wnerd_fine_super
-00001b10: 222c 0d0a 2020 2020 2020 2020 2320 5472  ",..        # Tr
-00001b20: 6169 6e69 6e67 2048 7970 6572 7061 7261  aining Hyperpara
-00001b30: 6d65 7465 7273 3a0d 0a20 2020 2020 2020  meters:..       
-00001b40: 206c 6561 726e 696e 675f 7261 7465 3d35   learning_rate=5
-00001b50: 652d 352c 0d0a 2020 2020 2020 2020 7065  e-5,..        pe
-00001b60: 725f 6465 7669 6365 5f74 7261 696e 5f62  r_device_train_b
-00001b70: 6174 6368 5f73 697a 653d 3332 2c0d 0a20  atch_size=32,.. 
-00001b80: 2020 2020 2020 2070 6572 5f64 6576 6963         per_devic
-00001b90: 655f 6576 616c 5f62 6174 6368 5f73 697a  e_eval_batch_siz
-00001ba0: 653d 3332 2c0d 0a20 2020 2020 2020 206e  e=32,..        n
-00001bb0: 756d 5f74 7261 696e 5f65 706f 6368 733d  um_train_epochs=
-00001bc0: 332c 0d0a 2020 2020 2020 2020 7765 6967  3,..        weig
-00001bd0: 6874 5f64 6563 6179 3d30 2e30 312c 0d0a  ht_decay=0.01,..
-00001be0: 2020 2020 2020 2020 7761 726d 7570 5f72          warmup_r
-00001bf0: 6174 696f 3d30 2e31 2c0d 0a20 2020 2020  atio=0.1,..     
-00001c00: 2020 2062 6631 363d 5472 7565 2c20 2023     bf16=True,  #
-00001c10: 2052 6570 6c61 6365 2060 6266 3136 6020   Replace `bf16` 
-00001c20: 7769 7468 2060 6670 3136 6020 6966 2079  with `fp16` if y
-00001c30: 6f75 7220 6861 7264 7761 7265 2063 616e  our hardware can
-00001c40: 2774 2075 7365 2062 6631 362e 0d0a 2020  't use bf16...  
-00001c50: 2020 2020 2020 2320 4f74 6865 7220 5472        # Other Tr
-00001c60: 6169 6e69 6e67 2070 6172 616d 6574 6572  aining parameter
-00001c70: 730d 0a20 2020 2020 2020 206c 6f67 6769  s..        loggi
-00001c80: 6e67 5f66 6972 7374 5f73 7465 703d 5472  ng_first_step=Tr
-00001c90: 7565 2c0d 0a20 2020 2020 2020 206c 6f67  ue,..        log
-00001ca0: 6769 6e67 5f73 7465 7073 3d35 302c 0d0a  ging_steps=50,..
-00001cb0: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
-00001cc0: 6f6e 5f73 7472 6174 6567 793d 2273 7465  on_strategy="ste
-00001cd0: 7073 222c 0d0a 2020 2020 2020 2020 7361  ps",..        sa
-00001ce0: 7665 5f73 7472 6174 6567 793d 2273 7465  ve_strategy="ste
-00001cf0: 7073 222c 0d0a 2020 2020 2020 2020 6576  ps",..        ev
-00001d00: 616c 5f73 7465 7073 3d33 3030 302c 0d0a  al_steps=3000,..
-00001d10: 2020 2020 2020 2020 7361 7665 5f74 6f74          save_tot
-00001d20: 616c 5f6c 696d 6974 3d32 2c0d 0a20 2020  al_limit=2,..   
-00001d30: 2020 2020 2064 6174 616c 6f61 6465 725f       dataloader_
-00001d40: 6e75 6d5f 776f 726b 6572 733d 322c 0d0a  num_workers=2,..
-00001d50: 2020 2020 290d 0a0d 0a20 2020 2023 2049      )....    # I
-00001d60: 6e69 7469 616c 697a 6520 7468 6520 7472  nitialize the tr
-00001d70: 6169 6e65 7220 7573 696e 6720 6f75 7220  ainer using our 
-00001d80: 6d6f 6465 6c2c 2074 7261 696e 696e 6720  model, training 
-00001d90: 6172 6773 2026 2064 6174 6173 6574 2c20  args & dataset, 
-00001da0: 616e 6420 7472 6169 6e0d 0a20 2020 2074  and train..    t
-00001db0: 7261 696e 6572 203d 2054 7261 696e 6572  rainer = Trainer
-00001dc0: 280d 0a20 2020 2020 2020 206d 6f64 656c  (..        model
-00001dd0: 3d6d 6f64 656c 2c0d 0a20 2020 2020 2020  =model,..       
-00001de0: 2061 7267 733d 6172 6773 2c0d 0a20 2020   args=args,..   
-00001df0: 2020 2020 2074 7261 696e 5f64 6174 6173       train_datas
-00001e00: 6574 3d64 6174 6173 6574 5b22 7472 6169  et=dataset["trai
-00001e10: 6e22 5d2c 0d0a 2020 2020 2020 2020 6576  n"],..        ev
-00001e20: 616c 5f64 6174 6173 6574 3d64 6174 6173  al_dataset=datas
-00001e30: 6574 5b22 7661 6c69 6461 7469 6f6e 225d  et["validation"]
-00001e40: 2c0d 0a20 2020 2029 0d0a 2020 2020 7472  ,..    )..    tr
-00001e50: 6169 6e65 722e 7472 6169 6e28 290d 0a20  ainer.train().. 
-00001e60: 2020 2074 7261 696e 6572 2e73 6176 655f     trainer.save_
-00001e70: 6d6f 6465 6c28 226d 6f64 656c 732f 7370  model("models/sp
-00001e80: 616e 5f6d 6172 6b65 725f 6265 7274 5f62  an_marker_bert_b
-00001e90: 6173 655f 6361 7365 645f 6665 776e 6572  ase_cased_fewner
-00001ea0: 645f 6669 6e65 5f73 7570 6572 2f63 6865  d_fine_super/che
-00001eb0: 636b 706f 696e 742d 6669 6e61 6c22 290d  ckpoint-final").
-00001ec0: 0a0d 0a20 2020 2023 2043 6f6d 7075 7465  ...    # Compute
-00001ed0: 2026 2073 6176 6520 7468 6520 6d65 7472   & save the metr
-00001ee0: 6963 7320 6f6e 2074 6865 2074 6573 7420  ics on the test 
-00001ef0: 7365 740d 0a20 2020 206d 6574 7269 6373  set..    metrics
-00001f00: 203d 2074 7261 696e 6572 2e65 7661 6c75   = trainer.evalu
-00001f10: 6174 6528 6461 7461 7365 745b 2274 6573  ate(dataset["tes
-00001f20: 7422 5d2c 206d 6574 7269 635f 6b65 795f  t"], metric_key_
-00001f30: 7072 6566 6978 3d22 7465 7374 2229 0d0a  prefix="test")..
-00001f40: 2020 2020 7472 6169 6e65 722e 7361 7665      trainer.save
-00001f50: 5f6d 6574 7269 6373 2822 7465 7374 222c  _metrics("test",
-00001f60: 206d 6574 7269 6373 290d 0a0d 0a0d 0a69   metrics)......i
-00001f70: 6620 5f5f 6e61 6d65 5f5f 203d 3d20 225f  f __name__ == "_
-00001f80: 5f6d 6169 6e5f 5f22 3a0d 0a20 2020 206d  _main__":..    m
-00001f90: 6169 6e28 290d 0a60 6060 0d0a 0d0a 2323  ain()..```....##
-00001fa0: 2320 496e 6665 7265 6e63 650d 0a60 6060  # Inference..```
-00001fb0: 7079 7468 6f6e 0d0a 6672 6f6d 2073 7061  python..from spa
-00001fc0: 6e5f 6d61 726b 6572 2069 6d70 6f72 7420  n_marker import 
-00001fd0: 5370 616e 4d61 726b 6572 4d6f 6465 6c0d  SpanMarkerModel.
-00001fe0: 0a0d 0a23 2044 6f77 6e6c 6f61 6420 6672  ...# Download fr
-00001ff0: 6f6d 2074 6865 20f0 9fa4 9720 4875 620d  om the .... Hub.
-00002000: 0a6d 6f64 656c 203d 2053 7061 6e4d 6172  .model = SpanMar
-00002010: 6b65 724d 6f64 656c 2e66 726f 6d5f 7072  kerModel.from_pr
-00002020: 6574 7261 696e 6564 2822 746f 6d61 6172  etrained("tomaar
-00002030: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
-00002040: 6265 7274 2d62 6173 652d 6665 776e 6572  bert-base-fewner
-00002050: 642d 6669 6e65 2d73 7570 6572 2229 0d0a  d-fine-super")..
-00002060: 2320 5275 6e20 696e 6665 7265 6e63 650d  # Run inference.
-00002070: 0a65 6e74 6974 6965 7320 3d20 6d6f 6465  .entities = mode
-00002080: 6c2e 7072 6564 6963 7428 2241 6d65 6c69  l.predict("Ameli
-00002090: 6120 4561 7268 6172 7420 666c 6577 2068  a Earhart flew h
-000020a0: 6572 2073 696e 676c 6520 656e 6769 6e65  er single engine
-000020b0: 204c 6f63 6b68 6565 6420 5665 6761 2035   Lockheed Vega 5
-000020c0: 4220 6163 726f 7373 2074 6865 2041 746c  B across the Atl
-000020d0: 616e 7469 6320 746f 2050 6172 6973 2e22  antic to Paris."
-000020e0: 290d 0a5b 7b27 7370 616e 273a 2027 416d  )..[{'span': 'Am
-000020f0: 656c 6961 2045 6172 6861 7274 272c 2027  elia Earhart', '
-00002100: 6c61 6265 6c27 3a20 2770 6572 736f 6e2d  label': 'person-
-00002110: 6f74 6865 7227 2c20 2773 636f 7265 273a  other', 'score':
-00002120: 2030 2e37 3635 3935 3937 3339 3638 3530   0.7659597396850
-00002130: 3538 362c 2027 6368 6172 5f73 7461 7274  586, 'char_start
-00002140: 5f69 6e64 6578 273a 2030 2c20 2763 6861  _index': 0, 'cha
-00002150: 725f 656e 645f 696e 6465 7827 3a20 3134  r_end_index': 14
-00002160: 7d2c 0d0a 207b 2773 7061 6e27 3a20 274c  },.. {'span': 'L
-00002170: 6f63 6b68 6565 6420 5665 6761 2035 4227  ockheed Vega 5B'
-00002180: 2c20 276c 6162 656c 273a 2027 7072 6f64  , 'label': 'prod
-00002190: 7563 742d 6169 7270 6c61 6e65 272c 2027  uct-airplane', '
-000021a0: 7363 6f72 6527 3a20 302e 3937 3235 3738  score': 0.972578
-000021b0: 3538 3531 3437 3835 3737 2c20 2763 6861  5851478577, 'cha
-000021c0: 725f 7374 6172 745f 696e 6465 7827 3a20  r_start_index': 
-000021d0: 3338 2c20 2763 6861 725f 656e 645f 696e  38, 'char_end_in
-000021e0: 6465 7827 3a20 3534 7d2c 0d0a 207b 2773  dex': 54},.. {'s
-000021f0: 7061 6e27 3a20 2741 746c 616e 7469 6327  pan': 'Atlantic'
-00002200: 2c20 276c 6162 656c 273a 2027 6c6f 6361  , 'label': 'loca
-00002210: 7469 6f6e 2d62 6f64 6965 736f 6677 6174  tion-bodiesofwat
-00002220: 6572 272c 2027 7363 6f72 6527 3a20 302e  er', 'score': 0.
-00002230: 3735 3837 3637 3930 3238 3531 3130 3437  7587679028511047
-00002240: 2c20 2763 6861 725f 7374 6172 745f 696e  , 'char_start_in
-00002250: 6465 7827 3a20 3636 2c20 2763 6861 725f  dex': 66, 'char_
-00002260: 656e 645f 696e 6465 7827 3a20 3734 7d2c  end_index': 74},
-00002270: 0d0a 207b 2773 7061 6e27 3a20 2750 6172  .. {'span': 'Par
-00002280: 6973 272c 2027 6c61 6265 6c27 3a20 276c  is', 'label': 'l
-00002290: 6f63 6174 696f 6e2d 4750 4527 2c20 2773  ocation-GPE', 's
-000022a0: 636f 7265 273a 2030 2e39 3839 3233 3930  core': 0.9892390
-000022b0: 3936 3634 3135 3430 352c 2027 6368 6172  966415405, 'char
-000022c0: 5f73 7461 7274 5f69 6e64 6578 273a 2037  _start_index': 7
-000022d0: 382c 2027 6368 6172 5f65 6e64 5f69 6e64  8, 'char_end_ind
-000022e0: 6578 273a 2038 337d 5d0d 0a60 6060 0d0a  ex': 83}]..```..
-000022f0: 0d0a 3c21 2d2d 2042 6563 6175 7365 2074  ..<!-- Because t
-00002300: 6869 7320 776f 726b 2069 7320 6261 7365  his work is base
-00002310: 6420 6f6e 205b 504c 2d4d 6172 6b65 725d  d on [PL-Marker]
-00002320: 2868 7474 7073 3a2f 2f61 7278 6976 2e6f  (https://arxiv.o
-00002330: 7267 2f70 6466 2f32 3130 392e 3036 3036  rg/pdf/2109.0606
-00002340: 3776 352e 7064 6629 2c20 796f 7520 6d61  7v5.pdf), you ma
-00002350: 7920 6578 7065 6374 2073 696d 696c 6172  y expect similar
-00002360: 2072 6573 756c 7473 2074 6f20 6974 7320   results to its 
-00002370: 5b50 6170 6572 7320 7769 7468 2043 6f64  [Papers with Cod
-00002380: 6520 4c65 6164 6572 626f 6172 645d 2868  e Leaderboard](h
-00002390: 7474 7073 3a2f 2f70 6170 6572 7377 6974  ttps://paperswit
-000023a0: 6863 6f64 652e 636f 6d2f 7061 7065 722f  hcode.com/paper/
-000023b0: 7061 636b 2d74 6f67 6574 6865 722d 656e  pack-together-en
-000023c0: 7469 7479 2d61 6e64 2d72 656c 6174 696f  tity-and-relatio
-000023d0: 6e2d 6578 7472 6163 7469 6f6e 2920 7265  n-extraction) re
-000023e0: 7375 6c74 732e 202d 2d3e 0d0a 0d0a 2323  sults. -->....##
-000023f0: 2050 7265 7472 6169 6e65 6420 4d6f 6465   Pretrained Mode
-00002400: 6c73 0d0a 0d0a 416c 6c20 6d6f 6465 6c73  ls....All models
-00002410: 2069 6e20 7468 6973 206c 6973 7420 636f   in this list co
-00002420: 6e74 6169 6e20 6074 7261 696e 2e70 7960  ntain `train.py`
-00002430: 2066 696c 6573 2074 6861 7420 7368 6f77   files that show
-00002440: 2074 6865 2074 7261 696e 696e 6720 7363   the training sc
-00002450: 7269 7074 7320 7573 6564 2074 6f20 6765  ripts used to ge
-00002460: 6e65 7261 7465 2074 6865 6d2e 2041 6464  nerate them. Add
-00002470: 6974 696f 6e61 6c6c 792c 2061 6c6c 2074  itionally, all t
-00002480: 7261 696e 696e 6720 7363 7269 7074 7320  raining scripts 
-00002490: 7573 6564 2061 7265 2073 746f 7265 6420  used are stored 
-000024a0: 696e 2074 6865 205b 7472 6169 6e69 6e67  in the [training
-000024b0: 5f73 6372 6970 7473 5d28 7472 6169 6e69  _scripts](traini
-000024c0: 6e67 5f73 6372 6970 7473 2920 6469 7265  ng_scripts) dire
-000024d0: 6374 6f72 792e 0d0a 5468 6573 6520 7472  ctory...These tr
-000024e0: 6169 6e65 6420 6d6f 6465 6c73 2068 6176  ained models hav
-000024f0: 6520 486f 7374 6564 2049 6e66 6572 656e  e Hosted Inferen
-00002500: 6365 2041 5049 2077 6964 6765 7473 2074  ce API widgets t
-00002510: 6861 7420 796f 7520 6361 6e20 7573 6520  hat you can use 
-00002520: 746f 2065 7870 6572 696d 656e 7420 7769  to experiment wi
-00002530: 7468 2074 6865 206d 6f64 656c 7320 6f6e  th the models on
-00002540: 2074 6865 6972 2048 7567 6769 6e67 2046   their Hugging F
-00002550: 6163 6520 6d6f 6465 6c20 7061 6765 732e  ace model pages.
-00002560: 2041 6464 6974 696f 6e61 6c6c 792c 2048   Additionally, H
-00002570: 7567 6769 6e67 2046 6163 6520 7072 6f76  ugging Face prov
-00002580: 6964 6573 2065 6163 6820 6d6f 6465 6c20  ides each model 
-00002590: 7769 7468 2061 2066 7265 6520 4150 4920  with a free API 
-000025a0: 2860 4465 706c 6f79 6020 3e20 6049 6e66  (`Deploy` > `Inf
-000025b0: 6572 656e 6365 2041 5049 6020 6f6e 2074  erence API` on t
-000025c0: 6865 206d 6f64 656c 2070 6167 6529 2e0d  he model page)..
-000025d0: 0a0d 0a23 2323 2046 6577 4e45 5244 0d0a  ...### FewNERD..
-000025e0: 2a20 5b60 746f 6d61 6172 7365 6e2f 7370  * [`tomaarsen/sp
-000025f0: 616e 2d6d 6172 6b65 722d 6265 7274 2d62  an-marker-bert-b
-00002600: 6173 652d 6665 776e 6572 642d 6669 6e65  ase-fewnerd-fine
-00002610: 2d73 7570 6572 605d 2868 7474 7073 3a2f  -super`](https:/
-00002620: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
-00002630: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
-00002640: 6172 6b65 722d 6265 7274 2d62 6173 652d  arker-bert-base-
-00002650: 6665 776e 6572 642d 6669 6e65 2d73 7570  fewnerd-fine-sup
-00002660: 6572 2920 6973 2061 206d 6f64 656c 2074  er) is a model t
-00002670: 6861 7420 4920 6861 7665 2074 7261 696e  hat I have train
-00002680: 6564 2069 6e20 3220 686f 7572 7320 6f6e  ed in 2 hours on
-00002690: 2074 6865 2066 696e 6567 7261 696e 6564   the finegrained
-000026a0: 2c20 7375 7065 7276 6973 6564 205b 4665  , supervised [Fe
-000026b0: 772d 4e45 5244 2064 6174 6173 6574 5d28  w-NERD dataset](
-000026c0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
-000026d0: 6163 652e 636f 2f64 6174 6173 6574 732f  ace.co/datasets/
-000026e0: 4446 4b49 2d53 4c54 2f66 6577 2d6e 6572  DFKI-SLT/few-ner
-000026f0: 6429 2e20 4974 2072 6561 6368 6564 2061  d). It reached a
-00002700: 2030 2e37 3035 3320 5465 7374 2046 312c   0.7053 Test F1,
-00002710: 2063 6f6d 7065 7469 7469 7665 2069 6e20   competitive in 
-00002720: 7468 6520 616c 6c2d 7469 6d65 205b 4665  the all-time [Fe
-00002730: 772d 4e45 5244 206c 6561 6465 7262 6f61  w-NERD leaderboa
-00002740: 7264 5d28 6874 7470 733a 2f2f 7061 7065  rd](https://pape
-00002750: 7273 7769 7468 636f 6465 2e63 6f6d 2f73  rswithcode.com/s
-00002760: 6f74 612f 6e61 6d65 642d 656e 7469 7479  ota/named-entity
-00002770: 2d72 6563 6f67 6e69 7469 6f6e 2d6f 6e2d  -recognition-on-
-00002780: 6665 772d 6e65 7264 2d73 7570 2920 7573  few-nerd-sup) us
-00002790: 696e 6720 6062 6572 742d 6261 7365 602e  ing `bert-base`.
-000027a0: 204d 7920 7472 6169 6e69 6e67 2073 6372   My training scr
-000027b0: 6970 7420 7265 7365 6d62 6c65 7320 7468  ipt resembles th
-000027c0: 6520 6f6e 6520 7468 6174 2079 6f75 2063  e one that you c
-000027d0: 616e 2073 6565 2061 626f 7665 2e0d 0a20  an see above... 
-000027e0: 202a 2054 7279 2074 6865 206d 6f64 656c   * Try the model
-000027f0: 206f 7574 206f 6e6c 696e 6520 7573 696e   out online usin
-00002800: 6720 7468 6973 205b f09f a497 2053 7061  g this [.... Spa
-00002810: 6365 5d28 6874 7470 733a 2f2f 746f 6d61  ce](https://toma
-00002820: 6172 7365 6e2d 7370 616e 2d6d 6172 6b65  arsen-span-marke
-00002830: 722d 6265 7274 2d62 6173 652d 6665 776e  r-bert-base-fewn
-00002840: 6572 642d 6669 6e65 2d73 7570 6572 2e68  erd-fine-super.h
-00002850: 662e 7370 6163 652f 292e 0d0a 0d0a 2a20  f.space/).....* 
-00002860: 5b60 746f 6d61 6172 7365 6e2f 7370 616e  [`tomaarsen/span
-00002870: 2d6d 6172 6b65 722d 726f 6265 7274 612d  -marker-roberta-
-00002880: 6c61 7267 652d 6665 776e 6572 642d 6669  large-fewnerd-fi
-00002890: 6e65 2d73 7570 6572 605d 2868 7474 7073  ne-super`](https
-000028a0: 3a2f 2f68 7567 6769 6e67 6661 6365 2e63  ://huggingface.c
-000028b0: 6f2f 746f 6d61 6172 7365 6e2f 7370 616e  o/tomaarsen/span
-000028c0: 2d6d 6172 6b65 722d 726f 6265 7274 612d  -marker-roberta-
-000028d0: 6c61 7267 652d 6665 776e 6572 642d 6669  large-fewnerd-fi
-000028e0: 6e65 2d73 7570 6572 2920 7761 7320 7472  ne-super) was tr
-000028f0: 6169 6e65 6420 696e 2036 2068 6f75 7273  ained in 6 hours
-00002900: 206f 6e20 7468 6520 6669 6e65 6772 6169   on the finegrai
-00002910: 6e65 642c 2073 7570 6572 7669 7365 6420  ned, supervised 
-00002920: 5b46 6577 2d4e 4552 4420 6461 7461 7365  [Few-NERD datase
-00002930: 745d 2868 7474 7073 3a2f 2f68 7567 6769  t](https://huggi
-00002940: 6e67 6661 6365 2e63 6f2f 6461 7461 7365  ngface.co/datase
-00002950: 7473 2f44 464b 492d 534c 542f 6665 772d  ts/DFKI-SLT/few-
-00002960: 6e65 7264 2920 7573 696e 6720 6072 6f62  nerd) using `rob
-00002970: 6572 7461 2d6c 6172 6765 602e 2049 7420  erta-large`. It 
-00002980: 7265 6163 6865 6420 6120 302e 3731 3033  reached a 0.7103
-00002990: 2054 6573 7420 4631 2c20 7265 6163 6869   Test F1, reachi
-000029a0: 6e67 2061 206e 6577 2073 7461 7465 206f  ng a new state o
-000029b0: 6620 7468 6520 6172 7420 696e 2074 6865  f the art in the
-000029c0: 2061 6c6c 2d74 696d 6520 5b46 6577 2d4e   all-time [Few-N
-000029d0: 4552 4420 6c65 6164 6572 626f 6172 645d  ERD leaderboard]
-000029e0: 2868 7474 7073 3a2f 2f70 6170 6572 7377  (https://papersw
-000029f0: 6974 6863 6f64 652e 636f 6d2f 736f 7461  ithcode.com/sota
-00002a00: 2f6e 616d 6564 2d65 6e74 6974 792d 7265  /named-entity-re
-00002a10: 636f 676e 6974 696f 6e2d 6f6e 2d66 6577  cognition-on-few
-00002a20: 2d6e 6572 642d 7375 7029 2e0d 0a2a 205b  -nerd-sup)...* [
-00002a30: 6074 6f6d 6161 7273 656e 2f73 7061 6e2d  `tomaarsen/span-
-00002a40: 6d61 726b 6572 2d78 6c6d 2d72 6f62 6572  marker-xlm-rober
-00002a50: 7461 2d62 6173 652d 6665 776e 6572 642d  ta-base-fewnerd-
-00002a60: 6669 6e65 2d73 7570 6572 605d 2868 7474  fine-super`](htt
-00002a70: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
-00002a80: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
-00002a90: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
-00002aa0: 6265 7274 612d 6261 7365 2d66 6577 6e65  berta-base-fewne
-00002ab0: 7264 2d66 696e 652d 7375 7065 7229 2069  rd-fine-super) i
-00002ac0: 7320 6120 6d75 6c74 696c 696e 6775 616c  s a multilingual
-00002ad0: 206d 6f64 656c 2074 6861 7420 4920 6861   model that I ha
-00002ae0: 7665 2074 7261 696e 6564 2069 6e20 312e  ve trained in 1.
-00002af0: 3520 686f 7572 7320 6f6e 2074 6865 2066  5 hours on the f
-00002b00: 696e 6567 7261 696e 6564 2c20 7375 7065  inegrained, supe
-00002b10: 7276 6973 6564 205b 4665 772d 4e45 5244  rvised [Few-NERD
-00002b20: 2064 6174 6173 6574 5d28 6874 7470 733a   dataset](https:
-00002b30: 2f2f 6875 6767 696e 6766 6163 652e 636f  //huggingface.co
-00002b40: 2f64 6174 6173 6574 732f 4446 4b49 2d53  /datasets/DFKI-S
-00002b50: 4c54 2f66 6577 2d6e 6572 6429 2e20 4974  LT/few-nerd). It
-00002b60: 2072 6561 6368 6564 2061 2030 2e36 3836   reached a 0.686
-00002b70: 2054 6573 7420 4631 206f 6e20 456e 676c   Test F1 on Engl
-00002b80: 6973 682c 2061 6e64 2077 6f72 6b73 2077  ish, and works w
-00002b90: 656c 6c20 6f6e 206f 7468 6572 206c 616e  ell on other lan
-00002ba0: 6775 6167 6573 206c 696b 6520 5370 616e  guages like Span
-00002bb0: 6973 682c 2046 7265 6e63 682c 2047 6572  ish, French, Ger
-00002bc0: 6d61 6e2c 2052 7573 7369 616e 2c20 4475  man, Russian, Du
-00002bd0: 7463 682c 2050 6f6c 6973 682c 2049 6365  tch, Polish, Ice
-00002be0: 6c61 6e64 6963 2c20 4772 6565 6b20 616e  landic, Greek an
-00002bf0: 6420 6d61 6e79 206d 6f72 652e 0d0a 0d0a  d many more.....
-00002c00: 2323 2320 4f6e 746f 4e6f 7465 7320 7635  ### OntoNotes v5
-00002c10: 2e30 0d0a 2a20 5b60 746f 6d61 6172 7365  .0..* [`tomaarse
-00002c20: 6e2f 7370 616e 2d6d 6172 6b65 722d 726f  n/span-marker-ro
-00002c30: 6265 7274 612d 6c61 7267 652d 6f6e 746f  berta-large-onto
-00002c40: 6e6f 7465 7335 605d 2868 7474 7073 3a2f  notes5`](https:/
-00002c50: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
-00002c60: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
-00002c70: 6172 6b65 722d 726f 6265 7274 612d 6c61  arker-roberta-la
-00002c80: 7267 652d 6f6e 746f 6e6f 7465 7335 2920  rge-ontonotes5) 
-00002c90: 7761 7320 7472 6169 6e65 6420 696e 2033  was trained in 3
-00002ca0: 2068 6f75 7273 206f 6e20 7468 6520 4f6e   hours on the On
-00002cb0: 746f 4e6f 7465 7320 7635 2e30 2064 6174  toNotes v5.0 dat
-00002cc0: 6173 6574 2c20 7265 6163 6869 6e67 2061  aset, reaching a
-00002cd0: 2070 6572 666f 726d 616e 6365 206f 6620   performance of 
-00002ce0: 302e 3931 3534 2046 312e 2046 6f72 2072  0.9154 F1. For r
-00002cf0: 6566 6572 656e 6365 2c20 7468 6520 6375  eference, the cu
-00002d00: 7272 656e 7420 7374 726f 6e67 6573 7420  rrent strongest 
-00002d10: 7370 6143 7920 6d6f 6465 6c20 2860 656e  spaCy model (`en
-00002d20: 5f63 6f72 655f 7765 625f 7472 6660 2920  _core_web_trf`) 
-00002d30: 7265 6163 6865 7320 302e 3839 382e 2054  reaches 0.898. T
-00002d40: 6869 7320 5370 616e 4d61 726b 6572 206d  his SpanMarker m
-00002d50: 6f64 656c 2075 7365 7320 6120 6072 6f62  odel uses a `rob
-00002d60: 6572 7461 2d6c 6172 6765 6020 656e 636f  erta-large` enco
-00002d70: 6465 7220 756e 6465 7220 7468 6520 686f  der under the ho
-00002d80: 6f64 2e0d 0a0d 0a23 2323 2043 6f4e 4c4c  od.....### CoNLL
-00002d90: 3033 0d0a 2a20 5b60 746f 6d61 6172 7365  03..* [`tomaarse
-00002da0: 6e2f 7370 616e 2d6d 6172 6b65 722d 786c  n/span-marker-xl
-00002db0: 6d2d 726f 6265 7274 612d 6c61 7267 652d  m-roberta-large-
-00002dc0: 636f 6e6c 6c30 3360 5d28 6874 7470 733a  conll03`](https:
-00002dd0: 2f2f 6875 6767 696e 6766 6163 652e 636f  //huggingface.co
-00002de0: 2f74 6f6d 6161 7273 656e 2f73 7061 6e2d  /tomaarsen/span-
-00002df0: 6d61 726b 6572 2d78 6c6d 2d72 6f62 6572  marker-xlm-rober
-00002e00: 7461 2d6c 6172 6765 2d63 6f6e 6c6c 3033  ta-large-conll03
-00002e10: 2920 6973 2061 2053 7061 6e4d 6172 6b65  ) is a SpanMarke
-00002e20: 7220 6d6f 6465 6c20 7573 696e 6720 6078  r model using `x
-00002e30: 6c6d 2d72 6f62 6572 7461 2d6c 6172 6765  lm-roberta-large
-00002e40: 6020 7468 6174 2077 6173 2074 7261 696e  ` that was train
-00002e50: 6564 2069 6e20 3435 206d 696e 7574 6573  ed in 45 minutes
-00002e60: 2e20 4974 2072 6561 6368 6573 2061 2073  . It reaches a s
-00002e70: 7461 7465 206f 6620 7468 6520 6172 7420  tate of the art 
-00002e80: 302e 3933 3120 4631 206f 6e20 436f 4e4c  0.931 F1 on CoNL
-00002e90: 4c30 3320 7769 7468 6f75 7420 7573 696e  L03 without usin
-00002ea0: 6720 646f 6375 6d65 6e74 2d6c 6576 656c  g document-level
-00002eb0: 2063 6f6e 7465 7874 2e20 466f 7220 7265   context. For re
-00002ec0: 6665 7265 6e63 652c 2074 6865 2063 7572  ference, the cur
-00002ed0: 7265 6e74 2073 7472 6f6e 6765 7374 2073  rent strongest s
-00002ee0: 7061 4379 206d 6f64 656c 2028 6065 6e5f  paCy model (`en_
-00002ef0: 636f 7265 5f77 6562 5f74 7266 6029 2072  core_web_trf`) r
-00002f00: 6561 6368 6573 2039 312e 362e 0d0a 2a20  eaches 91.6...* 
-00002f10: 5b60 746f 6d61 6172 7365 6e2f 7370 616e  [`tomaarsen/span
-00002f20: 2d6d 6172 6b65 722d 786c 6d2d 726f 6265  -marker-xlm-robe
-00002f30: 7274 612d 6c61 7267 652d 636f 6e6c 6c30  rta-large-conll0
-00002f40: 332d 646f 632d 636f 6e74 6578 7460 5d28  3-doc-context`](
-00002f50: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
-00002f60: 6163 652e 636f 2f74 6f6d 6161 7273 656e  ace.co/tomaarsen
-00002f70: 2f73 7061 6e2d 6d61 726b 6572 2d78 6c6d  /span-marker-xlm
-00002f80: 2d72 6f62 6572 7461 2d6c 6172 6765 2d63  -roberta-large-c
-00002f90: 6f6e 6c6c 3033 2d64 6f63 2d63 6f6e 7465  onll03-doc-conte
-00002fa0: 7874 2920 6973 2061 6e6f 7468 6572 2053  xt) is another S
-00002fb0: 7061 6e4d 6172 6b65 7220 6d6f 6465 6c20  panMarker model 
-00002fc0: 7573 696e 6720 7468 6520 6078 6c6d 2d72  using the `xlm-r
-00002fd0: 6f62 6572 7461 2d6c 6172 6765 6020 656e  oberta-large` en
-00002fe0: 636f 6465 722e 2049 7420 7573 6573 205b  coder. It uses [
-00002ff0: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
-00003000: 6f6e 7465 7874 5d28 6874 7470 733a 2f2f  ontext](https://
-00003010: 746f 6d61 6172 7365 6e2e 6769 7468 7562  tomaarsen.github
-00003020: 2e69 6f2f 5370 616e 4d61 726b 6572 4e45  .io/SpanMarkerNE
-00003030: 522f 6e6f 7465 626f 6f6b 732f 646f 6375  R/notebooks/docu
-00003040: 6d65 6e74 5f6c 6576 656c 5f63 6f6e 7465  ment_level_conte
-00003050: 7874 2e68 746d 6c29 2074 6f20 7265 6163  xt.html) to reac
-00003060: 6820 6120 7374 6174 6520 6f66 2074 6865  h a state of the
-00003070: 2061 7274 2030 2e39 3434 2046 312e 2046   art 0.944 F1. F
-00003080: 6f72 2074 6865 2062 6573 7420 7065 7266  or the best perf
-00003090: 6f72 6d61 6e63 652c 2069 6e66 6572 656e  ormance, inferen
-000030a0: 6365 2073 686f 756c 6420 6265 2070 6572  ce should be per
-000030b0: 666f 726d 6564 2075 7369 6e67 2064 6f63  formed using doc
-000030c0: 756d 656e 742d 6c65 7665 6c20 636f 6e74  ument-level cont
-000030d0: 6578 7420 285b 646f 6373 5d28 6874 7470  ext ([docs](http
-000030e0: 733a 2f2f 746f 6d61 6172 7365 6e2e 6769  s://tomaarsen.gi
-000030f0: 7468 7562 2e69 6f2f 5370 616e 4d61 726b  thub.io/SpanMark
-00003100: 6572 4e45 522f 6e6f 7465 626f 6f6b 732f  erNER/notebooks/
-00003110: 646f 6375 6d65 6e74 5f6c 6576 656c 5f63  document_level_c
-00003120: 6f6e 7465 7874 2e68 746d 6c23 496e 6665  ontext.html#Infe
-00003130: 7265 6e63 6529 292e 2054 6869 7320 6d6f  rence)). This mo
-00003140: 6465 6c20 7761 7320 7472 6169 6e65 6420  del was trained 
-00003150: 696e 2031 2068 6f75 722e 0d0a 0d0a 2323  in 1 hour.....##
-00003160: 2320 436f 4e4c 4c2b 2b0d 0a2a 205b 6074  # CoNLL++..* [`t
-00003170: 6f6d 6161 7273 656e 2f73 7061 6e2d 6d61  omaarsen/span-ma
-00003180: 726b 6572 2d78 6c6d 2d72 6f62 6572 7461  rker-xlm-roberta
-00003190: 2d6c 6172 6765 2d63 6f6e 6c6c 7070 2d64  -large-conllpp-d
-000031a0: 6f63 2d63 6f6e 7465 7874 605d 2868 7474  oc-context`](htt
-000031b0: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
-000031c0: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
-000031d0: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
-000031e0: 6265 7274 612d 6c61 7267 652d 636f 6e6c  berta-large-conl
-000031f0: 6c70 702d 646f 632d 636f 6e74 6578 7429  lpp-doc-context)
-00003200: 2077 6173 2074 7261 696e 6564 2069 6e20   was trained in 
-00003210: 616e 2068 6f75 7220 7573 696e 6720 7468  an hour using th
-00003220: 6520 6078 6c6d 2d72 6f62 6572 7461 2d6c  e `xlm-roberta-l
-00003230: 6172 6765 6020 656e 636f 6465 7220 6f6e  arge` encoder on
-00003240: 2074 6865 2043 6f4e 4c4c 2b2b 2064 6174   the CoNLL++ dat
-00003250: 6173 6574 2e20 5573 696e 6720 5b64 6f63  aset. Using [doc
-00003260: 756d 656e 742d 6c65 7665 6c20 636f 6e74  ument-level cont
-00003270: 6578 745d 2868 7474 7073 3a2f 2f74 6f6d  ext](https://tom
-00003280: 6161 7273 656e 2e67 6974 6875 622e 696f  aarsen.github.io
-00003290: 2f53 7061 6e4d 6172 6b65 724e 4552 2f6e  /SpanMarkerNER/n
-000032a0: 6f74 6562 6f6f 6b73 2f64 6f63 756d 656e  otebooks/documen
-000032b0: 745f 6c65 7665 6c5f 636f 6e74 6578 742e  t_level_context.
-000032c0: 6874 6d6c 292c 2069 7420 7265 6163 6865  html), it reache
-000032d0: 7320 6120 7665 7279 2063 6f6d 7065 7469  s a very competi
-000032e0: 7469 7665 2030 2e39 3535 2046 312e 2046  tive 0.955 F1. F
-000032f0: 6f72 2074 6865 2062 6573 7420 7065 7266  or the best perf
-00003300: 6f72 6d61 6e63 652c 2069 6e66 6572 656e  ormance, inferen
-00003310: 6365 2073 686f 756c 6420 6265 2070 6572  ce should be per
-00003320: 666f 726d 6564 2075 7369 6e67 2064 6f63  formed using doc
-00003330: 756d 656e 742d 6c65 7665 6c20 636f 6e74  ument-level cont
-00003340: 6578 7420 285b 646f 6373 5d28 6874 7470  ext ([docs](http
-00003350: 733a 2f2f 746f 6d61 6172 7365 6e2e 6769  s://tomaarsen.gi
-00003360: 7468 7562 2e69 6f2f 5370 616e 4d61 726b  thub.io/SpanMark
-00003370: 6572 4e45 522f 6e6f 7465 626f 6f6b 732f  erNER/notebooks/
-00003380: 646f 6375 6d65 6e74 5f6c 6576 656c 5f63  document_level_c
-00003390: 6f6e 7465 7874 2e68 746d 6c23 496e 6665  ontext.html#Infe
-000033a0: 7265 6e63 6529 292e 0d0a 0d0a 2323 2055  rence)).....## U
-000033b0: 7369 6e67 2070 7265 7472 6169 6e65 6420  sing pretrained 
-000033c0: 5370 616e 4d61 726b 6572 206d 6f64 656c  SpanMarker model
-000033d0: 7320 7769 7468 2073 7061 4379 0d0a 416c  s with spaCy..Al
-000033e0: 6c20 5b53 7061 6e4d 6172 6b65 7220 6d6f  l [SpanMarker mo
-000033f0: 6465 6c73 206f 6e20 7468 6520 4875 6767  dels on the Hugg
-00003400: 696e 6720 4661 6365 2048 7562 5d28 6874  ing Face Hub](ht
-00003410: 7470 733a 2f2f 6875 6767 696e 6766 6163  tps://huggingfac
-00003420: 652e 636f 2f6d 6f64 656c 733f 6c69 6272  e.co/models?libr
-00003430: 6172 793d 7370 616e 2d6d 6172 6b65 7229  ary=span-marker)
-00003440: 2063 616e 2061 6c73 6f20 6265 2065 6173   can also be eas
-00003450: 696c 7920 7573 6564 2069 6e20 7370 6143  ily used in spaC
-00003460: 792e 2049 7427 7320 6173 2073 696d 706c  y. It's as simpl
-00003470: 6520 6173 2069 6e63 6c75 6469 6e67 2031  e as including 1
-00003480: 206c 696e 6520 746f 2061 6464 2074 6865   line to add the
-00003490: 2060 7370 616e 5f6d 6172 6b65 7260 2070   `span_marker` p
-000034a0: 6970 656c 696e 652e 2053 6565 2074 6865  ipeline. See the
-000034b0: 2044 6f63 756d 656e 7461 7469 6f6e 206f   Documentation o
-000034c0: 7220 4150 4920 5265 6665 7265 6e63 6520  r API Reference 
-000034d0: 666f 7220 6d6f 7265 2069 6e66 6f72 6d61  for more informa
-000034e0: 7469 6f6e 2e0d 0a60 6060 7079 7468 6f6e  tion...```python
-000034f0: 0d0a 696d 706f 7274 2073 7061 6379 0d0a  ..import spacy..
-00003500: 0d0a 2320 4c6f 6164 2074 6865 2073 7061  ..# Load the spa
-00003510: 4379 206d 6f64 656c 2077 6974 6820 7468  Cy model with th
-00003520: 6520 7370 616e 5f6d 6172 6b65 7220 7069  e span_marker pi
-00003530: 7065 6c69 6e65 2063 6f6d 706f 6e65 6e74  peline component
-00003540: 0d0a 6e6c 7020 3d20 7370 6163 792e 6c6f  ..nlp = spacy.lo
-00003550: 6164 2822 656e 5f63 6f72 655f 7765 625f  ad("en_core_web_
-00003560: 736d 222c 2064 6973 6162 6c65 3d5b 226e  sm", disable=["n
-00003570: 6572 225d 290d 0a6e 6c70 2e61 6464 5f70  er"])..nlp.add_p
-00003580: 6970 6528 2273 7061 6e5f 6d61 726b 6572  ipe("span_marker
-00003590: 222c 2063 6f6e 6669 673d 7b22 6d6f 6465  ", config={"mode
-000035a0: 6c22 3a20 2274 6f6d 6161 7273 656e 2f73  l": "tomaarsen/s
-000035b0: 7061 6e2d 6d61 726b 6572 2d72 6f62 6572  pan-marker-rober
-000035c0: 7461 2d6c 6172 6765 2d6f 6e74 6f6e 6f74  ta-large-ontonot
-000035d0: 6573 3522 7d29 0d0a 0d0a 2320 4665 6564  es5"})....# Feed
-000035e0: 2073 6f6d 6520 7465 7874 2074 6872 6f75   some text throu
-000035f0: 6768 2074 6865 206d 6f64 656c 2074 6f20  gh the model to 
-00003600: 6765 7420 6120 7370 6163 7920 446f 630d  get a spacy Doc.
-00003610: 0a74 6578 7420 3d20 2222 2243 6c65 6f70  .text = """Cleop
-00003620: 6174 7261 2056 4949 2c20 616c 736f 206b  atra VII, also k
-00003630: 6e6f 776e 2061 7320 436c 656f 7061 7472  nown as Cleopatr
-00003640: 6120 7468 6520 4772 6561 742c 2077 6173  a the Great, was
-00003650: 2074 6865 206c 6173 7420 6163 7469 7665   the last active
-00003660: 2072 756c 6572 206f 6620 7468 6520 5c0d   ruler of the \.
-00003670: 0a50 746f 6c65 6d61 6963 204b 696e 6764  .Ptolemaic Kingd
-00003680: 6f6d 206f 6620 4567 7970 742e 2053 6865  om of Egypt. She
-00003690: 2077 6173 2062 6f72 6e20 696e 2036 3920   was born in 69 
-000036a0: 4243 4520 616e 6420 7275 6c65 6420 4567  BCE and ruled Eg
-000036b0: 7970 7420 6672 6f6d 2035 3120 4243 4520  ypt from 51 BCE 
-000036c0: 756e 7469 6c20 6865 7220 5c0d 0a64 6561  until her \..dea
-000036d0: 7468 2069 6e20 3330 2042 4345 2e22 2222  th in 30 BCE."""
-000036e0: 0d0a 646f 6320 3d20 6e6c 7028 7465 7874  ..doc = nlp(text
-000036f0: 290d 0a0d 0a23 2041 6e64 206c 6f6f 6b20  )....# And look 
-00003700: 6174 2074 6865 2065 6e74 6974 6965 730d  at the entities.
-00003710: 0a70 7269 6e74 285b 2865 6e74 6974 792c  .print([(entity,
-00003720: 2065 6e74 6974 792e 6c61 6265 6c5f 2920   entity.label_) 
-00003730: 666f 7220 656e 7469 7479 2069 6e20 646f  for entity in do
-00003740: 632e 656e 7473 5d29 0d0a 2222 220d 0a5b  c.ents]).."""..[
-00003750: 2843 6c65 6f70 6174 7261 2056 4949 2c20  (Cleopatra VII, 
-00003760: 2250 4552 534f 4e22 292c 2028 436c 656f  "PERSON"), (Cleo
-00003770: 7061 7472 6120 7468 6520 4772 6561 742c  patra the Great,
-00003780: 2022 5045 5253 4f4e 2229 2c20 2874 6865   "PERSON"), (the
-00003790: 2050 746f 6c65 6d61 6963 204b 696e 6764   Ptolemaic Kingd
-000037a0: 6f6d 206f 6620 4567 7970 742c 2022 4750  om of Egypt, "GP
-000037b0: 4522 292c 0d0a 2836 3920 4243 452c 2022  E"),..(69 BCE, "
-000037c0: 4441 5445 2229 2c20 2845 6779 7074 2c20  DATE"), (Egypt, 
-000037d0: 2247 5045 2229 2c20 2835 3120 4243 452c  "GPE"), (51 BCE,
-000037e0: 2022 4441 5445 2229 2c20 2833 3020 4243   "DATE"), (30 BC
-000037f0: 452c 2022 4441 5445 2229 5d0d 0a22 2222  E, "DATE")].."""
-00003800: 0d0a 6060 600d 0a21 5b69 6d61 6765 5d28  ..```..![image](
-00003810: 6874 7470 733a 2f2f 7573 6572 2d69 6d61  https://user-ima
-00003820: 6765 732e 6769 7468 7562 7573 6572 636f  ges.githubuserco
-00003830: 6e74 656e 742e 636f 6d2f 3337 3632 3134  ntent.com/376214
-00003840: 3931 2f32 3436 3137 3036 3233 2d36 3335  91/246170623-635
-00003850: 3163 6237 652d 6262 6230 2d34 3437 322d  1cb7e-bbb0-4472-
-00003860: 6166 3136 2d39 6133 3531 6132 3533 6461  af16-9a351a253da
-00003870: 392e 706e 6729 0d0a 0d0a 2323 2043 6f6e  9.png)....## Con
-00003880: 7465 7874 0d0a 3c68 3120 616c 6967 6e3d  text..<h1 align=
-00003890: 2263 656e 7465 7222 3e0d 0a20 2020 203c  "center">..    <
-000038a0: 6120 6872 6566 3d22 6874 7470 733a 2f2f  a href="https://
-000038b0: 6769 7468 7562 2e63 6f6d 2f61 7267 696c  github.com/argil
-000038c0: 6c61 2d69 6f2f 6172 6769 6c6c 6122 3e0d  la-io/argilla">.
-000038d0: 0a20 2020 203c 696d 6720 7372 633d 2268  .    <img src="h
-000038e0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
-000038f0: 6d2f 6476 7372 6570 6f2f 696d 6773 2f72  m/dvsrepo/imgs/r
-00003900: 6177 2f6d 6169 6e2f 7267 2e73 7667 2220  aw/main/rg.svg" 
-00003910: 616c 743d 2241 7267 696c 6c61 2220 7769  alt="Argilla" wi
-00003920: 6474 683d 2231 3530 223e 0d0a 2020 2020  dth="150">..    
-00003930: 3c2f 613e 0d0a 3c2f 6831 3e0d 0a0d 0a49  </a>..</h1>....I
-00003940: 2068 6176 6520 6465 7665 6c6f 7065 6420   have developed 
-00003950: 7468 6973 206c 6962 7261 7279 2061 7320  this library as 
-00003960: 6120 7061 7274 206f 6620 6d79 2074 6865  a part of my the
-00003970: 7369 7320 776f 726b 2061 7420 5b41 7267  sis work at [Arg
-00003980: 696c 6c61 5d28 6874 7470 733a 2f2f 6769  illa](https://gi
-00003990: 7468 7562 2e63 6f6d 2f61 7267 696c 6c61  thub.com/argilla
-000039a0: 2d69 6f2f 6172 6769 6c6c 6129 2e0d 0a46  -io/argilla)...F
-000039b0: 6565 6c20 6672 6565 2074 6f20 e2ad 9020  eel free to ... 
-000039c0: 7374 6172 206f 7220 7761 7463 6820 7468  star or watch th
-000039d0: 6520 5370 616e 4d61 726b 6572 2072 6570  e SpanMarker rep
-000039e0: 6f73 6974 6f72 7920 746f 2067 6574 206e  ository to get n
-000039f0: 6f74 6966 6965 6420 7768 656e 206d 7920  otified when my 
-00003a00: 7468 6573 6973 2069 7320 7075 626c 6973  thesis is publis
-00003a10: 6865 642e 0d0a 0d0a 2323 2043 6861 6e67  hed.....## Chang
-00003a20: 656c 6f67 0d0a 5365 6520 5b43 4841 4e47  elog..See [CHANG
-00003a30: 454c 4f47 2e6d 645d 2843 4841 4e47 454c  ELOG.md](CHANGEL
-00003a40: 4f47 2e6d 6429 2066 6f72 206e 6577 7320  OG.md) for news 
-00003a50: 6f6e 2061 6c6c 2053 7061 6e4d 6172 6b65  on all SpanMarke
-00003a60: 7220 7665 7273 696f 6e73 2e0d 0a0d 0a23  r versions.....#
-00003a70: 2320 4c69 6365 6e73 650d 0a53 6565 205b  # License..See [
-00003a80: 4c49 4345 4e53 455d 284c 4943 454e 5345  LICENSE](LICENSE
-00003a90: 2e6d 6429 2066 6f72 2074 6865 2063 7572  .md) for the cur
-00003aa0: 7265 6e74 206c 6963 656e 7365 2e0d 0a    rent license...
+000013d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001400: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001410: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001420: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001430: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001440: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001450: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001460: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d7c 3a2d  -------------|:-
+00001470: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001480: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001490: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001500: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001510: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001520: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001530: 7c3a 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  |:--------------
+00001540: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001550: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001560: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001570: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001580: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001590: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001600: 2d2d 2d2d 2d2d 2d2d 2d7c 0d0a 7c20 5b21  ---------|..| [!
+00001610: 5b4f 7065 6e20 496e 2043 6f6c 6162 5d28  [Open In Colab](
+00001620: 6874 7470 733a 2f2f 636f 6c61 622e 7265  https://colab.re
+00001630: 7365 6172 6368 2e67 6f6f 676c 652e 636f  search.google.co
+00001640: 6d2f 6173 7365 7473 2f63 6f6c 6162 2d62  m/assets/colab-b
+00001650: 6164 6765 2e73 7667 295d 2868 7474 7073  adge.svg)](https
+00001660: 3a2f 2f63 6f6c 6162 2e72 6573 6561 7263  ://colab.researc
+00001670: 682e 676f 6f67 6c65 2e63 6f6d 2f67 6974  h.google.com/git
+00001680: 6875 622f 746f 6d61 6172 7365 6e2f 5370  hub/tomaarsen/Sp
+00001690: 616e 4d61 726b 6572 4e45 522f 626c 6f62  anMarkerNER/blob
+000016a0: 2f6d 6169 6e2f 6e6f 7465 626f 6f6b 732f  /main/notebooks/
+000016b0: 6765 7474 696e 675f 7374 6172 7465 642e  getting_started.
+000016c0: 6970 796e 6229 2020 2020 2020 2020 2020  ipynb)          
+000016d0: 2020 2020 2020 2020 2020 2020 207c 205b               | [
+000016e0: 215b 4b61 6767 6c65 5d28 6874 7470 733a  ![Kaggle](https:
+000016f0: 2f2f 6b61 6767 6c65 2e63 6f6d 2f73 7461  //kaggle.com/sta
+00001700: 7469 632f 696d 6167 6573 2f6f 7065 6e2d  tic/images/open-
+00001710: 696e 2d6b 6167 676c 652e 7376 6729 5d28  in-kaggle.svg)](
+00001720: 6874 7470 733a 2f2f 6b61 6767 6c65 2e63  https://kaggle.c
+00001730: 6f6d 2f6b 6572 6e65 6c73 2f77 656c 636f  om/kernels/welco
+00001740: 6d65 3f73 7263 3d68 7474 7073 3a2f 2f67  me?src=https://g
+00001750: 6974 6875 622e 636f 6d2f 746f 6d61 6172  ithub.com/tomaar
+00001760: 7365 6e2f 5370 616e 4d61 726b 6572 4e45  sen/SpanMarkerNE
+00001770: 522f 626c 6f62 2f6d 6169 6e2f 6e6f 7465  R/blob/main/note
+00001780: 626f 6f6b 732f 6765 7474 696e 675f 7374  books/getting_st
+00001790: 6172 7465 642e 6970 796e 6229 2020 2020  arted.ipynb)    
+000017a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000017b0: 2020 207c 205b 215b 4772 6164 6965 6e74     | [![Gradient
+000017c0: 5d28 6874 7470 733a 2f2f 6173 7365 7473  ](https://assets
+000017d0: 2e70 6170 6572 7370 6163 652e 696f 2f69  .paperspace.io/i
+000017e0: 6d67 2f67 7261 6469 656e 742d 6261 6467  mg/gradient-badg
+000017f0: 652e 7376 6729 5d28 6874 7470 733a 2f2f  e.svg)](https://
+00001800: 636f 6e73 6f6c 652e 7061 7065 7273 7061  console.paperspa
+00001810: 6365 2e63 6f6d 2f67 6974 6875 622f 746f  ce.com/github/to
+00001820: 6d61 6172 7365 6e2f 5370 616e 4d61 726b  maarsen/SpanMark
+00001830: 6572 4e45 522f 626c 6f62 2f6d 6169 6e2f  erNER/blob/main/
+00001840: 6e6f 7465 626f 6f6b 732f 6765 7474 696e  notebooks/gettin
+00001850: 675f 7374 6172 7465 642e 6970 796e 6229  g_started.ipynb)
+00001860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001870: 2020 2020 2020 207c 205b 215b 4f70 656e         | [![Open
+00001880: 2049 6e20 5361 6765 4d61 6b65 7220 5374   In SageMaker St
+00001890: 7564 696f 204c 6162 5d28 6874 7470 733a  udio Lab](https:
+000018a0: 2f2f 7374 7564 696f 6c61 622e 7361 6765  //studiolab.sage
+000018b0: 6d61 6b65 722e 6177 732f 7374 7564 696f  maker.aws/studio
+000018c0: 6c61 622e 7376 6729 5d28 6874 7470 733a  lab.svg)](https:
+000018d0: 2f2f 7374 7564 696f 6c61 622e 7361 6765  //studiolab.sage
+000018e0: 6d61 6b65 722e 6177 732f 696d 706f 7274  maker.aws/import
+000018f0: 2f67 6974 6875 622f 746f 6d61 6172 7365  /github/tomaarse
+00001900: 6e2f 5370 616e 4d61 726b 6572 4e45 522f  n/SpanMarkerNER/
+00001910: 626c 6f62 2f6d 6169 6e2f 6e6f 7465 626f  blob/main/notebo
+00001920: 6f6b 732f 6765 7474 696e 675f 7374 6172  oks/getting_star
+00001930: 7465 642e 6970 796e 6229 2020 2020 2020  ted.ipynb)      
+00001940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001950: 207c 0d0a 0d0a 6060 6070 7974 686f 6e0d   |....```python.
+00001960: 0a66 726f 6d20 6461 7461 7365 7473 2069  .from datasets i
+00001970: 6d70 6f72 7420 6c6f 6164 5f64 6174 6173  mport load_datas
+00001980: 6574 0d0a 6672 6f6d 2074 7261 6e73 666f  et..from transfo
+00001990: 726d 6572 7320 696d 706f 7274 2054 7261  rmers import Tra
+000019a0: 696e 696e 6741 7267 756d 656e 7473 0d0a  iningArguments..
+000019b0: 6672 6f6d 2073 7061 6e5f 6d61 726b 6572  from span_marker
+000019c0: 2069 6d70 6f72 7420 5370 616e 4d61 726b   import SpanMark
+000019d0: 6572 4d6f 6465 6c2c 2054 7261 696e 6572  erModel, Trainer
+000019e0: 0d0a 0d0a 0d0a 6465 6620 6d61 696e 2829  ......def main()
+000019f0: 202d 3e20 4e6f 6e65 3a0d 0a20 2020 2023   -> None:..    #
+00001a00: 204c 6f61 6420 7468 6520 6461 7461 7365   Load the datase
+00001a10: 742c 2065 6e73 7572 6520 2274 6f6b 656e  t, ensure "token
+00001a20: 7322 2061 6e64 2022 6e65 725f 7461 6773  s" and "ner_tags
+00001a30: 2220 636f 6c75 6d6e 732c 2061 6e64 2067  " columns, and g
+00001a40: 6574 2061 206c 6973 7420 6f66 206c 6162  et a list of lab
+00001a50: 656c 730d 0a20 2020 2064 6174 6173 6574  els..    dataset
+00001a60: 203d 206c 6f61 645f 6461 7461 7365 7428   = load_dataset(
+00001a70: 2244 464b 492d 534c 542f 6665 772d 6e65  "DFKI-SLT/few-ne
+00001a80: 7264 222c 2022 7375 7065 7276 6973 6564  rd", "supervised
+00001a90: 2229 0d0a 2020 2020 6461 7461 7365 7420  ")..    dataset 
+00001aa0: 3d20 6461 7461 7365 742e 7265 6d6f 7665  = dataset.remove
+00001ab0: 5f63 6f6c 756d 6e73 2822 6e65 725f 7461  _columns("ner_ta
+00001ac0: 6773 2229 0d0a 2020 2020 6461 7461 7365  gs")..    datase
+00001ad0: 7420 3d20 6461 7461 7365 742e 7265 6e61  t = dataset.rena
+00001ae0: 6d65 5f63 6f6c 756d 6e28 2266 696e 655f  me_column("fine_
+00001af0: 6e65 725f 7461 6773 222c 2022 6e65 725f  ner_tags", "ner_
+00001b00: 7461 6773 2229 0d0a 2020 2020 6c61 6265  tags")..    labe
+00001b10: 6c73 203d 2064 6174 6173 6574 5b22 7472  ls = dataset["tr
+00001b20: 6169 6e22 5d2e 6665 6174 7572 6573 5b22  ain"].features["
+00001b30: 6e65 725f 7461 6773 225d 2e66 6561 7475  ner_tags"].featu
+00001b40: 7265 2e6e 616d 6573 0d0a 0d0a 2020 2020  re.names....    
+00001b50: 2320 496e 6974 6961 6c69 7a65 2061 2053  # Initialize a S
+00001b60: 7061 6e4d 6172 6b65 7220 6d6f 6465 6c20  panMarker model 
+00001b70: 7573 696e 6720 6120 7072 6574 7261 696e  using a pretrain
+00001b80: 6564 2042 4552 542d 7374 796c 6520 656e  ed BERT-style en
+00001b90: 636f 6465 720d 0a20 2020 206d 6f64 656c  coder..    model
+00001ba0: 5f6e 616d 6520 3d20 2262 6572 742d 6261  _name = "bert-ba
+00001bb0: 7365 2d63 6173 6564 220d 0a20 2020 206d  se-cased"..    m
+00001bc0: 6f64 656c 203d 2053 7061 6e4d 6172 6b65  odel = SpanMarke
+00001bd0: 724d 6f64 656c 2e66 726f 6d5f 7072 6574  rModel.from_pret
+00001be0: 7261 696e 6564 280d 0a20 2020 2020 2020  rained(..       
+00001bf0: 206d 6f64 656c 5f6e 616d 652c 0d0a 2020   model_name,..  
+00001c00: 2020 2020 2020 6c61 6265 6c73 3d6c 6162        labels=lab
+00001c10: 656c 732c 0d0a 2020 2020 2020 2020 2320  els,..        # 
+00001c20: 5370 616e 4d61 726b 6572 2068 7970 6572  SpanMarker hyper
+00001c30: 7061 7261 6d65 7465 7273 3a0d 0a20 2020  parameters:..   
+00001c40: 2020 2020 206d 6f64 656c 5f6d 6178 5f6c       model_max_l
+00001c50: 656e 6774 683d 3235 362c 0d0a 2020 2020  ength=256,..    
+00001c60: 2020 2020 6d61 726b 6572 5f6d 6178 5f6c      marker_max_l
+00001c70: 656e 6774 683d 3132 382c 0d0a 2020 2020  ength=128,..    
+00001c80: 2020 2020 656e 7469 7479 5f6d 6178 5f6c      entity_max_l
+00001c90: 656e 6774 683d 382c 0d0a 2020 2020 290d  ength=8,..    ).
+00001ca0: 0a0d 0a20 2020 2023 2050 7265 7061 7265  ...    # Prepare
+00001cb0: 2074 6865 20f0 9fa4 9720 7472 616e 7366   the .... transf
+00001cc0: 6f72 6d65 7273 2074 7261 696e 696e 6720  ormers training 
+00001cd0: 6172 6775 6d65 6e74 730d 0a20 2020 2061  arguments..    a
+00001ce0: 7267 7320 3d20 5472 6169 6e69 6e67 4172  rgs = TrainingAr
+00001cf0: 6775 6d65 6e74 7328 0d0a 2020 2020 2020  guments(..      
+00001d00: 2020 6f75 7470 7574 5f64 6972 3d22 6d6f    output_dir="mo
+00001d10: 6465 6c73 2f73 7061 6e5f 6d61 726b 6572  dels/span_marker
+00001d20: 5f62 6572 745f 6261 7365 5f63 6173 6564  _bert_base_cased
+00001d30: 5f66 6577 6e65 7264 5f66 696e 655f 7375  _fewnerd_fine_su
+00001d40: 7065 7222 2c0d 0a20 2020 2020 2020 2023  per",..        #
+00001d50: 2054 7261 696e 696e 6720 4879 7065 7270   Training Hyperp
+00001d60: 6172 616d 6574 6572 733a 0d0a 2020 2020  arameters:..    
+00001d70: 2020 2020 6c65 6172 6e69 6e67 5f72 6174      learning_rat
+00001d80: 653d 3565 2d35 2c0d 0a20 2020 2020 2020  e=5e-5,..       
+00001d90: 2070 6572 5f64 6576 6963 655f 7472 6169   per_device_trai
+00001da0: 6e5f 6261 7463 685f 7369 7a65 3d33 322c  n_batch_size=32,
+00001db0: 0d0a 2020 2020 2020 2020 7065 725f 6465  ..        per_de
+00001dc0: 7669 6365 5f65 7661 6c5f 6261 7463 685f  vice_eval_batch_
+00001dd0: 7369 7a65 3d33 322c 0d0a 2020 2020 2020  size=32,..      
+00001de0: 2020 6e75 6d5f 7472 6169 6e5f 6570 6f63    num_train_epoc
+00001df0: 6873 3d33 2c0d 0a20 2020 2020 2020 2077  hs=3,..        w
+00001e00: 6569 6768 745f 6465 6361 793d 302e 3031  eight_decay=0.01
+00001e10: 2c0d 0a20 2020 2020 2020 2077 6172 6d75  ,..        warmu
+00001e20: 705f 7261 7469 6f3d 302e 312c 0d0a 2020  p_ratio=0.1,..  
+00001e30: 2020 2020 2020 6266 3136 3d54 7275 652c        bf16=True,
+00001e40: 2020 2320 5265 706c 6163 6520 6062 6631    # Replace `bf1
+00001e50: 3660 2077 6974 6820 6066 7031 3660 2069  6` with `fp16` i
+00001e60: 6620 796f 7572 2068 6172 6477 6172 6520  f your hardware 
+00001e70: 6361 6e27 7420 7573 6520 6266 3136 2e0d  can't use bf16..
+00001e80: 0a20 2020 2020 2020 2023 204f 7468 6572  .        # Other
+00001e90: 2054 7261 696e 696e 6720 7061 7261 6d65   Training parame
+00001ea0: 7465 7273 0d0a 2020 2020 2020 2020 6c6f  ters..        lo
+00001eb0: 6767 696e 675f 6669 7273 745f 7374 6570  gging_first_step
+00001ec0: 3d54 7275 652c 0d0a 2020 2020 2020 2020  =True,..        
+00001ed0: 6c6f 6767 696e 675f 7374 6570 733d 3530  logging_steps=50
+00001ee0: 2c0d 0a20 2020 2020 2020 2065 7661 6c75  ,..        evalu
+00001ef0: 6174 696f 6e5f 7374 7261 7465 6779 3d22  ation_strategy="
+00001f00: 7374 6570 7322 2c0d 0a20 2020 2020 2020  steps",..       
+00001f10: 2073 6176 655f 7374 7261 7465 6779 3d22   save_strategy="
+00001f20: 7374 6570 7322 2c0d 0a20 2020 2020 2020  steps",..       
+00001f30: 2065 7661 6c5f 7374 6570 733d 3330 3030   eval_steps=3000
+00001f40: 2c0d 0a20 2020 2020 2020 2073 6176 655f  ,..        save_
+00001f50: 746f 7461 6c5f 6c69 6d69 743d 322c 0d0a  total_limit=2,..
+00001f60: 2020 2020 2020 2020 6461 7461 6c6f 6164          dataload
+00001f70: 6572 5f6e 756d 5f77 6f72 6b65 7273 3d32  er_num_workers=2
+00001f80: 2c0d 0a20 2020 2029 0d0a 0d0a 2020 2020  ,..    )....    
+00001f90: 2320 496e 6974 6961 6c69 7a65 2074 6865  # Initialize the
+00001fa0: 2074 7261 696e 6572 2075 7369 6e67 206f   trainer using o
+00001fb0: 7572 206d 6f64 656c 2c20 7472 6169 6e69  ur model, traini
+00001fc0: 6e67 2061 7267 7320 2620 6461 7461 7365  ng args & datase
+00001fd0: 742c 2061 6e64 2074 7261 696e 0d0a 2020  t, and train..  
+00001fe0: 2020 7472 6169 6e65 7220 3d20 5472 6169    trainer = Trai
+00001ff0: 6e65 7228 0d0a 2020 2020 2020 2020 6d6f  ner(..        mo
+00002000: 6465 6c3d 6d6f 6465 6c2c 0d0a 2020 2020  del=model,..    
+00002010: 2020 2020 6172 6773 3d61 7267 732c 0d0a      args=args,..
+00002020: 2020 2020 2020 2020 7472 6169 6e5f 6461          train_da
+00002030: 7461 7365 743d 6461 7461 7365 745b 2274  taset=dataset["t
+00002040: 7261 696e 225d 2c0d 0a20 2020 2020 2020  rain"],..       
+00002050: 2065 7661 6c5f 6461 7461 7365 743d 6461   eval_dataset=da
+00002060: 7461 7365 745b 2276 616c 6964 6174 696f  taset["validatio
+00002070: 6e22 5d2c 0d0a 2020 2020 290d 0a20 2020  n"],..    )..   
+00002080: 2074 7261 696e 6572 2e74 7261 696e 2829   trainer.train()
+00002090: 0d0a 2020 2020 7472 6169 6e65 722e 7361  ..    trainer.sa
+000020a0: 7665 5f6d 6f64 656c 2822 6d6f 6465 6c73  ve_model("models
+000020b0: 2f73 7061 6e5f 6d61 726b 6572 5f62 6572  /span_marker_ber
+000020c0: 745f 6261 7365 5f63 6173 6564 5f66 6577  t_base_cased_few
+000020d0: 6e65 7264 5f66 696e 655f 7375 7065 722f  nerd_fine_super/
+000020e0: 6368 6563 6b70 6f69 6e74 2d66 696e 616c  checkpoint-final
+000020f0: 2229 0d0a 0d0a 2020 2020 2320 436f 6d70  ")....    # Comp
+00002100: 7574 6520 2620 7361 7665 2074 6865 206d  ute & save the m
+00002110: 6574 7269 6373 206f 6e20 7468 6520 7465  etrics on the te
+00002120: 7374 2073 6574 0d0a 2020 2020 6d65 7472  st set..    metr
+00002130: 6963 7320 3d20 7472 6169 6e65 722e 6576  ics = trainer.ev
+00002140: 616c 7561 7465 2864 6174 6173 6574 5b22  aluate(dataset["
+00002150: 7465 7374 225d 2c20 6d65 7472 6963 5f6b  test"], metric_k
+00002160: 6579 5f70 7265 6669 783d 2274 6573 7422  ey_prefix="test"
+00002170: 290d 0a20 2020 2074 7261 696e 6572 2e73  )..    trainer.s
+00002180: 6176 655f 6d65 7472 6963 7328 2274 6573  ave_metrics("tes
+00002190: 7422 2c20 6d65 7472 6963 7329 0d0a 0d0a  t", metrics)....
+000021a0: 0d0a 6966 205f 5f6e 616d 655f 5f20 3d3d  ..if __name__ ==
+000021b0: 2022 5f5f 6d61 696e 5f5f 223a 0d0a 2020   "__main__":..  
+000021c0: 2020 6d61 696e 2829 0d0a 6060 600d 0a0d    main()..```...
+000021d0: 0a23 2323 2049 6e66 6572 656e 6365 0d0a  .### Inference..
+000021e0: 6060 6070 7974 686f 6e0d 0a66 726f 6d20  ```python..from 
+000021f0: 7370 616e 5f6d 6172 6b65 7220 696d 706f  span_marker impo
+00002200: 7274 2053 7061 6e4d 6172 6b65 724d 6f64  rt SpanMarkerMod
+00002210: 656c 0d0a 0d0a 2320 446f 776e 6c6f 6164  el....# Download
+00002220: 2066 726f 6d20 7468 6520 f09f a497 2048   from the .... H
+00002230: 7562 0d0a 6d6f 6465 6c20 3d20 5370 616e  ub..model = Span
+00002240: 4d61 726b 6572 4d6f 6465 6c2e 6672 6f6d  MarkerModel.from
+00002250: 5f70 7265 7472 6169 6e65 6428 2274 6f6d  _pretrained("tom
+00002260: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
+00002270: 6572 2d62 6572 742d 6261 7365 2d66 6577  er-bert-base-few
+00002280: 6e65 7264 2d66 696e 652d 7375 7065 7222  nerd-fine-super"
+00002290: 290d 0a23 2052 756e 2069 6e66 6572 656e  )..# Run inferen
+000022a0: 6365 0d0a 656e 7469 7469 6573 203d 206d  ce..entities = m
+000022b0: 6f64 656c 2e70 7265 6469 6374 2822 416d  odel.predict("Am
+000022c0: 656c 6961 2045 6172 6861 7274 2066 6c65  elia Earhart fle
+000022d0: 7720 6865 7220 7369 6e67 6c65 2065 6e67  w her single eng
+000022e0: 696e 6520 4c6f 636b 6865 6564 2056 6567  ine Lockheed Veg
+000022f0: 6120 3542 2061 6372 6f73 7320 7468 6520  a 5B across the 
+00002300: 4174 6c61 6e74 6963 2074 6f20 5061 7269  Atlantic to Pari
+00002310: 732e 2229 0d0a 5b7b 2773 7061 6e27 3a20  s.")..[{'span': 
+00002320: 2741 6d65 6c69 6120 4561 7268 6172 7427  'Amelia Earhart'
+00002330: 2c20 276c 6162 656c 273a 2027 7065 7273  , 'label': 'pers
+00002340: 6f6e 2d6f 7468 6572 272c 2027 7363 6f72  on-other', 'scor
+00002350: 6527 3a20 302e 3736 3539 3539 3733 3936  e': 0.7659597396
+00002360: 3835 3035 3836 2c20 2763 6861 725f 7374  850586, 'char_st
+00002370: 6172 745f 696e 6465 7827 3a20 302c 2027  art_index': 0, '
+00002380: 6368 6172 5f65 6e64 5f69 6e64 6578 273a  char_end_index':
+00002390: 2031 347d 2c0d 0a20 7b27 7370 616e 273a   14},.. {'span':
+000023a0: 2027 4c6f 636b 6865 6564 2056 6567 6120   'Lockheed Vega 
+000023b0: 3542 272c 2027 6c61 6265 6c27 3a20 2770  5B', 'label': 'p
+000023c0: 726f 6475 6374 2d61 6972 706c 616e 6527  roduct-airplane'
+000023d0: 2c20 2773 636f 7265 273a 2030 2e39 3732  , 'score': 0.972
+000023e0: 3537 3835 3835 3134 3738 3537 372c 2027  5785851478577, '
+000023f0: 6368 6172 5f73 7461 7274 5f69 6e64 6578  char_start_index
+00002400: 273a 2033 382c 2027 6368 6172 5f65 6e64  ': 38, 'char_end
+00002410: 5f69 6e64 6578 273a 2035 347d 2c0d 0a20  _index': 54},.. 
+00002420: 7b27 7370 616e 273a 2027 4174 6c61 6e74  {'span': 'Atlant
+00002430: 6963 272c 2027 6c61 6265 6c27 3a20 276c  ic', 'label': 'l
+00002440: 6f63 6174 696f 6e2d 626f 6469 6573 6f66  ocation-bodiesof
+00002450: 7761 7465 7227 2c20 2773 636f 7265 273a  water', 'score':
+00002460: 2030 2e37 3538 3736 3739 3032 3835 3131   0.7587679028511
+00002470: 3034 372c 2027 6368 6172 5f73 7461 7274  047, 'char_start
+00002480: 5f69 6e64 6578 273a 2036 362c 2027 6368  _index': 66, 'ch
+00002490: 6172 5f65 6e64 5f69 6e64 6578 273a 2037  ar_end_index': 7
+000024a0: 347d 2c0d 0a20 7b27 7370 616e 273a 2027  4},.. {'span': '
+000024b0: 5061 7269 7327 2c20 276c 6162 656c 273a  Paris', 'label':
+000024c0: 2027 6c6f 6361 7469 6f6e 2d47 5045 272c   'location-GPE',
+000024d0: 2027 7363 6f72 6527 3a20 302e 3938 3932   'score': 0.9892
+000024e0: 3339 3039 3636 3431 3534 3035 2c20 2763  390966415405, 'c
+000024f0: 6861 725f 7374 6172 745f 696e 6465 7827  har_start_index'
+00002500: 3a20 3738 2c20 2763 6861 725f 656e 645f  : 78, 'char_end_
+00002510: 696e 6465 7827 3a20 3833 7d5d 0d0a 6060  index': 83}]..``
+00002520: 600d 0a0d 0a3c 212d 2d20 4265 6361 7573  `....<!-- Becaus
+00002530: 6520 7468 6973 2077 6f72 6b20 6973 2062  e this work is b
+00002540: 6173 6564 206f 6e20 5b50 4c2d 4d61 726b  ased on [PL-Mark
+00002550: 6572 5d28 6874 7470 733a 2f2f 6172 7869  er](https://arxi
+00002560: 762e 6f72 672f 7064 662f 3231 3039 2e30  v.org/pdf/2109.0
+00002570: 3630 3637 7635 2e70 6466 292c 2079 6f75  6067v5.pdf), you
+00002580: 206d 6179 2065 7870 6563 7420 7369 6d69   may expect simi
+00002590: 6c61 7220 7265 7375 6c74 7320 746f 2069  lar results to i
+000025a0: 7473 205b 5061 7065 7273 2077 6974 6820  ts [Papers with 
+000025b0: 436f 6465 204c 6561 6465 7262 6f61 7264  Code Leaderboard
+000025c0: 5d28 6874 7470 733a 2f2f 7061 7065 7273  ](https://papers
+000025d0: 7769 7468 636f 6465 2e63 6f6d 2f70 6170  withcode.com/pap
+000025e0: 6572 2f70 6163 6b2d 746f 6765 7468 6572  er/pack-together
+000025f0: 2d65 6e74 6974 792d 616e 642d 7265 6c61  -entity-and-rela
+00002600: 7469 6f6e 2d65 7874 7261 6374 696f 6e29  tion-extraction)
+00002610: 2072 6573 756c 7473 2e20 2d2d 3e0d 0a0d   results. -->...
+00002620: 0a23 2320 5072 6574 7261 696e 6564 204d  .## Pretrained M
+00002630: 6f64 656c 730d 0a0d 0a41 6c6c 206d 6f64  odels....All mod
+00002640: 656c 7320 696e 2074 6869 7320 6c69 7374  els in this list
+00002650: 2063 6f6e 7461 696e 2060 7472 6169 6e2e   contain `train.
+00002660: 7079 6020 6669 6c65 7320 7468 6174 2073  py` files that s
+00002670: 686f 7720 7468 6520 7472 6169 6e69 6e67  how the training
+00002680: 2073 6372 6970 7473 2075 7365 6420 746f   scripts used to
+00002690: 2067 656e 6572 6174 6520 7468 656d 2e20   generate them. 
+000026a0: 4164 6469 7469 6f6e 616c 6c79 2c20 616c  Additionally, al
+000026b0: 6c20 7472 6169 6e69 6e67 2073 6372 6970  l training scrip
+000026c0: 7473 2075 7365 6420 6172 6520 7374 6f72  ts used are stor
+000026d0: 6564 2069 6e20 7468 6520 5b74 7261 696e  ed in the [train
+000026e0: 696e 675f 7363 7269 7074 735d 2874 7261  ing_scripts](tra
+000026f0: 696e 696e 675f 7363 7269 7074 7329 2064  ining_scripts) d
+00002700: 6972 6563 746f 7279 2e0d 0a54 6865 7365  irectory...These
+00002710: 2074 7261 696e 6564 206d 6f64 656c 7320   trained models 
+00002720: 6861 7665 2048 6f73 7465 6420 496e 6665  have Hosted Infe
+00002730: 7265 6e63 6520 4150 4920 7769 6467 6574  rence API widget
+00002740: 7320 7468 6174 2079 6f75 2063 616e 2075  s that you can u
+00002750: 7365 2074 6f20 6578 7065 7269 6d65 6e74  se to experiment
+00002760: 2077 6974 6820 7468 6520 6d6f 6465 6c73   with the models
+00002770: 206f 6e20 7468 6569 7220 4875 6767 696e   on their Huggin
+00002780: 6720 4661 6365 206d 6f64 656c 2070 6167  g Face model pag
+00002790: 6573 2e20 4164 6469 7469 6f6e 616c 6c79  es. Additionally
+000027a0: 2c20 4875 6767 696e 6720 4661 6365 2070  , Hugging Face p
+000027b0: 726f 7669 6465 7320 6561 6368 206d 6f64  rovides each mod
+000027c0: 656c 2077 6974 6820 6120 6672 6565 2041  el with a free A
+000027d0: 5049 2028 6044 6570 6c6f 7960 203e 2060  PI (`Deploy` > `
+000027e0: 496e 6665 7265 6e63 6520 4150 4960 206f  Inference API` o
+000027f0: 6e20 7468 6520 6d6f 6465 6c20 7061 6765  n the model page
+00002800: 292e 0d0a 0d0a 2323 2320 4665 774e 4552  ).....### FewNER
+00002810: 440d 0a2a 205b 6074 6f6d 6161 7273 656e  D..* [`tomaarsen
+00002820: 2f73 7061 6e2d 6d61 726b 6572 2d62 6572  /span-marker-ber
+00002830: 742d 6261 7365 2d66 6577 6e65 7264 2d66  t-base-fewnerd-f
+00002840: 696e 652d 7375 7065 7260 5d28 6874 7470  ine-super`](http
+00002850: 733a 2f2f 6875 6767 696e 6766 6163 652e  s://huggingface.
+00002860: 636f 2f74 6f6d 6161 7273 656e 2f73 7061  co/tomaarsen/spa
+00002870: 6e2d 6d61 726b 6572 2d62 6572 742d 6261  n-marker-bert-ba
+00002880: 7365 2d66 6577 6e65 7264 2d66 696e 652d  se-fewnerd-fine-
+00002890: 7375 7065 7229 2069 7320 6120 6d6f 6465  super) is a mode
+000028a0: 6c20 7468 6174 2049 2068 6176 6520 7472  l that I have tr
+000028b0: 6169 6e65 6420 696e 2032 2068 6f75 7273  ained in 2 hours
+000028c0: 206f 6e20 7468 6520 6669 6e65 6772 6169   on the finegrai
+000028d0: 6e65 642c 2073 7570 6572 7669 7365 6420  ned, supervised 
+000028e0: 5b46 6577 2d4e 4552 4420 6461 7461 7365  [Few-NERD datase
+000028f0: 745d 2868 7474 7073 3a2f 2f68 7567 6769  t](https://huggi
+00002900: 6e67 6661 6365 2e63 6f2f 6461 7461 7365  ngface.co/datase
+00002910: 7473 2f44 464b 492d 534c 542f 6665 772d  ts/DFKI-SLT/few-
+00002920: 6e65 7264 292e 2049 7420 7265 6163 6865  nerd). It reache
+00002930: 6420 6120 302e 3730 3533 2054 6573 7420  d a 0.7053 Test 
+00002940: 4631 2c20 636f 6d70 6574 6974 6976 6520  F1, competitive 
+00002950: 696e 2074 6865 2061 6c6c 2d74 696d 6520  in the all-time 
+00002960: 5b46 6577 2d4e 4552 4420 6c65 6164 6572  [Few-NERD leader
+00002970: 626f 6172 645d 2868 7474 7073 3a2f 2f70  board](https://p
+00002980: 6170 6572 7377 6974 6863 6f64 652e 636f  aperswithcode.co
+00002990: 6d2f 736f 7461 2f6e 616d 6564 2d65 6e74  m/sota/named-ent
+000029a0: 6974 792d 7265 636f 676e 6974 696f 6e2d  ity-recognition-
+000029b0: 6f6e 2d66 6577 2d6e 6572 642d 7375 7029  on-few-nerd-sup)
+000029c0: 2075 7369 6e67 2060 6265 7274 2d62 6173   using `bert-bas
+000029d0: 6560 2e20 4d79 2074 7261 696e 696e 6720  e`. My training 
+000029e0: 7363 7269 7074 2072 6573 656d 626c 6573  script resembles
+000029f0: 2074 6865 206f 6e65 2074 6861 7420 796f   the one that yo
+00002a00: 7520 6361 6e20 7365 6520 6162 6f76 652e  u can see above.
+00002a10: 0d0a 2020 2a20 5472 7920 7468 6520 6d6f  ..  * Try the mo
+00002a20: 6465 6c20 6f75 7420 6f6e 6c69 6e65 2075  del out online u
+00002a30: 7369 6e67 2074 6869 7320 5bf0 9fa4 9720  sing this [.... 
+00002a40: 5370 6163 655d 2868 7474 7073 3a2f 2f74  Space](https://t
+00002a50: 6f6d 6161 7273 656e 2d73 7061 6e2d 6d61  omaarsen-span-ma
+00002a60: 726b 6572 2d62 6572 742d 6261 7365 2d66  rker-bert-base-f
+00002a70: 6577 6e65 7264 2d66 696e 652d 7375 7065  ewnerd-fine-supe
+00002a80: 722e 6866 2e73 7061 6365 2f29 2e0d 0a0d  r.hf.space/)....
+00002a90: 0a2a 205b 6074 6f6d 6161 7273 656e 2f73  .* [`tomaarsen/s
+00002aa0: 7061 6e2d 6d61 726b 6572 2d72 6f62 6572  pan-marker-rober
+00002ab0: 7461 2d6c 6172 6765 2d66 6577 6e65 7264  ta-large-fewnerd
+00002ac0: 2d66 696e 652d 7375 7065 7260 5d28 6874  -fine-super`](ht
+00002ad0: 7470 733a 2f2f 6875 6767 696e 6766 6163  tps://huggingfac
+00002ae0: 652e 636f 2f74 6f6d 6161 7273 656e 2f73  e.co/tomaarsen/s
+00002af0: 7061 6e2d 6d61 726b 6572 2d72 6f62 6572  pan-marker-rober
+00002b00: 7461 2d6c 6172 6765 2d66 6577 6e65 7264  ta-large-fewnerd
+00002b10: 2d66 696e 652d 7375 7065 7229 2077 6173  -fine-super) was
+00002b20: 2074 7261 696e 6564 2069 6e20 3620 686f   trained in 6 ho
+00002b30: 7572 7320 6f6e 2074 6865 2066 696e 6567  urs on the fineg
+00002b40: 7261 696e 6564 2c20 7375 7065 7276 6973  rained, supervis
+00002b50: 6564 205b 4665 772d 4e45 5244 2064 6174  ed [Few-NERD dat
+00002b60: 6173 6574 5d28 6874 7470 733a 2f2f 6875  aset](https://hu
+00002b70: 6767 696e 6766 6163 652e 636f 2f64 6174  ggingface.co/dat
+00002b80: 6173 6574 732f 4446 4b49 2d53 4c54 2f66  asets/DFKI-SLT/f
+00002b90: 6577 2d6e 6572 6429 2075 7369 6e67 2060  ew-nerd) using `
+00002ba0: 726f 6265 7274 612d 6c61 7267 6560 2e20  roberta-large`. 
+00002bb0: 4974 2072 6561 6368 6564 2061 2030 2e37  It reached a 0.7
+00002bc0: 3130 3320 5465 7374 2046 312c 2072 6561  103 Test F1, rea
+00002bd0: 6368 696e 6720 6120 6e65 7720 7374 6174  ching a new stat
+00002be0: 6520 6f66 2074 6865 2061 7274 2069 6e20  e of the art in 
+00002bf0: 7468 6520 616c 6c2d 7469 6d65 205b 4665  the all-time [Fe
+00002c00: 772d 4e45 5244 206c 6561 6465 7262 6f61  w-NERD leaderboa
+00002c10: 7264 5d28 6874 7470 733a 2f2f 7061 7065  rd](https://pape
+00002c20: 7273 7769 7468 636f 6465 2e63 6f6d 2f73  rswithcode.com/s
+00002c30: 6f74 612f 6e61 6d65 642d 656e 7469 7479  ota/named-entity
+00002c40: 2d72 6563 6f67 6e69 7469 6f6e 2d6f 6e2d  -recognition-on-
+00002c50: 6665 772d 6e65 7264 2d73 7570 292e 0d0a  few-nerd-sup)...
+00002c60: 2a20 5b60 746f 6d61 6172 7365 6e2f 7370  * [`tomaarsen/sp
+00002c70: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
+00002c80: 6265 7274 612d 6261 7365 2d66 6577 6e65  berta-base-fewne
+00002c90: 7264 2d66 696e 652d 7375 7065 7260 5d28  rd-fine-super`](
+00002ca0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
+00002cb0: 6163 652e 636f 2f74 6f6d 6161 7273 656e  ace.co/tomaarsen
+00002cc0: 2f73 7061 6e2d 6d61 726b 6572 2d78 6c6d  /span-marker-xlm
+00002cd0: 2d72 6f62 6572 7461 2d62 6173 652d 6665  -roberta-base-fe
+00002ce0: 776e 6572 642d 6669 6e65 2d73 7570 6572  wnerd-fine-super
+00002cf0: 2920 6973 2061 206d 756c 7469 6c69 6e67  ) is a multiling
+00002d00: 7561 6c20 6d6f 6465 6c20 7468 6174 2049  ual model that I
+00002d10: 2068 6176 6520 7472 6169 6e65 6420 696e   have trained in
+00002d20: 2031 2e35 2068 6f75 7273 206f 6e20 7468   1.5 hours on th
+00002d30: 6520 6669 6e65 6772 6169 6e65 642c 2073  e finegrained, s
+00002d40: 7570 6572 7669 7365 6420 5b46 6577 2d4e  upervised [Few-N
+00002d50: 4552 4420 6461 7461 7365 745d 2868 7474  ERD dataset](htt
+00002d60: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+00002d70: 2e63 6f2f 6461 7461 7365 7473 2f44 464b  .co/datasets/DFK
+00002d80: 492d 534c 542f 6665 772d 6e65 7264 292e  I-SLT/few-nerd).
+00002d90: 2049 7420 7265 6163 6865 6420 6120 302e   It reached a 0.
+00002da0: 3638 3620 5465 7374 2046 3120 6f6e 2045  686 Test F1 on E
+00002db0: 6e67 6c69 7368 2c20 616e 6420 776f 726b  nglish, and work
+00002dc0: 7320 7765 6c6c 206f 6e20 6f74 6865 7220  s well on other 
+00002dd0: 6c61 6e67 7561 6765 7320 6c69 6b65 2053  languages like S
+00002de0: 7061 6e69 7368 2c20 4672 656e 6368 2c20  panish, French, 
+00002df0: 4765 726d 616e 2c20 5275 7373 6961 6e2c  German, Russian,
+00002e00: 2044 7574 6368 2c20 506f 6c69 7368 2c20   Dutch, Polish, 
+00002e10: 4963 656c 616e 6469 632c 2047 7265 656b  Icelandic, Greek
+00002e20: 2061 6e64 206d 616e 7920 6d6f 7265 2e0d   and many more..
+00002e30: 0a0d 0a23 2323 204f 6e74 6f4e 6f74 6573  ...### OntoNotes
+00002e40: 2076 352e 300d 0a2a 205b 6074 6f6d 6161   v5.0..* [`tomaa
+00002e50: 7273 656e 2f73 7061 6e2d 6d61 726b 6572  rsen/span-marker
+00002e60: 2d72 6f62 6572 7461 2d6c 6172 6765 2d6f  -roberta-large-o
+00002e70: 6e74 6f6e 6f74 6573 3560 5d28 6874 7470  ntonotes5`](http
+00002e80: 733a 2f2f 6875 6767 696e 6766 6163 652e  s://huggingface.
+00002e90: 636f 2f74 6f6d 6161 7273 656e 2f73 7061  co/tomaarsen/spa
+00002ea0: 6e2d 6d61 726b 6572 2d72 6f62 6572 7461  n-marker-roberta
+00002eb0: 2d6c 6172 6765 2d6f 6e74 6f6e 6f74 6573  -large-ontonotes
+00002ec0: 3529 2077 6173 2074 7261 696e 6564 2069  5) was trained i
+00002ed0: 6e20 3320 686f 7572 7320 6f6e 2074 6865  n 3 hours on the
+00002ee0: 204f 6e74 6f4e 6f74 6573 2076 352e 3020   OntoNotes v5.0 
+00002ef0: 6461 7461 7365 742c 2072 6561 6368 696e  dataset, reachin
+00002f00: 6720 6120 7065 7266 6f72 6d61 6e63 6520  g a performance 
+00002f10: 6f66 2030 2e39 3135 3420 4631 2e20 466f  of 0.9154 F1. Fo
+00002f20: 7220 7265 6665 7265 6e63 652c 2074 6865  r reference, the
+00002f30: 2063 7572 7265 6e74 2073 7472 6f6e 6765   current stronge
+00002f40: 7374 2073 7061 4379 206d 6f64 656c 2028  st spaCy model (
+00002f50: 6065 6e5f 636f 7265 5f77 6562 5f74 7266  `en_core_web_trf
+00002f60: 6029 2072 6561 6368 6573 2030 2e38 3938  `) reaches 0.898
+00002f70: 2e20 5468 6973 2053 7061 6e4d 6172 6b65  . This SpanMarke
+00002f80: 7220 6d6f 6465 6c20 7573 6573 2061 2060  r model uses a `
+00002f90: 726f 6265 7274 612d 6c61 7267 6560 2065  roberta-large` e
+00002fa0: 6e63 6f64 6572 2075 6e64 6572 2074 6865  ncoder under the
+00002fb0: 2068 6f6f 642e 0d0a 0d0a 2323 2320 436f   hood.....### Co
+00002fc0: 4e4c 4c30 330d 0a2a 205b 6074 6f6d 6161  NLL03..* [`tomaa
+00002fd0: 7273 656e 2f73 7061 6e2d 6d61 726b 6572  rsen/span-marker
+00002fe0: 2d78 6c6d 2d72 6f62 6572 7461 2d6c 6172  -xlm-roberta-lar
+00002ff0: 6765 2d63 6f6e 6c6c 3033 605d 2868 7474  ge-conll03`](htt
+00003000: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+00003010: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
+00003020: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
+00003030: 6265 7274 612d 6c61 7267 652d 636f 6e6c  berta-large-conl
+00003040: 6c30 3329 2069 7320 6120 5370 616e 4d61  l03) is a SpanMa
+00003050: 726b 6572 206d 6f64 656c 2075 7369 6e67  rker model using
+00003060: 2060 786c 6d2d 726f 6265 7274 612d 6c61   `xlm-roberta-la
+00003070: 7267 6560 2074 6861 7420 7761 7320 7472  rge` that was tr
+00003080: 6169 6e65 6420 696e 2034 3520 6d69 6e75  ained in 45 minu
+00003090: 7465 732e 2049 7420 7265 6163 6865 7320  tes. It reaches 
+000030a0: 6120 7374 6174 6520 6f66 2074 6865 2061  a state of the a
+000030b0: 7274 2030 2e39 3331 2046 3120 6f6e 2043  rt 0.931 F1 on C
+000030c0: 6f4e 4c4c 3033 2077 6974 686f 7574 2075  oNLL03 without u
+000030d0: 7369 6e67 2064 6f63 756d 656e 742d 6c65  sing document-le
+000030e0: 7665 6c20 636f 6e74 6578 742e 2046 6f72  vel context. For
+000030f0: 2072 6566 6572 656e 6365 2c20 7468 6520   reference, the 
+00003100: 6375 7272 656e 7420 7374 726f 6e67 6573  current stronges
+00003110: 7420 7370 6143 7920 6d6f 6465 6c20 2860  t spaCy model (`
+00003120: 656e 5f63 6f72 655f 7765 625f 7472 6660  en_core_web_trf`
+00003130: 2920 7265 6163 6865 7320 3931 2e36 2e0d  ) reaches 91.6..
+00003140: 0a2a 205b 6074 6f6d 6161 7273 656e 2f73  .* [`tomaarsen/s
+00003150: 7061 6e2d 6d61 726b 6572 2d78 6c6d 2d72  pan-marker-xlm-r
+00003160: 6f62 6572 7461 2d6c 6172 6765 2d63 6f6e  oberta-large-con
+00003170: 6c6c 3033 2d64 6f63 2d63 6f6e 7465 7874  ll03-doc-context
+00003180: 605d 2868 7474 7073 3a2f 2f68 7567 6769  `](https://huggi
+00003190: 6e67 6661 6365 2e63 6f2f 746f 6d61 6172  ngface.co/tomaar
+000031a0: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
+000031b0: 786c 6d2d 726f 6265 7274 612d 6c61 7267  xlm-roberta-larg
+000031c0: 652d 636f 6e6c 6c30 332d 646f 632d 636f  e-conll03-doc-co
+000031d0: 6e74 6578 7429 2069 7320 616e 6f74 6865  ntext) is anothe
+000031e0: 7220 5370 616e 4d61 726b 6572 206d 6f64  r SpanMarker mod
+000031f0: 656c 2075 7369 6e67 2074 6865 2060 786c  el using the `xl
+00003200: 6d2d 726f 6265 7274 612d 6c61 7267 6560  m-roberta-large`
+00003210: 2065 6e63 6f64 6572 2e20 4974 2075 7365   encoder. It use
+00003220: 7320 5b64 6f63 756d 656e 742d 6c65 7665  s [document-leve
+00003230: 6c20 636f 6e74 6578 745d 2868 7474 7073  l context](https
+00003240: 3a2f 2f74 6f6d 6161 7273 656e 2e67 6974  ://tomaarsen.git
+00003250: 6875 622e 696f 2f53 7061 6e4d 6172 6b65  hub.io/SpanMarke
+00003260: 724e 4552 2f6e 6f74 6562 6f6f 6b73 2f64  rNER/notebooks/d
+00003270: 6f63 756d 656e 745f 6c65 7665 6c5f 636f  ocument_level_co
+00003280: 6e74 6578 742e 6874 6d6c 2920 746f 2072  ntext.html) to r
+00003290: 6561 6368 2061 2073 7461 7465 206f 6620  each a state of 
+000032a0: 7468 6520 6172 7420 302e 3934 3420 4631  the art 0.944 F1
+000032b0: 2e20 466f 7220 7468 6520 6265 7374 2070  . For the best p
+000032c0: 6572 666f 726d 616e 6365 2c20 696e 6665  erformance, infe
+000032d0: 7265 6e63 6520 7368 6f75 6c64 2062 6520  rence should be 
+000032e0: 7065 7266 6f72 6d65 6420 7573 696e 6720  performed using 
+000032f0: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
+00003300: 6f6e 7465 7874 2028 5b64 6f63 735d 2868  ontext ([docs](h
+00003310: 7474 7073 3a2f 2f74 6f6d 6161 7273 656e  ttps://tomaarsen
+00003320: 2e67 6974 6875 622e 696f 2f53 7061 6e4d  .github.io/SpanM
+00003330: 6172 6b65 724e 4552 2f6e 6f74 6562 6f6f  arkerNER/noteboo
+00003340: 6b73 2f64 6f63 756d 656e 745f 6c65 7665  ks/document_leve
+00003350: 6c5f 636f 6e74 6578 742e 6874 6d6c 2349  l_context.html#I
+00003360: 6e66 6572 656e 6365 2929 2e20 5468 6973  nference)). This
+00003370: 206d 6f64 656c 2077 6173 2074 7261 696e   model was train
+00003380: 6564 2069 6e20 3120 686f 7572 2e0d 0a0d  ed in 1 hour....
+00003390: 0a23 2323 2043 6f4e 4c4c 2b2b 0d0a 2a20  .### CoNLL++..* 
+000033a0: 5b60 746f 6d61 6172 7365 6e2f 7370 616e  [`tomaarsen/span
+000033b0: 2d6d 6172 6b65 722d 786c 6d2d 726f 6265  -marker-xlm-robe
+000033c0: 7274 612d 6c61 7267 652d 636f 6e6c 6c70  rta-large-conllp
+000033d0: 702d 646f 632d 636f 6e74 6578 7460 5d28  p-doc-context`](
+000033e0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
+000033f0: 6163 652e 636f 2f74 6f6d 6161 7273 656e  ace.co/tomaarsen
+00003400: 2f73 7061 6e2d 6d61 726b 6572 2d78 6c6d  /span-marker-xlm
+00003410: 2d72 6f62 6572 7461 2d6c 6172 6765 2d63  -roberta-large-c
+00003420: 6f6e 6c6c 7070 2d64 6f63 2d63 6f6e 7465  onllpp-doc-conte
+00003430: 7874 2920 7761 7320 7472 6169 6e65 6420  xt) was trained 
+00003440: 696e 2061 6e20 686f 7572 2075 7369 6e67  in an hour using
+00003450: 2074 6865 2060 786c 6d2d 726f 6265 7274   the `xlm-robert
+00003460: 612d 6c61 7267 6560 2065 6e63 6f64 6572  a-large` encoder
+00003470: 206f 6e20 7468 6520 436f 4e4c 4c2b 2b20   on the CoNLL++ 
+00003480: 6461 7461 7365 742e 2055 7369 6e67 205b  dataset. Using [
+00003490: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
+000034a0: 6f6e 7465 7874 5d28 6874 7470 733a 2f2f  ontext](https://
+000034b0: 746f 6d61 6172 7365 6e2e 6769 7468 7562  tomaarsen.github
+000034c0: 2e69 6f2f 5370 616e 4d61 726b 6572 4e45  .io/SpanMarkerNE
+000034d0: 522f 6e6f 7465 626f 6f6b 732f 646f 6375  R/notebooks/docu
+000034e0: 6d65 6e74 5f6c 6576 656c 5f63 6f6e 7465  ment_level_conte
+000034f0: 7874 2e68 746d 6c29 2c20 6974 2072 6561  xt.html), it rea
+00003500: 6368 6573 2061 2076 6572 7920 636f 6d70  ches a very comp
+00003510: 6574 6974 6976 6520 302e 3935 3520 4631  etitive 0.955 F1
+00003520: 2e20 466f 7220 7468 6520 6265 7374 2070  . For the best p
+00003530: 6572 666f 726d 616e 6365 2c20 696e 6665  erformance, infe
+00003540: 7265 6e63 6520 7368 6f75 6c64 2062 6520  rence should be 
+00003550: 7065 7266 6f72 6d65 6420 7573 696e 6720  performed using 
+00003560: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
+00003570: 6f6e 7465 7874 2028 5b64 6f63 735d 2868  ontext ([docs](h
+00003580: 7474 7073 3a2f 2f74 6f6d 6161 7273 656e  ttps://tomaarsen
+00003590: 2e67 6974 6875 622e 696f 2f53 7061 6e4d  .github.io/SpanM
+000035a0: 6172 6b65 724e 4552 2f6e 6f74 6562 6f6f  arkerNER/noteboo
+000035b0: 6b73 2f64 6f63 756d 656e 745f 6c65 7665  ks/document_leve
+000035c0: 6c5f 636f 6e74 6578 742e 6874 6d6c 2349  l_context.html#I
+000035d0: 6e66 6572 656e 6365 2929 2e0d 0a0d 0a23  nference)).....#
+000035e0: 2320 5573 696e 6720 7072 6574 7261 696e  # Using pretrain
+000035f0: 6564 2053 7061 6e4d 6172 6b65 7220 6d6f  ed SpanMarker mo
+00003600: 6465 6c73 2077 6974 6820 7370 6143 790d  dels with spaCy.
+00003610: 0a41 6c6c 205b 5370 616e 4d61 726b 6572  .All [SpanMarker
+00003620: 206d 6f64 656c 7320 6f6e 2074 6865 2048   models on the H
+00003630: 7567 6769 6e67 2046 6163 6520 4875 625d  ugging Face Hub]
+00003640: 2868 7474 7073 3a2f 2f68 7567 6769 6e67  (https://hugging
+00003650: 6661 6365 2e63 6f2f 6d6f 6465 6c73 3f6c  face.co/models?l
+00003660: 6962 7261 7279 3d73 7061 6e2d 6d61 726b  ibrary=span-mark
+00003670: 6572 2920 6361 6e20 616c 736f 2062 6520  er) can also be 
+00003680: 6561 7369 6c79 2075 7365 6420 696e 2073  easily used in s
+00003690: 7061 4379 2e20 4974 2773 2061 7320 7369  paCy. It's as si
+000036a0: 6d70 6c65 2061 7320 696e 636c 7564 696e  mple as includin
+000036b0: 6720 3120 6c69 6e65 2074 6f20 6164 6420  g 1 line to add 
+000036c0: 7468 6520 6073 7061 6e5f 6d61 726b 6572  the `span_marker
+000036d0: 6020 7069 7065 6c69 6e65 2e20 5365 6520  ` pipeline. See 
+000036e0: 7468 6520 446f 6375 6d65 6e74 6174 696f  the Documentatio
+000036f0: 6e20 6f72 2041 5049 2052 6566 6572 656e  n or API Referen
+00003700: 6365 2066 6f72 206d 6f72 6520 696e 666f  ce for more info
+00003710: 726d 6174 696f 6e2e 0d0a 6060 6070 7974  rmation...```pyt
+00003720: 686f 6e0d 0a69 6d70 6f72 7420 7370 6163  hon..import spac
+00003730: 790d 0a0d 0a23 204c 6f61 6420 7468 6520  y....# Load the 
+00003740: 7370 6143 7920 6d6f 6465 6c20 7769 7468  spaCy model with
+00003750: 2074 6865 2073 7061 6e5f 6d61 726b 6572   the span_marker
+00003760: 2070 6970 656c 696e 6520 636f 6d70 6f6e   pipeline compon
+00003770: 656e 740d 0a6e 6c70 203d 2073 7061 6379  ent..nlp = spacy
+00003780: 2e6c 6f61 6428 2265 6e5f 636f 7265 5f77  .load("en_core_w
+00003790: 6562 5f73 6d22 2c20 6469 7361 626c 653d  eb_sm", disable=
+000037a0: 5b22 6e65 7222 5d29 0d0a 6e6c 702e 6164  ["ner"])..nlp.ad
+000037b0: 645f 7069 7065 2822 7370 616e 5f6d 6172  d_pipe("span_mar
+000037c0: 6b65 7222 2c20 636f 6e66 6967 3d7b 226d  ker", config={"m
+000037d0: 6f64 656c 223a 2022 746f 6d61 6172 7365  odel": "tomaarse
+000037e0: 6e2f 7370 616e 2d6d 6172 6b65 722d 726f  n/span-marker-ro
+000037f0: 6265 7274 612d 6c61 7267 652d 6f6e 746f  berta-large-onto
+00003800: 6e6f 7465 7335 227d 290d 0a0d 0a23 2046  notes5"})....# F
+00003810: 6565 6420 736f 6d65 2074 6578 7420 7468  eed some text th
+00003820: 726f 7567 6820 7468 6520 6d6f 6465 6c20  rough the model 
+00003830: 746f 2067 6574 2061 2073 7061 6379 2044  to get a spacy D
+00003840: 6f63 0d0a 7465 7874 203d 2022 2222 436c  oc..text = """Cl
+00003850: 656f 7061 7472 6120 5649 492c 2061 6c73  eopatra VII, als
+00003860: 6f20 6b6e 6f77 6e20 6173 2043 6c65 6f70  o known as Cleop
+00003870: 6174 7261 2074 6865 2047 7265 6174 2c20  atra the Great, 
+00003880: 7761 7320 7468 6520 6c61 7374 2061 6374  was the last act
+00003890: 6976 6520 7275 6c65 7220 6f66 2074 6865  ive ruler of the
+000038a0: 205c 0d0a 5074 6f6c 656d 6169 6320 4b69   \..Ptolemaic Ki
+000038b0: 6e67 646f 6d20 6f66 2045 6779 7074 2e20  ngdom of Egypt. 
+000038c0: 5368 6520 7761 7320 626f 726e 2069 6e20  She was born in 
+000038d0: 3639 2042 4345 2061 6e64 2072 756c 6564  69 BCE and ruled
+000038e0: 2045 6779 7074 2066 726f 6d20 3531 2042   Egypt from 51 B
+000038f0: 4345 2075 6e74 696c 2068 6572 205c 0d0a  CE until her \..
+00003900: 6465 6174 6820 696e 2033 3020 4243 452e  death in 30 BCE.
+00003910: 2222 220d 0a64 6f63 203d 206e 6c70 2874  """..doc = nlp(t
+00003920: 6578 7429 0d0a 0d0a 2320 416e 6420 6c6f  ext)....# And lo
+00003930: 6f6b 2061 7420 7468 6520 656e 7469 7469  ok at the entiti
+00003940: 6573 0d0a 7072 696e 7428 5b28 656e 7469  es..print([(enti
+00003950: 7479 2c20 656e 7469 7479 2e6c 6162 656c  ty, entity.label
+00003960: 5f29 2066 6f72 2065 6e74 6974 7920 696e  _) for entity in
+00003970: 2064 6f63 2e65 6e74 735d 290d 0a22 2222   doc.ents]).."""
+00003980: 0d0a 5b28 436c 656f 7061 7472 6120 5649  ..[(Cleopatra VI
+00003990: 492c 2022 5045 5253 4f4e 2229 2c20 2843  I, "PERSON"), (C
+000039a0: 6c65 6f70 6174 7261 2074 6865 2047 7265  leopatra the Gre
+000039b0: 6174 2c20 2250 4552 534f 4e22 292c 2028  at, "PERSON"), (
+000039c0: 7468 6520 5074 6f6c 656d 6169 6320 4b69  the Ptolemaic Ki
+000039d0: 6e67 646f 6d20 6f66 2045 6779 7074 2c20  ngdom of Egypt, 
+000039e0: 2247 5045 2229 2c0d 0a28 3639 2042 4345  "GPE"),..(69 BCE
+000039f0: 2c20 2244 4154 4522 292c 2028 4567 7970  , "DATE"), (Egyp
+00003a00: 742c 2022 4750 4522 292c 2028 3531 2042  t, "GPE"), (51 B
+00003a10: 4345 2c20 2244 4154 4522 292c 2028 3330  CE, "DATE"), (30
+00003a20: 2042 4345 2c20 2244 4154 4522 295d 0d0a   BCE, "DATE")]..
+00003a30: 2222 220d 0a60 6060 0d0a 215b 696d 6167  """..```..![imag
+00003a40: 655d 2868 7474 7073 3a2f 2f75 7365 722d  e](https://user-
+00003a50: 696d 6167 6573 2e67 6974 6875 6275 7365  images.githubuse
+00003a60: 7263 6f6e 7465 6e74 2e63 6f6d 2f33 3736  rcontent.com/376
+00003a70: 3231 3439 312f 3234 3631 3730 3632 332d  21491/246170623-
+00003a80: 3633 3531 6362 3765 2d62 6262 302d 3434  6351cb7e-bbb0-44
+00003a90: 3732 2d61 6631 362d 3961 3335 3161 3235  72-af16-9a351a25
+00003aa0: 3364 6139 2e70 6e67 290d 0a0d 0a23 2320  3da9.png)....## 
+00003ab0: 436f 6e74 6578 740d 0a3c 6831 2061 6c69  Context..<h1 ali
+00003ac0: 676e 3d22 6365 6e74 6572 223e 0d0a 2020  gn="center">..  
+00003ad0: 2020 3c61 2068 7265 663d 2268 7474 7073    <a href="https
+00003ae0: 3a2f 2f67 6974 6875 622e 636f 6d2f 6172  ://github.com/ar
+00003af0: 6769 6c6c 612d 696f 2f61 7267 696c 6c61  gilla-io/argilla
+00003b00: 223e 0d0a 2020 2020 3c69 6d67 2073 7263  ">..    <img src
+00003b10: 3d22 6874 7470 733a 2f2f 6769 7468 7562  ="https://github
+00003b20: 2e63 6f6d 2f64 7673 7265 706f 2f69 6d67  .com/dvsrepo/img
+00003b30: 732f 7261 772f 6d61 696e 2f72 672e 7376  s/raw/main/rg.sv
+00003b40: 6722 2061 6c74 3d22 4172 6769 6c6c 6122  g" alt="Argilla"
+00003b50: 2077 6964 7468 3d22 3135 3022 3e0d 0a20   width="150">.. 
+00003b60: 2020 203c 2f61 3e0d 0a3c 2f68 313e 0d0a     </a>..</h1>..
+00003b70: 0d0a 4920 6861 7665 2064 6576 656c 6f70  ..I have develop
+00003b80: 6564 2074 6869 7320 6c69 6272 6172 7920  ed this library 
+00003b90: 6173 2061 2070 6172 7420 6f66 206d 7920  as a part of my 
+00003ba0: 7468 6573 6973 2077 6f72 6b20 6174 205b  thesis work at [
+00003bb0: 4172 6769 6c6c 615d 2868 7474 7073 3a2f  Argilla](https:/
+00003bc0: 2f67 6974 6875 622e 636f 6d2f 6172 6769  /github.com/argi
+00003bd0: 6c6c 612d 696f 2f61 7267 696c 6c61 292e  lla-io/argilla).
+00003be0: 0d0a 4665 656c 2066 7265 6520 746f 20e2  ..Feel free to .
+00003bf0: ad90 2073 7461 7220 6f72 2077 6174 6368  .. star or watch
+00003c00: 2074 6865 2053 7061 6e4d 6172 6b65 7220   the SpanMarker 
+00003c10: 7265 706f 7369 746f 7279 2074 6f20 6765  repository to ge
+00003c20: 7420 6e6f 7469 6669 6564 2077 6865 6e20  t notified when 
+00003c30: 6d79 2074 6865 7369 7320 6973 2070 7562  my thesis is pub
+00003c40: 6c69 7368 6564 2e0d 0a0d 0a23 2320 4368  lished.....## Ch
+00003c50: 616e 6765 6c6f 670d 0a53 6565 205b 4348  angelog..See [CH
+00003c60: 414e 4745 4c4f 472e 6d64 5d28 4348 414e  ANGELOG.md](CHAN
+00003c70: 4745 4c4f 472e 6d64 2920 666f 7220 6e65  GELOG.md) for ne
+00003c80: 7773 206f 6e20 616c 6c20 5370 616e 4d61  ws on all SpanMa
+00003c90: 726b 6572 2076 6572 7369 6f6e 732e 0d0a  rker versions...
+00003ca0: 0d0a 2323 204c 6963 656e 7365 0d0a 5365  ..## License..Se
+00003cb0: 6520 5b4c 4943 454e 5345 5d28 4c49 4345  e [LICENSE](LICE
+00003cc0: 4e53 452e 6d64 2920 666f 7220 7468 6520  NSE.md) for the 
+00003cd0: 6375 7272 656e 7420 6c69 6365 6e73 652e  current license.
+00003ce0: 0d0a                                     ..
```

### Comparing `span_marker-1.2.2/README.md` & `span_marker-1.2.3/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,904 +1,939 @@
-00000000: 3c68 3120 616c 6967 6e3d 2263 656e 7465  <h1 align="cente
-00000010: 7222 3e0d 0a53 7061 6e4d 6172 6b65 7220  r">..SpanMarker 
-00000020: 666f 7220 4e61 6d65 6420 456e 7469 7479  for Named Entity
-00000030: 2052 6563 6f67 6e69 7469 6f6e 0d0a 3c2f   Recognition..</
-00000040: 6831 3e0d 0a3c 6469 7620 616c 6967 6e3d  h1>..<div align=
-00000050: 2263 656e 7465 7222 3e0d 0a0d 0a5b f09f  "center">....[..
-00000060: a497 204d 6f64 656c 735d 2868 7474 7073  .. Models](https
-00000070: 3a2f 2f68 7567 6769 6e67 6661 6365 2e63  ://huggingface.c
-00000080: 6f2f 6d6f 6465 6c73 3f6f 7468 6572 3d73  o/models?other=s
-00000090: 7061 6e2d 6d61 726b 6572 2920 7c0d 0a5b  pan-marker) |..[
-000000a0: f09f 9ba0 efb8 8f20 4765 7474 696e 6720  ....... Getting 
-000000b0: 5374 6172 7465 6420 496e 2047 6f6f 676c  Started In Googl
-000000c0: 6520 436f 6c61 625d 2868 7474 7073 3a2f  e Colab](https:/
-000000d0: 2f63 6f6c 6162 2e72 6573 6561 7263 682e  /colab.research.
-000000e0: 676f 6f67 6c65 2e63 6f6d 2f67 6974 6875  google.com/githu
-000000f0: 622f 746f 6d61 6172 7365 6e2f 5370 616e  b/tomaarsen/Span
-00000100: 4d61 726b 6572 4e45 522f 626c 6f62 2f6d  MarkerNER/blob/m
-00000110: 6169 6e2f 6e6f 7465 626f 6f6b 732f 6765  ain/notebooks/ge
-00000120: 7474 696e 675f 7374 6172 7465 642e 6970  tting_started.ip
-00000130: 796e 6229 207c 0d0a 5bf0 9f93 8420 446f  ynb) |..[.... Do
-00000140: 6375 6d65 6e74 6174 696f 6e5d 2868 7474  cumentation](htt
-00000150: 7073 3a2f 2f74 6f6d 6161 7273 656e 2e67  ps://tomaarsen.g
-00000160: 6974 6875 622e 696f 2f53 7061 6e4d 6172  ithub.io/SpanMar
-00000170: 6b65 724e 4552 290d 0a3c 2f64 6976 3e0d  kerNER)..</div>.
-00000180: 0a0d 0a53 7061 6e4d 6172 6b65 7220 6973  ...SpanMarker is
-00000190: 2061 2066 7261 6d65 776f 726b 2066 6f72   a framework for
-000001a0: 2074 7261 696e 696e 6720 706f 7765 7266   training powerf
-000001b0: 756c 204e 616d 6564 2045 6e74 6974 7920  ul Named Entity 
-000001c0: 5265 636f 676e 6974 696f 6e20 6d6f 6465  Recognition mode
-000001d0: 6c73 2075 7369 6e67 2066 616d 696c 6961  ls using familia
-000001e0: 7220 656e 636f 6465 7273 2073 7563 6820  r encoders such 
-000001f0: 6173 2042 4552 542c 2052 6f42 4552 5461  as BERT, RoBERTa
-00000200: 2061 6e64 2044 6542 4552 5461 2e0d 0a54   and DeBERTa...T
-00000210: 6967 6874 6c79 2069 6d70 6c65 6d65 6e74  ightly implement
-00000220: 6564 206f 6e20 746f 7020 6f66 2074 6865  ed on top of the
-00000230: 205b f09f a497 2054 7261 6e73 666f 726d   [.... Transform
-00000240: 6572 735d 2868 7474 7073 3a2f 2f67 6974  ers](https://git
-00000250: 6875 622e 636f 6d2f 6875 6767 696e 6766  hub.com/huggingf
-00000260: 6163 652f 7472 616e 7366 6f72 6d65 7273  ace/transformers
-00000270: 2f29 206c 6962 7261 7279 2c20 5370 616e  /) library, Span
-00000280: 4d61 726b 6572 2063 616e 2074 616b 6520  Marker can take 
-00000290: 6164 7661 6e74 6167 6520 6f66 2069 7473  advantage of its
-000002a0: 2076 616c 7561 626c 6520 6675 6e63 7469   valuable functi
-000002b0: 6f6e 616c 6974 792e 0d0a 3c21 2d2d 206c  onality...<!-- l
-000002c0: 696b 6520 7065 7266 6f72 6d61 6e63 6520  ike performance 
-000002d0: 6461 7368 626f 6172 6420 696e 7465 6772  dashboard integr
-000002e0: 6174 696f 6e2c 2061 7574 6f6d 6174 6963  ation, automatic
-000002f0: 206d 6978 6564 2070 7265 6369 7369 6f6e   mixed precision
-00000300: 2c20 382d 6269 7420 696e 6665 7265 6e63  , 8-bit inferenc
-00000310: 652d 2d3e 0d0a 0d0a 4261 7365 6420 6f6e  e-->....Based on
-00000320: 2074 6865 205b 504c 2d4d 6172 6b65 725d   the [PL-Marker]
-00000330: 2868 7474 7073 3a2f 2f61 7278 6976 2e6f  (https://arxiv.o
-00000340: 7267 2f70 6466 2f32 3130 392e 3036 3036  rg/pdf/2109.0606
-00000350: 372e 7064 6629 2070 6170 6572 2c20 5370  7.pdf) paper, Sp
-00000360: 616e 4d61 726b 6572 2062 7265 616b 7320  anMarker breaks 
-00000370: 7468 6520 6d6f 6c64 2074 6872 6f75 6768  the mold through
-00000380: 2069 7473 2061 6363 6573 7369 6269 6c69   its accessibili
-00000390: 7479 2061 6e64 2065 6173 6520 6f66 2075  ty and ease of u
-000003a0: 7365 2e20 4372 7563 6961 6c6c 792c 2053  se. Crucially, S
-000003b0: 7061 6e4d 6172 6b65 7220 776f 726b 7320  panMarker works 
-000003c0: 6f75 7420 6f66 2074 6865 2062 6f78 2077  out of the box w
-000003d0: 6974 6820 6d61 6e79 2063 6f6d 6d6f 6e20  ith many common 
-000003e0: 656e 636f 6465 7273 2073 7563 6820 6173  encoders such as
-000003f0: 2060 6265 7274 2d62 6173 652d 6361 7365   `bert-base-case
-00000400: 6460 2061 6e64 2060 726f 6265 7274 612d  d` and `roberta-
-00000410: 6c61 7267 6560 2c20 616e 6420 6175 746f  large`, and auto
-00000420: 6d61 7469 6361 6c6c 7920 776f 726b 7320  matically works 
-00000430: 7769 7468 2064 6174 6173 6574 7320 7573  with datasets us
-00000440: 696e 6720 7468 6520 6049 4f42 602c 2060  ing the `IOB`, `
-00000450: 494f 4232 602c 2060 4249 4f45 5360 2c20  IOB2`, `BIOES`, 
-00000460: 6042 494c 4f55 6020 6f72 206e 6f20 6c61  `BILOU` or no la
-00000470: 6265 6c20 616e 6e6f 7461 7469 6f6e 2073  bel annotation s
-00000480: 6368 656d 652e 0d0a 0d0a 4164 6469 7469  cheme.....Additi
-00000490: 6f6e 616c 6c79 2c20 7468 6520 5370 616e  onally, the Span
-000004a0: 4d61 726b 6572 206c 6962 7261 7279 2068  Marker library h
-000004b0: 6173 2062 6565 6e20 696e 7465 6772 6174  as been integrat
-000004c0: 6564 2077 6974 6820 7468 6520 4875 6767  ed with the Hugg
-000004d0: 696e 6720 4661 6365 2048 7562 2061 6e64  ing Face Hub and
-000004e0: 2074 6865 2048 7567 6769 6e67 2046 6163   the Hugging Fac
-000004f0: 6520 496e 6665 7265 6e63 6520 4150 492e  e Inference API.
-00000500: 2053 6565 2074 6865 2053 7061 6e4d 6172   See the SpanMar
-00000510: 6b65 7220 646f 6375 6d65 6e74 6174 696f  ker documentatio
-00000520: 6e20 6f6e 205b 4875 6767 696e 6720 4661  n on [Hugging Fa
-00000530: 6365 5d28 6874 7470 733a 2f2f 6875 6767  ce](https://hugg
-00000540: 696e 6766 6163 652e 636f 2f64 6f63 732f  ingface.co/docs/
-00000550: 6875 622f 7370 616e 5f6d 6172 6b65 7229  hub/span_marker)
-00000560: 206f 7220 7365 6520 5b61 6c6c 2053 7061   or see [all Spa
-00000570: 6e4d 6172 6b65 7220 6d6f 6465 6c73 206f  nMarker models o
-00000580: 6e20 7468 6520 4875 6767 696e 6720 4661  n the Hugging Fa
-00000590: 6365 2048 7562 5d28 6874 7470 733a 2f2f  ce Hub](https://
-000005a0: 6875 6767 696e 6766 6163 652e 636f 2f6d  huggingface.co/m
-000005b0: 6f64 656c 733f 6c69 6272 6172 793d 7370  odels?library=sp
-000005c0: 616e 2d6d 6172 6b65 7229 2e0d 0a0d 0a54  an-marker).....T
-000005d0: 6872 6f75 6768 2074 6865 2049 6e66 6572  hrough the Infer
-000005e0: 656e 6365 2041 5049 2069 6e74 6567 7261  ence API integra
-000005f0: 7469 6f6e 2c20 7573 6572 7320 6361 6e20  tion, users can 
-00000600: 7465 7374 2061 6e79 2053 7061 6e4d 6172  test any SpanMar
-00000610: 6b65 7220 6d6f 6465 6c20 6f6e 2074 6865  ker model on the
-00000620: 2048 7567 6769 6e67 2046 6163 6520 4875   Hugging Face Hu
-00000630: 6220 666f 7220 6672 6565 2075 7369 6e67  b for free using
-00000640: 2061 2077 6964 6765 7420 6f6e 2074 6865   a widget on the
-00000650: 205b 6d6f 6465 6c20 7061 6765 5d28 6874   [model page](ht
-00000660: 7470 733a 2f2f 6875 6767 696e 6766 6163  tps://huggingfac
-00000670: 652e 636f 2f74 6f6d 6161 7273 656e 2f73  e.co/tomaarsen/s
-00000680: 7061 6e2d 6d61 726b 6572 2d62 6572 742d  pan-marker-bert-
-00000690: 6261 7365 2d66 6577 6e65 7264 2d66 696e  base-fewnerd-fin
-000006a0: 652d 7375 7065 7229 2e20 4675 7274 6865  e-super). Furthe
-000006b0: 726d 6f72 652c 2065 6163 6820 7075 626c  rmore, each publ
-000006c0: 6963 2053 7061 6e4d 6172 6b65 7220 6d6f  ic SpanMarker mo
-000006d0: 6465 6c20 6f66 6665 7273 2061 2066 7265  del offers a fre
-000006e0: 6520 4150 4920 666f 7220 6661 7374 2070  e API for fast p
-000006f0: 726f 746f 7479 7069 6e67 2061 6e64 2063  rototyping and c
-00000700: 616e 2062 6520 6465 706c 6f79 6564 2074  an be deployed t
-00000710: 6f20 7072 6f64 7563 7469 6f6e 2075 7369  o production usi
-00000720: 6e67 2048 7567 6769 6e67 2046 6163 6520  ng Hugging Face 
-00000730: 496e 6665 7265 6e63 6520 456e 6470 6f69  Inference Endpoi
-00000740: 6e74 732e 0d0a 0d0a 7c20 496e 6665 7265  nts.....| Infere
-00000750: 6e63 6520 4150 4920 5769 6467 6574 2028  nce API Widget (
-00000760: 6f6e 2061 206d 6f64 656c 2070 6167 6529  on a model page)
-00000770: 207c 2046 7265 6520 496e 6665 7265 6e63   | Free Inferenc
-00000780: 6520 4150 4920 2860 4465 706c 6f79 6020  e API (`Deploy` 
-00000790: 3e20 6049 6e66 6572 656e 6365 2041 5049  > `Inference API
-000007a0: 6020 6f6e 2061 206d 6f64 656c 2070 6167  ` on a model pag
-000007b0: 6529 207c 0d0a 7c20 2d2d 2d2d 2d2d 2d2d  e) |..| --------
-000007c0: 2d2d 2d2d 2d20 7c20 2d2d 2d2d 2d2d 2d2d  ----- | --------
-000007d0: 2d2d 2d2d 2d20 7c0d 0a7c 2020 215b 696d  ----- |..|  ![im
-000007e0: 6167 655d 2868 7474 7073 3a2f 2f67 6974  age](https://git
-000007f0: 6875 622e 636f 6d2f 746f 6d61 6172 7365  hub.com/tomaarse
-00000800: 6e2f 5370 616e 4d61 726b 6572 4e45 522f  n/SpanMarkerNER/
-00000810: 6173 7365 7473 2f33 3736 3231 3439 312f  assets/37621491/
-00000820: 3233 3430 3738 6237 2d32 3263 382d 3439  234078b7-22c8-49
-00000830: 3163 2d38 3638 362d 6661 6363 6433 3934  1c-8686-faccd394
-00000840: 6636 3833 2920 7c20 2021 5b69 6d61 6765  f683) |  ![image
-00000850: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
-00000860: 2e63 6f6d 2f74 6f6d 6161 7273 656e 2f53  .com/tomaarsen/S
-00000870: 7061 6e4d 6172 6b65 724e 4552 2f61 7373  panMarkerNER/ass
-00000880: 6574 732f 3337 3632 3134 3931 2f34 3130  ets/37621491/410
-00000890: 6535 3139 312d 3933 3534 2d34 6532 372d  e5191-9354-4e27-
-000008a0: 6237 3138 2d32 6436 3961 6636 3738 6562  b718-2d69af678eb
-000008b0: 3729 207c 0d0a 0d0a 2323 2044 6f63 756d  7) |....## Docum
-000008c0: 656e 7461 7469 6f6e 0d0a 4665 656c 2066  entation..Feel f
-000008d0: 7265 6520 746f 2068 6176 6520 6120 6c6f  ree to have a lo
-000008e0: 6f6b 2061 7420 7468 6520 5b64 6f63 756d  ok at the [docum
-000008f0: 656e 7461 7469 6f6e 5d28 6874 7470 733a  entation](https:
-00000900: 2f2f 746f 6d61 6172 7365 6e2e 6769 7468  //tomaarsen.gith
-00000910: 7562 2e69 6f2f 5370 616e 4d61 726b 6572  ub.io/SpanMarker
-00000920: 4e45 5229 2e0d 0a0d 0a23 2320 496e 7374  NER).....## Inst
-00000930: 616c 6c61 7469 6f6e 0d0a 596f 7520 6d61  allation..You ma
-00000940: 7920 696e 7374 616c 6c20 7468 6520 5b60  y install the [`
-00000950: 7370 616e 5f6d 6172 6b65 7260 5d28 6874  span_marker`](ht
-00000960: 7470 733a 2f2f 7079 7069 2e6f 7267 2f70  tps://pypi.org/p
-00000970: 726f 6a65 6374 2f73 7061 6e2d 6d61 726b  roject/span-mark
-00000980: 6572 2920 5079 7468 6f6e 206d 6f64 756c  er) Python modul
-00000990: 6520 7669 6120 6070 6970 6020 6c69 6b65  e via `pip` like
-000009a0: 2073 6f3a 0d0a 6060 600d 0a70 6970 2069   so:..```..pip i
-000009b0: 6e73 7461 6c6c 2073 7061 6e5f 6d61 726b  nstall span_mark
-000009c0: 6572 0d0a 6060 600d 0a0d 0a23 2320 5175  er..```....## Qu
-000009d0: 6963 6b20 5374 6172 740d 0a23 2323 2054  ick Start..### T
-000009e0: 7261 696e 696e 670d 0a50 6c65 6173 6520  raining..Please 
-000009f0: 6861 7665 2061 206c 6f6f 6b20 6174 206f  have a look at o
-00000a00: 7572 205b 4765 7474 696e 6720 5374 6172  ur [Getting Star
-00000a10: 7465 645d 286e 6f74 6562 6f6f 6b73 2f67  ted](notebooks/g
-00000a20: 6574 7469 6e67 5f73 7461 7274 6564 2e69  etting_started.i
-00000a30: 7079 6e62 2920 6e6f 7465 626f 6f6b 2066  pynb) notebook f
-00000a40: 6f72 2064 6574 6169 6c73 206f 6e20 686f  or details on ho
-00000a50: 7720 5370 616e 4d61 726b 6572 2069 7320  w SpanMarker is 
-00000a60: 636f 6d6d 6f6e 6c79 2075 7365 642e 2049  commonly used. I
-00000a70: 7420 6578 706c 6169 6e73 2074 6865 2066  t explains the f
-00000a80: 6f6c 6c6f 7769 6e67 2073 6e69 7070 6574  ollowing snippet
-00000a90: 2069 6e20 6d6f 7265 2064 6574 6169 6c2e   in more detail.
-00000aa0: 2041 6c74 6572 6e61 7469 7665 6c79 2c20   Alternatively, 
-00000ab0: 6861 7665 2061 206c 6f6f 6b20 6174 2074  have a look at t
-00000ac0: 6865 205b 7472 6169 6e69 6e67 2073 6372  he [training scr
-00000ad0: 6970 7473 5d28 7472 6169 6e69 6e67 5f73  ipts](training_s
-00000ae0: 6372 6970 7473 2920 7468 6174 2068 6176  cripts) that hav
-00000af0: 6520 6265 656e 2073 7563 6365 7373 6675  e been successfu
-00000b00: 6c6c 7920 7573 6564 2069 6e20 7468 6520  lly used in the 
-00000b10: 7061 7374 2e0d 0a0d 0a7c 2043 6f6c 6162  past.....| Colab
-00000b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000be0: 2020 2020 2020 2020 207c 204b 6167 676c           | Kaggl
-00000bf0: 6520 2020 2020 2020 2020 2020 2020 2020  e               
-00000c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000cb0: 2020 2020 2020 2020 2020 2020 2020 7c20                | 
-00000cc0: 4772 6164 6965 6e74 2020 2020 2020 2020  Gradient        
-00000cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000000: 3c64 6976 2061 6c69 676e 3d22 6365 6e74  <div align="cent
+00000010: 6572 223e 0d0a 3c68 313e 0d0a 5370 616e  er">..<h1>..Span
+00000020: 4d61 726b 6572 2066 6f72 204e 616d 6564  Marker for Named
+00000030: 2045 6e74 6974 7920 5265 636f 676e 6974   Entity Recognit
+00000040: 696f 6e0d 0a3c 2f68 313e 0d0a 3c61 2068  ion..</h1>..<a h
+00000050: 7265 663d 2268 7474 7073 3a2f 2f68 7567  ref="https://hug
+00000060: 6769 6e67 6661 6365 2e63 6f2f 746f 6d61  gingface.co/toma
+00000070: 6172 7365 6e2f 7370 616e 2d6d 6172 6b65  arsen/span-marke
+00000080: 722d 726f 6265 7274 612d 6c61 7267 652d  r-roberta-large-
+00000090: 6f6e 746f 6e6f 7465 7335 2220 7461 7267  ontonotes5" targ
+000000a0: 6574 3d22 5f62 6c61 6e6b 223e 0d0a 2020  et="_blank">..  
+000000b0: 2020 3c69 6d67 2073 7263 3d22 6874 7470    <img src="http
+000000c0: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f74  s://github.com/t
+000000d0: 6f6d 6161 7273 656e 2f53 7061 6e4d 6172  omaarsen/SpanMar
+000000e0: 6b65 724e 4552 2f61 7373 6574 732f 3337  kerNER/assets/37
+000000f0: 3632 3134 3931 2f63 3736 6436 3339 332d  621491/c76d6393-
+00000100: 6262 3062 2d34 3463 332d 3934 3132 2d66  bb0b-44c3-9412-f
+00000110: 6439 6338 3331 3364 6363 3122 3e0d 0a3c  d9c8313dcc1">..<
+00000120: 2f61 3e0d 0a0d 0a5b f09f a497 204d 6f64  /a>....[.... Mod
+00000130: 656c 735d 2868 7474 7073 3a2f 2f68 7567  els](https://hug
+00000140: 6769 6e67 6661 6365 2e63 6f2f 6d6f 6465  gingface.co/mode
+00000150: 6c73 3f6c 6962 7261 7279 3d73 7061 6e2d  ls?library=span-
+00000160: 6d61 726b 6572 2920 7c0d 0a5b f09f 9ba0  marker) |..[....
+00000170: efb8 8f20 4765 7474 696e 6720 5374 6172  ... Getting Star
+00000180: 7465 6420 496e 2047 6f6f 676c 6520 436f  ted In Google Co
+00000190: 6c61 625d 2868 7474 7073 3a2f 2f63 6f6c  lab](https://col
+000001a0: 6162 2e72 6573 6561 7263 682e 676f 6f67  ab.research.goog
+000001b0: 6c65 2e63 6f6d 2f67 6974 6875 622f 746f  le.com/github/to
+000001c0: 6d61 6172 7365 6e2f 5370 616e 4d61 726b  maarsen/SpanMark
+000001d0: 6572 4e45 522f 626c 6f62 2f6d 6169 6e2f  erNER/blob/main/
+000001e0: 6e6f 7465 626f 6f6b 732f 6765 7474 696e  notebooks/gettin
+000001f0: 675f 7374 6172 7465 642e 6970 796e 6229  g_started.ipynb)
+00000200: 207c 0d0a 5bf0 9f93 8420 446f 6375 6d65   |..[.... Docume
+00000210: 6e74 6174 696f 6e5d 2868 7474 7073 3a2f  ntation](https:/
+00000220: 2f74 6f6d 6161 7273 656e 2e67 6974 6875  /tomaarsen.githu
+00000230: 622e 696f 2f53 7061 6e4d 6172 6b65 724e  b.io/SpanMarkerN
+00000240: 4552 290d 0a3c 2f64 6976 3e0d 0a0d 0a53  ER)..</div>....S
+00000250: 7061 6e4d 6172 6b65 7220 6973 2061 2066  panMarker is a f
+00000260: 7261 6d65 776f 726b 2066 6f72 2074 7261  ramework for tra
+00000270: 696e 696e 6720 706f 7765 7266 756c 204e  ining powerful N
+00000280: 616d 6564 2045 6e74 6974 7920 5265 636f  amed Entity Reco
+00000290: 676e 6974 696f 6e20 6d6f 6465 6c73 2075  gnition models u
+000002a0: 7369 6e67 2066 616d 696c 6961 7220 656e  sing familiar en
+000002b0: 636f 6465 7273 2073 7563 6820 6173 2042  coders such as B
+000002c0: 4552 542c 2052 6f42 4552 5461 2061 6e64  ERT, RoBERTa and
+000002d0: 2044 6542 4552 5461 2e0d 0a42 7569 6c74   DeBERTa...Built
+000002e0: 206f 6e20 746f 7020 6f66 2074 6865 2066   on top of the f
+000002f0: 616d 696c 6961 7220 5bf0 9fa4 9720 5472  amiliar [.... Tr
+00000300: 616e 7366 6f72 6d65 7273 5d28 6874 7470  ansformers](http
+00000310: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f68  s://github.com/h
+00000320: 7567 6769 6e67 6661 6365 2f74 7261 6e73  uggingface/trans
+00000330: 666f 726d 6572 7329 206c 6962 7261 7279  formers) library
+00000340: 2c20 5370 616e 4d61 726b 6572 2069 6e68  , SpanMarker inh
+00000350: 6572 6974 7320 6120 7769 6465 2072 616e  erits a wide ran
+00000360: 6765 206f 6620 706f 7765 7266 756c 2066  ge of powerful f
+00000370: 756e 6374 696f 6e61 6c69 7469 6573 2c20  unctionalities, 
+00000380: 7375 6368 2061 7320 6561 7369 6c79 206c  such as easily l
+00000390: 6f61 6469 6e67 2061 6e64 2073 6176 696e  oading and savin
+000003a0: 6720 6d6f 6465 6c73 2c20 6879 7065 7270  g models, hyperp
+000003b0: 6172 616d 6574 6572 206f 7074 696d 697a  arameter optimiz
+000003c0: 6174 696f 6e2c 2061 7574 6f6d 6174 6963  ation, automatic
+000003d0: 206c 6f67 6769 6e67 2069 6e20 7661 7269   logging in vari
+000003e0: 6f75 7320 746f 6f6c 732c 2063 6865 636b  ous tools, check
+000003f0: 706f 696e 7469 6e67 2c20 6361 6c6c 6261  pointing, callba
+00000400: 636b 732c 206d 6978 6564 2070 7265 6369  cks, mixed preci
+00000410: 7369 6f6e 2074 7261 696e 696e 672c 2038  sion training, 8
+00000420: 2d62 6974 2069 6e66 6572 656e 6365 2c20  -bit inference, 
+00000430: 616e 6420 6d6f 7265 2e0d 0a0d 0a3c 212d  and more.....<!-
+00000440: 2d54 6967 6874 6c79 2069 6d70 6c65 6d65  -Tightly impleme
+00000450: 6e74 6564 206f 6e20 746f 7020 6f66 2074  nted on top of t
+00000460: 6865 205b f09f a497 2054 7261 6e73 666f  he [.... Transfo
+00000470: 726d 6572 735d 2868 7474 7073 3a2f 2f67  rmers](https://g
+00000480: 6974 6875 622e 636f 6d2f 6875 6767 696e  ithub.com/huggin
+00000490: 6766 6163 652f 7472 616e 7366 6f72 6d65  gface/transforme
+000004a0: 7273 2f29 206c 6962 7261 7279 2c20 5370  rs/) library, Sp
+000004b0: 616e 4d61 726b 6572 2063 616e 2074 616b  anMarker can tak
+000004c0: 6520 6164 7661 6e74 6167 6520 6f66 2069  e advantage of i
+000004d0: 7473 2076 616c 7561 626c 6520 6675 6e63  ts valuable func
+000004e0: 7469 6f6e 616c 6974 792e 2d2d 3e0d 0a3c  tionality.-->..<
+000004f0: 212d 2d20 6c69 6b65 2070 6572 666f 726d  !-- like perform
+00000500: 616e 6365 2064 6173 6862 6f61 7264 2069  ance dashboard i
+00000510: 6e74 6567 7261 7469 6f6e 2c20 6175 746f  ntegration, auto
+00000520: 6d61 7469 6320 6d69 7865 6420 7072 6563  matic mixed prec
+00000530: 6973 696f 6e2c 2038 2d62 6974 2069 6e66  ision, 8-bit inf
+00000540: 6572 656e 6365 2d2d 3e0d 0a0d 0a42 6173  erence-->....Bas
+00000550: 6564 206f 6e20 7468 6520 5b50 4c2d 4d61  ed on the [PL-Ma
+00000560: 726b 6572 5d28 6874 7470 733a 2f2f 6172  rker](https://ar
+00000570: 7869 762e 6f72 672f 7064 662f 3231 3039  xiv.org/pdf/2109
+00000580: 2e30 3630 3637 2e70 6466 2920 7061 7065  .06067.pdf) pape
+00000590: 722c 2053 7061 6e4d 6172 6b65 7220 6272  r, SpanMarker br
+000005a0: 6561 6b73 2074 6865 206d 6f6c 6420 7468  eaks the mold th
+000005b0: 726f 7567 6820 6974 7320 6163 6365 7373  rough its access
+000005c0: 6962 696c 6974 7920 616e 6420 6561 7365  ibility and ease
+000005d0: 206f 6620 7573 652e 2043 7275 6369 616c   of use. Crucial
+000005e0: 6c79 2c20 5370 616e 4d61 726b 6572 2077  ly, SpanMarker w
+000005f0: 6f72 6b73 206f 7574 206f 6620 7468 6520  orks out of the 
+00000600: 626f 7820 7769 7468 206d 616e 7920 636f  box with many co
+00000610: 6d6d 6f6e 2065 6e63 6f64 6572 7320 7375  mmon encoders su
+00000620: 6368 2061 7320 6062 6572 742d 6261 7365  ch as `bert-base
+00000630: 2d63 6173 6564 6020 616e 6420 6072 6f62  -cased` and `rob
+00000640: 6572 7461 2d6c 6172 6765 602c 2061 6e64  erta-large`, and
+00000650: 2061 7574 6f6d 6174 6963 616c 6c79 2077   automatically w
+00000660: 6f72 6b73 2077 6974 6820 6461 7461 7365  orks with datase
+00000670: 7473 2075 7369 6e67 2074 6865 2060 494f  ts using the `IO
+00000680: 4260 2c20 6049 4f42 3260 2c20 6042 494f  B`, `IOB2`, `BIO
+00000690: 4553 602c 2060 4249 4c4f 5560 206f 7220  ES`, `BILOU` or 
+000006a0: 6e6f 206c 6162 656c 2061 6e6e 6f74 6174  no label annotat
+000006b0: 696f 6e20 7363 6865 6d65 2e0d 0a0d 0a41  ion scheme.....A
+000006c0: 6464 6974 696f 6e61 6c6c 792c 2074 6865  dditionally, the
+000006d0: 2053 7061 6e4d 6172 6b65 7220 6c69 6272   SpanMarker libr
+000006e0: 6172 7920 6861 7320 6265 656e 2069 6e74  ary has been int
+000006f0: 6567 7261 7465 6420 7769 7468 2074 6865  egrated with the
+00000700: 2048 7567 6769 6e67 2046 6163 6520 4875   Hugging Face Hu
+00000710: 6220 616e 6420 7468 6520 4875 6767 696e  b and the Huggin
+00000720: 6720 4661 6365 2049 6e66 6572 656e 6365  g Face Inference
+00000730: 2041 5049 2e20 5365 6520 7468 6520 5370   API. See the Sp
+00000740: 616e 4d61 726b 6572 2064 6f63 756d 656e  anMarker documen
+00000750: 7461 7469 6f6e 206f 6e20 5b48 7567 6769  tation on [Huggi
+00000760: 6e67 2046 6163 655d 2868 7474 7073 3a2f  ng Face](https:/
+00000770: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
+00000780: 646f 6373 2f68 7562 2f73 7061 6e5f 6d61  docs/hub/span_ma
+00000790: 726b 6572 2920 6f72 2073 6565 205b 616c  rker) or see [al
+000007a0: 6c20 5370 616e 4d61 726b 6572 206d 6f64  l SpanMarker mod
+000007b0: 656c 7320 6f6e 2074 6865 2048 7567 6769  els on the Huggi
+000007c0: 6e67 2046 6163 6520 4875 625d 2868 7474  ng Face Hub](htt
+000007d0: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+000007e0: 2e63 6f2f 6d6f 6465 6c73 3f6c 6962 7261  .co/models?libra
+000007f0: 7279 3d73 7061 6e2d 6d61 726b 6572 292e  ry=span-marker).
+00000800: 0d0a 5468 726f 7567 6820 7468 6520 496e  ..Through the In
+00000810: 6665 7265 6e63 6520 4150 4920 696e 7465  ference API inte
+00000820: 6772 6174 696f 6e2c 2075 7365 7273 2063  gration, users c
+00000830: 616e 2074 6573 7420 616e 7920 5370 616e  an test any Span
+00000840: 4d61 726b 6572 206d 6f64 656c 206f 6e20  Marker model on 
+00000850: 7468 6520 4875 6767 696e 6720 4661 6365  the Hugging Face
+00000860: 2048 7562 2066 6f72 2066 7265 6520 7573   Hub for free us
+00000870: 696e 6720 6120 7769 6467 6574 206f 6e20  ing a widget on 
+00000880: 7468 6520 5b6d 6f64 656c 2070 6167 655d  the [model page]
+00000890: 2868 7474 7073 3a2f 2f68 7567 6769 6e67  (https://hugging
+000008a0: 6661 6365 2e63 6f2f 746f 6d61 6172 7365  face.co/tomaarse
+000008b0: 6e2f 7370 616e 2d6d 6172 6b65 722d 6265  n/span-marker-be
+000008c0: 7274 2d62 6173 652d 6665 776e 6572 642d  rt-base-fewnerd-
+000008d0: 6669 6e65 2d73 7570 6572 292e 2046 7572  fine-super). Fur
+000008e0: 7468 6572 6d6f 7265 2c20 6561 6368 2070  thermore, each p
+000008f0: 7562 6c69 6320 5370 616e 4d61 726b 6572  ublic SpanMarker
+00000900: 206d 6f64 656c 206f 6666 6572 7320 6120   model offers a 
+00000910: 6672 6565 2041 5049 2066 6f72 2066 6173  free API for fas
+00000920: 7420 7072 6f74 6f74 7970 696e 6720 616e  t prototyping an
+00000930: 6420 6361 6e20 6265 2064 6570 6c6f 7965  d can be deploye
+00000940: 6420 746f 2070 726f 6475 6374 696f 6e20  d to production 
+00000950: 7573 696e 6720 4875 6767 696e 6720 4661  using Hugging Fa
+00000960: 6365 2049 6e66 6572 656e 6365 2045 6e64  ce Inference End
+00000970: 706f 696e 7473 2e0d 0a0d 0a7c 2049 6e66  points.....| Inf
+00000980: 6572 656e 6365 2041 5049 2057 6964 6765  erence API Widge
+00000990: 7420 286f 6e20 6120 6d6f 6465 6c20 7061  t (on a model pa
+000009a0: 6765 2920 7c20 4672 6565 2049 6e66 6572  ge) | Free Infer
+000009b0: 656e 6365 2041 5049 2028 6044 6570 6c6f  ence API (`Deplo
+000009c0: 7960 203e 2060 496e 6665 7265 6e63 6520  y` > `Inference 
+000009d0: 4150 4960 206f 6e20 6120 6d6f 6465 6c20  API` on a model 
+000009e0: 7061 6765 2920 7c0d 0a7c 202d 2d2d 2d2d  page) |..| -----
+000009f0: 2d2d 2d2d 2d2d 2d2d 207c 202d 2d2d 2d2d  -------- | -----
+00000a00: 2d2d 2d2d 2d2d 2d2d 207c 0d0a 7c20 2021  -------- |..|  !
+00000a10: 5b69 6d61 6765 5d28 6874 7470 733a 2f2f  [image](https://
+00000a20: 6769 7468 7562 2e63 6f6d 2f74 6f6d 6161  github.com/tomaa
+00000a30: 7273 656e 2f53 7061 6e4d 6172 6b65 724e  rsen/SpanMarkerN
+00000a40: 4552 2f61 7373 6574 732f 3337 3632 3134  ER/assets/376214
+00000a50: 3931 2f32 3334 3037 3862 372d 3232 6338  91/234078b7-22c8
+00000a60: 2d34 3931 632d 3836 3836 2d66 6163 6364  -491c-8686-faccd
+00000a70: 3339 3466 3638 3329 207c 2020 215b 696d  394f683) |  ![im
+00000a80: 6167 655d 2868 7474 7073 3a2f 2f67 6974  age](https://git
+00000a90: 6875 622e 636f 6d2f 746f 6d61 6172 7365  hub.com/tomaarse
+00000aa0: 6e2f 5370 616e 4d61 726b 6572 4e45 522f  n/SpanMarkerNER/
+00000ab0: 6173 7365 7473 2f33 3736 3231 3439 312f  assets/37621491/
+00000ac0: 3431 3065 3531 3931 2d39 3335 342d 3465  410e5191-9354-4e
+00000ad0: 3237 2d62 3731 382d 3264 3639 6166 3637  27-b718-2d69af67
+00000ae0: 3865 6237 2920 7c0d 0a0d 0a23 2320 446f  8eb7) |....## Do
+00000af0: 6375 6d65 6e74 6174 696f 6e0d 0a46 6565  cumentation..Fee
+00000b00: 6c20 6672 6565 2074 6f20 6861 7665 2061  l free to have a
+00000b10: 206c 6f6f 6b20 6174 2074 6865 205b 646f   look at the [do
+00000b20: 6375 6d65 6e74 6174 696f 6e5d 2868 7474  cumentation](htt
+00000b30: 7073 3a2f 2f74 6f6d 6161 7273 656e 2e67  ps://tomaarsen.g
+00000b40: 6974 6875 622e 696f 2f53 7061 6e4d 6172  ithub.io/SpanMar
+00000b50: 6b65 724e 4552 292e 0d0a 0d0a 2323 2049  kerNER).....## I
+00000b60: 6e73 7461 6c6c 6174 696f 6e0d 0a59 6f75  nstallation..You
+00000b70: 206d 6179 2069 6e73 7461 6c6c 2074 6865   may install the
+00000b80: 205b 6073 7061 6e5f 6d61 726b 6572 605d   [`span_marker`]
+00000b90: 2868 7474 7073 3a2f 2f70 7970 692e 6f72  (https://pypi.or
+00000ba0: 672f 7072 6f6a 6563 742f 7370 616e 2d6d  g/project/span-m
+00000bb0: 6172 6b65 7229 2050 7974 686f 6e20 6d6f  arker) Python mo
+00000bc0: 6475 6c65 2076 6961 2060 7069 7060 206c  dule via `pip` l
+00000bd0: 696b 6520 736f 3a0d 0a60 6060 0d0a 7069  ike so:..```..pi
+00000be0: 7020 696e 7374 616c 6c20 7370 616e 5f6d  p install span_m
+00000bf0: 6172 6b65 720d 0a60 6060 0d0a 0d0a 2323  arker..```....##
+00000c00: 2051 7569 636b 2053 7461 7274 0d0a 2323   Quick Start..##
+00000c10: 2320 5472 6169 6e69 6e67 0d0a 506c 6561  # Training..Plea
+00000c20: 7365 2068 6176 6520 6120 6c6f 6f6b 2061  se have a look a
+00000c30: 7420 6f75 7220 5b47 6574 7469 6e67 2053  t our [Getting S
+00000c40: 7461 7274 6564 5d28 6e6f 7465 626f 6f6b  tarted](notebook
+00000c50: 732f 6765 7474 696e 675f 7374 6172 7465  s/getting_starte
+00000c60: 642e 6970 796e 6229 206e 6f74 6562 6f6f  d.ipynb) noteboo
+00000c70: 6b20 666f 7220 6465 7461 696c 7320 6f6e  k for details on
+00000c80: 2068 6f77 2053 7061 6e4d 6172 6b65 7220   how SpanMarker 
+00000c90: 6973 2063 6f6d 6d6f 6e6c 7920 7573 6564  is commonly used
+00000ca0: 2e20 4974 2065 7870 6c61 696e 7320 7468  . It explains th
+00000cb0: 6520 666f 6c6c 6f77 696e 6720 736e 6970  e following snip
+00000cc0: 7065 7420 696e 206d 6f72 6520 6465 7461  pet in more deta
+00000cd0: 696c 2e20 416c 7465 726e 6174 6976 656c  il. Alternativel
+00000ce0: 792c 2068 6176 6520 6120 6c6f 6f6b 2061  y, have a look a
+00000cf0: 7420 7468 6520 5b74 7261 696e 696e 6720  t the [training 
+00000d00: 7363 7269 7074 735d 2874 7261 696e 696e  scripts](trainin
+00000d10: 675f 7363 7269 7074 7329 2074 6861 7420  g_scripts) that 
+00000d20: 6861 7665 2062 6565 6e20 7375 6363 6573  have been succes
+00000d30: 7366 756c 6c79 2075 7365 6420 696e 2074  sfully used in t
+00000d40: 6865 2070 6173 742e 0d0a 0d0a 7c20 436f  he past.....| Co
+00000d50: 6c61 6220 2020 2020 2020 2020 2020 2020  lab             
 00000d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d80: 207c 2053 7475 6469 6f20 4c61 6220 2020   | Studio Lab   
+00000d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000e10: 2020 2020 2020 2020 2020 2020 7c20 4b61              | Ka
+00000e20: 6767 6c65 2020 2020 2020 2020 2020 2020  ggle            
 00000e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e50: 2020 2020 2020 2020 2020 7c0d 0a7c 3a2d            |..|:-
-00000e60: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000e70: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000e80: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000e90: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ea0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000eb0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ec0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ed0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ee0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ef0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f00: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f10: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f20: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d7c 3a2d  -------------|:-
-00000f30: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f60: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f70: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f80: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000f90: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000fa0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000fb0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000fc0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000fd0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000fe0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00000ff0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001000: 2d2d 7c3a 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  --|:------------
-00001010: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001020: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001030: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001040: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001050: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001060: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001070: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001080: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001090: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00000e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ef0: 207c 2047 7261 6469 656e 7420 2020 2020   | Gradient     
+00000f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000fb0: 2020 2020 7c20 5374 7564 696f 204c 6162      | Studio Lab
+00000fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001030: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001080: 2020 2020 2020 2020 2020 2020 207c 0d0a               |..
+00001090: 7c3a 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  |:--------------
 000010a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000010b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010c0: 2d2d 2d2d 2d7c 3a2d 2d2d 2d2d 2d2d 2d2d  -----|:---------
+000010c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000010d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000010e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000010f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001100: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001110: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001120: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001130: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001140: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001150: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001160: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001160: 7c3a 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  |:--------------
 00001170: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001180: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001190: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 7c0d  --------------|.
-000011a0: 0a7c 205b 215b 4f70 656e 2049 6e20 436f  .| [![Open In Co
-000011b0: 6c61 625d 2868 7474 7073 3a2f 2f63 6f6c  lab](https://col
-000011c0: 6162 2e72 6573 6561 7263 682e 676f 6f67  ab.research.goog
-000011d0: 6c65 2e63 6f6d 2f61 7373 6574 732f 636f  le.com/assets/co
-000011e0: 6c61 622d 6261 6467 652e 7376 6729 5d28  lab-badge.svg)](
-000011f0: 6874 7470 733a 2f2f 636f 6c61 622e 7265  https://colab.re
-00001200: 7365 6172 6368 2e67 6f6f 676c 652e 636f  search.google.co
-00001210: 6d2f 6769 7468 7562 2f74 6f6d 6161 7273  m/github/tomaars
-00001220: 656e 2f53 7061 6e4d 6172 6b65 724e 4552  en/SpanMarkerNER
-00001230: 2f62 6c6f 622f 6d61 696e 2f6e 6f74 6562  /blob/main/noteb
-00001240: 6f6f 6b73 2f67 6574 7469 6e67 5f73 7461  ooks/getting_sta
-00001250: 7274 6564 2e69 7079 6e62 2920 2020 2020  rted.ipynb)     
-00001260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001270: 2020 7c20 5b21 5b4b 6167 676c 655d 2868    | [![Kaggle](h
-00001280: 7474 7073 3a2f 2f6b 6167 676c 652e 636f  ttps://kaggle.co
-00001290: 6d2f 7374 6174 6963 2f69 6d61 6765 732f  m/static/images/
-000012a0: 6f70 656e 2d69 6e2d 6b61 6767 6c65 2e73  open-in-kaggle.s
-000012b0: 7667 295d 2868 7474 7073 3a2f 2f6b 6167  vg)](https://kag
-000012c0: 676c 652e 636f 6d2f 6b65 726e 656c 732f  gle.com/kernels/
-000012d0: 7765 6c63 6f6d 653f 7372 633d 6874 7470  welcome?src=http
-000012e0: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f74  s://github.com/t
-000012f0: 6f6d 6161 7273 656e 2f53 7061 6e4d 6172  omaarsen/SpanMar
-00001300: 6b65 724e 4552 2f62 6c6f 622f 6d61 696e  kerNER/blob/main
-00001310: 2f6e 6f74 6562 6f6f 6b73 2f67 6574 7469  /notebooks/getti
-00001320: 6e67 5f73 7461 7274 6564 2e69 7079 6e62  ng_started.ipynb
-00001330: 2920 2020 2020 2020 2020 2020 2020 2020  )               
-00001340: 2020 2020 2020 2020 7c20 5b21 5b47 7261          | [![Gra
-00001350: 6469 656e 745d 2868 7474 7073 3a2f 2f61  dient](https://a
-00001360: 7373 6574 732e 7061 7065 7273 7061 6365  ssets.paperspace
-00001370: 2e69 6f2f 696d 672f 6772 6164 6965 6e74  .io/img/gradient
-00001380: 2d62 6164 6765 2e73 7667 295d 2868 7474  -badge.svg)](htt
-00001390: 7073 3a2f 2f63 6f6e 736f 6c65 2e70 6170  ps://console.pap
-000013a0: 6572 7370 6163 652e 636f 6d2f 6769 7468  erspace.com/gith
-000013b0: 7562 2f74 6f6d 6161 7273 656e 2f53 7061  ub/tomaarsen/Spa
-000013c0: 6e4d 6172 6b65 724e 4552 2f62 6c6f 622f  nMarkerNER/blob/
-000013d0: 6d61 696e 2f6e 6f74 6562 6f6f 6b73 2f67  main/notebooks/g
-000013e0: 6574 7469 6e67 5f73 7461 7274 6564 2e69  etting_started.i
-000013f0: 7079 6e62 2920 2020 2020 2020 2020 2020  pynb)           
-00001400: 2020 2020 2020 2020 2020 2020 7c20 5b21              | [!
-00001410: 5b4f 7065 6e20 496e 2053 6167 654d 616b  [Open In SageMak
-00001420: 6572 2053 7475 6469 6f20 4c61 625d 2868  er Studio Lab](h
-00001430: 7474 7073 3a2f 2f73 7475 6469 6f6c 6162  ttps://studiolab
-00001440: 2e73 6167 656d 616b 6572 2e61 7773 2f73  .sagemaker.aws/s
-00001450: 7475 6469 6f6c 6162 2e73 7667 295d 2868  tudiolab.svg)](h
-00001460: 7474 7073 3a2f 2f73 7475 6469 6f6c 6162  ttps://studiolab
-00001470: 2e73 6167 656d 616b 6572 2e61 7773 2f69  .sagemaker.aws/i
-00001480: 6d70 6f72 742f 6769 7468 7562 2f74 6f6d  mport/github/tom
-00001490: 6161 7273 656e 2f53 7061 6e4d 6172 6b65  aarsen/SpanMarke
-000014a0: 724e 4552 2f62 6c6f 622f 6d61 696e 2f6e  rNER/blob/main/n
-000014b0: 6f74 6562 6f6f 6b73 2f67 6574 7469 6e67  otebooks/getting
-000014c0: 5f73 7461 7274 6564 2e69 7079 6e62 2920  _started.ipynb) 
-000014d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000014e0: 2020 2020 2020 7c0d 0a0d 0a60 6060 7079        |....```py
-000014f0: 7468 6f6e 0d0a 6672 6f6d 2064 6174 6173  thon..from datas
-00001500: 6574 7320 696d 706f 7274 206c 6f61 645f  ets import load_
-00001510: 6461 7461 7365 740d 0a66 726f 6d20 7472  dataset..from tr
-00001520: 616e 7366 6f72 6d65 7273 2069 6d70 6f72  ansformers impor
-00001530: 7420 5472 6169 6e69 6e67 4172 6775 6d65  t TrainingArgume
-00001540: 6e74 730d 0a66 726f 6d20 7370 616e 5f6d  nts..from span_m
-00001550: 6172 6b65 7220 696d 706f 7274 2053 7061  arker import Spa
-00001560: 6e4d 6172 6b65 724d 6f64 656c 2c20 5472  nMarkerModel, Tr
-00001570: 6169 6e65 720d 0a0d 0a0d 0a64 6566 206d  ainer......def m
-00001580: 6169 6e28 2920 2d3e 204e 6f6e 653a 0d0a  ain() -> None:..
-00001590: 2020 2020 2320 4c6f 6164 2074 6865 2064      # Load the d
-000015a0: 6174 6173 6574 2c20 656e 7375 7265 2022  ataset, ensure "
-000015b0: 746f 6b65 6e73 2220 616e 6420 226e 6572  tokens" and "ner
-000015c0: 5f74 6167 7322 2063 6f6c 756d 6e73 2c20  _tags" columns, 
-000015d0: 616e 6420 6765 7420 6120 6c69 7374 206f  and get a list o
-000015e0: 6620 6c61 6265 6c73 0d0a 2020 2020 6461  f labels..    da
-000015f0: 7461 7365 7420 3d20 6c6f 6164 5f64 6174  taset = load_dat
-00001600: 6173 6574 2822 4446 4b49 2d53 4c54 2f66  aset("DFKI-SLT/f
-00001610: 6577 2d6e 6572 6422 2c20 2273 7570 6572  ew-nerd", "super
-00001620: 7669 7365 6422 290d 0a20 2020 2064 6174  vised")..    dat
-00001630: 6173 6574 203d 2064 6174 6173 6574 2e72  aset = dataset.r
-00001640: 656d 6f76 655f 636f 6c75 6d6e 7328 226e  emove_columns("n
-00001650: 6572 5f74 6167 7322 290d 0a20 2020 2064  er_tags")..    d
-00001660: 6174 6173 6574 203d 2064 6174 6173 6574  ataset = dataset
-00001670: 2e72 656e 616d 655f 636f 6c75 6d6e 2822  .rename_column("
-00001680: 6669 6e65 5f6e 6572 5f74 6167 7322 2c20  fine_ner_tags", 
-00001690: 226e 6572 5f74 6167 7322 290d 0a20 2020  "ner_tags")..   
-000016a0: 206c 6162 656c 7320 3d20 6461 7461 7365   labels = datase
-000016b0: 745b 2274 7261 696e 225d 2e66 6561 7475  t["train"].featu
-000016c0: 7265 735b 226e 6572 5f74 6167 7322 5d2e  res["ner_tags"].
-000016d0: 6665 6174 7572 652e 6e61 6d65 730d 0a0d  feature.names...
-000016e0: 0a20 2020 2023 2049 6e69 7469 616c 697a  .    # Initializ
-000016f0: 6520 6120 5370 616e 4d61 726b 6572 206d  e a SpanMarker m
-00001700: 6f64 656c 2075 7369 6e67 2061 2070 7265  odel using a pre
-00001710: 7472 6169 6e65 6420 4245 5254 2d73 7479  trained BERT-sty
-00001720: 6c65 2065 6e63 6f64 6572 0d0a 2020 2020  le encoder..    
-00001730: 6d6f 6465 6c5f 6e61 6d65 203d 2022 6265  model_name = "be
-00001740: 7274 2d62 6173 652d 6361 7365 6422 0d0a  rt-base-cased"..
-00001750: 2020 2020 6d6f 6465 6c20 3d20 5370 616e      model = Span
-00001760: 4d61 726b 6572 4d6f 6465 6c2e 6672 6f6d  MarkerModel.from
-00001770: 5f70 7265 7472 6169 6e65 6428 0d0a 2020  _pretrained(..  
-00001780: 2020 2020 2020 6d6f 6465 6c5f 6e61 6d65        model_name
-00001790: 2c0d 0a20 2020 2020 2020 206c 6162 656c  ,..        label
-000017a0: 733d 6c61 6265 6c73 2c0d 0a20 2020 2020  s=labels,..     
-000017b0: 2020 2023 2053 7061 6e4d 6172 6b65 7220     # SpanMarker 
-000017c0: 6879 7065 7270 6172 616d 6574 6572 733a  hyperparameters:
-000017d0: 0d0a 2020 2020 2020 2020 6d6f 6465 6c5f  ..        model_
-000017e0: 6d61 785f 6c65 6e67 7468 3d32 3536 2c0d  max_length=256,.
-000017f0: 0a20 2020 2020 2020 206d 6172 6b65 725f  .        marker_
-00001800: 6d61 785f 6c65 6e67 7468 3d31 3238 2c0d  max_length=128,.
-00001810: 0a20 2020 2020 2020 2065 6e74 6974 795f  .        entity_
-00001820: 6d61 785f 6c65 6e67 7468 3d38 2c0d 0a20  max_length=8,.. 
-00001830: 2020 2029 0d0a 0d0a 2020 2020 2320 5072     )....    # Pr
-00001840: 6570 6172 6520 7468 6520 f09f a497 2074  epare the .... t
-00001850: 7261 6e73 666f 726d 6572 7320 7472 6169  ransformers trai
-00001860: 6e69 6e67 2061 7267 756d 656e 7473 0d0a  ning arguments..
-00001870: 2020 2020 6172 6773 203d 2054 7261 696e      args = Train
-00001880: 696e 6741 7267 756d 656e 7473 280d 0a20  ingArguments(.. 
-00001890: 2020 2020 2020 206f 7574 7075 745f 6469         output_di
-000018a0: 723d 226d 6f64 656c 732f 7370 616e 5f6d  r="models/span_m
-000018b0: 6172 6b65 725f 6265 7274 5f62 6173 655f  arker_bert_base_
-000018c0: 6361 7365 645f 6665 776e 6572 645f 6669  cased_fewnerd_fi
-000018d0: 6e65 5f73 7570 6572 222c 0d0a 2020 2020  ne_super",..    
-000018e0: 2020 2020 2320 5472 6169 6e69 6e67 2048      # Training H
-000018f0: 7970 6572 7061 7261 6d65 7465 7273 3a0d  yperparameters:.
-00001900: 0a20 2020 2020 2020 206c 6561 726e 696e  .        learnin
-00001910: 675f 7261 7465 3d35 652d 352c 0d0a 2020  g_rate=5e-5,..  
-00001920: 2020 2020 2020 7065 725f 6465 7669 6365        per_device
-00001930: 5f74 7261 696e 5f62 6174 6368 5f73 697a  _train_batch_siz
-00001940: 653d 3332 2c0d 0a20 2020 2020 2020 2070  e=32,..        p
-00001950: 6572 5f64 6576 6963 655f 6576 616c 5f62  er_device_eval_b
-00001960: 6174 6368 5f73 697a 653d 3332 2c0d 0a20  atch_size=32,.. 
-00001970: 2020 2020 2020 206e 756d 5f74 7261 696e         num_train
-00001980: 5f65 706f 6368 733d 332c 0d0a 2020 2020  _epochs=3,..    
-00001990: 2020 2020 7765 6967 6874 5f64 6563 6179      weight_decay
-000019a0: 3d30 2e30 312c 0d0a 2020 2020 2020 2020  =0.01,..        
-000019b0: 7761 726d 7570 5f72 6174 696f 3d30 2e31  warmup_ratio=0.1
-000019c0: 2c0d 0a20 2020 2020 2020 2062 6631 363d  ,..        bf16=
-000019d0: 5472 7565 2c20 2023 2052 6570 6c61 6365  True,  # Replace
-000019e0: 2060 6266 3136 6020 7769 7468 2060 6670   `bf16` with `fp
-000019f0: 3136 6020 6966 2079 6f75 7220 6861 7264  16` if your hard
-00001a00: 7761 7265 2063 616e 2774 2075 7365 2062  ware can't use b
-00001a10: 6631 362e 0d0a 2020 2020 2020 2020 2320  f16...        # 
-00001a20: 4f74 6865 7220 5472 6169 6e69 6e67 2070  Other Training p
-00001a30: 6172 616d 6574 6572 730d 0a20 2020 2020  arameters..     
-00001a40: 2020 206c 6f67 6769 6e67 5f66 6972 7374     logging_first
-00001a50: 5f73 7465 703d 5472 7565 2c0d 0a20 2020  _step=True,..   
-00001a60: 2020 2020 206c 6f67 6769 6e67 5f73 7465       logging_ste
-00001a70: 7073 3d35 302c 0d0a 2020 2020 2020 2020  ps=50,..        
-00001a80: 6576 616c 7561 7469 6f6e 5f73 7472 6174  evaluation_strat
-00001a90: 6567 793d 2273 7465 7073 222c 0d0a 2020  egy="steps",..  
-00001aa0: 2020 2020 2020 7361 7665 5f73 7472 6174        save_strat
-00001ab0: 6567 793d 2273 7465 7073 222c 0d0a 2020  egy="steps",..  
-00001ac0: 2020 2020 2020 6576 616c 5f73 7465 7073        eval_steps
-00001ad0: 3d33 3030 302c 0d0a 2020 2020 2020 2020  =3000,..        
-00001ae0: 7361 7665 5f74 6f74 616c 5f6c 696d 6974  save_total_limit
-00001af0: 3d32 2c0d 0a20 2020 2020 2020 2064 6174  =2,..        dat
-00001b00: 616c 6f61 6465 725f 6e75 6d5f 776f 726b  aloader_num_work
-00001b10: 6572 733d 322c 0d0a 2020 2020 290d 0a0d  ers=2,..    )...
-00001b20: 0a20 2020 2023 2049 6e69 7469 616c 697a  .    # Initializ
-00001b30: 6520 7468 6520 7472 6169 6e65 7220 7573  e the trainer us
-00001b40: 696e 6720 6f75 7220 6d6f 6465 6c2c 2074  ing our model, t
-00001b50: 7261 696e 696e 6720 6172 6773 2026 2064  raining args & d
-00001b60: 6174 6173 6574 2c20 616e 6420 7472 6169  ataset, and trai
-00001b70: 6e0d 0a20 2020 2074 7261 696e 6572 203d  n..    trainer =
-00001b80: 2054 7261 696e 6572 280d 0a20 2020 2020   Trainer(..     
-00001b90: 2020 206d 6f64 656c 3d6d 6f64 656c 2c0d     model=model,.
-00001ba0: 0a20 2020 2020 2020 2061 7267 733d 6172  .        args=ar
-00001bb0: 6773 2c0d 0a20 2020 2020 2020 2074 7261  gs,..        tra
-00001bc0: 696e 5f64 6174 6173 6574 3d64 6174 6173  in_dataset=datas
-00001bd0: 6574 5b22 7472 6169 6e22 5d2c 0d0a 2020  et["train"],..  
-00001be0: 2020 2020 2020 6576 616c 5f64 6174 6173        eval_datas
-00001bf0: 6574 3d64 6174 6173 6574 5b22 7661 6c69  et=dataset["vali
-00001c00: 6461 7469 6f6e 225d 2c0d 0a20 2020 2029  dation"],..    )
-00001c10: 0d0a 2020 2020 7472 6169 6e65 722e 7472  ..    trainer.tr
-00001c20: 6169 6e28 290d 0a20 2020 2074 7261 696e  ain()..    train
-00001c30: 6572 2e73 6176 655f 6d6f 6465 6c28 226d  er.save_model("m
-00001c40: 6f64 656c 732f 7370 616e 5f6d 6172 6b65  odels/span_marke
-00001c50: 725f 6265 7274 5f62 6173 655f 6361 7365  r_bert_base_case
-00001c60: 645f 6665 776e 6572 645f 6669 6e65 5f73  d_fewnerd_fine_s
-00001c70: 7570 6572 2f63 6865 636b 706f 696e 742d  uper/checkpoint-
-00001c80: 6669 6e61 6c22 290d 0a0d 0a20 2020 2023  final")....    #
-00001c90: 2043 6f6d 7075 7465 2026 2073 6176 6520   Compute & save 
-00001ca0: 7468 6520 6d65 7472 6963 7320 6f6e 2074  the metrics on t
-00001cb0: 6865 2074 6573 7420 7365 740d 0a20 2020  he test set..   
-00001cc0: 206d 6574 7269 6373 203d 2074 7261 696e   metrics = train
-00001cd0: 6572 2e65 7661 6c75 6174 6528 6461 7461  er.evaluate(data
-00001ce0: 7365 745b 2274 6573 7422 5d2c 206d 6574  set["test"], met
-00001cf0: 7269 635f 6b65 795f 7072 6566 6978 3d22  ric_key_prefix="
-00001d00: 7465 7374 2229 0d0a 2020 2020 7472 6169  test")..    trai
-00001d10: 6e65 722e 7361 7665 5f6d 6574 7269 6373  ner.save_metrics
-00001d20: 2822 7465 7374 222c 206d 6574 7269 6373  ("test", metrics
-00001d30: 290d 0a0d 0a0d 0a69 6620 5f5f 6e61 6d65  )......if __name
-00001d40: 5f5f 203d 3d20 225f 5f6d 6169 6e5f 5f22  __ == "__main__"
-00001d50: 3a0d 0a20 2020 206d 6169 6e28 290d 0a60  :..    main()..`
-00001d60: 6060 0d0a 0d0a 2323 2320 496e 6665 7265  ``....### Infere
-00001d70: 6e63 650d 0a60 6060 7079 7468 6f6e 0d0a  nce..```python..
-00001d80: 6672 6f6d 2073 7061 6e5f 6d61 726b 6572  from span_marker
-00001d90: 2069 6d70 6f72 7420 5370 616e 4d61 726b   import SpanMark
-00001da0: 6572 4d6f 6465 6c0d 0a0d 0a23 2044 6f77  erModel....# Dow
-00001db0: 6e6c 6f61 6420 6672 6f6d 2074 6865 20f0  nload from the .
-00001dc0: 9fa4 9720 4875 620d 0a6d 6f64 656c 203d  ... Hub..model =
-00001dd0: 2053 7061 6e4d 6172 6b65 724d 6f64 656c   SpanMarkerModel
-00001de0: 2e66 726f 6d5f 7072 6574 7261 696e 6564  .from_pretrained
-00001df0: 2822 746f 6d61 6172 7365 6e2f 7370 616e  ("tomaarsen/span
-00001e00: 2d6d 6172 6b65 722d 6265 7274 2d62 6173  -marker-bert-bas
-00001e10: 652d 6665 776e 6572 642d 6669 6e65 2d73  e-fewnerd-fine-s
-00001e20: 7570 6572 2229 0d0a 2320 5275 6e20 696e  uper")..# Run in
-00001e30: 6665 7265 6e63 650d 0a65 6e74 6974 6965  ference..entitie
-00001e40: 7320 3d20 6d6f 6465 6c2e 7072 6564 6963  s = model.predic
-00001e50: 7428 2241 6d65 6c69 6120 4561 7268 6172  t("Amelia Earhar
-00001e60: 7420 666c 6577 2068 6572 2073 696e 676c  t flew her singl
-00001e70: 6520 656e 6769 6e65 204c 6f63 6b68 6565  e engine Lockhee
-00001e80: 6420 5665 6761 2035 4220 6163 726f 7373  d Vega 5B across
-00001e90: 2074 6865 2041 746c 616e 7469 6320 746f   the Atlantic to
-00001ea0: 2050 6172 6973 2e22 290d 0a5b 7b27 7370   Paris.")..[{'sp
-00001eb0: 616e 273a 2027 416d 656c 6961 2045 6172  an': 'Amelia Ear
-00001ec0: 6861 7274 272c 2027 6c61 6265 6c27 3a20  hart', 'label': 
-00001ed0: 2770 6572 736f 6e2d 6f74 6865 7227 2c20  'person-other', 
-00001ee0: 2773 636f 7265 273a 2030 2e37 3635 3935  'score': 0.76595
-00001ef0: 3937 3339 3638 3530 3538 362c 2027 6368  97396850586, 'ch
-00001f00: 6172 5f73 7461 7274 5f69 6e64 6578 273a  ar_start_index':
-00001f10: 2030 2c20 2763 6861 725f 656e 645f 696e   0, 'char_end_in
-00001f20: 6465 7827 3a20 3134 7d2c 0d0a 207b 2773  dex': 14},.. {'s
-00001f30: 7061 6e27 3a20 274c 6f63 6b68 6565 6420  pan': 'Lockheed 
-00001f40: 5665 6761 2035 4227 2c20 276c 6162 656c  Vega 5B', 'label
-00001f50: 273a 2027 7072 6f64 7563 742d 6169 7270  ': 'product-airp
-00001f60: 6c61 6e65 272c 2027 7363 6f72 6527 3a20  lane', 'score': 
-00001f70: 302e 3937 3235 3738 3538 3531 3437 3835  0.97257858514785
-00001f80: 3737 2c20 2763 6861 725f 7374 6172 745f  77, 'char_start_
-00001f90: 696e 6465 7827 3a20 3338 2c20 2763 6861  index': 38, 'cha
-00001fa0: 725f 656e 645f 696e 6465 7827 3a20 3534  r_end_index': 54
-00001fb0: 7d2c 0d0a 207b 2773 7061 6e27 3a20 2741  },.. {'span': 'A
-00001fc0: 746c 616e 7469 6327 2c20 276c 6162 656c  tlantic', 'label
-00001fd0: 273a 2027 6c6f 6361 7469 6f6e 2d62 6f64  ': 'location-bod
-00001fe0: 6965 736f 6677 6174 6572 272c 2027 7363  iesofwater', 'sc
-00001ff0: 6f72 6527 3a20 302e 3735 3837 3637 3930  ore': 0.75876790
-00002000: 3238 3531 3130 3437 2c20 2763 6861 725f  28511047, 'char_
-00002010: 7374 6172 745f 696e 6465 7827 3a20 3636  start_index': 66
-00002020: 2c20 2763 6861 725f 656e 645f 696e 6465  , 'char_end_inde
-00002030: 7827 3a20 3734 7d2c 0d0a 207b 2773 7061  x': 74},.. {'spa
-00002040: 6e27 3a20 2750 6172 6973 272c 2027 6c61  n': 'Paris', 'la
-00002050: 6265 6c27 3a20 276c 6f63 6174 696f 6e2d  bel': 'location-
-00002060: 4750 4527 2c20 2773 636f 7265 273a 2030  GPE', 'score': 0
-00002070: 2e39 3839 3233 3930 3936 3634 3135 3430  .989239096641540
-00002080: 352c 2027 6368 6172 5f73 7461 7274 5f69  5, 'char_start_i
-00002090: 6e64 6578 273a 2037 382c 2027 6368 6172  ndex': 78, 'char
-000020a0: 5f65 6e64 5f69 6e64 6578 273a 2038 337d  _end_index': 83}
-000020b0: 5d0d 0a60 6060 0d0a 0d0a 3c21 2d2d 2042  ]..```....<!-- B
-000020c0: 6563 6175 7365 2074 6869 7320 776f 726b  ecause this work
-000020d0: 2069 7320 6261 7365 6420 6f6e 205b 504c   is based on [PL
-000020e0: 2d4d 6172 6b65 725d 2868 7474 7073 3a2f  -Marker](https:/
-000020f0: 2f61 7278 6976 2e6f 7267 2f70 6466 2f32  /arxiv.org/pdf/2
-00002100: 3130 392e 3036 3036 3776 352e 7064 6629  109.06067v5.pdf)
-00002110: 2c20 796f 7520 6d61 7920 6578 7065 6374  , you may expect
-00002120: 2073 696d 696c 6172 2072 6573 756c 7473   similar results
-00002130: 2074 6f20 6974 7320 5b50 6170 6572 7320   to its [Papers 
-00002140: 7769 7468 2043 6f64 6520 4c65 6164 6572  with Code Leader
-00002150: 626f 6172 645d 2868 7474 7073 3a2f 2f70  board](https://p
-00002160: 6170 6572 7377 6974 6863 6f64 652e 636f  aperswithcode.co
-00002170: 6d2f 7061 7065 722f 7061 636b 2d74 6f67  m/paper/pack-tog
-00002180: 6574 6865 722d 656e 7469 7479 2d61 6e64  ether-entity-and
-00002190: 2d72 656c 6174 696f 6e2d 6578 7472 6163  -relation-extrac
-000021a0: 7469 6f6e 2920 7265 7375 6c74 732e 202d  tion) results. -
-000021b0: 2d3e 0d0a 0d0a 2323 2050 7265 7472 6169  ->....## Pretrai
-000021c0: 6e65 6420 4d6f 6465 6c73 0d0a 0d0a 416c  ned Models....Al
-000021d0: 6c20 6d6f 6465 6c73 2069 6e20 7468 6973  l models in this
-000021e0: 206c 6973 7420 636f 6e74 6169 6e20 6074   list contain `t
-000021f0: 7261 696e 2e70 7960 2066 696c 6573 2074  rain.py` files t
-00002200: 6861 7420 7368 6f77 2074 6865 2074 7261  hat show the tra
-00002210: 696e 696e 6720 7363 7269 7074 7320 7573  ining scripts us
-00002220: 6564 2074 6f20 6765 6e65 7261 7465 2074  ed to generate t
-00002230: 6865 6d2e 2041 6464 6974 696f 6e61 6c6c  hem. Additionall
-00002240: 792c 2061 6c6c 2074 7261 696e 696e 6720  y, all training 
-00002250: 7363 7269 7074 7320 7573 6564 2061 7265  scripts used are
-00002260: 2073 746f 7265 6420 696e 2074 6865 205b   stored in the [
-00002270: 7472 6169 6e69 6e67 5f73 6372 6970 7473  training_scripts
-00002280: 5d28 7472 6169 6e69 6e67 5f73 6372 6970  ](training_scrip
-00002290: 7473 2920 6469 7265 6374 6f72 792e 0d0a  ts) directory...
-000022a0: 5468 6573 6520 7472 6169 6e65 6420 6d6f  These trained mo
-000022b0: 6465 6c73 2068 6176 6520 486f 7374 6564  dels have Hosted
-000022c0: 2049 6e66 6572 656e 6365 2041 5049 2077   Inference API w
-000022d0: 6964 6765 7473 2074 6861 7420 796f 7520  idgets that you 
-000022e0: 6361 6e20 7573 6520 746f 2065 7870 6572  can use to exper
-000022f0: 696d 656e 7420 7769 7468 2074 6865 206d  iment with the m
-00002300: 6f64 656c 7320 6f6e 2074 6865 6972 2048  odels on their H
-00002310: 7567 6769 6e67 2046 6163 6520 6d6f 6465  ugging Face mode
-00002320: 6c20 7061 6765 732e 2041 6464 6974 696f  l pages. Additio
-00002330: 6e61 6c6c 792c 2048 7567 6769 6e67 2046  nally, Hugging F
-00002340: 6163 6520 7072 6f76 6964 6573 2065 6163  ace provides eac
-00002350: 6820 6d6f 6465 6c20 7769 7468 2061 2066  h model with a f
-00002360: 7265 6520 4150 4920 2860 4465 706c 6f79  ree API (`Deploy
-00002370: 6020 3e20 6049 6e66 6572 656e 6365 2041  ` > `Inference A
-00002380: 5049 6020 6f6e 2074 6865 206d 6f64 656c  PI` on the model
-00002390: 2070 6167 6529 2e0d 0a0d 0a23 2323 2046   page).....### F
-000023a0: 6577 4e45 5244 0d0a 2a20 5b60 746f 6d61  ewNERD..* [`toma
-000023b0: 6172 7365 6e2f 7370 616e 2d6d 6172 6b65  arsen/span-marke
-000023c0: 722d 6265 7274 2d62 6173 652d 6665 776e  r-bert-base-fewn
-000023d0: 6572 642d 6669 6e65 2d73 7570 6572 605d  erd-fine-super`]
-000023e0: 2868 7474 7073 3a2f 2f68 7567 6769 6e67  (https://hugging
-000023f0: 6661 6365 2e63 6f2f 746f 6d61 6172 7365  face.co/tomaarse
-00002400: 6e2f 7370 616e 2d6d 6172 6b65 722d 6265  n/span-marker-be
-00002410: 7274 2d62 6173 652d 6665 776e 6572 642d  rt-base-fewnerd-
-00002420: 6669 6e65 2d73 7570 6572 2920 6973 2061  fine-super) is a
-00002430: 206d 6f64 656c 2074 6861 7420 4920 6861   model that I ha
-00002440: 7665 2074 7261 696e 6564 2069 6e20 3220  ve trained in 2 
-00002450: 686f 7572 7320 6f6e 2074 6865 2066 696e  hours on the fin
-00002460: 6567 7261 696e 6564 2c20 7375 7065 7276  egrained, superv
-00002470: 6973 6564 205b 4665 772d 4e45 5244 2064  ised [Few-NERD d
-00002480: 6174 6173 6574 5d28 6874 7470 733a 2f2f  ataset](https://
-00002490: 6875 6767 696e 6766 6163 652e 636f 2f64  huggingface.co/d
-000024a0: 6174 6173 6574 732f 4446 4b49 2d53 4c54  atasets/DFKI-SLT
-000024b0: 2f66 6577 2d6e 6572 6429 2e20 4974 2072  /few-nerd). It r
-000024c0: 6561 6368 6564 2061 2030 2e37 3035 3320  eached a 0.7053 
-000024d0: 5465 7374 2046 312c 2063 6f6d 7065 7469  Test F1, competi
-000024e0: 7469 7665 2069 6e20 7468 6520 616c 6c2d  tive in the all-
-000024f0: 7469 6d65 205b 4665 772d 4e45 5244 206c  time [Few-NERD l
-00002500: 6561 6465 7262 6f61 7264 5d28 6874 7470  eaderboard](http
-00002510: 733a 2f2f 7061 7065 7273 7769 7468 636f  s://paperswithco
-00002520: 6465 2e63 6f6d 2f73 6f74 612f 6e61 6d65  de.com/sota/name
-00002530: 642d 656e 7469 7479 2d72 6563 6f67 6e69  d-entity-recogni
-00002540: 7469 6f6e 2d6f 6e2d 6665 772d 6e65 7264  tion-on-few-nerd
-00002550: 2d73 7570 2920 7573 696e 6720 6062 6572  -sup) using `ber
-00002560: 742d 6261 7365 602e 204d 7920 7472 6169  t-base`. My trai
-00002570: 6e69 6e67 2073 6372 6970 7420 7265 7365  ning script rese
-00002580: 6d62 6c65 7320 7468 6520 6f6e 6520 7468  mbles the one th
-00002590: 6174 2079 6f75 2063 616e 2073 6565 2061  at you can see a
-000025a0: 626f 7665 2e0d 0a20 202a 2054 7279 2074  bove...  * Try t
-000025b0: 6865 206d 6f64 656c 206f 7574 206f 6e6c  he model out onl
-000025c0: 696e 6520 7573 696e 6720 7468 6973 205b  ine using this [
-000025d0: f09f a497 2053 7061 6365 5d28 6874 7470  .... Space](http
-000025e0: 733a 2f2f 746f 6d61 6172 7365 6e2d 7370  s://tomaarsen-sp
-000025f0: 616e 2d6d 6172 6b65 722d 6265 7274 2d62  an-marker-bert-b
-00002600: 6173 652d 6665 776e 6572 642d 6669 6e65  ase-fewnerd-fine
-00002610: 2d73 7570 6572 2e68 662e 7370 6163 652f  -super.hf.space/
-00002620: 292e 0d0a 0d0a 2a20 5b60 746f 6d61 6172  ).....* [`tomaar
-00002630: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
-00002640: 726f 6265 7274 612d 6c61 7267 652d 6665  roberta-large-fe
-00002650: 776e 6572 642d 6669 6e65 2d73 7570 6572  wnerd-fine-super
-00002660: 605d 2868 7474 7073 3a2f 2f68 7567 6769  `](https://huggi
-00002670: 6e67 6661 6365 2e63 6f2f 746f 6d61 6172  ngface.co/tomaar
-00002680: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
-00002690: 726f 6265 7274 612d 6c61 7267 652d 6665  roberta-large-fe
-000026a0: 776e 6572 642d 6669 6e65 2d73 7570 6572  wnerd-fine-super
-000026b0: 2920 7761 7320 7472 6169 6e65 6420 696e  ) was trained in
-000026c0: 2036 2068 6f75 7273 206f 6e20 7468 6520   6 hours on the 
-000026d0: 6669 6e65 6772 6169 6e65 642c 2073 7570  finegrained, sup
-000026e0: 6572 7669 7365 6420 5b46 6577 2d4e 4552  ervised [Few-NER
-000026f0: 4420 6461 7461 7365 745d 2868 7474 7073  D dataset](https
-00002700: 3a2f 2f68 7567 6769 6e67 6661 6365 2e63  ://huggingface.c
-00002710: 6f2f 6461 7461 7365 7473 2f44 464b 492d  o/datasets/DFKI-
-00002720: 534c 542f 6665 772d 6e65 7264 2920 7573  SLT/few-nerd) us
-00002730: 696e 6720 6072 6f62 6572 7461 2d6c 6172  ing `roberta-lar
-00002740: 6765 602e 2049 7420 7265 6163 6865 6420  ge`. It reached 
-00002750: 6120 302e 3731 3033 2054 6573 7420 4631  a 0.7103 Test F1
-00002760: 2c20 7265 6163 6869 6e67 2061 206e 6577  , reaching a new
-00002770: 2073 7461 7465 206f 6620 7468 6520 6172   state of the ar
-00002780: 7420 696e 2074 6865 2061 6c6c 2d74 696d  t in the all-tim
-00002790: 6520 5b46 6577 2d4e 4552 4420 6c65 6164  e [Few-NERD lead
-000027a0: 6572 626f 6172 645d 2868 7474 7073 3a2f  erboard](https:/
-000027b0: 2f70 6170 6572 7377 6974 6863 6f64 652e  /paperswithcode.
-000027c0: 636f 6d2f 736f 7461 2f6e 616d 6564 2d65  com/sota/named-e
-000027d0: 6e74 6974 792d 7265 636f 676e 6974 696f  ntity-recognitio
-000027e0: 6e2d 6f6e 2d66 6577 2d6e 6572 642d 7375  n-on-few-nerd-su
-000027f0: 7029 2e0d 0a2a 205b 6074 6f6d 6161 7273  p)...* [`tomaars
-00002800: 656e 2f73 7061 6e2d 6d61 726b 6572 2d78  en/span-marker-x
-00002810: 6c6d 2d72 6f62 6572 7461 2d62 6173 652d  lm-roberta-base-
-00002820: 6665 776e 6572 642d 6669 6e65 2d73 7570  fewnerd-fine-sup
-00002830: 6572 605d 2868 7474 7073 3a2f 2f68 7567  er`](https://hug
-00002840: 6769 6e67 6661 6365 2e63 6f2f 746f 6d61  gingface.co/toma
-00002850: 6172 7365 6e2f 7370 616e 2d6d 6172 6b65  arsen/span-marke
-00002860: 722d 786c 6d2d 726f 6265 7274 612d 6261  r-xlm-roberta-ba
-00002870: 7365 2d66 6577 6e65 7264 2d66 696e 652d  se-fewnerd-fine-
-00002880: 7375 7065 7229 2069 7320 6120 6d75 6c74  super) is a mult
-00002890: 696c 696e 6775 616c 206d 6f64 656c 2074  ilingual model t
-000028a0: 6861 7420 4920 6861 7665 2074 7261 696e  hat I have train
-000028b0: 6564 2069 6e20 312e 3520 686f 7572 7320  ed in 1.5 hours 
-000028c0: 6f6e 2074 6865 2066 696e 6567 7261 696e  on the finegrain
-000028d0: 6564 2c20 7375 7065 7276 6973 6564 205b  ed, supervised [
-000028e0: 4665 772d 4e45 5244 2064 6174 6173 6574  Few-NERD dataset
-000028f0: 5d28 6874 7470 733a 2f2f 6875 6767 696e  ](https://huggin
-00002900: 6766 6163 652e 636f 2f64 6174 6173 6574  gface.co/dataset
-00002910: 732f 4446 4b49 2d53 4c54 2f66 6577 2d6e  s/DFKI-SLT/few-n
-00002920: 6572 6429 2e20 4974 2072 6561 6368 6564  erd). It reached
-00002930: 2061 2030 2e36 3836 2054 6573 7420 4631   a 0.686 Test F1
-00002940: 206f 6e20 456e 676c 6973 682c 2061 6e64   on English, and
-00002950: 2077 6f72 6b73 2077 656c 6c20 6f6e 206f   works well on o
-00002960: 7468 6572 206c 616e 6775 6167 6573 206c  ther languages l
-00002970: 696b 6520 5370 616e 6973 682c 2046 7265  ike Spanish, Fre
-00002980: 6e63 682c 2047 6572 6d61 6e2c 2052 7573  nch, German, Rus
-00002990: 7369 616e 2c20 4475 7463 682c 2050 6f6c  sian, Dutch, Pol
-000029a0: 6973 682c 2049 6365 6c61 6e64 6963 2c20  ish, Icelandic, 
-000029b0: 4772 6565 6b20 616e 6420 6d61 6e79 206d  Greek and many m
-000029c0: 6f72 652e 0d0a 0d0a 2323 2320 4f6e 746f  ore.....### Onto
-000029d0: 4e6f 7465 7320 7635 2e30 0d0a 2a20 5b60  Notes v5.0..* [`
-000029e0: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
-000029f0: 6172 6b65 722d 726f 6265 7274 612d 6c61  arker-roberta-la
-00002a00: 7267 652d 6f6e 746f 6e6f 7465 7335 605d  rge-ontonotes5`]
-00002a10: 2868 7474 7073 3a2f 2f68 7567 6769 6e67  (https://hugging
-00002a20: 6661 6365 2e63 6f2f 746f 6d61 6172 7365  face.co/tomaarse
-00002a30: 6e2f 7370 616e 2d6d 6172 6b65 722d 726f  n/span-marker-ro
-00002a40: 6265 7274 612d 6c61 7267 652d 6f6e 746f  berta-large-onto
-00002a50: 6e6f 7465 7335 2920 7761 7320 7472 6169  notes5) was trai
-00002a60: 6e65 6420 696e 2033 2068 6f75 7273 206f  ned in 3 hours o
-00002a70: 6e20 7468 6520 4f6e 746f 4e6f 7465 7320  n the OntoNotes 
-00002a80: 7635 2e30 2064 6174 6173 6574 2c20 7265  v5.0 dataset, re
-00002a90: 6163 6869 6e67 2061 2070 6572 666f 726d  aching a perform
-00002aa0: 616e 6365 206f 6620 302e 3931 3534 2046  ance of 0.9154 F
-00002ab0: 312e 2046 6f72 2072 6566 6572 656e 6365  1. For reference
-00002ac0: 2c20 7468 6520 6375 7272 656e 7420 7374  , the current st
-00002ad0: 726f 6e67 6573 7420 7370 6143 7920 6d6f  rongest spaCy mo
-00002ae0: 6465 6c20 2860 656e 5f63 6f72 655f 7765  del (`en_core_we
-00002af0: 625f 7472 6660 2920 7265 6163 6865 7320  b_trf`) reaches 
-00002b00: 302e 3839 382e 2054 6869 7320 5370 616e  0.898. This Span
-00002b10: 4d61 726b 6572 206d 6f64 656c 2075 7365  Marker model use
-00002b20: 7320 6120 6072 6f62 6572 7461 2d6c 6172  s a `roberta-lar
-00002b30: 6765 6020 656e 636f 6465 7220 756e 6465  ge` encoder unde
-00002b40: 7220 7468 6520 686f 6f64 2e0d 0a0d 0a23  r the hood.....#
-00002b50: 2323 2043 6f4e 4c4c 3033 0d0a 2a20 5b60  ## CoNLL03..* [`
-00002b60: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
-00002b70: 6172 6b65 722d 786c 6d2d 726f 6265 7274  arker-xlm-robert
-00002b80: 612d 6c61 7267 652d 636f 6e6c 6c30 3360  a-large-conll03`
-00002b90: 5d28 6874 7470 733a 2f2f 6875 6767 696e  ](https://huggin
-00002ba0: 6766 6163 652e 636f 2f74 6f6d 6161 7273  gface.co/tomaars
-00002bb0: 656e 2f73 7061 6e2d 6d61 726b 6572 2d78  en/span-marker-x
-00002bc0: 6c6d 2d72 6f62 6572 7461 2d6c 6172 6765  lm-roberta-large
-00002bd0: 2d63 6f6e 6c6c 3033 2920 6973 2061 2053  -conll03) is a S
-00002be0: 7061 6e4d 6172 6b65 7220 6d6f 6465 6c20  panMarker model 
-00002bf0: 7573 696e 6720 6078 6c6d 2d72 6f62 6572  using `xlm-rober
-00002c00: 7461 2d6c 6172 6765 6020 7468 6174 2077  ta-large` that w
-00002c10: 6173 2074 7261 696e 6564 2069 6e20 3435  as trained in 45
-00002c20: 206d 696e 7574 6573 2e20 4974 2072 6561   minutes. It rea
-00002c30: 6368 6573 2061 2073 7461 7465 206f 6620  ches a state of 
-00002c40: 7468 6520 6172 7420 302e 3933 3120 4631  the art 0.931 F1
-00002c50: 206f 6e20 436f 4e4c 4c30 3320 7769 7468   on CoNLL03 with
-00002c60: 6f75 7420 7573 696e 6720 646f 6375 6d65  out using docume
-00002c70: 6e74 2d6c 6576 656c 2063 6f6e 7465 7874  nt-level context
-00002c80: 2e20 466f 7220 7265 6665 7265 6e63 652c  . For reference,
-00002c90: 2074 6865 2063 7572 7265 6e74 2073 7472   the current str
-00002ca0: 6f6e 6765 7374 2073 7061 4379 206d 6f64  ongest spaCy mod
-00002cb0: 656c 2028 6065 6e5f 636f 7265 5f77 6562  el (`en_core_web
-00002cc0: 5f74 7266 6029 2072 6561 6368 6573 2039  _trf`) reaches 9
-00002cd0: 312e 362e 0d0a 2a20 5b60 746f 6d61 6172  1.6...* [`tomaar
-00002ce0: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
-00002cf0: 786c 6d2d 726f 6265 7274 612d 6c61 7267  xlm-roberta-larg
-00002d00: 652d 636f 6e6c 6c30 332d 646f 632d 636f  e-conll03-doc-co
-00002d10: 6e74 6578 7460 5d28 6874 7470 733a 2f2f  ntext`](https://
-00002d20: 6875 6767 696e 6766 6163 652e 636f 2f74  huggingface.co/t
-00002d30: 6f6d 6161 7273 656e 2f73 7061 6e2d 6d61  omaarsen/span-ma
-00002d40: 726b 6572 2d78 6c6d 2d72 6f62 6572 7461  rker-xlm-roberta
-00002d50: 2d6c 6172 6765 2d63 6f6e 6c6c 3033 2d64  -large-conll03-d
-00002d60: 6f63 2d63 6f6e 7465 7874 2920 6973 2061  oc-context) is a
-00002d70: 6e6f 7468 6572 2053 7061 6e4d 6172 6b65  nother SpanMarke
-00002d80: 7220 6d6f 6465 6c20 7573 696e 6720 7468  r model using th
-00002d90: 6520 6078 6c6d 2d72 6f62 6572 7461 2d6c  e `xlm-roberta-l
-00002da0: 6172 6765 6020 656e 636f 6465 722e 2049  arge` encoder. I
-00002db0: 7420 7573 6573 205b 646f 6375 6d65 6e74  t uses [document
-00002dc0: 2d6c 6576 656c 2063 6f6e 7465 7874 5d28  -level context](
-00002dd0: 6874 7470 733a 2f2f 746f 6d61 6172 7365  https://tomaarse
-00002de0: 6e2e 6769 7468 7562 2e69 6f2f 5370 616e  n.github.io/Span
-00002df0: 4d61 726b 6572 4e45 522f 6e6f 7465 626f  MarkerNER/notebo
-00002e00: 6f6b 732f 646f 6375 6d65 6e74 5f6c 6576  oks/document_lev
-00002e10: 656c 5f63 6f6e 7465 7874 2e68 746d 6c29  el_context.html)
-00002e20: 2074 6f20 7265 6163 6820 6120 7374 6174   to reach a stat
-00002e30: 6520 6f66 2074 6865 2061 7274 2030 2e39  e of the art 0.9
-00002e40: 3434 2046 312e 2046 6f72 2074 6865 2062  44 F1. For the b
-00002e50: 6573 7420 7065 7266 6f72 6d61 6e63 652c  est performance,
-00002e60: 2069 6e66 6572 656e 6365 2073 686f 756c   inference shoul
-00002e70: 6420 6265 2070 6572 666f 726d 6564 2075  d be performed u
-00002e80: 7369 6e67 2064 6f63 756d 656e 742d 6c65  sing document-le
-00002e90: 7665 6c20 636f 6e74 6578 7420 285b 646f  vel context ([do
-00002ea0: 6373 5d28 6874 7470 733a 2f2f 746f 6d61  cs](https://toma
-00002eb0: 6172 7365 6e2e 6769 7468 7562 2e69 6f2f  arsen.github.io/
-00002ec0: 5370 616e 4d61 726b 6572 4e45 522f 6e6f  SpanMarkerNER/no
-00002ed0: 7465 626f 6f6b 732f 646f 6375 6d65 6e74  tebooks/document
-00002ee0: 5f6c 6576 656c 5f63 6f6e 7465 7874 2e68  _level_context.h
-00002ef0: 746d 6c23 496e 6665 7265 6e63 6529 292e  tml#Inference)).
-00002f00: 2054 6869 7320 6d6f 6465 6c20 7761 7320   This model was 
-00002f10: 7472 6169 6e65 6420 696e 2031 2068 6f75  trained in 1 hou
-00002f20: 722e 0d0a 0d0a 2323 2320 436f 4e4c 4c2b  r.....### CoNLL+
-00002f30: 2b0d 0a2a 205b 6074 6f6d 6161 7273 656e  +..* [`tomaarsen
-00002f40: 2f73 7061 6e2d 6d61 726b 6572 2d78 6c6d  /span-marker-xlm
-00002f50: 2d72 6f62 6572 7461 2d6c 6172 6765 2d63  -roberta-large-c
-00002f60: 6f6e 6c6c 7070 2d64 6f63 2d63 6f6e 7465  onllpp-doc-conte
-00002f70: 7874 605d 2868 7474 7073 3a2f 2f68 7567  xt`](https://hug
-00002f80: 6769 6e67 6661 6365 2e63 6f2f 746f 6d61  gingface.co/toma
-00002f90: 6172 7365 6e2f 7370 616e 2d6d 6172 6b65  arsen/span-marke
-00002fa0: 722d 786c 6d2d 726f 6265 7274 612d 6c61  r-xlm-roberta-la
-00002fb0: 7267 652d 636f 6e6c 6c70 702d 646f 632d  rge-conllpp-doc-
-00002fc0: 636f 6e74 6578 7429 2077 6173 2074 7261  context) was tra
-00002fd0: 696e 6564 2069 6e20 616e 2068 6f75 7220  ined in an hour 
-00002fe0: 7573 696e 6720 7468 6520 6078 6c6d 2d72  using the `xlm-r
-00002ff0: 6f62 6572 7461 2d6c 6172 6765 6020 656e  oberta-large` en
-00003000: 636f 6465 7220 6f6e 2074 6865 2043 6f4e  coder on the CoN
-00003010: 4c4c 2b2b 2064 6174 6173 6574 2e20 5573  LL++ dataset. Us
-00003020: 696e 6720 5b64 6f63 756d 656e 742d 6c65  ing [document-le
-00003030: 7665 6c20 636f 6e74 6578 745d 2868 7474  vel context](htt
-00003040: 7073 3a2f 2f74 6f6d 6161 7273 656e 2e67  ps://tomaarsen.g
-00003050: 6974 6875 622e 696f 2f53 7061 6e4d 6172  ithub.io/SpanMar
-00003060: 6b65 724e 4552 2f6e 6f74 6562 6f6f 6b73  kerNER/notebooks
-00003070: 2f64 6f63 756d 656e 745f 6c65 7665 6c5f  /document_level_
-00003080: 636f 6e74 6578 742e 6874 6d6c 292c 2069  context.html), i
-00003090: 7420 7265 6163 6865 7320 6120 7665 7279  t reaches a very
-000030a0: 2063 6f6d 7065 7469 7469 7665 2030 2e39   competitive 0.9
-000030b0: 3535 2046 312e 2046 6f72 2074 6865 2062  55 F1. For the b
-000030c0: 6573 7420 7065 7266 6f72 6d61 6e63 652c  est performance,
-000030d0: 2069 6e66 6572 656e 6365 2073 686f 756c   inference shoul
-000030e0: 6420 6265 2070 6572 666f 726d 6564 2075  d be performed u
-000030f0: 7369 6e67 2064 6f63 756d 656e 742d 6c65  sing document-le
-00003100: 7665 6c20 636f 6e74 6578 7420 285b 646f  vel context ([do
-00003110: 6373 5d28 6874 7470 733a 2f2f 746f 6d61  cs](https://toma
-00003120: 6172 7365 6e2e 6769 7468 7562 2e69 6f2f  arsen.github.io/
-00003130: 5370 616e 4d61 726b 6572 4e45 522f 6e6f  SpanMarkerNER/no
-00003140: 7465 626f 6f6b 732f 646f 6375 6d65 6e74  tebooks/document
-00003150: 5f6c 6576 656c 5f63 6f6e 7465 7874 2e68  _level_context.h
-00003160: 746d 6c23 496e 6665 7265 6e63 6529 292e  tml#Inference)).
-00003170: 0d0a 0d0a 2323 2055 7369 6e67 2070 7265  ....## Using pre
-00003180: 7472 6169 6e65 6420 5370 616e 4d61 726b  trained SpanMark
-00003190: 6572 206d 6f64 656c 7320 7769 7468 2073  er models with s
-000031a0: 7061 4379 0d0a 416c 6c20 5b53 7061 6e4d  paCy..All [SpanM
-000031b0: 6172 6b65 7220 6d6f 6465 6c73 206f 6e20  arker models on 
-000031c0: 7468 6520 4875 6767 696e 6720 4661 6365  the Hugging Face
-000031d0: 2048 7562 5d28 6874 7470 733a 2f2f 6875   Hub](https://hu
-000031e0: 6767 696e 6766 6163 652e 636f 2f6d 6f64  ggingface.co/mod
-000031f0: 656c 733f 6c69 6272 6172 793d 7370 616e  els?library=span
-00003200: 2d6d 6172 6b65 7229 2063 616e 2061 6c73  -marker) can als
-00003210: 6f20 6265 2065 6173 696c 7920 7573 6564  o be easily used
-00003220: 2069 6e20 7370 6143 792e 2049 7427 7320   in spaCy. It's 
-00003230: 6173 2073 696d 706c 6520 6173 2069 6e63  as simple as inc
-00003240: 6c75 6469 6e67 2031 206c 696e 6520 746f  luding 1 line to
-00003250: 2061 6464 2074 6865 2060 7370 616e 5f6d   add the `span_m
-00003260: 6172 6b65 7260 2070 6970 656c 696e 652e  arker` pipeline.
-00003270: 2053 6565 2074 6865 2044 6f63 756d 656e   See the Documen
-00003280: 7461 7469 6f6e 206f 7220 4150 4920 5265  tation or API Re
-00003290: 6665 7265 6e63 6520 666f 7220 6d6f 7265  ference for more
-000032a0: 2069 6e66 6f72 6d61 7469 6f6e 2e0d 0a60   information...`
-000032b0: 6060 7079 7468 6f6e 0d0a 696d 706f 7274  ``python..import
-000032c0: 2073 7061 6379 0d0a 0d0a 2320 4c6f 6164   spacy....# Load
-000032d0: 2074 6865 2073 7061 4379 206d 6f64 656c   the spaCy model
-000032e0: 2077 6974 6820 7468 6520 7370 616e 5f6d   with the span_m
-000032f0: 6172 6b65 7220 7069 7065 6c69 6e65 2063  arker pipeline c
-00003300: 6f6d 706f 6e65 6e74 0d0a 6e6c 7020 3d20  omponent..nlp = 
-00003310: 7370 6163 792e 6c6f 6164 2822 656e 5f63  spacy.load("en_c
-00003320: 6f72 655f 7765 625f 736d 222c 2064 6973  ore_web_sm", dis
-00003330: 6162 6c65 3d5b 226e 6572 225d 290d 0a6e  able=["ner"])..n
-00003340: 6c70 2e61 6464 5f70 6970 6528 2273 7061  lp.add_pipe("spa
-00003350: 6e5f 6d61 726b 6572 222c 2063 6f6e 6669  n_marker", confi
-00003360: 673d 7b22 6d6f 6465 6c22 3a20 2274 6f6d  g={"model": "tom
-00003370: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
-00003380: 6572 2d72 6f62 6572 7461 2d6c 6172 6765  er-roberta-large
-00003390: 2d6f 6e74 6f6e 6f74 6573 3522 7d29 0d0a  -ontonotes5"})..
-000033a0: 0d0a 2320 4665 6564 2073 6f6d 6520 7465  ..# Feed some te
-000033b0: 7874 2074 6872 6f75 6768 2074 6865 206d  xt through the m
-000033c0: 6f64 656c 2074 6f20 6765 7420 6120 7370  odel to get a sp
-000033d0: 6163 7920 446f 630d 0a74 6578 7420 3d20  acy Doc..text = 
-000033e0: 2222 2243 6c65 6f70 6174 7261 2056 4949  """Cleopatra VII
-000033f0: 2c20 616c 736f 206b 6e6f 776e 2061 7320  , also known as 
-00003400: 436c 656f 7061 7472 6120 7468 6520 4772  Cleopatra the Gr
-00003410: 6561 742c 2077 6173 2074 6865 206c 6173  eat, was the las
-00003420: 7420 6163 7469 7665 2072 756c 6572 206f  t active ruler o
-00003430: 6620 7468 6520 5c0d 0a50 746f 6c65 6d61  f the \..Ptolema
-00003440: 6963 204b 696e 6764 6f6d 206f 6620 4567  ic Kingdom of Eg
-00003450: 7970 742e 2053 6865 2077 6173 2062 6f72  ypt. She was bor
-00003460: 6e20 696e 2036 3920 4243 4520 616e 6420  n in 69 BCE and 
-00003470: 7275 6c65 6420 4567 7970 7420 6672 6f6d  ruled Egypt from
-00003480: 2035 3120 4243 4520 756e 7469 6c20 6865   51 BCE until he
-00003490: 7220 5c0d 0a64 6561 7468 2069 6e20 3330  r \..death in 30
-000034a0: 2042 4345 2e22 2222 0d0a 646f 6320 3d20   BCE."""..doc = 
-000034b0: 6e6c 7028 7465 7874 290d 0a0d 0a23 2041  nlp(text)....# A
-000034c0: 6e64 206c 6f6f 6b20 6174 2074 6865 2065  nd look at the e
-000034d0: 6e74 6974 6965 730d 0a70 7269 6e74 285b  ntities..print([
-000034e0: 2865 6e74 6974 792c 2065 6e74 6974 792e  (entity, entity.
-000034f0: 6c61 6265 6c5f 2920 666f 7220 656e 7469  label_) for enti
-00003500: 7479 2069 6e20 646f 632e 656e 7473 5d29  ty in doc.ents])
-00003510: 0d0a 2222 220d 0a5b 2843 6c65 6f70 6174  .."""..[(Cleopat
-00003520: 7261 2056 4949 2c20 2250 4552 534f 4e22  ra VII, "PERSON"
-00003530: 292c 2028 436c 656f 7061 7472 6120 7468  ), (Cleopatra th
-00003540: 6520 4772 6561 742c 2022 5045 5253 4f4e  e Great, "PERSON
-00003550: 2229 2c20 2874 6865 2050 746f 6c65 6d61  "), (the Ptolema
-00003560: 6963 204b 696e 6764 6f6d 206f 6620 4567  ic Kingdom of Eg
-00003570: 7970 742c 2022 4750 4522 292c 0d0a 2836  ypt, "GPE"),..(6
-00003580: 3920 4243 452c 2022 4441 5445 2229 2c20  9 BCE, "DATE"), 
-00003590: 2845 6779 7074 2c20 2247 5045 2229 2c20  (Egypt, "GPE"), 
-000035a0: 2835 3120 4243 452c 2022 4441 5445 2229  (51 BCE, "DATE")
-000035b0: 2c20 2833 3020 4243 452c 2022 4441 5445  , (30 BCE, "DATE
-000035c0: 2229 5d0d 0a22 2222 0d0a 6060 600d 0a21  ")].."""..```..!
-000035d0: 5b69 6d61 6765 5d28 6874 7470 733a 2f2f  [image](https://
-000035e0: 7573 6572 2d69 6d61 6765 732e 6769 7468  user-images.gith
-000035f0: 7562 7573 6572 636f 6e74 656e 742e 636f  ubusercontent.co
-00003600: 6d2f 3337 3632 3134 3931 2f32 3436 3137  m/37621491/24617
-00003610: 3036 3233 2d36 3335 3163 6237 652d 6262  0623-6351cb7e-bb
-00003620: 6230 2d34 3437 322d 6166 3136 2d39 6133  b0-4472-af16-9a3
-00003630: 3531 6132 3533 6461 392e 706e 6729 0d0a  51a253da9.png)..
-00003640: 0d0a 2323 2043 6f6e 7465 7874 0d0a 3c68  ..## Context..<h
-00003650: 3120 616c 6967 6e3d 2263 656e 7465 7222  1 align="center"
-00003660: 3e0d 0a20 2020 203c 6120 6872 6566 3d22  >..    <a href="
-00003670: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
-00003680: 6f6d 2f61 7267 696c 6c61 2d69 6f2f 6172  om/argilla-io/ar
-00003690: 6769 6c6c 6122 3e0d 0a20 2020 203c 696d  gilla">..    <im
-000036a0: 6720 7372 633d 2268 7474 7073 3a2f 2f67  g src="https://g
-000036b0: 6974 6875 622e 636f 6d2f 6476 7372 6570  ithub.com/dvsrep
-000036c0: 6f2f 696d 6773 2f72 6177 2f6d 6169 6e2f  o/imgs/raw/main/
-000036d0: 7267 2e73 7667 2220 616c 743d 2241 7267  rg.svg" alt="Arg
-000036e0: 696c 6c61 2220 7769 6474 683d 2231 3530  illa" width="150
-000036f0: 223e 0d0a 2020 2020 3c2f 613e 0d0a 3c2f  ">..    </a>..</
-00003700: 6831 3e0d 0a0d 0a49 2068 6176 6520 6465  h1>....I have de
-00003710: 7665 6c6f 7065 6420 7468 6973 206c 6962  veloped this lib
-00003720: 7261 7279 2061 7320 6120 7061 7274 206f  rary as a part o
-00003730: 6620 6d79 2074 6865 7369 7320 776f 726b  f my thesis work
-00003740: 2061 7420 5b41 7267 696c 6c61 5d28 6874   at [Argilla](ht
-00003750: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
-00003760: 2f61 7267 696c 6c61 2d69 6f2f 6172 6769  /argilla-io/argi
-00003770: 6c6c 6129 2e0d 0a46 6565 6c20 6672 6565  lla)...Feel free
-00003780: 2074 6f20 e2ad 9020 7374 6172 206f 7220   to ... star or 
-00003790: 7761 7463 6820 7468 6520 5370 616e 4d61  watch the SpanMa
-000037a0: 726b 6572 2072 6570 6f73 6974 6f72 7920  rker repository 
-000037b0: 746f 2067 6574 206e 6f74 6966 6965 6420  to get notified 
-000037c0: 7768 656e 206d 7920 7468 6573 6973 2069  when my thesis i
-000037d0: 7320 7075 626c 6973 6865 642e 0d0a 0d0a  s published.....
-000037e0: 2323 2043 6861 6e67 656c 6f67 0d0a 5365  ## Changelog..Se
-000037f0: 6520 5b43 4841 4e47 454c 4f47 2e6d 645d  e [CHANGELOG.md]
-00003800: 2843 4841 4e47 454c 4f47 2e6d 6429 2066  (CHANGELOG.md) f
-00003810: 6f72 206e 6577 7320 6f6e 2061 6c6c 2053  or news on all S
-00003820: 7061 6e4d 6172 6b65 7220 7665 7273 696f  panMarker versio
-00003830: 6e73 2e0d 0a0d 0a23 2320 4c69 6365 6e73  ns.....## Licens
-00003840: 650d 0a53 6565 205b 4c49 4345 4e53 455d  e..See [LICENSE]
-00003850: 284c 4943 454e 5345 2e6d 6429 2066 6f72  (LICENSE.md) for
-00003860: 2074 6865 2063 7572 7265 6e74 206c 6963   the current lic
-00003870: 656e 7365 2e0d 0a                        ense...
+00001190: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000011a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000011b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000011c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000011d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000011e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000011f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001200: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001210: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001220: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001230: 2d2d 2d2d 2d7c 3a2d 2d2d 2d2d 2d2d 2d2d  -----|:---------
+00001240: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001250: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001260: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001270: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001280: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001290: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000012f0: 2d2d 2d2d 2d2d 2d2d 7c3a 2d2d 2d2d 2d2d  --------|:------
+00001300: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001310: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001320: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001330: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001340: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001350: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001360: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001370: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001380: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001390: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013d0: 2d7c 0d0a 7c20 5b21 5b4f 7065 6e20 496e  -|..| [![Open In
+000013e0: 2043 6f6c 6162 5d28 6874 7470 733a 2f2f   Colab](https://
+000013f0: 636f 6c61 622e 7265 7365 6172 6368 2e67  colab.research.g
+00001400: 6f6f 676c 652e 636f 6d2f 6173 7365 7473  oogle.com/assets
+00001410: 2f63 6f6c 6162 2d62 6164 6765 2e73 7667  /colab-badge.svg
+00001420: 295d 2868 7474 7073 3a2f 2f63 6f6c 6162  )](https://colab
+00001430: 2e72 6573 6561 7263 682e 676f 6f67 6c65  .research.google
+00001440: 2e63 6f6d 2f67 6974 6875 622f 746f 6d61  .com/github/toma
+00001450: 6172 7365 6e2f 5370 616e 4d61 726b 6572  arsen/SpanMarker
+00001460: 4e45 522f 626c 6f62 2f6d 6169 6e2f 6e6f  NER/blob/main/no
+00001470: 7465 626f 6f6b 732f 6765 7474 696e 675f  tebooks/getting_
+00001480: 7374 6172 7465 642e 6970 796e 6229 2020  started.ipynb)  
+00001490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000014a0: 2020 2020 207c 205b 215b 4b61 6767 6c65       | [![Kaggle
+000014b0: 5d28 6874 7470 733a 2f2f 6b61 6767 6c65  ](https://kaggle
+000014c0: 2e63 6f6d 2f73 7461 7469 632f 696d 6167  .com/static/imag
+000014d0: 6573 2f6f 7065 6e2d 696e 2d6b 6167 676c  es/open-in-kaggl
+000014e0: 652e 7376 6729 5d28 6874 7470 733a 2f2f  e.svg)](https://
+000014f0: 6b61 6767 6c65 2e63 6f6d 2f6b 6572 6e65  kaggle.com/kerne
+00001500: 6c73 2f77 656c 636f 6d65 3f73 7263 3d68  ls/welcome?src=h
+00001510: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
+00001520: 6d2f 746f 6d61 6172 7365 6e2f 5370 616e  m/tomaarsen/Span
+00001530: 4d61 726b 6572 4e45 522f 626c 6f62 2f6d  MarkerNER/blob/m
+00001540: 6169 6e2f 6e6f 7465 626f 6f6b 732f 6765  ain/notebooks/ge
+00001550: 7474 696e 675f 7374 6172 7465 642e 6970  tting_started.ip
+00001560: 796e 6229 2020 2020 2020 2020 2020 2020  ynb)            
+00001570: 2020 2020 2020 2020 2020 207c 205b 215b             | [![
+00001580: 4772 6164 6965 6e74 5d28 6874 7470 733a  Gradient](https:
+00001590: 2f2f 6173 7365 7473 2e70 6170 6572 7370  //assets.papersp
+000015a0: 6163 652e 696f 2f69 6d67 2f67 7261 6469  ace.io/img/gradi
+000015b0: 656e 742d 6261 6467 652e 7376 6729 5d28  ent-badge.svg)](
+000015c0: 6874 7470 733a 2f2f 636f 6e73 6f6c 652e  https://console.
+000015d0: 7061 7065 7273 7061 6365 2e63 6f6d 2f67  paperspace.com/g
+000015e0: 6974 6875 622f 746f 6d61 6172 7365 6e2f  ithub/tomaarsen/
+000015f0: 5370 616e 4d61 726b 6572 4e45 522f 626c  SpanMarkerNER/bl
+00001600: 6f62 2f6d 6169 6e2f 6e6f 7465 626f 6f6b  ob/main/notebook
+00001610: 732f 6765 7474 696e 675f 7374 6172 7465  s/getting_starte
+00001620: 642e 6970 796e 6229 2020 2020 2020 2020  d.ipynb)        
+00001630: 2020 2020 2020 2020 2020 2020 2020 207c                 |
+00001640: 205b 215b 4f70 656e 2049 6e20 5361 6765   [![Open In Sage
+00001650: 4d61 6b65 7220 5374 7564 696f 204c 6162  Maker Studio Lab
+00001660: 5d28 6874 7470 733a 2f2f 7374 7564 696f  ](https://studio
+00001670: 6c61 622e 7361 6765 6d61 6b65 722e 6177  lab.sagemaker.aw
+00001680: 732f 7374 7564 696f 6c61 622e 7376 6729  s/studiolab.svg)
+00001690: 5d28 6874 7470 733a 2f2f 7374 7564 696f  ](https://studio
+000016a0: 6c61 622e 7361 6765 6d61 6b65 722e 6177  lab.sagemaker.aw
+000016b0: 732f 696d 706f 7274 2f67 6974 6875 622f  s/import/github/
+000016c0: 746f 6d61 6172 7365 6e2f 5370 616e 4d61  tomaarsen/SpanMa
+000016d0: 726b 6572 4e45 522f 626c 6f62 2f6d 6169  rkerNER/blob/mai
+000016e0: 6e2f 6e6f 7465 626f 6f6b 732f 6765 7474  n/notebooks/gett
+000016f0: 696e 675f 7374 6172 7465 642e 6970 796e  ing_started.ipyn
+00001700: 6229 2020 2020 2020 2020 2020 2020 2020  b)              
+00001710: 2020 2020 2020 2020 207c 0d0a 0d0a 6060           |....``
+00001720: 6070 7974 686f 6e0d 0a66 726f 6d20 6461  `python..from da
+00001730: 7461 7365 7473 2069 6d70 6f72 7420 6c6f  tasets import lo
+00001740: 6164 5f64 6174 6173 6574 0d0a 6672 6f6d  ad_dataset..from
+00001750: 2074 7261 6e73 666f 726d 6572 7320 696d   transformers im
+00001760: 706f 7274 2054 7261 696e 696e 6741 7267  port TrainingArg
+00001770: 756d 656e 7473 0d0a 6672 6f6d 2073 7061  uments..from spa
+00001780: 6e5f 6d61 726b 6572 2069 6d70 6f72 7420  n_marker import 
+00001790: 5370 616e 4d61 726b 6572 4d6f 6465 6c2c  SpanMarkerModel,
+000017a0: 2054 7261 696e 6572 0d0a 0d0a 0d0a 6465   Trainer......de
+000017b0: 6620 6d61 696e 2829 202d 3e20 4e6f 6e65  f main() -> None
+000017c0: 3a0d 0a20 2020 2023 204c 6f61 6420 7468  :..    # Load th
+000017d0: 6520 6461 7461 7365 742c 2065 6e73 7572  e dataset, ensur
+000017e0: 6520 2274 6f6b 656e 7322 2061 6e64 2022  e "tokens" and "
+000017f0: 6e65 725f 7461 6773 2220 636f 6c75 6d6e  ner_tags" column
+00001800: 732c 2061 6e64 2067 6574 2061 206c 6973  s, and get a lis
+00001810: 7420 6f66 206c 6162 656c 730d 0a20 2020  t of labels..   
+00001820: 2064 6174 6173 6574 203d 206c 6f61 645f   dataset = load_
+00001830: 6461 7461 7365 7428 2244 464b 492d 534c  dataset("DFKI-SL
+00001840: 542f 6665 772d 6e65 7264 222c 2022 7375  T/few-nerd", "su
+00001850: 7065 7276 6973 6564 2229 0d0a 2020 2020  pervised")..    
+00001860: 6461 7461 7365 7420 3d20 6461 7461 7365  dataset = datase
+00001870: 742e 7265 6d6f 7665 5f63 6f6c 756d 6e73  t.remove_columns
+00001880: 2822 6e65 725f 7461 6773 2229 0d0a 2020  ("ner_tags")..  
+00001890: 2020 6461 7461 7365 7420 3d20 6461 7461    dataset = data
+000018a0: 7365 742e 7265 6e61 6d65 5f63 6f6c 756d  set.rename_colum
+000018b0: 6e28 2266 696e 655f 6e65 725f 7461 6773  n("fine_ner_tags
+000018c0: 222c 2022 6e65 725f 7461 6773 2229 0d0a  ", "ner_tags")..
+000018d0: 2020 2020 6c61 6265 6c73 203d 2064 6174      labels = dat
+000018e0: 6173 6574 5b22 7472 6169 6e22 5d2e 6665  aset["train"].fe
+000018f0: 6174 7572 6573 5b22 6e65 725f 7461 6773  atures["ner_tags
+00001900: 225d 2e66 6561 7475 7265 2e6e 616d 6573  "].feature.names
+00001910: 0d0a 0d0a 2020 2020 2320 496e 6974 6961  ....    # Initia
+00001920: 6c69 7a65 2061 2053 7061 6e4d 6172 6b65  lize a SpanMarke
+00001930: 7220 6d6f 6465 6c20 7573 696e 6720 6120  r model using a 
+00001940: 7072 6574 7261 696e 6564 2042 4552 542d  pretrained BERT-
+00001950: 7374 796c 6520 656e 636f 6465 720d 0a20  style encoder.. 
+00001960: 2020 206d 6f64 656c 5f6e 616d 6520 3d20     model_name = 
+00001970: 2262 6572 742d 6261 7365 2d63 6173 6564  "bert-base-cased
+00001980: 220d 0a20 2020 206d 6f64 656c 203d 2053  "..    model = S
+00001990: 7061 6e4d 6172 6b65 724d 6f64 656c 2e66  panMarkerModel.f
+000019a0: 726f 6d5f 7072 6574 7261 696e 6564 280d  rom_pretrained(.
+000019b0: 0a20 2020 2020 2020 206d 6f64 656c 5f6e  .        model_n
+000019c0: 616d 652c 0d0a 2020 2020 2020 2020 6c61  ame,..        la
+000019d0: 6265 6c73 3d6c 6162 656c 732c 0d0a 2020  bels=labels,..  
+000019e0: 2020 2020 2020 2320 5370 616e 4d61 726b        # SpanMark
+000019f0: 6572 2068 7970 6572 7061 7261 6d65 7465  er hyperparamete
+00001a00: 7273 3a0d 0a20 2020 2020 2020 206d 6f64  rs:..        mod
+00001a10: 656c 5f6d 6178 5f6c 656e 6774 683d 3235  el_max_length=25
+00001a20: 362c 0d0a 2020 2020 2020 2020 6d61 726b  6,..        mark
+00001a30: 6572 5f6d 6178 5f6c 656e 6774 683d 3132  er_max_length=12
+00001a40: 382c 0d0a 2020 2020 2020 2020 656e 7469  8,..        enti
+00001a50: 7479 5f6d 6178 5f6c 656e 6774 683d 382c  ty_max_length=8,
+00001a60: 0d0a 2020 2020 290d 0a0d 0a20 2020 2023  ..    )....    #
+00001a70: 2050 7265 7061 7265 2074 6865 20f0 9fa4   Prepare the ...
+00001a80: 9720 7472 616e 7366 6f72 6d65 7273 2074  . transformers t
+00001a90: 7261 696e 696e 6720 6172 6775 6d65 6e74  raining argument
+00001aa0: 730d 0a20 2020 2061 7267 7320 3d20 5472  s..    args = Tr
+00001ab0: 6169 6e69 6e67 4172 6775 6d65 6e74 7328  ainingArguments(
+00001ac0: 0d0a 2020 2020 2020 2020 6f75 7470 7574  ..        output
+00001ad0: 5f64 6972 3d22 6d6f 6465 6c73 2f73 7061  _dir="models/spa
+00001ae0: 6e5f 6d61 726b 6572 5f62 6572 745f 6261  n_marker_bert_ba
+00001af0: 7365 5f63 6173 6564 5f66 6577 6e65 7264  se_cased_fewnerd
+00001b00: 5f66 696e 655f 7375 7065 7222 2c0d 0a20  _fine_super",.. 
+00001b10: 2020 2020 2020 2023 2054 7261 696e 696e         # Trainin
+00001b20: 6720 4879 7065 7270 6172 616d 6574 6572  g Hyperparameter
+00001b30: 733a 0d0a 2020 2020 2020 2020 6c65 6172  s:..        lear
+00001b40: 6e69 6e67 5f72 6174 653d 3565 2d35 2c0d  ning_rate=5e-5,.
+00001b50: 0a20 2020 2020 2020 2070 6572 5f64 6576  .        per_dev
+00001b60: 6963 655f 7472 6169 6e5f 6261 7463 685f  ice_train_batch_
+00001b70: 7369 7a65 3d33 322c 0d0a 2020 2020 2020  size=32,..      
+00001b80: 2020 7065 725f 6465 7669 6365 5f65 7661    per_device_eva
+00001b90: 6c5f 6261 7463 685f 7369 7a65 3d33 322c  l_batch_size=32,
+00001ba0: 0d0a 2020 2020 2020 2020 6e75 6d5f 7472  ..        num_tr
+00001bb0: 6169 6e5f 6570 6f63 6873 3d33 2c0d 0a20  ain_epochs=3,.. 
+00001bc0: 2020 2020 2020 2077 6569 6768 745f 6465         weight_de
+00001bd0: 6361 793d 302e 3031 2c0d 0a20 2020 2020  cay=0.01,..     
+00001be0: 2020 2077 6172 6d75 705f 7261 7469 6f3d     warmup_ratio=
+00001bf0: 302e 312c 0d0a 2020 2020 2020 2020 6266  0.1,..        bf
+00001c00: 3136 3d54 7275 652c 2020 2320 5265 706c  16=True,  # Repl
+00001c10: 6163 6520 6062 6631 3660 2077 6974 6820  ace `bf16` with 
+00001c20: 6066 7031 3660 2069 6620 796f 7572 2068  `fp16` if your h
+00001c30: 6172 6477 6172 6520 6361 6e27 7420 7573  ardware can't us
+00001c40: 6520 6266 3136 2e0d 0a20 2020 2020 2020  e bf16...       
+00001c50: 2023 204f 7468 6572 2054 7261 696e 696e   # Other Trainin
+00001c60: 6720 7061 7261 6d65 7465 7273 0d0a 2020  g parameters..  
+00001c70: 2020 2020 2020 6c6f 6767 696e 675f 6669        logging_fi
+00001c80: 7273 745f 7374 6570 3d54 7275 652c 0d0a  rst_step=True,..
+00001c90: 2020 2020 2020 2020 6c6f 6767 696e 675f          logging_
+00001ca0: 7374 6570 733d 3530 2c0d 0a20 2020 2020  steps=50,..     
+00001cb0: 2020 2065 7661 6c75 6174 696f 6e5f 7374     evaluation_st
+00001cc0: 7261 7465 6779 3d22 7374 6570 7322 2c0d  rategy="steps",.
+00001cd0: 0a20 2020 2020 2020 2073 6176 655f 7374  .        save_st
+00001ce0: 7261 7465 6779 3d22 7374 6570 7322 2c0d  rategy="steps",.
+00001cf0: 0a20 2020 2020 2020 2065 7661 6c5f 7374  .        eval_st
+00001d00: 6570 733d 3330 3030 2c0d 0a20 2020 2020  eps=3000,..     
+00001d10: 2020 2073 6176 655f 746f 7461 6c5f 6c69     save_total_li
+00001d20: 6d69 743d 322c 0d0a 2020 2020 2020 2020  mit=2,..        
+00001d30: 6461 7461 6c6f 6164 6572 5f6e 756d 5f77  dataloader_num_w
+00001d40: 6f72 6b65 7273 3d32 2c0d 0a20 2020 2029  orkers=2,..    )
+00001d50: 0d0a 0d0a 2020 2020 2320 496e 6974 6961  ....    # Initia
+00001d60: 6c69 7a65 2074 6865 2074 7261 696e 6572  lize the trainer
+00001d70: 2075 7369 6e67 206f 7572 206d 6f64 656c   using our model
+00001d80: 2c20 7472 6169 6e69 6e67 2061 7267 7320  , training args 
+00001d90: 2620 6461 7461 7365 742c 2061 6e64 2074  & dataset, and t
+00001da0: 7261 696e 0d0a 2020 2020 7472 6169 6e65  rain..    traine
+00001db0: 7220 3d20 5472 6169 6e65 7228 0d0a 2020  r = Trainer(..  
+00001dc0: 2020 2020 2020 6d6f 6465 6c3d 6d6f 6465        model=mode
+00001dd0: 6c2c 0d0a 2020 2020 2020 2020 6172 6773  l,..        args
+00001de0: 3d61 7267 732c 0d0a 2020 2020 2020 2020  =args,..        
+00001df0: 7472 6169 6e5f 6461 7461 7365 743d 6461  train_dataset=da
+00001e00: 7461 7365 745b 2274 7261 696e 225d 2c0d  taset["train"],.
+00001e10: 0a20 2020 2020 2020 2065 7661 6c5f 6461  .        eval_da
+00001e20: 7461 7365 743d 6461 7461 7365 745b 2276  taset=dataset["v
+00001e30: 616c 6964 6174 696f 6e22 5d2c 0d0a 2020  alidation"],..  
+00001e40: 2020 290d 0a20 2020 2074 7261 696e 6572    )..    trainer
+00001e50: 2e74 7261 696e 2829 0d0a 2020 2020 7472  .train()..    tr
+00001e60: 6169 6e65 722e 7361 7665 5f6d 6f64 656c  ainer.save_model
+00001e70: 2822 6d6f 6465 6c73 2f73 7061 6e5f 6d61  ("models/span_ma
+00001e80: 726b 6572 5f62 6572 745f 6261 7365 5f63  rker_bert_base_c
+00001e90: 6173 6564 5f66 6577 6e65 7264 5f66 696e  ased_fewnerd_fin
+00001ea0: 655f 7375 7065 722f 6368 6563 6b70 6f69  e_super/checkpoi
+00001eb0: 6e74 2d66 696e 616c 2229 0d0a 0d0a 2020  nt-final")....  
+00001ec0: 2020 2320 436f 6d70 7574 6520 2620 7361    # Compute & sa
+00001ed0: 7665 2074 6865 206d 6574 7269 6373 206f  ve the metrics o
+00001ee0: 6e20 7468 6520 7465 7374 2073 6574 0d0a  n the test set..
+00001ef0: 2020 2020 6d65 7472 6963 7320 3d20 7472      metrics = tr
+00001f00: 6169 6e65 722e 6576 616c 7561 7465 2864  ainer.evaluate(d
+00001f10: 6174 6173 6574 5b22 7465 7374 225d 2c20  ataset["test"], 
+00001f20: 6d65 7472 6963 5f6b 6579 5f70 7265 6669  metric_key_prefi
+00001f30: 783d 2274 6573 7422 290d 0a20 2020 2074  x="test")..    t
+00001f40: 7261 696e 6572 2e73 6176 655f 6d65 7472  rainer.save_metr
+00001f50: 6963 7328 2274 6573 7422 2c20 6d65 7472  ics("test", metr
+00001f60: 6963 7329 0d0a 0d0a 0d0a 6966 205f 5f6e  ics)......if __n
+00001f70: 616d 655f 5f20 3d3d 2022 5f5f 6d61 696e  ame__ == "__main
+00001f80: 5f5f 223a 0d0a 2020 2020 6d61 696e 2829  __":..    main()
+00001f90: 0d0a 6060 600d 0a0d 0a23 2323 2049 6e66  ..```....### Inf
+00001fa0: 6572 656e 6365 0d0a 6060 6070 7974 686f  erence..```pytho
+00001fb0: 6e0d 0a66 726f 6d20 7370 616e 5f6d 6172  n..from span_mar
+00001fc0: 6b65 7220 696d 706f 7274 2053 7061 6e4d  ker import SpanM
+00001fd0: 6172 6b65 724d 6f64 656c 0d0a 0d0a 2320  arkerModel....# 
+00001fe0: 446f 776e 6c6f 6164 2066 726f 6d20 7468  Download from th
+00001ff0: 6520 f09f a497 2048 7562 0d0a 6d6f 6465  e .... Hub..mode
+00002000: 6c20 3d20 5370 616e 4d61 726b 6572 4d6f  l = SpanMarkerMo
+00002010: 6465 6c2e 6672 6f6d 5f70 7265 7472 6169  del.from_pretrai
+00002020: 6e65 6428 2274 6f6d 6161 7273 656e 2f73  ned("tomaarsen/s
+00002030: 7061 6e2d 6d61 726b 6572 2d62 6572 742d  pan-marker-bert-
+00002040: 6261 7365 2d66 6577 6e65 7264 2d66 696e  base-fewnerd-fin
+00002050: 652d 7375 7065 7222 290d 0a23 2052 756e  e-super")..# Run
+00002060: 2069 6e66 6572 656e 6365 0d0a 656e 7469   inference..enti
+00002070: 7469 6573 203d 206d 6f64 656c 2e70 7265  ties = model.pre
+00002080: 6469 6374 2822 416d 656c 6961 2045 6172  dict("Amelia Ear
+00002090: 6861 7274 2066 6c65 7720 6865 7220 7369  hart flew her si
+000020a0: 6e67 6c65 2065 6e67 696e 6520 4c6f 636b  ngle engine Lock
+000020b0: 6865 6564 2056 6567 6120 3542 2061 6372  heed Vega 5B acr
+000020c0: 6f73 7320 7468 6520 4174 6c61 6e74 6963  oss the Atlantic
+000020d0: 2074 6f20 5061 7269 732e 2229 0d0a 5b7b   to Paris.")..[{
+000020e0: 2773 7061 6e27 3a20 2741 6d65 6c69 6120  'span': 'Amelia 
+000020f0: 4561 7268 6172 7427 2c20 276c 6162 656c  Earhart', 'label
+00002100: 273a 2027 7065 7273 6f6e 2d6f 7468 6572  ': 'person-other
+00002110: 272c 2027 7363 6f72 6527 3a20 302e 3736  ', 'score': 0.76
+00002120: 3539 3539 3733 3936 3835 3035 3836 2c20  59597396850586, 
+00002130: 2763 6861 725f 7374 6172 745f 696e 6465  'char_start_inde
+00002140: 7827 3a20 302c 2027 6368 6172 5f65 6e64  x': 0, 'char_end
+00002150: 5f69 6e64 6578 273a 2031 347d 2c0d 0a20  _index': 14},.. 
+00002160: 7b27 7370 616e 273a 2027 4c6f 636b 6865  {'span': 'Lockhe
+00002170: 6564 2056 6567 6120 3542 272c 2027 6c61  ed Vega 5B', 'la
+00002180: 6265 6c27 3a20 2770 726f 6475 6374 2d61  bel': 'product-a
+00002190: 6972 706c 616e 6527 2c20 2773 636f 7265  irplane', 'score
+000021a0: 273a 2030 2e39 3732 3537 3835 3835 3134  ': 0.97257858514
+000021b0: 3738 3537 372c 2027 6368 6172 5f73 7461  78577, 'char_sta
+000021c0: 7274 5f69 6e64 6578 273a 2033 382c 2027  rt_index': 38, '
+000021d0: 6368 6172 5f65 6e64 5f69 6e64 6578 273a  char_end_index':
+000021e0: 2035 347d 2c0d 0a20 7b27 7370 616e 273a   54},.. {'span':
+000021f0: 2027 4174 6c61 6e74 6963 272c 2027 6c61   'Atlantic', 'la
+00002200: 6265 6c27 3a20 276c 6f63 6174 696f 6e2d  bel': 'location-
+00002210: 626f 6469 6573 6f66 7761 7465 7227 2c20  bodiesofwater', 
+00002220: 2773 636f 7265 273a 2030 2e37 3538 3736  'score': 0.75876
+00002230: 3739 3032 3835 3131 3034 372c 2027 6368  79028511047, 'ch
+00002240: 6172 5f73 7461 7274 5f69 6e64 6578 273a  ar_start_index':
+00002250: 2036 362c 2027 6368 6172 5f65 6e64 5f69   66, 'char_end_i
+00002260: 6e64 6578 273a 2037 347d 2c0d 0a20 7b27  ndex': 74},.. {'
+00002270: 7370 616e 273a 2027 5061 7269 7327 2c20  span': 'Paris', 
+00002280: 276c 6162 656c 273a 2027 6c6f 6361 7469  'label': 'locati
+00002290: 6f6e 2d47 5045 272c 2027 7363 6f72 6527  on-GPE', 'score'
+000022a0: 3a20 302e 3938 3932 3339 3039 3636 3431  : 0.989239096641
+000022b0: 3534 3035 2c20 2763 6861 725f 7374 6172  5405, 'char_star
+000022c0: 745f 696e 6465 7827 3a20 3738 2c20 2763  t_index': 78, 'c
+000022d0: 6861 725f 656e 645f 696e 6465 7827 3a20  har_end_index': 
+000022e0: 3833 7d5d 0d0a 6060 600d 0a0d 0a3c 212d  83}]..```....<!-
+000022f0: 2d20 4265 6361 7573 6520 7468 6973 2077  - Because this w
+00002300: 6f72 6b20 6973 2062 6173 6564 206f 6e20  ork is based on 
+00002310: 5b50 4c2d 4d61 726b 6572 5d28 6874 7470  [PL-Marker](http
+00002320: 733a 2f2f 6172 7869 762e 6f72 672f 7064  s://arxiv.org/pd
+00002330: 662f 3231 3039 2e30 3630 3637 7635 2e70  f/2109.06067v5.p
+00002340: 6466 292c 2079 6f75 206d 6179 2065 7870  df), you may exp
+00002350: 6563 7420 7369 6d69 6c61 7220 7265 7375  ect similar resu
+00002360: 6c74 7320 746f 2069 7473 205b 5061 7065  lts to its [Pape
+00002370: 7273 2077 6974 6820 436f 6465 204c 6561  rs with Code Lea
+00002380: 6465 7262 6f61 7264 5d28 6874 7470 733a  derboard](https:
+00002390: 2f2f 7061 7065 7273 7769 7468 636f 6465  //paperswithcode
+000023a0: 2e63 6f6d 2f70 6170 6572 2f70 6163 6b2d  .com/paper/pack-
+000023b0: 746f 6765 7468 6572 2d65 6e74 6974 792d  together-entity-
+000023c0: 616e 642d 7265 6c61 7469 6f6e 2d65 7874  and-relation-ext
+000023d0: 7261 6374 696f 6e29 2072 6573 756c 7473  raction) results
+000023e0: 2e20 2d2d 3e0d 0a0d 0a23 2320 5072 6574  . -->....## Pret
+000023f0: 7261 696e 6564 204d 6f64 656c 730d 0a0d  rained Models...
+00002400: 0a41 6c6c 206d 6f64 656c 7320 696e 2074  .All models in t
+00002410: 6869 7320 6c69 7374 2063 6f6e 7461 696e  his list contain
+00002420: 2060 7472 6169 6e2e 7079 6020 6669 6c65   `train.py` file
+00002430: 7320 7468 6174 2073 686f 7720 7468 6520  s that show the 
+00002440: 7472 6169 6e69 6e67 2073 6372 6970 7473  training scripts
+00002450: 2075 7365 6420 746f 2067 656e 6572 6174   used to generat
+00002460: 6520 7468 656d 2e20 4164 6469 7469 6f6e  e them. Addition
+00002470: 616c 6c79 2c20 616c 6c20 7472 6169 6e69  ally, all traini
+00002480: 6e67 2073 6372 6970 7473 2075 7365 6420  ng scripts used 
+00002490: 6172 6520 7374 6f72 6564 2069 6e20 7468  are stored in th
+000024a0: 6520 5b74 7261 696e 696e 675f 7363 7269  e [training_scri
+000024b0: 7074 735d 2874 7261 696e 696e 675f 7363  pts](training_sc
+000024c0: 7269 7074 7329 2064 6972 6563 746f 7279  ripts) directory
+000024d0: 2e0d 0a54 6865 7365 2074 7261 696e 6564  ...These trained
+000024e0: 206d 6f64 656c 7320 6861 7665 2048 6f73   models have Hos
+000024f0: 7465 6420 496e 6665 7265 6e63 6520 4150  ted Inference AP
+00002500: 4920 7769 6467 6574 7320 7468 6174 2079  I widgets that y
+00002510: 6f75 2063 616e 2075 7365 2074 6f20 6578  ou can use to ex
+00002520: 7065 7269 6d65 6e74 2077 6974 6820 7468  periment with th
+00002530: 6520 6d6f 6465 6c73 206f 6e20 7468 6569  e models on thei
+00002540: 7220 4875 6767 696e 6720 4661 6365 206d  r Hugging Face m
+00002550: 6f64 656c 2070 6167 6573 2e20 4164 6469  odel pages. Addi
+00002560: 7469 6f6e 616c 6c79 2c20 4875 6767 696e  tionally, Huggin
+00002570: 6720 4661 6365 2070 726f 7669 6465 7320  g Face provides 
+00002580: 6561 6368 206d 6f64 656c 2077 6974 6820  each model with 
+00002590: 6120 6672 6565 2041 5049 2028 6044 6570  a free API (`Dep
+000025a0: 6c6f 7960 203e 2060 496e 6665 7265 6e63  loy` > `Inferenc
+000025b0: 6520 4150 4960 206f 6e20 7468 6520 6d6f  e API` on the mo
+000025c0: 6465 6c20 7061 6765 292e 0d0a 0d0a 2323  del page).....##
+000025d0: 2320 4665 774e 4552 440d 0a2a 205b 6074  # FewNERD..* [`t
+000025e0: 6f6d 6161 7273 656e 2f73 7061 6e2d 6d61  omaarsen/span-ma
+000025f0: 726b 6572 2d62 6572 742d 6261 7365 2d66  rker-bert-base-f
+00002600: 6577 6e65 7264 2d66 696e 652d 7375 7065  ewnerd-fine-supe
+00002610: 7260 5d28 6874 7470 733a 2f2f 6875 6767  r`](https://hugg
+00002620: 696e 6766 6163 652e 636f 2f74 6f6d 6161  ingface.co/tomaa
+00002630: 7273 656e 2f73 7061 6e2d 6d61 726b 6572  rsen/span-marker
+00002640: 2d62 6572 742d 6261 7365 2d66 6577 6e65  -bert-base-fewne
+00002650: 7264 2d66 696e 652d 7375 7065 7229 2069  rd-fine-super) i
+00002660: 7320 6120 6d6f 6465 6c20 7468 6174 2049  s a model that I
+00002670: 2068 6176 6520 7472 6169 6e65 6420 696e   have trained in
+00002680: 2032 2068 6f75 7273 206f 6e20 7468 6520   2 hours on the 
+00002690: 6669 6e65 6772 6169 6e65 642c 2073 7570  finegrained, sup
+000026a0: 6572 7669 7365 6420 5b46 6577 2d4e 4552  ervised [Few-NER
+000026b0: 4420 6461 7461 7365 745d 2868 7474 7073  D dataset](https
+000026c0: 3a2f 2f68 7567 6769 6e67 6661 6365 2e63  ://huggingface.c
+000026d0: 6f2f 6461 7461 7365 7473 2f44 464b 492d  o/datasets/DFKI-
+000026e0: 534c 542f 6665 772d 6e65 7264 292e 2049  SLT/few-nerd). I
+000026f0: 7420 7265 6163 6865 6420 6120 302e 3730  t reached a 0.70
+00002700: 3533 2054 6573 7420 4631 2c20 636f 6d70  53 Test F1, comp
+00002710: 6574 6974 6976 6520 696e 2074 6865 2061  etitive in the a
+00002720: 6c6c 2d74 696d 6520 5b46 6577 2d4e 4552  ll-time [Few-NER
+00002730: 4420 6c65 6164 6572 626f 6172 645d 2868  D leaderboard](h
+00002740: 7474 7073 3a2f 2f70 6170 6572 7377 6974  ttps://paperswit
+00002750: 6863 6f64 652e 636f 6d2f 736f 7461 2f6e  hcode.com/sota/n
+00002760: 616d 6564 2d65 6e74 6974 792d 7265 636f  amed-entity-reco
+00002770: 676e 6974 696f 6e2d 6f6e 2d66 6577 2d6e  gnition-on-few-n
+00002780: 6572 642d 7375 7029 2075 7369 6e67 2060  erd-sup) using `
+00002790: 6265 7274 2d62 6173 6560 2e20 4d79 2074  bert-base`. My t
+000027a0: 7261 696e 696e 6720 7363 7269 7074 2072  raining script r
+000027b0: 6573 656d 626c 6573 2074 6865 206f 6e65  esembles the one
+000027c0: 2074 6861 7420 796f 7520 6361 6e20 7365   that you can se
+000027d0: 6520 6162 6f76 652e 0d0a 2020 2a20 5472  e above...  * Tr
+000027e0: 7920 7468 6520 6d6f 6465 6c20 6f75 7420  y the model out 
+000027f0: 6f6e 6c69 6e65 2075 7369 6e67 2074 6869  online using thi
+00002800: 7320 5bf0 9fa4 9720 5370 6163 655d 2868  s [.... Space](h
+00002810: 7474 7073 3a2f 2f74 6f6d 6161 7273 656e  ttps://tomaarsen
+00002820: 2d73 7061 6e2d 6d61 726b 6572 2d62 6572  -span-marker-ber
+00002830: 742d 6261 7365 2d66 6577 6e65 7264 2d66  t-base-fewnerd-f
+00002840: 696e 652d 7375 7065 722e 6866 2e73 7061  ine-super.hf.spa
+00002850: 6365 2f29 2e0d 0a0d 0a2a 205b 6074 6f6d  ce/).....* [`tom
+00002860: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
+00002870: 6572 2d72 6f62 6572 7461 2d6c 6172 6765  er-roberta-large
+00002880: 2d66 6577 6e65 7264 2d66 696e 652d 7375  -fewnerd-fine-su
+00002890: 7065 7260 5d28 6874 7470 733a 2f2f 6875  per`](https://hu
+000028a0: 6767 696e 6766 6163 652e 636f 2f74 6f6d  ggingface.co/tom
+000028b0: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
+000028c0: 6572 2d72 6f62 6572 7461 2d6c 6172 6765  er-roberta-large
+000028d0: 2d66 6577 6e65 7264 2d66 696e 652d 7375  -fewnerd-fine-su
+000028e0: 7065 7229 2077 6173 2074 7261 696e 6564  per) was trained
+000028f0: 2069 6e20 3620 686f 7572 7320 6f6e 2074   in 6 hours on t
+00002900: 6865 2066 696e 6567 7261 696e 6564 2c20  he finegrained, 
+00002910: 7375 7065 7276 6973 6564 205b 4665 772d  supervised [Few-
+00002920: 4e45 5244 2064 6174 6173 6574 5d28 6874  NERD dataset](ht
+00002930: 7470 733a 2f2f 6875 6767 696e 6766 6163  tps://huggingfac
+00002940: 652e 636f 2f64 6174 6173 6574 732f 4446  e.co/datasets/DF
+00002950: 4b49 2d53 4c54 2f66 6577 2d6e 6572 6429  KI-SLT/few-nerd)
+00002960: 2075 7369 6e67 2060 726f 6265 7274 612d   using `roberta-
+00002970: 6c61 7267 6560 2e20 4974 2072 6561 6368  large`. It reach
+00002980: 6564 2061 2030 2e37 3130 3320 5465 7374  ed a 0.7103 Test
+00002990: 2046 312c 2072 6561 6368 696e 6720 6120   F1, reaching a 
+000029a0: 6e65 7720 7374 6174 6520 6f66 2074 6865  new state of the
+000029b0: 2061 7274 2069 6e20 7468 6520 616c 6c2d   art in the all-
+000029c0: 7469 6d65 205b 4665 772d 4e45 5244 206c  time [Few-NERD l
+000029d0: 6561 6465 7262 6f61 7264 5d28 6874 7470  eaderboard](http
+000029e0: 733a 2f2f 7061 7065 7273 7769 7468 636f  s://paperswithco
+000029f0: 6465 2e63 6f6d 2f73 6f74 612f 6e61 6d65  de.com/sota/name
+00002a00: 642d 656e 7469 7479 2d72 6563 6f67 6e69  d-entity-recogni
+00002a10: 7469 6f6e 2d6f 6e2d 6665 772d 6e65 7264  tion-on-few-nerd
+00002a20: 2d73 7570 292e 0d0a 2a20 5b60 746f 6d61  -sup)...* [`toma
+00002a30: 6172 7365 6e2f 7370 616e 2d6d 6172 6b65  arsen/span-marke
+00002a40: 722d 786c 6d2d 726f 6265 7274 612d 6261  r-xlm-roberta-ba
+00002a50: 7365 2d66 6577 6e65 7264 2d66 696e 652d  se-fewnerd-fine-
+00002a60: 7375 7065 7260 5d28 6874 7470 733a 2f2f  super`](https://
+00002a70: 6875 6767 696e 6766 6163 652e 636f 2f74  huggingface.co/t
+00002a80: 6f6d 6161 7273 656e 2f73 7061 6e2d 6d61  omaarsen/span-ma
+00002a90: 726b 6572 2d78 6c6d 2d72 6f62 6572 7461  rker-xlm-roberta
+00002aa0: 2d62 6173 652d 6665 776e 6572 642d 6669  -base-fewnerd-fi
+00002ab0: 6e65 2d73 7570 6572 2920 6973 2061 206d  ne-super) is a m
+00002ac0: 756c 7469 6c69 6e67 7561 6c20 6d6f 6465  ultilingual mode
+00002ad0: 6c20 7468 6174 2049 2068 6176 6520 7472  l that I have tr
+00002ae0: 6169 6e65 6420 696e 2031 2e35 2068 6f75  ained in 1.5 hou
+00002af0: 7273 206f 6e20 7468 6520 6669 6e65 6772  rs on the finegr
+00002b00: 6169 6e65 642c 2073 7570 6572 7669 7365  ained, supervise
+00002b10: 6420 5b46 6577 2d4e 4552 4420 6461 7461  d [Few-NERD data
+00002b20: 7365 745d 2868 7474 7073 3a2f 2f68 7567  set](https://hug
+00002b30: 6769 6e67 6661 6365 2e63 6f2f 6461 7461  gingface.co/data
+00002b40: 7365 7473 2f44 464b 492d 534c 542f 6665  sets/DFKI-SLT/fe
+00002b50: 772d 6e65 7264 292e 2049 7420 7265 6163  w-nerd). It reac
+00002b60: 6865 6420 6120 302e 3638 3620 5465 7374  hed a 0.686 Test
+00002b70: 2046 3120 6f6e 2045 6e67 6c69 7368 2c20   F1 on English, 
+00002b80: 616e 6420 776f 726b 7320 7765 6c6c 206f  and works well o
+00002b90: 6e20 6f74 6865 7220 6c61 6e67 7561 6765  n other language
+00002ba0: 7320 6c69 6b65 2053 7061 6e69 7368 2c20  s like Spanish, 
+00002bb0: 4672 656e 6368 2c20 4765 726d 616e 2c20  French, German, 
+00002bc0: 5275 7373 6961 6e2c 2044 7574 6368 2c20  Russian, Dutch, 
+00002bd0: 506f 6c69 7368 2c20 4963 656c 616e 6469  Polish, Icelandi
+00002be0: 632c 2047 7265 656b 2061 6e64 206d 616e  c, Greek and man
+00002bf0: 7920 6d6f 7265 2e0d 0a0d 0a23 2323 204f  y more.....### O
+00002c00: 6e74 6f4e 6f74 6573 2076 352e 300d 0a2a  ntoNotes v5.0..*
+00002c10: 205b 6074 6f6d 6161 7273 656e 2f73 7061   [`tomaarsen/spa
+00002c20: 6e2d 6d61 726b 6572 2d72 6f62 6572 7461  n-marker-roberta
+00002c30: 2d6c 6172 6765 2d6f 6e74 6f6e 6f74 6573  -large-ontonotes
+00002c40: 3560 5d28 6874 7470 733a 2f2f 6875 6767  5`](https://hugg
+00002c50: 696e 6766 6163 652e 636f 2f74 6f6d 6161  ingface.co/tomaa
+00002c60: 7273 656e 2f73 7061 6e2d 6d61 726b 6572  rsen/span-marker
+00002c70: 2d72 6f62 6572 7461 2d6c 6172 6765 2d6f  -roberta-large-o
+00002c80: 6e74 6f6e 6f74 6573 3529 2077 6173 2074  ntonotes5) was t
+00002c90: 7261 696e 6564 2069 6e20 3320 686f 7572  rained in 3 hour
+00002ca0: 7320 6f6e 2074 6865 204f 6e74 6f4e 6f74  s on the OntoNot
+00002cb0: 6573 2076 352e 3020 6461 7461 7365 742c  es v5.0 dataset,
+00002cc0: 2072 6561 6368 696e 6720 6120 7065 7266   reaching a perf
+00002cd0: 6f72 6d61 6e63 6520 6f66 2030 2e39 3135  ormance of 0.915
+00002ce0: 3420 4631 2e20 466f 7220 7265 6665 7265  4 F1. For refere
+00002cf0: 6e63 652c 2074 6865 2063 7572 7265 6e74  nce, the current
+00002d00: 2073 7472 6f6e 6765 7374 2073 7061 4379   strongest spaCy
+00002d10: 206d 6f64 656c 2028 6065 6e5f 636f 7265   model (`en_core
+00002d20: 5f77 6562 5f74 7266 6029 2072 6561 6368  _web_trf`) reach
+00002d30: 6573 2030 2e38 3938 2e20 5468 6973 2053  es 0.898. This S
+00002d40: 7061 6e4d 6172 6b65 7220 6d6f 6465 6c20  panMarker model 
+00002d50: 7573 6573 2061 2060 726f 6265 7274 612d  uses a `roberta-
+00002d60: 6c61 7267 6560 2065 6e63 6f64 6572 2075  large` encoder u
+00002d70: 6e64 6572 2074 6865 2068 6f6f 642e 0d0a  nder the hood...
+00002d80: 0d0a 2323 2320 436f 4e4c 4c30 330d 0a2a  ..### CoNLL03..*
+00002d90: 205b 6074 6f6d 6161 7273 656e 2f73 7061   [`tomaarsen/spa
+00002da0: 6e2d 6d61 726b 6572 2d78 6c6d 2d72 6f62  n-marker-xlm-rob
+00002db0: 6572 7461 2d6c 6172 6765 2d63 6f6e 6c6c  erta-large-conll
+00002dc0: 3033 605d 2868 7474 7073 3a2f 2f68 7567  03`](https://hug
+00002dd0: 6769 6e67 6661 6365 2e63 6f2f 746f 6d61  gingface.co/toma
+00002de0: 6172 7365 6e2f 7370 616e 2d6d 6172 6b65  arsen/span-marke
+00002df0: 722d 786c 6d2d 726f 6265 7274 612d 6c61  r-xlm-roberta-la
+00002e00: 7267 652d 636f 6e6c 6c30 3329 2069 7320  rge-conll03) is 
+00002e10: 6120 5370 616e 4d61 726b 6572 206d 6f64  a SpanMarker mod
+00002e20: 656c 2075 7369 6e67 2060 786c 6d2d 726f  el using `xlm-ro
+00002e30: 6265 7274 612d 6c61 7267 6560 2074 6861  berta-large` tha
+00002e40: 7420 7761 7320 7472 6169 6e65 6420 696e  t was trained in
+00002e50: 2034 3520 6d69 6e75 7465 732e 2049 7420   45 minutes. It 
+00002e60: 7265 6163 6865 7320 6120 7374 6174 6520  reaches a state 
+00002e70: 6f66 2074 6865 2061 7274 2030 2e39 3331  of the art 0.931
+00002e80: 2046 3120 6f6e 2043 6f4e 4c4c 3033 2077   F1 on CoNLL03 w
+00002e90: 6974 686f 7574 2075 7369 6e67 2064 6f63  ithout using doc
+00002ea0: 756d 656e 742d 6c65 7665 6c20 636f 6e74  ument-level cont
+00002eb0: 6578 742e 2046 6f72 2072 6566 6572 656e  ext. For referen
+00002ec0: 6365 2c20 7468 6520 6375 7272 656e 7420  ce, the current 
+00002ed0: 7374 726f 6e67 6573 7420 7370 6143 7920  strongest spaCy 
+00002ee0: 6d6f 6465 6c20 2860 656e 5f63 6f72 655f  model (`en_core_
+00002ef0: 7765 625f 7472 6660 2920 7265 6163 6865  web_trf`) reache
+00002f00: 7320 3931 2e36 2e0d 0a2a 205b 6074 6f6d  s 91.6...* [`tom
+00002f10: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
+00002f20: 6572 2d78 6c6d 2d72 6f62 6572 7461 2d6c  er-xlm-roberta-l
+00002f30: 6172 6765 2d63 6f6e 6c6c 3033 2d64 6f63  arge-conll03-doc
+00002f40: 2d63 6f6e 7465 7874 605d 2868 7474 7073  -context`](https
+00002f50: 3a2f 2f68 7567 6769 6e67 6661 6365 2e63  ://huggingface.c
+00002f60: 6f2f 746f 6d61 6172 7365 6e2f 7370 616e  o/tomaarsen/span
+00002f70: 2d6d 6172 6b65 722d 786c 6d2d 726f 6265  -marker-xlm-robe
+00002f80: 7274 612d 6c61 7267 652d 636f 6e6c 6c30  rta-large-conll0
+00002f90: 332d 646f 632d 636f 6e74 6578 7429 2069  3-doc-context) i
+00002fa0: 7320 616e 6f74 6865 7220 5370 616e 4d61  s another SpanMa
+00002fb0: 726b 6572 206d 6f64 656c 2075 7369 6e67  rker model using
+00002fc0: 2074 6865 2060 786c 6d2d 726f 6265 7274   the `xlm-robert
+00002fd0: 612d 6c61 7267 6560 2065 6e63 6f64 6572  a-large` encoder
+00002fe0: 2e20 4974 2075 7365 7320 5b64 6f63 756d  . It uses [docum
+00002ff0: 656e 742d 6c65 7665 6c20 636f 6e74 6578  ent-level contex
+00003000: 745d 2868 7474 7073 3a2f 2f74 6f6d 6161  t](https://tomaa
+00003010: 7273 656e 2e67 6974 6875 622e 696f 2f53  rsen.github.io/S
+00003020: 7061 6e4d 6172 6b65 724e 4552 2f6e 6f74  panMarkerNER/not
+00003030: 6562 6f6f 6b73 2f64 6f63 756d 656e 745f  ebooks/document_
+00003040: 6c65 7665 6c5f 636f 6e74 6578 742e 6874  level_context.ht
+00003050: 6d6c 2920 746f 2072 6561 6368 2061 2073  ml) to reach a s
+00003060: 7461 7465 206f 6620 7468 6520 6172 7420  tate of the art 
+00003070: 302e 3934 3420 4631 2e20 466f 7220 7468  0.944 F1. For th
+00003080: 6520 6265 7374 2070 6572 666f 726d 616e  e best performan
+00003090: 6365 2c20 696e 6665 7265 6e63 6520 7368  ce, inference sh
+000030a0: 6f75 6c64 2062 6520 7065 7266 6f72 6d65  ould be performe
+000030b0: 6420 7573 696e 6720 646f 6375 6d65 6e74  d using document
+000030c0: 2d6c 6576 656c 2063 6f6e 7465 7874 2028  -level context (
+000030d0: 5b64 6f63 735d 2868 7474 7073 3a2f 2f74  [docs](https://t
+000030e0: 6f6d 6161 7273 656e 2e67 6974 6875 622e  omaarsen.github.
+000030f0: 696f 2f53 7061 6e4d 6172 6b65 724e 4552  io/SpanMarkerNER
+00003100: 2f6e 6f74 6562 6f6f 6b73 2f64 6f63 756d  /notebooks/docum
+00003110: 656e 745f 6c65 7665 6c5f 636f 6e74 6578  ent_level_contex
+00003120: 742e 6874 6d6c 2349 6e66 6572 656e 6365  t.html#Inference
+00003130: 2929 2e20 5468 6973 206d 6f64 656c 2077  )). This model w
+00003140: 6173 2074 7261 696e 6564 2069 6e20 3120  as trained in 1 
+00003150: 686f 7572 2e0d 0a0d 0a23 2323 2043 6f4e  hour.....### CoN
+00003160: 4c4c 2b2b 0d0a 2a20 5b60 746f 6d61 6172  LL++..* [`tomaar
+00003170: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
+00003180: 786c 6d2d 726f 6265 7274 612d 6c61 7267  xlm-roberta-larg
+00003190: 652d 636f 6e6c 6c70 702d 646f 632d 636f  e-conllpp-doc-co
+000031a0: 6e74 6578 7460 5d28 6874 7470 733a 2f2f  ntext`](https://
+000031b0: 6875 6767 696e 6766 6163 652e 636f 2f74  huggingface.co/t
+000031c0: 6f6d 6161 7273 656e 2f73 7061 6e2d 6d61  omaarsen/span-ma
+000031d0: 726b 6572 2d78 6c6d 2d72 6f62 6572 7461  rker-xlm-roberta
+000031e0: 2d6c 6172 6765 2d63 6f6e 6c6c 7070 2d64  -large-conllpp-d
+000031f0: 6f63 2d63 6f6e 7465 7874 2920 7761 7320  oc-context) was 
+00003200: 7472 6169 6e65 6420 696e 2061 6e20 686f  trained in an ho
+00003210: 7572 2075 7369 6e67 2074 6865 2060 786c  ur using the `xl
+00003220: 6d2d 726f 6265 7274 612d 6c61 7267 6560  m-roberta-large`
+00003230: 2065 6e63 6f64 6572 206f 6e20 7468 6520   encoder on the 
+00003240: 436f 4e4c 4c2b 2b20 6461 7461 7365 742e  CoNLL++ dataset.
+00003250: 2055 7369 6e67 205b 646f 6375 6d65 6e74   Using [document
+00003260: 2d6c 6576 656c 2063 6f6e 7465 7874 5d28  -level context](
+00003270: 6874 7470 733a 2f2f 746f 6d61 6172 7365  https://tomaarse
+00003280: 6e2e 6769 7468 7562 2e69 6f2f 5370 616e  n.github.io/Span
+00003290: 4d61 726b 6572 4e45 522f 6e6f 7465 626f  MarkerNER/notebo
+000032a0: 6f6b 732f 646f 6375 6d65 6e74 5f6c 6576  oks/document_lev
+000032b0: 656c 5f63 6f6e 7465 7874 2e68 746d 6c29  el_context.html)
+000032c0: 2c20 6974 2072 6561 6368 6573 2061 2076  , it reaches a v
+000032d0: 6572 7920 636f 6d70 6574 6974 6976 6520  ery competitive 
+000032e0: 302e 3935 3520 4631 2e20 466f 7220 7468  0.955 F1. For th
+000032f0: 6520 6265 7374 2070 6572 666f 726d 616e  e best performan
+00003300: 6365 2c20 696e 6665 7265 6e63 6520 7368  ce, inference sh
+00003310: 6f75 6c64 2062 6520 7065 7266 6f72 6d65  ould be performe
+00003320: 6420 7573 696e 6720 646f 6375 6d65 6e74  d using document
+00003330: 2d6c 6576 656c 2063 6f6e 7465 7874 2028  -level context (
+00003340: 5b64 6f63 735d 2868 7474 7073 3a2f 2f74  [docs](https://t
+00003350: 6f6d 6161 7273 656e 2e67 6974 6875 622e  omaarsen.github.
+00003360: 696f 2f53 7061 6e4d 6172 6b65 724e 4552  io/SpanMarkerNER
+00003370: 2f6e 6f74 6562 6f6f 6b73 2f64 6f63 756d  /notebooks/docum
+00003380: 656e 745f 6c65 7665 6c5f 636f 6e74 6578  ent_level_contex
+00003390: 742e 6874 6d6c 2349 6e66 6572 656e 6365  t.html#Inference
+000033a0: 2929 2e0d 0a0d 0a23 2320 5573 696e 6720  )).....## Using 
+000033b0: 7072 6574 7261 696e 6564 2053 7061 6e4d  pretrained SpanM
+000033c0: 6172 6b65 7220 6d6f 6465 6c73 2077 6974  arker models wit
+000033d0: 6820 7370 6143 790d 0a41 6c6c 205b 5370  h spaCy..All [Sp
+000033e0: 616e 4d61 726b 6572 206d 6f64 656c 7320  anMarker models 
+000033f0: 6f6e 2074 6865 2048 7567 6769 6e67 2046  on the Hugging F
+00003400: 6163 6520 4875 625d 2868 7474 7073 3a2f  ace Hub](https:/
+00003410: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
+00003420: 6d6f 6465 6c73 3f6c 6962 7261 7279 3d73  models?library=s
+00003430: 7061 6e2d 6d61 726b 6572 2920 6361 6e20  pan-marker) can 
+00003440: 616c 736f 2062 6520 6561 7369 6c79 2075  also be easily u
+00003450: 7365 6420 696e 2073 7061 4379 2e20 4974  sed in spaCy. It
+00003460: 2773 2061 7320 7369 6d70 6c65 2061 7320  's as simple as 
+00003470: 696e 636c 7564 696e 6720 3120 6c69 6e65  including 1 line
+00003480: 2074 6f20 6164 6420 7468 6520 6073 7061   to add the `spa
+00003490: 6e5f 6d61 726b 6572 6020 7069 7065 6c69  n_marker` pipeli
+000034a0: 6e65 2e20 5365 6520 7468 6520 446f 6375  ne. See the Docu
+000034b0: 6d65 6e74 6174 696f 6e20 6f72 2041 5049  mentation or API
+000034c0: 2052 6566 6572 656e 6365 2066 6f72 206d   Reference for m
+000034d0: 6f72 6520 696e 666f 726d 6174 696f 6e2e  ore information.
+000034e0: 0d0a 6060 6070 7974 686f 6e0d 0a69 6d70  ..```python..imp
+000034f0: 6f72 7420 7370 6163 790d 0a0d 0a23 204c  ort spacy....# L
+00003500: 6f61 6420 7468 6520 7370 6143 7920 6d6f  oad the spaCy mo
+00003510: 6465 6c20 7769 7468 2074 6865 2073 7061  del with the spa
+00003520: 6e5f 6d61 726b 6572 2070 6970 656c 696e  n_marker pipelin
+00003530: 6520 636f 6d70 6f6e 656e 740d 0a6e 6c70  e component..nlp
+00003540: 203d 2073 7061 6379 2e6c 6f61 6428 2265   = spacy.load("e
+00003550: 6e5f 636f 7265 5f77 6562 5f73 6d22 2c20  n_core_web_sm", 
+00003560: 6469 7361 626c 653d 5b22 6e65 7222 5d29  disable=["ner"])
+00003570: 0d0a 6e6c 702e 6164 645f 7069 7065 2822  ..nlp.add_pipe("
+00003580: 7370 616e 5f6d 6172 6b65 7222 2c20 636f  span_marker", co
+00003590: 6e66 6967 3d7b 226d 6f64 656c 223a 2022  nfig={"model": "
+000035a0: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
+000035b0: 6172 6b65 722d 726f 6265 7274 612d 6c61  arker-roberta-la
+000035c0: 7267 652d 6f6e 746f 6e6f 7465 7335 227d  rge-ontonotes5"}
+000035d0: 290d 0a0d 0a23 2046 6565 6420 736f 6d65  )....# Feed some
+000035e0: 2074 6578 7420 7468 726f 7567 6820 7468   text through th
+000035f0: 6520 6d6f 6465 6c20 746f 2067 6574 2061  e model to get a
+00003600: 2073 7061 6379 2044 6f63 0d0a 7465 7874   spacy Doc..text
+00003610: 203d 2022 2222 436c 656f 7061 7472 6120   = """Cleopatra 
+00003620: 5649 492c 2061 6c73 6f20 6b6e 6f77 6e20  VII, also known 
+00003630: 6173 2043 6c65 6f70 6174 7261 2074 6865  as Cleopatra the
+00003640: 2047 7265 6174 2c20 7761 7320 7468 6520   Great, was the 
+00003650: 6c61 7374 2061 6374 6976 6520 7275 6c65  last active rule
+00003660: 7220 6f66 2074 6865 205c 0d0a 5074 6f6c  r of the \..Ptol
+00003670: 656d 6169 6320 4b69 6e67 646f 6d20 6f66  emaic Kingdom of
+00003680: 2045 6779 7074 2e20 5368 6520 7761 7320   Egypt. She was 
+00003690: 626f 726e 2069 6e20 3639 2042 4345 2061  born in 69 BCE a
+000036a0: 6e64 2072 756c 6564 2045 6779 7074 2066  nd ruled Egypt f
+000036b0: 726f 6d20 3531 2042 4345 2075 6e74 696c  rom 51 BCE until
+000036c0: 2068 6572 205c 0d0a 6465 6174 6820 696e   her \..death in
+000036d0: 2033 3020 4243 452e 2222 220d 0a64 6f63   30 BCE."""..doc
+000036e0: 203d 206e 6c70 2874 6578 7429 0d0a 0d0a   = nlp(text)....
+000036f0: 2320 416e 6420 6c6f 6f6b 2061 7420 7468  # And look at th
+00003700: 6520 656e 7469 7469 6573 0d0a 7072 696e  e entities..prin
+00003710: 7428 5b28 656e 7469 7479 2c20 656e 7469  t([(entity, enti
+00003720: 7479 2e6c 6162 656c 5f29 2066 6f72 2065  ty.label_) for e
+00003730: 6e74 6974 7920 696e 2064 6f63 2e65 6e74  ntity in doc.ent
+00003740: 735d 290d 0a22 2222 0d0a 5b28 436c 656f  s]).."""..[(Cleo
+00003750: 7061 7472 6120 5649 492c 2022 5045 5253  patra VII, "PERS
+00003760: 4f4e 2229 2c20 2843 6c65 6f70 6174 7261  ON"), (Cleopatra
+00003770: 2074 6865 2047 7265 6174 2c20 2250 4552   the Great, "PER
+00003780: 534f 4e22 292c 2028 7468 6520 5074 6f6c  SON"), (the Ptol
+00003790: 656d 6169 6320 4b69 6e67 646f 6d20 6f66  emaic Kingdom of
+000037a0: 2045 6779 7074 2c20 2247 5045 2229 2c0d   Egypt, "GPE"),.
+000037b0: 0a28 3639 2042 4345 2c20 2244 4154 4522  .(69 BCE, "DATE"
+000037c0: 292c 2028 4567 7970 742c 2022 4750 4522  ), (Egypt, "GPE"
+000037d0: 292c 2028 3531 2042 4345 2c20 2244 4154  ), (51 BCE, "DAT
+000037e0: 4522 292c 2028 3330 2042 4345 2c20 2244  E"), (30 BCE, "D
+000037f0: 4154 4522 295d 0d0a 2222 220d 0a60 6060  ATE")].."""..```
+00003800: 0d0a 215b 696d 6167 655d 2868 7474 7073  ..![image](https
+00003810: 3a2f 2f75 7365 722d 696d 6167 6573 2e67  ://user-images.g
+00003820: 6974 6875 6275 7365 7263 6f6e 7465 6e74  ithubusercontent
+00003830: 2e63 6f6d 2f33 3736 3231 3439 312f 3234  .com/37621491/24
+00003840: 3631 3730 3632 332d 3633 3531 6362 3765  6170623-6351cb7e
+00003850: 2d62 6262 302d 3434 3732 2d61 6631 362d  -bbb0-4472-af16-
+00003860: 3961 3335 3161 3235 3364 6139 2e70 6e67  9a351a253da9.png
+00003870: 290d 0a0d 0a23 2320 436f 6e74 6578 740d  )....## Context.
+00003880: 0a3c 6831 2061 6c69 676e 3d22 6365 6e74  .<h1 align="cent
+00003890: 6572 223e 0d0a 2020 2020 3c61 2068 7265  er">..    <a hre
+000038a0: 663d 2268 7474 7073 3a2f 2f67 6974 6875  f="https://githu
+000038b0: 622e 636f 6d2f 6172 6769 6c6c 612d 696f  b.com/argilla-io
+000038c0: 2f61 7267 696c 6c61 223e 0d0a 2020 2020  /argilla">..    
+000038d0: 3c69 6d67 2073 7263 3d22 6874 7470 733a  <img src="https:
+000038e0: 2f2f 6769 7468 7562 2e63 6f6d 2f64 7673  //github.com/dvs
+000038f0: 7265 706f 2f69 6d67 732f 7261 772f 6d61  repo/imgs/raw/ma
+00003900: 696e 2f72 672e 7376 6722 2061 6c74 3d22  in/rg.svg" alt="
+00003910: 4172 6769 6c6c 6122 2077 6964 7468 3d22  Argilla" width="
+00003920: 3135 3022 3e0d 0a20 2020 203c 2f61 3e0d  150">..    </a>.
+00003930: 0a3c 2f68 313e 0d0a 0d0a 4920 6861 7665  .</h1>....I have
+00003940: 2064 6576 656c 6f70 6564 2074 6869 7320   developed this 
+00003950: 6c69 6272 6172 7920 6173 2061 2070 6172  library as a par
+00003960: 7420 6f66 206d 7920 7468 6573 6973 2077  t of my thesis w
+00003970: 6f72 6b20 6174 205b 4172 6769 6c6c 615d  ork at [Argilla]
+00003980: 2868 7474 7073 3a2f 2f67 6974 6875 622e  (https://github.
+00003990: 636f 6d2f 6172 6769 6c6c 612d 696f 2f61  com/argilla-io/a
+000039a0: 7267 696c 6c61 292e 0d0a 4665 656c 2066  rgilla)...Feel f
+000039b0: 7265 6520 746f 20e2 ad90 2073 7461 7220  ree to ... star 
+000039c0: 6f72 2077 6174 6368 2074 6865 2053 7061  or watch the Spa
+000039d0: 6e4d 6172 6b65 7220 7265 706f 7369 746f  nMarker reposito
+000039e0: 7279 2074 6f20 6765 7420 6e6f 7469 6669  ry to get notifi
+000039f0: 6564 2077 6865 6e20 6d79 2074 6865 7369  ed when my thesi
+00003a00: 7320 6973 2070 7562 6c69 7368 6564 2e0d  s is published..
+00003a10: 0a0d 0a23 2320 4368 616e 6765 6c6f 670d  ...## Changelog.
+00003a20: 0a53 6565 205b 4348 414e 4745 4c4f 472e  .See [CHANGELOG.
+00003a30: 6d64 5d28 4348 414e 4745 4c4f 472e 6d64  md](CHANGELOG.md
+00003a40: 2920 666f 7220 6e65 7773 206f 6e20 616c  ) for news on al
+00003a50: 6c20 5370 616e 4d61 726b 6572 2076 6572  l SpanMarker ver
+00003a60: 7369 6f6e 732e 0d0a 0d0a 2323 204c 6963  sions.....## Lic
+00003a70: 656e 7365 0d0a 5365 6520 5b4c 4943 454e  ense..See [LICEN
+00003a80: 5345 5d28 4c49 4345 4e53 452e 6d64 2920  SE](LICENSE.md) 
+00003a90: 666f 7220 7468 6520 6375 7272 656e 7420  for the current 
+00003aa0: 6c69 6365 6e73 652e 0d0a                 license...
```

### Comparing `span_marker-1.2.2/pyproject.toml` & `span_marker-1.2.3/pyproject.toml`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/__init__.py` & `span_marker-1.2.3/span_marker/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-__version__ = "1.2.2"
+__version__ = "1.2.3"
 
 import logging
 from typing import Optional, Union
 
 import torch
 from transformers import AutoConfig, AutoModel, TrainingArguments
```

### Comparing `span_marker-1.2.2/span_marker/configuration.py` & `span_marker-1.2.3/span_marker/configuration.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/data_collator.py` & `span_marker-1.2.3/span_marker/data_collator.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/evaluation.py` & `span_marker-1.2.3/span_marker/evaluation.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/label_normalizer.py` & `span_marker-1.2.3/span_marker/label_normalizer.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/model_card.py` & `span_marker-1.2.3/span_marker/model_card.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/modeling.py` & `span_marker-1.2.3/span_marker/modeling.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/output.py` & `span_marker-1.2.3/span_marker/output.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/spacy_integration.py` & `span_marker-1.2.3/span_marker/spacy_integration.py`

 * *Files 4% similar despite different names*

```diff
@@ -70,15 +70,15 @@
         elif torch.cuda.is_available():
             self.model.to("cuda")
         self.batch_size = batch_size
 
     def __call__(self, doc: Doc) -> Doc:
         """Fill `doc.ents` and `span.label_` using the chosen SpanMarker model."""
         sents = list(doc.sents)
-        inputs = [[token.text for token in sent] for sent in sents]
+        inputs = [[token.text if not token.is_space else "" for token in sent] for sent in sents]
         # use document-level context in the inference if the model was also trained that way
         if self.model.config.trained_with_document_context:
             inputs = Dataset.from_dict(
                 {
                     "tokens": inputs,
                     "document_id": [0] * len(inputs),
                     "sentence_id": range(len(inputs)),
```

### Comparing `span_marker-1.2.2/span_marker/tokenizer.py` & `span_marker-1.2.3/span_marker/tokenizer.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker/trainer.py` & `span_marker-1.2.3/span_marker/trainer.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/span_marker.egg-info/PKG-INFO` & `span_marker-1.2.3/span_marker.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 00000000: 4d65 7461 6461 7461 2d56 6572 7369 6f6e  Metadata-Version
 00000010: 3a20 322e 310d 0a4e 616d 653a 2073 7061  : 2.1..Name: spa
 00000020: 6e2d 6d61 726b 6572 0d0a 5665 7273 696f  n-marker..Versio
-00000030: 6e3a 2031 2e32 2e32 0d0a 5375 6d6d 6172  n: 1.2.2..Summar
+00000030: 6e3a 2031 2e32 2e33 0d0a 5375 6d6d 6172  n: 1.2.3..Summar
 00000040: 793a 204e 616d 6564 2045 6e74 6974 7920  y: Named Entity 
 00000050: 5265 636f 676e 6974 696f 6e20 7573 696e  Recognition usin
 00000060: 6720 5370 616e 204d 6172 6b65 7273 0d0a  g Span Markers..
 00000070: 4175 7468 6f72 3a20 546f 6d20 4161 7273  Author: Tom Aars
 00000080: 656e 0d0a 4d61 696e 7461 696e 6572 3a20  en..Maintainer: 
 00000090: 546f 6d20 4161 7273 656e 0d0a 5072 6f6a  Tom Aarsen..Proj
 000000a0: 6563 742d 5552 4c3a 2044 6f63 756d 656e  ect-URL: Documen
@@ -29,911 +29,947 @@
 000001c0: 436f 6e74 656e 742d 5479 7065 3a20 7465  Content-Type: te
 000001d0: 7874 2f6d 6172 6b64 6f77 6e0d 0a50 726f  xt/markdown..Pro
 000001e0: 7669 6465 732d 4578 7472 613a 2064 6576  vides-Extra: dev
 000001f0: 0d0a 5072 6f76 6964 6573 2d45 7874 7261  ..Provides-Extra
 00000200: 3a20 646f 6373 0d0a 5072 6f76 6964 6573  : docs..Provides
 00000210: 2d45 7874 7261 3a20 7761 6e64 620d 0a4c  -Extra: wandb..L
 00000220: 6963 656e 7365 2d46 696c 653a 204c 4943  icense-File: LIC
-00000230: 454e 5345 0d0a 0d0a 3c68 3120 616c 6967  ENSE....<h1 alig
-00000240: 6e3d 2263 656e 7465 7222 3e0d 0a53 7061  n="center">..Spa
-00000250: 6e4d 6172 6b65 7220 666f 7220 4e61 6d65  nMarker for Name
-00000260: 6420 456e 7469 7479 2052 6563 6f67 6e69  d Entity Recogni
-00000270: 7469 6f6e 0d0a 3c2f 6831 3e0d 0a3c 6469  tion..</h1>..<di
-00000280: 7620 616c 6967 6e3d 2263 656e 7465 7222  v align="center"
-00000290: 3e0d 0a0d 0a5b f09f a497 204d 6f64 656c  >....[.... Model
-000002a0: 735d 2868 7474 7073 3a2f 2f68 7567 6769  s](https://huggi
-000002b0: 6e67 6661 6365 2e63 6f2f 6d6f 6465 6c73  ngface.co/models
-000002c0: 3f6f 7468 6572 3d73 7061 6e2d 6d61 726b  ?other=span-mark
-000002d0: 6572 2920 7c0d 0a5b f09f 9ba0 efb8 8f20  er) |..[....... 
-000002e0: 4765 7474 696e 6720 5374 6172 7465 6420  Getting Started 
-000002f0: 496e 2047 6f6f 676c 6520 436f 6c61 625d  In Google Colab]
-00000300: 2868 7474 7073 3a2f 2f63 6f6c 6162 2e72  (https://colab.r
-00000310: 6573 6561 7263 682e 676f 6f67 6c65 2e63  esearch.google.c
-00000320: 6f6d 2f67 6974 6875 622f 746f 6d61 6172  om/github/tomaar
-00000330: 7365 6e2f 5370 616e 4d61 726b 6572 4e45  sen/SpanMarkerNE
-00000340: 522f 626c 6f62 2f6d 6169 6e2f 6e6f 7465  R/blob/main/note
-00000350: 626f 6f6b 732f 6765 7474 696e 675f 7374  books/getting_st
-00000360: 6172 7465 642e 6970 796e 6229 207c 0d0a  arted.ipynb) |..
-00000370: 5bf0 9f93 8420 446f 6375 6d65 6e74 6174  [.... Documentat
-00000380: 696f 6e5d 2868 7474 7073 3a2f 2f74 6f6d  ion](https://tom
-00000390: 6161 7273 656e 2e67 6974 6875 622e 696f  aarsen.github.io
-000003a0: 2f53 7061 6e4d 6172 6b65 724e 4552 290d  /SpanMarkerNER).
-000003b0: 0a3c 2f64 6976 3e0d 0a0d 0a53 7061 6e4d  .</div>....SpanM
-000003c0: 6172 6b65 7220 6973 2061 2066 7261 6d65  arker is a frame
-000003d0: 776f 726b 2066 6f72 2074 7261 696e 696e  work for trainin
-000003e0: 6720 706f 7765 7266 756c 204e 616d 6564  g powerful Named
-000003f0: 2045 6e74 6974 7920 5265 636f 676e 6974   Entity Recognit
-00000400: 696f 6e20 6d6f 6465 6c73 2075 7369 6e67  ion models using
-00000410: 2066 616d 696c 6961 7220 656e 636f 6465   familiar encode
-00000420: 7273 2073 7563 6820 6173 2042 4552 542c  rs such as BERT,
-00000430: 2052 6f42 4552 5461 2061 6e64 2044 6542   RoBERTa and DeB
-00000440: 4552 5461 2e0d 0a54 6967 6874 6c79 2069  ERTa...Tightly i
-00000450: 6d70 6c65 6d65 6e74 6564 206f 6e20 746f  mplemented on to
-00000460: 7020 6f66 2074 6865 205b f09f a497 2054  p of the [.... T
-00000470: 7261 6e73 666f 726d 6572 735d 2868 7474  ransformers](htt
-00000480: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-00000490: 6875 6767 696e 6766 6163 652f 7472 616e  huggingface/tran
-000004a0: 7366 6f72 6d65 7273 2f29 206c 6962 7261  sformers/) libra
-000004b0: 7279 2c20 5370 616e 4d61 726b 6572 2063  ry, SpanMarker c
-000004c0: 616e 2074 616b 6520 6164 7661 6e74 6167  an take advantag
-000004d0: 6520 6f66 2069 7473 2076 616c 7561 626c  e of its valuabl
-000004e0: 6520 6675 6e63 7469 6f6e 616c 6974 792e  e functionality.
-000004f0: 0d0a 3c21 2d2d 206c 696b 6520 7065 7266  ..<!-- like perf
-00000500: 6f72 6d61 6e63 6520 6461 7368 626f 6172  ormance dashboar
-00000510: 6420 696e 7465 6772 6174 696f 6e2c 2061  d integration, a
-00000520: 7574 6f6d 6174 6963 206d 6978 6564 2070  utomatic mixed p
-00000530: 7265 6369 7369 6f6e 2c20 382d 6269 7420  recision, 8-bit 
-00000540: 696e 6665 7265 6e63 652d 2d3e 0d0a 0d0a  inference-->....
-00000550: 4261 7365 6420 6f6e 2074 6865 205b 504c  Based on the [PL
-00000560: 2d4d 6172 6b65 725d 2868 7474 7073 3a2f  -Marker](https:/
-00000570: 2f61 7278 6976 2e6f 7267 2f70 6466 2f32  /arxiv.org/pdf/2
-00000580: 3130 392e 3036 3036 372e 7064 6629 2070  109.06067.pdf) p
-00000590: 6170 6572 2c20 5370 616e 4d61 726b 6572  aper, SpanMarker
-000005a0: 2062 7265 616b 7320 7468 6520 6d6f 6c64   breaks the mold
-000005b0: 2074 6872 6f75 6768 2069 7473 2061 6363   through its acc
-000005c0: 6573 7369 6269 6c69 7479 2061 6e64 2065  essibility and e
-000005d0: 6173 6520 6f66 2075 7365 2e20 4372 7563  ase of use. Cruc
-000005e0: 6961 6c6c 792c 2053 7061 6e4d 6172 6b65  ially, SpanMarke
-000005f0: 7220 776f 726b 7320 6f75 7420 6f66 2074  r works out of t
-00000600: 6865 2062 6f78 2077 6974 6820 6d61 6e79  he box with many
-00000610: 2063 6f6d 6d6f 6e20 656e 636f 6465 7273   common encoders
-00000620: 2073 7563 6820 6173 2060 6265 7274 2d62   such as `bert-b
-00000630: 6173 652d 6361 7365 6460 2061 6e64 2060  ase-cased` and `
-00000640: 726f 6265 7274 612d 6c61 7267 6560 2c20  roberta-large`, 
-00000650: 616e 6420 6175 746f 6d61 7469 6361 6c6c  and automaticall
-00000660: 7920 776f 726b 7320 7769 7468 2064 6174  y works with dat
-00000670: 6173 6574 7320 7573 696e 6720 7468 6520  asets using the 
-00000680: 6049 4f42 602c 2060 494f 4232 602c 2060  `IOB`, `IOB2`, `
-00000690: 4249 4f45 5360 2c20 6042 494c 4f55 6020  BIOES`, `BILOU` 
-000006a0: 6f72 206e 6f20 6c61 6265 6c20 616e 6e6f  or no label anno
-000006b0: 7461 7469 6f6e 2073 6368 656d 652e 0d0a  tation scheme...
-000006c0: 0d0a 4164 6469 7469 6f6e 616c 6c79 2c20  ..Additionally, 
-000006d0: 7468 6520 5370 616e 4d61 726b 6572 206c  the SpanMarker l
-000006e0: 6962 7261 7279 2068 6173 2062 6565 6e20  ibrary has been 
-000006f0: 696e 7465 6772 6174 6564 2077 6974 6820  integrated with 
-00000700: 7468 6520 4875 6767 696e 6720 4661 6365  the Hugging Face
-00000710: 2048 7562 2061 6e64 2074 6865 2048 7567   Hub and the Hug
-00000720: 6769 6e67 2046 6163 6520 496e 6665 7265  ging Face Infere
-00000730: 6e63 6520 4150 492e 2053 6565 2074 6865  nce API. See the
-00000740: 2053 7061 6e4d 6172 6b65 7220 646f 6375   SpanMarker docu
-00000750: 6d65 6e74 6174 696f 6e20 6f6e 205b 4875  mentation on [Hu
-00000760: 6767 696e 6720 4661 6365 5d28 6874 7470  gging Face](http
-00000770: 733a 2f2f 6875 6767 696e 6766 6163 652e  s://huggingface.
-00000780: 636f 2f64 6f63 732f 6875 622f 7370 616e  co/docs/hub/span
-00000790: 5f6d 6172 6b65 7229 206f 7220 7365 6520  _marker) or see 
-000007a0: 5b61 6c6c 2053 7061 6e4d 6172 6b65 7220  [all SpanMarker 
-000007b0: 6d6f 6465 6c73 206f 6e20 7468 6520 4875  models on the Hu
-000007c0: 6767 696e 6720 4661 6365 2048 7562 5d28  gging Face Hub](
-000007d0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
-000007e0: 6163 652e 636f 2f6d 6f64 656c 733f 6c69  ace.co/models?li
-000007f0: 6272 6172 793d 7370 616e 2d6d 6172 6b65  brary=span-marke
-00000800: 7229 2e0d 0a0d 0a54 6872 6f75 6768 2074  r).....Through t
-00000810: 6865 2049 6e66 6572 656e 6365 2041 5049  he Inference API
-00000820: 2069 6e74 6567 7261 7469 6f6e 2c20 7573   integration, us
-00000830: 6572 7320 6361 6e20 7465 7374 2061 6e79  ers can test any
-00000840: 2053 7061 6e4d 6172 6b65 7220 6d6f 6465   SpanMarker mode
-00000850: 6c20 6f6e 2074 6865 2048 7567 6769 6e67  l on the Hugging
-00000860: 2046 6163 6520 4875 6220 666f 7220 6672   Face Hub for fr
-00000870: 6565 2075 7369 6e67 2061 2077 6964 6765  ee using a widge
-00000880: 7420 6f6e 2074 6865 205b 6d6f 6465 6c20  t on the [model 
-00000890: 7061 6765 5d28 6874 7470 733a 2f2f 6875  page](https://hu
-000008a0: 6767 696e 6766 6163 652e 636f 2f74 6f6d  ggingface.co/tom
-000008b0: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
-000008c0: 6572 2d62 6572 742d 6261 7365 2d66 6577  er-bert-base-few
-000008d0: 6e65 7264 2d66 696e 652d 7375 7065 7229  nerd-fine-super)
-000008e0: 2e20 4675 7274 6865 726d 6f72 652c 2065  . Furthermore, e
-000008f0: 6163 6820 7075 626c 6963 2053 7061 6e4d  ach public SpanM
-00000900: 6172 6b65 7220 6d6f 6465 6c20 6f66 6665  arker model offe
-00000910: 7273 2061 2066 7265 6520 4150 4920 666f  rs a free API fo
-00000920: 7220 6661 7374 2070 726f 746f 7479 7069  r fast prototypi
-00000930: 6e67 2061 6e64 2063 616e 2062 6520 6465  ng and can be de
-00000940: 706c 6f79 6564 2074 6f20 7072 6f64 7563  ployed to produc
-00000950: 7469 6f6e 2075 7369 6e67 2048 7567 6769  tion using Huggi
-00000960: 6e67 2046 6163 6520 496e 6665 7265 6e63  ng Face Inferenc
-00000970: 6520 456e 6470 6f69 6e74 732e 0d0a 0d0a  e Endpoints.....
-00000980: 7c20 496e 6665 7265 6e63 6520 4150 4920  | Inference API 
-00000990: 5769 6467 6574 2028 6f6e 2061 206d 6f64  Widget (on a mod
-000009a0: 656c 2070 6167 6529 207c 2046 7265 6520  el page) | Free 
-000009b0: 496e 6665 7265 6e63 6520 4150 4920 2860  Inference API (`
-000009c0: 4465 706c 6f79 6020 3e20 6049 6e66 6572  Deploy` > `Infer
-000009d0: 656e 6365 2041 5049 6020 6f6e 2061 206d  ence API` on a m
-000009e0: 6f64 656c 2070 6167 6529 207c 0d0a 7c20  odel page) |..| 
-000009f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d20 7c20  ------------- | 
-00000a00: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d20 7c0d  ------------- |.
-00000a10: 0a7c 2020 215b 696d 6167 655d 2868 7474  .|  ![image](htt
-00000a20: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-00000a30: 746f 6d61 6172 7365 6e2f 5370 616e 4d61  tomaarsen/SpanMa
-00000a40: 726b 6572 4e45 522f 6173 7365 7473 2f33  rkerNER/assets/3
-00000a50: 3736 3231 3439 312f 3233 3430 3738 6237  7621491/234078b7
-00000a60: 2d32 3263 382d 3439 3163 2d38 3638 362d  -22c8-491c-8686-
-00000a70: 6661 6363 6433 3934 6636 3833 2920 7c20  faccd394f683) | 
-00000a80: 2021 5b69 6d61 6765 5d28 6874 7470 733a   ![image](https:
-00000a90: 2f2f 6769 7468 7562 2e63 6f6d 2f74 6f6d  //github.com/tom
-00000aa0: 6161 7273 656e 2f53 7061 6e4d 6172 6b65  aarsen/SpanMarke
-00000ab0: 724e 4552 2f61 7373 6574 732f 3337 3632  rNER/assets/3762
-00000ac0: 3134 3931 2f34 3130 6535 3139 312d 3933  1491/410e5191-93
-00000ad0: 3534 2d34 6532 372d 6237 3138 2d32 6436  54-4e27-b718-2d6
-00000ae0: 3961 6636 3738 6562 3729 207c 0d0a 0d0a  9af678eb7) |....
-00000af0: 2323 2044 6f63 756d 656e 7461 7469 6f6e  ## Documentation
-00000b00: 0d0a 4665 656c 2066 7265 6520 746f 2068  ..Feel free to h
-00000b10: 6176 6520 6120 6c6f 6f6b 2061 7420 7468  ave a look at th
-00000b20: 6520 5b64 6f63 756d 656e 7461 7469 6f6e  e [documentation
-00000b30: 5d28 6874 7470 733a 2f2f 746f 6d61 6172  ](https://tomaar
-00000b40: 7365 6e2e 6769 7468 7562 2e69 6f2f 5370  sen.github.io/Sp
-00000b50: 616e 4d61 726b 6572 4e45 5229 2e0d 0a0d  anMarkerNER)....
-00000b60: 0a23 2320 496e 7374 616c 6c61 7469 6f6e  .## Installation
-00000b70: 0d0a 596f 7520 6d61 7920 696e 7374 616c  ..You may instal
-00000b80: 6c20 7468 6520 5b60 7370 616e 5f6d 6172  l the [`span_mar
-00000b90: 6b65 7260 5d28 6874 7470 733a 2f2f 7079  ker`](https://py
-00000ba0: 7069 2e6f 7267 2f70 726f 6a65 6374 2f73  pi.org/project/s
-00000bb0: 7061 6e2d 6d61 726b 6572 2920 5079 7468  pan-marker) Pyth
-00000bc0: 6f6e 206d 6f64 756c 6520 7669 6120 6070  on module via `p
-00000bd0: 6970 6020 6c69 6b65 2073 6f3a 0d0a 6060  ip` like so:..``
-00000be0: 600d 0a70 6970 2069 6e73 7461 6c6c 2073  `..pip install s
-00000bf0: 7061 6e5f 6d61 726b 6572 0d0a 6060 600d  pan_marker..```.
-00000c00: 0a0d 0a23 2320 5175 6963 6b20 5374 6172  ...## Quick Star
-00000c10: 740d 0a23 2323 2054 7261 696e 696e 670d  t..### Training.
-00000c20: 0a50 6c65 6173 6520 6861 7665 2061 206c  .Please have a l
-00000c30: 6f6f 6b20 6174 206f 7572 205b 4765 7474  ook at our [Gett
-00000c40: 696e 6720 5374 6172 7465 645d 286e 6f74  ing Started](not
-00000c50: 6562 6f6f 6b73 2f67 6574 7469 6e67 5f73  ebooks/getting_s
-00000c60: 7461 7274 6564 2e69 7079 6e62 2920 6e6f  tarted.ipynb) no
-00000c70: 7465 626f 6f6b 2066 6f72 2064 6574 6169  tebook for detai
-00000c80: 6c73 206f 6e20 686f 7720 5370 616e 4d61  ls on how SpanMa
-00000c90: 726b 6572 2069 7320 636f 6d6d 6f6e 6c79  rker is commonly
-00000ca0: 2075 7365 642e 2049 7420 6578 706c 6169   used. It explai
-00000cb0: 6e73 2074 6865 2066 6f6c 6c6f 7769 6e67  ns the following
-00000cc0: 2073 6e69 7070 6574 2069 6e20 6d6f 7265   snippet in more
-00000cd0: 2064 6574 6169 6c2e 2041 6c74 6572 6e61   detail. Alterna
-00000ce0: 7469 7665 6c79 2c20 6861 7665 2061 206c  tively, have a l
-00000cf0: 6f6f 6b20 6174 2074 6865 205b 7472 6169  ook at the [trai
-00000d00: 6e69 6e67 2073 6372 6970 7473 5d28 7472  ning scripts](tr
-00000d10: 6169 6e69 6e67 5f73 6372 6970 7473 2920  aining_scripts) 
-00000d20: 7468 6174 2068 6176 6520 6265 656e 2073  that have been s
-00000d30: 7563 6365 7373 6675 6c6c 7920 7573 6564  uccessfully used
-00000d40: 2069 6e20 7468 6520 7061 7374 2e0d 0a0d   in the past....
-00000d50: 0a7c 2043 6f6c 6162 2020 2020 2020 2020  .| Colab        
-00000d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e20: 207c 204b 6167 676c 6520 2020 2020 2020   | Kaggle       
-00000e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ef0: 2020 2020 2020 7c20 4772 6164 6965 6e74        | Gradient
-00000f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000230: 454e 5345 0d0a 0d0a 3c64 6976 2061 6c69  ENSE....<div ali
+00000240: 676e 3d22 6365 6e74 6572 223e 0d0a 3c68  gn="center">..<h
+00000250: 313e 0d0a 5370 616e 4d61 726b 6572 2066  1>..SpanMarker f
+00000260: 6f72 204e 616d 6564 2045 6e74 6974 7920  or Named Entity 
+00000270: 5265 636f 676e 6974 696f 6e0d 0a3c 2f68  Recognition..</h
+00000280: 313e 0d0a 3c61 2068 7265 663d 2268 7474  1>..<a href="htt
+00000290: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+000002a0: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
+000002b0: 616e 2d6d 6172 6b65 722d 726f 6265 7274  an-marker-robert
+000002c0: 612d 6c61 7267 652d 6f6e 746f 6e6f 7465  a-large-ontonote
+000002d0: 7335 2220 7461 7267 6574 3d22 5f62 6c61  s5" target="_bla
+000002e0: 6e6b 223e 0d0a 2020 2020 3c69 6d67 2073  nk">..    <img s
+000002f0: 7263 3d22 6874 7470 733a 2f2f 6769 7468  rc="https://gith
+00000300: 7562 2e63 6f6d 2f74 6f6d 6161 7273 656e  ub.com/tomaarsen
+00000310: 2f53 7061 6e4d 6172 6b65 724e 4552 2f61  /SpanMarkerNER/a
+00000320: 7373 6574 732f 3337 3632 3134 3931 2f63  ssets/37621491/c
+00000330: 3736 6436 3339 332d 6262 3062 2d34 3463  76d6393-bb0b-44c
+00000340: 332d 3934 3132 2d66 6439 6338 3331 3364  3-9412-fd9c8313d
+00000350: 6363 3122 3e0d 0a3c 2f61 3e0d 0a0d 0a5b  cc1">..</a>....[
+00000360: f09f a497 204d 6f64 656c 735d 2868 7474  .... Models](htt
+00000370: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+00000380: 2e63 6f2f 6d6f 6465 6c73 3f6c 6962 7261  .co/models?libra
+00000390: 7279 3d73 7061 6e2d 6d61 726b 6572 2920  ry=span-marker) 
+000003a0: 7c0d 0a5b f09f 9ba0 efb8 8f20 4765 7474  |..[....... Gett
+000003b0: 696e 6720 5374 6172 7465 6420 496e 2047  ing Started In G
+000003c0: 6f6f 676c 6520 436f 6c61 625d 2868 7474  oogle Colab](htt
+000003d0: 7073 3a2f 2f63 6f6c 6162 2e72 6573 6561  ps://colab.resea
+000003e0: 7263 682e 676f 6f67 6c65 2e63 6f6d 2f67  rch.google.com/g
+000003f0: 6974 6875 622f 746f 6d61 6172 7365 6e2f  ithub/tomaarsen/
+00000400: 5370 616e 4d61 726b 6572 4e45 522f 626c  SpanMarkerNER/bl
+00000410: 6f62 2f6d 6169 6e2f 6e6f 7465 626f 6f6b  ob/main/notebook
+00000420: 732f 6765 7474 696e 675f 7374 6172 7465  s/getting_starte
+00000430: 642e 6970 796e 6229 207c 0d0a 5bf0 9f93  d.ipynb) |..[...
+00000440: 8420 446f 6375 6d65 6e74 6174 696f 6e5d  . Documentation]
+00000450: 2868 7474 7073 3a2f 2f74 6f6d 6161 7273  (https://tomaars
+00000460: 656e 2e67 6974 6875 622e 696f 2f53 7061  en.github.io/Spa
+00000470: 6e4d 6172 6b65 724e 4552 290d 0a3c 2f64  nMarkerNER)..</d
+00000480: 6976 3e0d 0a0d 0a53 7061 6e4d 6172 6b65  iv>....SpanMarke
+00000490: 7220 6973 2061 2066 7261 6d65 776f 726b  r is a framework
+000004a0: 2066 6f72 2074 7261 696e 696e 6720 706f   for training po
+000004b0: 7765 7266 756c 204e 616d 6564 2045 6e74  werful Named Ent
+000004c0: 6974 7920 5265 636f 676e 6974 696f 6e20  ity Recognition 
+000004d0: 6d6f 6465 6c73 2075 7369 6e67 2066 616d  models using fam
+000004e0: 696c 6961 7220 656e 636f 6465 7273 2073  iliar encoders s
+000004f0: 7563 6820 6173 2042 4552 542c 2052 6f42  uch as BERT, RoB
+00000500: 4552 5461 2061 6e64 2044 6542 4552 5461  ERTa and DeBERTa
+00000510: 2e0d 0a42 7569 6c74 206f 6e20 746f 7020  ...Built on top 
+00000520: 6f66 2074 6865 2066 616d 696c 6961 7220  of the familiar 
+00000530: 5bf0 9fa4 9720 5472 616e 7366 6f72 6d65  [.... Transforme
+00000540: 7273 5d28 6874 7470 733a 2f2f 6769 7468  rs](https://gith
+00000550: 7562 2e63 6f6d 2f68 7567 6769 6e67 6661  ub.com/huggingfa
+00000560: 6365 2f74 7261 6e73 666f 726d 6572 7329  ce/transformers)
+00000570: 206c 6962 7261 7279 2c20 5370 616e 4d61   library, SpanMa
+00000580: 726b 6572 2069 6e68 6572 6974 7320 6120  rker inherits a 
+00000590: 7769 6465 2072 616e 6765 206f 6620 706f  wide range of po
+000005a0: 7765 7266 756c 2066 756e 6374 696f 6e61  werful functiona
+000005b0: 6c69 7469 6573 2c20 7375 6368 2061 7320  lities, such as 
+000005c0: 6561 7369 6c79 206c 6f61 6469 6e67 2061  easily loading a
+000005d0: 6e64 2073 6176 696e 6720 6d6f 6465 6c73  nd saving models
+000005e0: 2c20 6879 7065 7270 6172 616d 6574 6572  , hyperparameter
+000005f0: 206f 7074 696d 697a 6174 696f 6e2c 2061   optimization, a
+00000600: 7574 6f6d 6174 6963 206c 6f67 6769 6e67  utomatic logging
+00000610: 2069 6e20 7661 7269 6f75 7320 746f 6f6c   in various tool
+00000620: 732c 2063 6865 636b 706f 696e 7469 6e67  s, checkpointing
+00000630: 2c20 6361 6c6c 6261 636b 732c 206d 6978  , callbacks, mix
+00000640: 6564 2070 7265 6369 7369 6f6e 2074 7261  ed precision tra
+00000650: 696e 696e 672c 2038 2d62 6974 2069 6e66  ining, 8-bit inf
+00000660: 6572 656e 6365 2c20 616e 6420 6d6f 7265  erence, and more
+00000670: 2e0d 0a0d 0a3c 212d 2d54 6967 6874 6c79  .....<!--Tightly
+00000680: 2069 6d70 6c65 6d65 6e74 6564 206f 6e20   implemented on 
+00000690: 746f 7020 6f66 2074 6865 205b f09f a497  top of the [....
+000006a0: 2054 7261 6e73 666f 726d 6572 735d 2868   Transformers](h
+000006b0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
+000006c0: 6d2f 6875 6767 696e 6766 6163 652f 7472  m/huggingface/tr
+000006d0: 616e 7366 6f72 6d65 7273 2f29 206c 6962  ansformers/) lib
+000006e0: 7261 7279 2c20 5370 616e 4d61 726b 6572  rary, SpanMarker
+000006f0: 2063 616e 2074 616b 6520 6164 7661 6e74   can take advant
+00000700: 6167 6520 6f66 2069 7473 2076 616c 7561  age of its valua
+00000710: 626c 6520 6675 6e63 7469 6f6e 616c 6974  ble functionalit
+00000720: 792e 2d2d 3e0d 0a3c 212d 2d20 6c69 6b65  y.-->..<!-- like
+00000730: 2070 6572 666f 726d 616e 6365 2064 6173   performance das
+00000740: 6862 6f61 7264 2069 6e74 6567 7261 7469  hboard integrati
+00000750: 6f6e 2c20 6175 746f 6d61 7469 6320 6d69  on, automatic mi
+00000760: 7865 6420 7072 6563 6973 696f 6e2c 2038  xed precision, 8
+00000770: 2d62 6974 2069 6e66 6572 656e 6365 2d2d  -bit inference--
+00000780: 3e0d 0a0d 0a42 6173 6564 206f 6e20 7468  >....Based on th
+00000790: 6520 5b50 4c2d 4d61 726b 6572 5d28 6874  e [PL-Marker](ht
+000007a0: 7470 733a 2f2f 6172 7869 762e 6f72 672f  tps://arxiv.org/
+000007b0: 7064 662f 3231 3039 2e30 3630 3637 2e70  pdf/2109.06067.p
+000007c0: 6466 2920 7061 7065 722c 2053 7061 6e4d  df) paper, SpanM
+000007d0: 6172 6b65 7220 6272 6561 6b73 2074 6865  arker breaks the
+000007e0: 206d 6f6c 6420 7468 726f 7567 6820 6974   mold through it
+000007f0: 7320 6163 6365 7373 6962 696c 6974 7920  s accessibility 
+00000800: 616e 6420 6561 7365 206f 6620 7573 652e  and ease of use.
+00000810: 2043 7275 6369 616c 6c79 2c20 5370 616e   Crucially, Span
+00000820: 4d61 726b 6572 2077 6f72 6b73 206f 7574  Marker works out
+00000830: 206f 6620 7468 6520 626f 7820 7769 7468   of the box with
+00000840: 206d 616e 7920 636f 6d6d 6f6e 2065 6e63   many common enc
+00000850: 6f64 6572 7320 7375 6368 2061 7320 6062  oders such as `b
+00000860: 6572 742d 6261 7365 2d63 6173 6564 6020  ert-base-cased` 
+00000870: 616e 6420 6072 6f62 6572 7461 2d6c 6172  and `roberta-lar
+00000880: 6765 602c 2061 6e64 2061 7574 6f6d 6174  ge`, and automat
+00000890: 6963 616c 6c79 2077 6f72 6b73 2077 6974  ically works wit
+000008a0: 6820 6461 7461 7365 7473 2075 7369 6e67  h datasets using
+000008b0: 2074 6865 2060 494f 4260 2c20 6049 4f42   the `IOB`, `IOB
+000008c0: 3260 2c20 6042 494f 4553 602c 2060 4249  2`, `BIOES`, `BI
+000008d0: 4c4f 5560 206f 7220 6e6f 206c 6162 656c  LOU` or no label
+000008e0: 2061 6e6e 6f74 6174 696f 6e20 7363 6865   annotation sche
+000008f0: 6d65 2e0d 0a0d 0a41 6464 6974 696f 6e61  me.....Additiona
+00000900: 6c6c 792c 2074 6865 2053 7061 6e4d 6172  lly, the SpanMar
+00000910: 6b65 7220 6c69 6272 6172 7920 6861 7320  ker library has 
+00000920: 6265 656e 2069 6e74 6567 7261 7465 6420  been integrated 
+00000930: 7769 7468 2074 6865 2048 7567 6769 6e67  with the Hugging
+00000940: 2046 6163 6520 4875 6220 616e 6420 7468   Face Hub and th
+00000950: 6520 4875 6767 696e 6720 4661 6365 2049  e Hugging Face I
+00000960: 6e66 6572 656e 6365 2041 5049 2e20 5365  nference API. Se
+00000970: 6520 7468 6520 5370 616e 4d61 726b 6572  e the SpanMarker
+00000980: 2064 6f63 756d 656e 7461 7469 6f6e 206f   documentation o
+00000990: 6e20 5b48 7567 6769 6e67 2046 6163 655d  n [Hugging Face]
+000009a0: 2868 7474 7073 3a2f 2f68 7567 6769 6e67  (https://hugging
+000009b0: 6661 6365 2e63 6f2f 646f 6373 2f68 7562  face.co/docs/hub
+000009c0: 2f73 7061 6e5f 6d61 726b 6572 2920 6f72  /span_marker) or
+000009d0: 2073 6565 205b 616c 6c20 5370 616e 4d61   see [all SpanMa
+000009e0: 726b 6572 206d 6f64 656c 7320 6f6e 2074  rker models on t
+000009f0: 6865 2048 7567 6769 6e67 2046 6163 6520  he Hugging Face 
+00000a00: 4875 625d 2868 7474 7073 3a2f 2f68 7567  Hub](https://hug
+00000a10: 6769 6e67 6661 6365 2e63 6f2f 6d6f 6465  gingface.co/mode
+00000a20: 6c73 3f6c 6962 7261 7279 3d73 7061 6e2d  ls?library=span-
+00000a30: 6d61 726b 6572 292e 0d0a 5468 726f 7567  marker)...Throug
+00000a40: 6820 7468 6520 496e 6665 7265 6e63 6520  h the Inference 
+00000a50: 4150 4920 696e 7465 6772 6174 696f 6e2c  API integration,
+00000a60: 2075 7365 7273 2063 616e 2074 6573 7420   users can test 
+00000a70: 616e 7920 5370 616e 4d61 726b 6572 206d  any SpanMarker m
+00000a80: 6f64 656c 206f 6e20 7468 6520 4875 6767  odel on the Hugg
+00000a90: 696e 6720 4661 6365 2048 7562 2066 6f72  ing Face Hub for
+00000aa0: 2066 7265 6520 7573 696e 6720 6120 7769   free using a wi
+00000ab0: 6467 6574 206f 6e20 7468 6520 5b6d 6f64  dget on the [mod
+00000ac0: 656c 2070 6167 655d 2868 7474 7073 3a2f  el page](https:/
+00000ad0: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
+00000ae0: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
+00000af0: 6172 6b65 722d 6265 7274 2d62 6173 652d  arker-bert-base-
+00000b00: 6665 776e 6572 642d 6669 6e65 2d73 7570  fewnerd-fine-sup
+00000b10: 6572 292e 2046 7572 7468 6572 6d6f 7265  er). Furthermore
+00000b20: 2c20 6561 6368 2070 7562 6c69 6320 5370  , each public Sp
+00000b30: 616e 4d61 726b 6572 206d 6f64 656c 206f  anMarker model o
+00000b40: 6666 6572 7320 6120 6672 6565 2041 5049  ffers a free API
+00000b50: 2066 6f72 2066 6173 7420 7072 6f74 6f74   for fast protot
+00000b60: 7970 696e 6720 616e 6420 6361 6e20 6265  yping and can be
+00000b70: 2064 6570 6c6f 7965 6420 746f 2070 726f   deployed to pro
+00000b80: 6475 6374 696f 6e20 7573 696e 6720 4875  duction using Hu
+00000b90: 6767 696e 6720 4661 6365 2049 6e66 6572  gging Face Infer
+00000ba0: 656e 6365 2045 6e64 706f 696e 7473 2e0d  ence Endpoints..
+00000bb0: 0a0d 0a7c 2049 6e66 6572 656e 6365 2041  ...| Inference A
+00000bc0: 5049 2057 6964 6765 7420 286f 6e20 6120  PI Widget (on a 
+00000bd0: 6d6f 6465 6c20 7061 6765 2920 7c20 4672  model page) | Fr
+00000be0: 6565 2049 6e66 6572 656e 6365 2041 5049  ee Inference API
+00000bf0: 2028 6044 6570 6c6f 7960 203e 2060 496e   (`Deploy` > `In
+00000c00: 6665 7265 6e63 6520 4150 4960 206f 6e20  ference API` on 
+00000c10: 6120 6d6f 6465 6c20 7061 6765 2920 7c0d  a model page) |.
+00000c20: 0a7c 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  .| -------------
+00000c30: 207c 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d   | -------------
+00000c40: 207c 0d0a 7c20 2021 5b69 6d61 6765 5d28   |..|  ![image](
+00000c50: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
+00000c60: 6f6d 2f74 6f6d 6161 7273 656e 2f53 7061  om/tomaarsen/Spa
+00000c70: 6e4d 6172 6b65 724e 4552 2f61 7373 6574  nMarkerNER/asset
+00000c80: 732f 3337 3632 3134 3931 2f32 3334 3037  s/37621491/23407
+00000c90: 3862 372d 3232 6338 2d34 3931 632d 3836  8b7-22c8-491c-86
+00000ca0: 3836 2d66 6163 6364 3339 3466 3638 3329  86-faccd394f683)
+00000cb0: 207c 2020 215b 696d 6167 655d 2868 7474   |  ![image](htt
+00000cc0: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
+00000cd0: 746f 6d61 6172 7365 6e2f 5370 616e 4d61  tomaarsen/SpanMa
+00000ce0: 726b 6572 4e45 522f 6173 7365 7473 2f33  rkerNER/assets/3
+00000cf0: 3736 3231 3439 312f 3431 3065 3531 3931  7621491/410e5191
+00000d00: 2d39 3335 342d 3465 3237 2d62 3731 382d  -9354-4e27-b718-
+00000d10: 3264 3639 6166 3637 3865 6237 2920 7c0d  2d69af678eb7) |.
+00000d20: 0a0d 0a23 2320 446f 6375 6d65 6e74 6174  ...## Documentat
+00000d30: 696f 6e0d 0a46 6565 6c20 6672 6565 2074  ion..Feel free t
+00000d40: 6f20 6861 7665 2061 206c 6f6f 6b20 6174  o have a look at
+00000d50: 2074 6865 205b 646f 6375 6d65 6e74 6174   the [documentat
+00000d60: 696f 6e5d 2868 7474 7073 3a2f 2f74 6f6d  ion](https://tom
+00000d70: 6161 7273 656e 2e67 6974 6875 622e 696f  aarsen.github.io
+00000d80: 2f53 7061 6e4d 6172 6b65 724e 4552 292e  /SpanMarkerNER).
+00000d90: 0d0a 0d0a 2323 2049 6e73 7461 6c6c 6174  ....## Installat
+00000da0: 696f 6e0d 0a59 6f75 206d 6179 2069 6e73  ion..You may ins
+00000db0: 7461 6c6c 2074 6865 205b 6073 7061 6e5f  tall the [`span_
+00000dc0: 6d61 726b 6572 605d 2868 7474 7073 3a2f  marker`](https:/
+00000dd0: 2f70 7970 692e 6f72 672f 7072 6f6a 6563  /pypi.org/projec
+00000de0: 742f 7370 616e 2d6d 6172 6b65 7229 2050  t/span-marker) P
+00000df0: 7974 686f 6e20 6d6f 6475 6c65 2076 6961  ython module via
+00000e00: 2060 7069 7060 206c 696b 6520 736f 3a0d   `pip` like so:.
+00000e10: 0a60 6060 0d0a 7069 7020 696e 7374 616c  .```..pip instal
+00000e20: 6c20 7370 616e 5f6d 6172 6b65 720d 0a60  l span_marker..`
+00000e30: 6060 0d0a 0d0a 2323 2051 7569 636b 2053  ``....## Quick S
+00000e40: 7461 7274 0d0a 2323 2320 5472 6169 6e69  tart..### Traini
+00000e50: 6e67 0d0a 506c 6561 7365 2068 6176 6520  ng..Please have 
+00000e60: 6120 6c6f 6f6b 2061 7420 6f75 7220 5b47  a look at our [G
+00000e70: 6574 7469 6e67 2053 7461 7274 6564 5d28  etting Started](
+00000e80: 6e6f 7465 626f 6f6b 732f 6765 7474 696e  notebooks/gettin
+00000e90: 675f 7374 6172 7465 642e 6970 796e 6229  g_started.ipynb)
+00000ea0: 206e 6f74 6562 6f6f 6b20 666f 7220 6465   notebook for de
+00000eb0: 7461 696c 7320 6f6e 2068 6f77 2053 7061  tails on how Spa
+00000ec0: 6e4d 6172 6b65 7220 6973 2063 6f6d 6d6f  nMarker is commo
+00000ed0: 6e6c 7920 7573 6564 2e20 4974 2065 7870  nly used. It exp
+00000ee0: 6c61 696e 7320 7468 6520 666f 6c6c 6f77  lains the follow
+00000ef0: 696e 6720 736e 6970 7065 7420 696e 206d  ing snippet in m
+00000f00: 6f72 6520 6465 7461 696c 2e20 416c 7465  ore detail. Alte
+00000f10: 726e 6174 6976 656c 792c 2068 6176 6520  rnatively, have 
+00000f20: 6120 6c6f 6f6b 2061 7420 7468 6520 5b74  a look at the [t
+00000f30: 7261 696e 696e 6720 7363 7269 7074 735d  raining scripts]
+00000f40: 2874 7261 696e 696e 675f 7363 7269 7074  (training_script
+00000f50: 7329 2074 6861 7420 6861 7665 2062 6565  s) that have bee
+00000f60: 6e20 7375 6363 6573 7366 756c 6c79 2075  n successfully u
+00000f70: 7365 6420 696e 2074 6865 2070 6173 742e  sed in the past.
+00000f80: 0d0a 0d0a 7c20 436f 6c61 6220 2020 2020  ....| Colab     
 00000f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000fb0: 2020 2020 2020 2020 207c 2053 7475 6469           | Studi
-00000fc0: 6f20 4c61 6220 2020 2020 2020 2020 2020  o Lab           
+00000fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00000ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001000: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001010: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001020: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001030: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001050: 2020 2020 7c20 4b61 6767 6c65 2020 2020      | Kaggle    
 00001060: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001070: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00001080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001090: 2020 7c0d 0a7c 3a2d 2d2d 2d2d 2d2d 2d2d    |..|:---------
-000010a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000010f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001100: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001110: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001120: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001130: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001140: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001150: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001160: 2d2d 2d2d 2d7c 3a2d 2d2d 2d2d 2d2d 2d2d  -----|:---------
-00001170: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001180: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001190: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000011f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001200: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001210: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001220: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001230: 2d2d 2d2d 2d2d 2d2d 2d2d 7c3a 2d2d 2d2d  ----------|:----
-00001240: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001250: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001260: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001270: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001280: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001290: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001120: 2020 2020 2020 2020 207c 2047 7261 6469           | Gradi
+00001130: 656e 7420 2020 2020 2020 2020 2020 2020  ent             
+00001140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011e0: 2020 2020 2020 2020 2020 2020 7c20 5374              | St
+000011f0: 7564 696f 204c 6162 2020 2020 2020 2020  udio Lab        
+00001200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000012a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000012b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000012c0: 2020 2020 207c 0d0a 7c3a 2d2d 2d2d 2d2d       |..|:------
 000012d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000012e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000012f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d7c 3a2d  -------------|:-
+000012f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001300: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001310: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001320: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001330: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001340: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001350: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001360: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001370: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 00001380: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001390: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001390: 2d2d 2d2d 2d2d 2d2d 7c3a 2d2d 2d2d 2d2d  --------|:------
 000013a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000013b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
 000013c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-000013d0: 2d2d 2d2d 2d2d 7c0d 0a7c 205b 215b 4f70  ------|..| [![Op
-000013e0: 656e 2049 6e20 436f 6c61 625d 2868 7474  en In Colab](htt
-000013f0: 7073 3a2f 2f63 6f6c 6162 2e72 6573 6561  ps://colab.resea
-00001400: 7263 682e 676f 6f67 6c65 2e63 6f6d 2f61  rch.google.com/a
-00001410: 7373 6574 732f 636f 6c61 622d 6261 6467  ssets/colab-badg
-00001420: 652e 7376 6729 5d28 6874 7470 733a 2f2f  e.svg)](https://
-00001430: 636f 6c61 622e 7265 7365 6172 6368 2e67  colab.research.g
-00001440: 6f6f 676c 652e 636f 6d2f 6769 7468 7562  oogle.com/github
-00001450: 2f74 6f6d 6161 7273 656e 2f53 7061 6e4d  /tomaarsen/SpanM
-00001460: 6172 6b65 724e 4552 2f62 6c6f 622f 6d61  arkerNER/blob/ma
-00001470: 696e 2f6e 6f74 6562 6f6f 6b73 2f67 6574  in/notebooks/get
-00001480: 7469 6e67 5f73 7461 7274 6564 2e69 7079  ting_started.ipy
-00001490: 6e62 2920 2020 2020 2020 2020 2020 2020  nb)             
-000014a0: 2020 2020 2020 2020 2020 7c20 5b21 5b4b            | [![K
-000014b0: 6167 676c 655d 2868 7474 7073 3a2f 2f6b  aggle](https://k
-000014c0: 6167 676c 652e 636f 6d2f 7374 6174 6963  aggle.com/static
-000014d0: 2f69 6d61 6765 732f 6f70 656e 2d69 6e2d  /images/open-in-
-000014e0: 6b61 6767 6c65 2e73 7667 295d 2868 7474  kaggle.svg)](htt
-000014f0: 7073 3a2f 2f6b 6167 676c 652e 636f 6d2f  ps://kaggle.com/
-00001500: 6b65 726e 656c 732f 7765 6c63 6f6d 653f  kernels/welcome?
-00001510: 7372 633d 6874 7470 733a 2f2f 6769 7468  src=https://gith
-00001520: 7562 2e63 6f6d 2f74 6f6d 6161 7273 656e  ub.com/tomaarsen
-00001530: 2f53 7061 6e4d 6172 6b65 724e 4552 2f62  /SpanMarkerNER/b
-00001540: 6c6f 622f 6d61 696e 2f6e 6f74 6562 6f6f  lob/main/noteboo
-00001550: 6b73 2f67 6574 7469 6e67 5f73 7461 7274  ks/getting_start
-00001560: 6564 2e69 7079 6e62 2920 2020 2020 2020  ed.ipynb)       
-00001570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001580: 7c20 5b21 5b47 7261 6469 656e 745d 2868  | [![Gradient](h
-00001590: 7474 7073 3a2f 2f61 7373 6574 732e 7061  ttps://assets.pa
-000015a0: 7065 7273 7061 6365 2e69 6f2f 696d 672f  perspace.io/img/
-000015b0: 6772 6164 6965 6e74 2d62 6164 6765 2e73  gradient-badge.s
-000015c0: 7667 295d 2868 7474 7073 3a2f 2f63 6f6e  vg)](https://con
-000015d0: 736f 6c65 2e70 6170 6572 7370 6163 652e  sole.paperspace.
-000015e0: 636f 6d2f 6769 7468 7562 2f74 6f6d 6161  com/github/tomaa
-000015f0: 7273 656e 2f53 7061 6e4d 6172 6b65 724e  rsen/SpanMarkerN
-00001600: 4552 2f62 6c6f 622f 6d61 696e 2f6e 6f74  ER/blob/main/not
-00001610: 6562 6f6f 6b73 2f67 6574 7469 6e67 5f73  ebooks/getting_s
-00001620: 7461 7274 6564 2e69 7079 6e62 2920 2020  tarted.ipynb)   
-00001630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001640: 2020 2020 7c20 5b21 5b4f 7065 6e20 496e      | [![Open In
-00001650: 2053 6167 654d 616b 6572 2053 7475 6469   SageMaker Studi
-00001660: 6f20 4c61 625d 2868 7474 7073 3a2f 2f73  o Lab](https://s
-00001670: 7475 6469 6f6c 6162 2e73 6167 656d 616b  tudiolab.sagemak
-00001680: 6572 2e61 7773 2f73 7475 6469 6f6c 6162  er.aws/studiolab
-00001690: 2e73 7667 295d 2868 7474 7073 3a2f 2f73  .svg)](https://s
-000016a0: 7475 6469 6f6c 6162 2e73 6167 656d 616b  tudiolab.sagemak
-000016b0: 6572 2e61 7773 2f69 6d70 6f72 742f 6769  er.aws/import/gi
-000016c0: 7468 7562 2f74 6f6d 6161 7273 656e 2f53  thub/tomaarsen/S
-000016d0: 7061 6e4d 6172 6b65 724e 4552 2f62 6c6f  panMarkerNER/blo
-000016e0: 622f 6d61 696e 2f6e 6f74 6562 6f6f 6b73  b/main/notebooks
-000016f0: 2f67 6574 7469 6e67 5f73 7461 7274 6564  /getting_started
-00001700: 2e69 7079 6e62 2920 2020 2020 2020 2020  .ipynb)         
-00001710: 2020 2020 2020 2020 2020 2020 2020 7c0d                |.
-00001720: 0a0d 0a60 6060 7079 7468 6f6e 0d0a 6672  ...```python..fr
-00001730: 6f6d 2064 6174 6173 6574 7320 696d 706f  om datasets impo
-00001740: 7274 206c 6f61 645f 6461 7461 7365 740d  rt load_dataset.
-00001750: 0a66 726f 6d20 7472 616e 7366 6f72 6d65  .from transforme
-00001760: 7273 2069 6d70 6f72 7420 5472 6169 6e69  rs import Traini
-00001770: 6e67 4172 6775 6d65 6e74 730d 0a66 726f  ngArguments..fro
-00001780: 6d20 7370 616e 5f6d 6172 6b65 7220 696d  m span_marker im
-00001790: 706f 7274 2053 7061 6e4d 6172 6b65 724d  port SpanMarkerM
-000017a0: 6f64 656c 2c20 5472 6169 6e65 720d 0a0d  odel, Trainer...
-000017b0: 0a0d 0a64 6566 206d 6169 6e28 2920 2d3e  ...def main() ->
-000017c0: 204e 6f6e 653a 0d0a 2020 2020 2320 4c6f   None:..    # Lo
-000017d0: 6164 2074 6865 2064 6174 6173 6574 2c20  ad the dataset, 
-000017e0: 656e 7375 7265 2022 746f 6b65 6e73 2220  ensure "tokens" 
-000017f0: 616e 6420 226e 6572 5f74 6167 7322 2063  and "ner_tags" c
-00001800: 6f6c 756d 6e73 2c20 616e 6420 6765 7420  olumns, and get 
-00001810: 6120 6c69 7374 206f 6620 6c61 6265 6c73  a list of labels
-00001820: 0d0a 2020 2020 6461 7461 7365 7420 3d20  ..    dataset = 
-00001830: 6c6f 6164 5f64 6174 6173 6574 2822 4446  load_dataset("DF
-00001840: 4b49 2d53 4c54 2f66 6577 2d6e 6572 6422  KI-SLT/few-nerd"
-00001850: 2c20 2273 7570 6572 7669 7365 6422 290d  , "supervised").
-00001860: 0a20 2020 2064 6174 6173 6574 203d 2064  .    dataset = d
-00001870: 6174 6173 6574 2e72 656d 6f76 655f 636f  ataset.remove_co
-00001880: 6c75 6d6e 7328 226e 6572 5f74 6167 7322  lumns("ner_tags"
-00001890: 290d 0a20 2020 2064 6174 6173 6574 203d  )..    dataset =
-000018a0: 2064 6174 6173 6574 2e72 656e 616d 655f   dataset.rename_
-000018b0: 636f 6c75 6d6e 2822 6669 6e65 5f6e 6572  column("fine_ner
-000018c0: 5f74 6167 7322 2c20 226e 6572 5f74 6167  _tags", "ner_tag
-000018d0: 7322 290d 0a20 2020 206c 6162 656c 7320  s")..    labels 
-000018e0: 3d20 6461 7461 7365 745b 2274 7261 696e  = dataset["train
-000018f0: 225d 2e66 6561 7475 7265 735b 226e 6572  "].features["ner
-00001900: 5f74 6167 7322 5d2e 6665 6174 7572 652e  _tags"].feature.
-00001910: 6e61 6d65 730d 0a0d 0a20 2020 2023 2049  names....    # I
-00001920: 6e69 7469 616c 697a 6520 6120 5370 616e  nitialize a Span
-00001930: 4d61 726b 6572 206d 6f64 656c 2075 7369  Marker model usi
-00001940: 6e67 2061 2070 7265 7472 6169 6e65 6420  ng a pretrained 
-00001950: 4245 5254 2d73 7479 6c65 2065 6e63 6f64  BERT-style encod
-00001960: 6572 0d0a 2020 2020 6d6f 6465 6c5f 6e61  er..    model_na
-00001970: 6d65 203d 2022 6265 7274 2d62 6173 652d  me = "bert-base-
-00001980: 6361 7365 6422 0d0a 2020 2020 6d6f 6465  cased"..    mode
-00001990: 6c20 3d20 5370 616e 4d61 726b 6572 4d6f  l = SpanMarkerMo
-000019a0: 6465 6c2e 6672 6f6d 5f70 7265 7472 6169  del.from_pretrai
-000019b0: 6e65 6428 0d0a 2020 2020 2020 2020 6d6f  ned(..        mo
-000019c0: 6465 6c5f 6e61 6d65 2c0d 0a20 2020 2020  del_name,..     
-000019d0: 2020 206c 6162 656c 733d 6c61 6265 6c73     labels=labels
-000019e0: 2c0d 0a20 2020 2020 2020 2023 2053 7061  ,..        # Spa
-000019f0: 6e4d 6172 6b65 7220 6879 7065 7270 6172  nMarker hyperpar
-00001a00: 616d 6574 6572 733a 0d0a 2020 2020 2020  ameters:..      
-00001a10: 2020 6d6f 6465 6c5f 6d61 785f 6c65 6e67    model_max_leng
-00001a20: 7468 3d32 3536 2c0d 0a20 2020 2020 2020  th=256,..       
-00001a30: 206d 6172 6b65 725f 6d61 785f 6c65 6e67   marker_max_leng
-00001a40: 7468 3d31 3238 2c0d 0a20 2020 2020 2020  th=128,..       
-00001a50: 2065 6e74 6974 795f 6d61 785f 6c65 6e67   entity_max_leng
-00001a60: 7468 3d38 2c0d 0a20 2020 2029 0d0a 0d0a  th=8,..    )....
-00001a70: 2020 2020 2320 5072 6570 6172 6520 7468      # Prepare th
-00001a80: 6520 f09f a497 2074 7261 6e73 666f 726d  e .... transform
-00001a90: 6572 7320 7472 6169 6e69 6e67 2061 7267  ers training arg
-00001aa0: 756d 656e 7473 0d0a 2020 2020 6172 6773  uments..    args
-00001ab0: 203d 2054 7261 696e 696e 6741 7267 756d   = TrainingArgum
-00001ac0: 656e 7473 280d 0a20 2020 2020 2020 206f  ents(..        o
-00001ad0: 7574 7075 745f 6469 723d 226d 6f64 656c  utput_dir="model
-00001ae0: 732f 7370 616e 5f6d 6172 6b65 725f 6265  s/span_marker_be
-00001af0: 7274 5f62 6173 655f 6361 7365 645f 6665  rt_base_cased_fe
-00001b00: 776e 6572 645f 6669 6e65 5f73 7570 6572  wnerd_fine_super
-00001b10: 222c 0d0a 2020 2020 2020 2020 2320 5472  ",..        # Tr
-00001b20: 6169 6e69 6e67 2048 7970 6572 7061 7261  aining Hyperpara
-00001b30: 6d65 7465 7273 3a0d 0a20 2020 2020 2020  meters:..       
-00001b40: 206c 6561 726e 696e 675f 7261 7465 3d35   learning_rate=5
-00001b50: 652d 352c 0d0a 2020 2020 2020 2020 7065  e-5,..        pe
-00001b60: 725f 6465 7669 6365 5f74 7261 696e 5f62  r_device_train_b
-00001b70: 6174 6368 5f73 697a 653d 3332 2c0d 0a20  atch_size=32,.. 
-00001b80: 2020 2020 2020 2070 6572 5f64 6576 6963         per_devic
-00001b90: 655f 6576 616c 5f62 6174 6368 5f73 697a  e_eval_batch_siz
-00001ba0: 653d 3332 2c0d 0a20 2020 2020 2020 206e  e=32,..        n
-00001bb0: 756d 5f74 7261 696e 5f65 706f 6368 733d  um_train_epochs=
-00001bc0: 332c 0d0a 2020 2020 2020 2020 7765 6967  3,..        weig
-00001bd0: 6874 5f64 6563 6179 3d30 2e30 312c 0d0a  ht_decay=0.01,..
-00001be0: 2020 2020 2020 2020 7761 726d 7570 5f72          warmup_r
-00001bf0: 6174 696f 3d30 2e31 2c0d 0a20 2020 2020  atio=0.1,..     
-00001c00: 2020 2062 6631 363d 5472 7565 2c20 2023     bf16=True,  #
-00001c10: 2052 6570 6c61 6365 2060 6266 3136 6020   Replace `bf16` 
-00001c20: 7769 7468 2060 6670 3136 6020 6966 2079  with `fp16` if y
-00001c30: 6f75 7220 6861 7264 7761 7265 2063 616e  our hardware can
-00001c40: 2774 2075 7365 2062 6631 362e 0d0a 2020  't use bf16...  
-00001c50: 2020 2020 2020 2320 4f74 6865 7220 5472        # Other Tr
-00001c60: 6169 6e69 6e67 2070 6172 616d 6574 6572  aining parameter
-00001c70: 730d 0a20 2020 2020 2020 206c 6f67 6769  s..        loggi
-00001c80: 6e67 5f66 6972 7374 5f73 7465 703d 5472  ng_first_step=Tr
-00001c90: 7565 2c0d 0a20 2020 2020 2020 206c 6f67  ue,..        log
-00001ca0: 6769 6e67 5f73 7465 7073 3d35 302c 0d0a  ging_steps=50,..
-00001cb0: 2020 2020 2020 2020 6576 616c 7561 7469          evaluati
-00001cc0: 6f6e 5f73 7472 6174 6567 793d 2273 7465  on_strategy="ste
-00001cd0: 7073 222c 0d0a 2020 2020 2020 2020 7361  ps",..        sa
-00001ce0: 7665 5f73 7472 6174 6567 793d 2273 7465  ve_strategy="ste
-00001cf0: 7073 222c 0d0a 2020 2020 2020 2020 6576  ps",..        ev
-00001d00: 616c 5f73 7465 7073 3d33 3030 302c 0d0a  al_steps=3000,..
-00001d10: 2020 2020 2020 2020 7361 7665 5f74 6f74          save_tot
-00001d20: 616c 5f6c 696d 6974 3d32 2c0d 0a20 2020  al_limit=2,..   
-00001d30: 2020 2020 2064 6174 616c 6f61 6465 725f       dataloader_
-00001d40: 6e75 6d5f 776f 726b 6572 733d 322c 0d0a  num_workers=2,..
-00001d50: 2020 2020 290d 0a0d 0a20 2020 2023 2049      )....    # I
-00001d60: 6e69 7469 616c 697a 6520 7468 6520 7472  nitialize the tr
-00001d70: 6169 6e65 7220 7573 696e 6720 6f75 7220  ainer using our 
-00001d80: 6d6f 6465 6c2c 2074 7261 696e 696e 6720  model, training 
-00001d90: 6172 6773 2026 2064 6174 6173 6574 2c20  args & dataset, 
-00001da0: 616e 6420 7472 6169 6e0d 0a20 2020 2074  and train..    t
-00001db0: 7261 696e 6572 203d 2054 7261 696e 6572  rainer = Trainer
-00001dc0: 280d 0a20 2020 2020 2020 206d 6f64 656c  (..        model
-00001dd0: 3d6d 6f64 656c 2c0d 0a20 2020 2020 2020  =model,..       
-00001de0: 2061 7267 733d 6172 6773 2c0d 0a20 2020   args=args,..   
-00001df0: 2020 2020 2074 7261 696e 5f64 6174 6173       train_datas
-00001e00: 6574 3d64 6174 6173 6574 5b22 7472 6169  et=dataset["trai
-00001e10: 6e22 5d2c 0d0a 2020 2020 2020 2020 6576  n"],..        ev
-00001e20: 616c 5f64 6174 6173 6574 3d64 6174 6173  al_dataset=datas
-00001e30: 6574 5b22 7661 6c69 6461 7469 6f6e 225d  et["validation"]
-00001e40: 2c0d 0a20 2020 2029 0d0a 2020 2020 7472  ,..    )..    tr
-00001e50: 6169 6e65 722e 7472 6169 6e28 290d 0a20  ainer.train().. 
-00001e60: 2020 2074 7261 696e 6572 2e73 6176 655f     trainer.save_
-00001e70: 6d6f 6465 6c28 226d 6f64 656c 732f 7370  model("models/sp
-00001e80: 616e 5f6d 6172 6b65 725f 6265 7274 5f62  an_marker_bert_b
-00001e90: 6173 655f 6361 7365 645f 6665 776e 6572  ase_cased_fewner
-00001ea0: 645f 6669 6e65 5f73 7570 6572 2f63 6865  d_fine_super/che
-00001eb0: 636b 706f 696e 742d 6669 6e61 6c22 290d  ckpoint-final").
-00001ec0: 0a0d 0a20 2020 2023 2043 6f6d 7075 7465  ...    # Compute
-00001ed0: 2026 2073 6176 6520 7468 6520 6d65 7472   & save the metr
-00001ee0: 6963 7320 6f6e 2074 6865 2074 6573 7420  ics on the test 
-00001ef0: 7365 740d 0a20 2020 206d 6574 7269 6373  set..    metrics
-00001f00: 203d 2074 7261 696e 6572 2e65 7661 6c75   = trainer.evalu
-00001f10: 6174 6528 6461 7461 7365 745b 2274 6573  ate(dataset["tes
-00001f20: 7422 5d2c 206d 6574 7269 635f 6b65 795f  t"], metric_key_
-00001f30: 7072 6566 6978 3d22 7465 7374 2229 0d0a  prefix="test")..
-00001f40: 2020 2020 7472 6169 6e65 722e 7361 7665      trainer.save
-00001f50: 5f6d 6574 7269 6373 2822 7465 7374 222c  _metrics("test",
-00001f60: 206d 6574 7269 6373 290d 0a0d 0a0d 0a69   metrics)......i
-00001f70: 6620 5f5f 6e61 6d65 5f5f 203d 3d20 225f  f __name__ == "_
-00001f80: 5f6d 6169 6e5f 5f22 3a0d 0a20 2020 206d  _main__":..    m
-00001f90: 6169 6e28 290d 0a60 6060 0d0a 0d0a 2323  ain()..```....##
-00001fa0: 2320 496e 6665 7265 6e63 650d 0a60 6060  # Inference..```
-00001fb0: 7079 7468 6f6e 0d0a 6672 6f6d 2073 7061  python..from spa
-00001fc0: 6e5f 6d61 726b 6572 2069 6d70 6f72 7420  n_marker import 
-00001fd0: 5370 616e 4d61 726b 6572 4d6f 6465 6c0d  SpanMarkerModel.
-00001fe0: 0a0d 0a23 2044 6f77 6e6c 6f61 6420 6672  ...# Download fr
-00001ff0: 6f6d 2074 6865 20f0 9fa4 9720 4875 620d  om the .... Hub.
-00002000: 0a6d 6f64 656c 203d 2053 7061 6e4d 6172  .model = SpanMar
-00002010: 6b65 724d 6f64 656c 2e66 726f 6d5f 7072  kerModel.from_pr
-00002020: 6574 7261 696e 6564 2822 746f 6d61 6172  etrained("tomaar
-00002030: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
-00002040: 6265 7274 2d62 6173 652d 6665 776e 6572  bert-base-fewner
-00002050: 642d 6669 6e65 2d73 7570 6572 2229 0d0a  d-fine-super")..
-00002060: 2320 5275 6e20 696e 6665 7265 6e63 650d  # Run inference.
-00002070: 0a65 6e74 6974 6965 7320 3d20 6d6f 6465  .entities = mode
-00002080: 6c2e 7072 6564 6963 7428 2241 6d65 6c69  l.predict("Ameli
-00002090: 6120 4561 7268 6172 7420 666c 6577 2068  a Earhart flew h
-000020a0: 6572 2073 696e 676c 6520 656e 6769 6e65  er single engine
-000020b0: 204c 6f63 6b68 6565 6420 5665 6761 2035   Lockheed Vega 5
-000020c0: 4220 6163 726f 7373 2074 6865 2041 746c  B across the Atl
-000020d0: 616e 7469 6320 746f 2050 6172 6973 2e22  antic to Paris."
-000020e0: 290d 0a5b 7b27 7370 616e 273a 2027 416d  )..[{'span': 'Am
-000020f0: 656c 6961 2045 6172 6861 7274 272c 2027  elia Earhart', '
-00002100: 6c61 6265 6c27 3a20 2770 6572 736f 6e2d  label': 'person-
-00002110: 6f74 6865 7227 2c20 2773 636f 7265 273a  other', 'score':
-00002120: 2030 2e37 3635 3935 3937 3339 3638 3530   0.7659597396850
-00002130: 3538 362c 2027 6368 6172 5f73 7461 7274  586, 'char_start
-00002140: 5f69 6e64 6578 273a 2030 2c20 2763 6861  _index': 0, 'cha
-00002150: 725f 656e 645f 696e 6465 7827 3a20 3134  r_end_index': 14
-00002160: 7d2c 0d0a 207b 2773 7061 6e27 3a20 274c  },.. {'span': 'L
-00002170: 6f63 6b68 6565 6420 5665 6761 2035 4227  ockheed Vega 5B'
-00002180: 2c20 276c 6162 656c 273a 2027 7072 6f64  , 'label': 'prod
-00002190: 7563 742d 6169 7270 6c61 6e65 272c 2027  uct-airplane', '
-000021a0: 7363 6f72 6527 3a20 302e 3937 3235 3738  score': 0.972578
-000021b0: 3538 3531 3437 3835 3737 2c20 2763 6861  5851478577, 'cha
-000021c0: 725f 7374 6172 745f 696e 6465 7827 3a20  r_start_index': 
-000021d0: 3338 2c20 2763 6861 725f 656e 645f 696e  38, 'char_end_in
-000021e0: 6465 7827 3a20 3534 7d2c 0d0a 207b 2773  dex': 54},.. {'s
-000021f0: 7061 6e27 3a20 2741 746c 616e 7469 6327  pan': 'Atlantic'
-00002200: 2c20 276c 6162 656c 273a 2027 6c6f 6361  , 'label': 'loca
-00002210: 7469 6f6e 2d62 6f64 6965 736f 6677 6174  tion-bodiesofwat
-00002220: 6572 272c 2027 7363 6f72 6527 3a20 302e  er', 'score': 0.
-00002230: 3735 3837 3637 3930 3238 3531 3130 3437  7587679028511047
-00002240: 2c20 2763 6861 725f 7374 6172 745f 696e  , 'char_start_in
-00002250: 6465 7827 3a20 3636 2c20 2763 6861 725f  dex': 66, 'char_
-00002260: 656e 645f 696e 6465 7827 3a20 3734 7d2c  end_index': 74},
-00002270: 0d0a 207b 2773 7061 6e27 3a20 2750 6172  .. {'span': 'Par
-00002280: 6973 272c 2027 6c61 6265 6c27 3a20 276c  is', 'label': 'l
-00002290: 6f63 6174 696f 6e2d 4750 4527 2c20 2773  ocation-GPE', 's
-000022a0: 636f 7265 273a 2030 2e39 3839 3233 3930  core': 0.9892390
-000022b0: 3936 3634 3135 3430 352c 2027 6368 6172  966415405, 'char
-000022c0: 5f73 7461 7274 5f69 6e64 6578 273a 2037  _start_index': 7
-000022d0: 382c 2027 6368 6172 5f65 6e64 5f69 6e64  8, 'char_end_ind
-000022e0: 6578 273a 2038 337d 5d0d 0a60 6060 0d0a  ex': 83}]..```..
-000022f0: 0d0a 3c21 2d2d 2042 6563 6175 7365 2074  ..<!-- Because t
-00002300: 6869 7320 776f 726b 2069 7320 6261 7365  his work is base
-00002310: 6420 6f6e 205b 504c 2d4d 6172 6b65 725d  d on [PL-Marker]
-00002320: 2868 7474 7073 3a2f 2f61 7278 6976 2e6f  (https://arxiv.o
-00002330: 7267 2f70 6466 2f32 3130 392e 3036 3036  rg/pdf/2109.0606
-00002340: 3776 352e 7064 6629 2c20 796f 7520 6d61  7v5.pdf), you ma
-00002350: 7920 6578 7065 6374 2073 696d 696c 6172  y expect similar
-00002360: 2072 6573 756c 7473 2074 6f20 6974 7320   results to its 
-00002370: 5b50 6170 6572 7320 7769 7468 2043 6f64  [Papers with Cod
-00002380: 6520 4c65 6164 6572 626f 6172 645d 2868  e Leaderboard](h
-00002390: 7474 7073 3a2f 2f70 6170 6572 7377 6974  ttps://paperswit
-000023a0: 6863 6f64 652e 636f 6d2f 7061 7065 722f  hcode.com/paper/
-000023b0: 7061 636b 2d74 6f67 6574 6865 722d 656e  pack-together-en
-000023c0: 7469 7479 2d61 6e64 2d72 656c 6174 696f  tity-and-relatio
-000023d0: 6e2d 6578 7472 6163 7469 6f6e 2920 7265  n-extraction) re
-000023e0: 7375 6c74 732e 202d 2d3e 0d0a 0d0a 2323  sults. -->....##
-000023f0: 2050 7265 7472 6169 6e65 6420 4d6f 6465   Pretrained Mode
-00002400: 6c73 0d0a 0d0a 416c 6c20 6d6f 6465 6c73  ls....All models
-00002410: 2069 6e20 7468 6973 206c 6973 7420 636f   in this list co
-00002420: 6e74 6169 6e20 6074 7261 696e 2e70 7960  ntain `train.py`
-00002430: 2066 696c 6573 2074 6861 7420 7368 6f77   files that show
-00002440: 2074 6865 2074 7261 696e 696e 6720 7363   the training sc
-00002450: 7269 7074 7320 7573 6564 2074 6f20 6765  ripts used to ge
-00002460: 6e65 7261 7465 2074 6865 6d2e 2041 6464  nerate them. Add
-00002470: 6974 696f 6e61 6c6c 792c 2061 6c6c 2074  itionally, all t
-00002480: 7261 696e 696e 6720 7363 7269 7074 7320  raining scripts 
-00002490: 7573 6564 2061 7265 2073 746f 7265 6420  used are stored 
-000024a0: 696e 2074 6865 205b 7472 6169 6e69 6e67  in the [training
-000024b0: 5f73 6372 6970 7473 5d28 7472 6169 6e69  _scripts](traini
-000024c0: 6e67 5f73 6372 6970 7473 2920 6469 7265  ng_scripts) dire
-000024d0: 6374 6f72 792e 0d0a 5468 6573 6520 7472  ctory...These tr
-000024e0: 6169 6e65 6420 6d6f 6465 6c73 2068 6176  ained models hav
-000024f0: 6520 486f 7374 6564 2049 6e66 6572 656e  e Hosted Inferen
-00002500: 6365 2041 5049 2077 6964 6765 7473 2074  ce API widgets t
-00002510: 6861 7420 796f 7520 6361 6e20 7573 6520  hat you can use 
-00002520: 746f 2065 7870 6572 696d 656e 7420 7769  to experiment wi
-00002530: 7468 2074 6865 206d 6f64 656c 7320 6f6e  th the models on
-00002540: 2074 6865 6972 2048 7567 6769 6e67 2046   their Hugging F
-00002550: 6163 6520 6d6f 6465 6c20 7061 6765 732e  ace model pages.
-00002560: 2041 6464 6974 696f 6e61 6c6c 792c 2048   Additionally, H
-00002570: 7567 6769 6e67 2046 6163 6520 7072 6f76  ugging Face prov
-00002580: 6964 6573 2065 6163 6820 6d6f 6465 6c20  ides each model 
-00002590: 7769 7468 2061 2066 7265 6520 4150 4920  with a free API 
-000025a0: 2860 4465 706c 6f79 6020 3e20 6049 6e66  (`Deploy` > `Inf
-000025b0: 6572 656e 6365 2041 5049 6020 6f6e 2074  erence API` on t
-000025c0: 6865 206d 6f64 656c 2070 6167 6529 2e0d  he model page)..
-000025d0: 0a0d 0a23 2323 2046 6577 4e45 5244 0d0a  ...### FewNERD..
-000025e0: 2a20 5b60 746f 6d61 6172 7365 6e2f 7370  * [`tomaarsen/sp
-000025f0: 616e 2d6d 6172 6b65 722d 6265 7274 2d62  an-marker-bert-b
-00002600: 6173 652d 6665 776e 6572 642d 6669 6e65  ase-fewnerd-fine
-00002610: 2d73 7570 6572 605d 2868 7474 7073 3a2f  -super`](https:/
-00002620: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
-00002630: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
-00002640: 6172 6b65 722d 6265 7274 2d62 6173 652d  arker-bert-base-
-00002650: 6665 776e 6572 642d 6669 6e65 2d73 7570  fewnerd-fine-sup
-00002660: 6572 2920 6973 2061 206d 6f64 656c 2074  er) is a model t
-00002670: 6861 7420 4920 6861 7665 2074 7261 696e  hat I have train
-00002680: 6564 2069 6e20 3220 686f 7572 7320 6f6e  ed in 2 hours on
-00002690: 2074 6865 2066 696e 6567 7261 696e 6564   the finegrained
-000026a0: 2c20 7375 7065 7276 6973 6564 205b 4665  , supervised [Fe
-000026b0: 772d 4e45 5244 2064 6174 6173 6574 5d28  w-NERD dataset](
-000026c0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
-000026d0: 6163 652e 636f 2f64 6174 6173 6574 732f  ace.co/datasets/
-000026e0: 4446 4b49 2d53 4c54 2f66 6577 2d6e 6572  DFKI-SLT/few-ner
-000026f0: 6429 2e20 4974 2072 6561 6368 6564 2061  d). It reached a
-00002700: 2030 2e37 3035 3320 5465 7374 2046 312c   0.7053 Test F1,
-00002710: 2063 6f6d 7065 7469 7469 7665 2069 6e20   competitive in 
-00002720: 7468 6520 616c 6c2d 7469 6d65 205b 4665  the all-time [Fe
-00002730: 772d 4e45 5244 206c 6561 6465 7262 6f61  w-NERD leaderboa
-00002740: 7264 5d28 6874 7470 733a 2f2f 7061 7065  rd](https://pape
-00002750: 7273 7769 7468 636f 6465 2e63 6f6d 2f73  rswithcode.com/s
-00002760: 6f74 612f 6e61 6d65 642d 656e 7469 7479  ota/named-entity
-00002770: 2d72 6563 6f67 6e69 7469 6f6e 2d6f 6e2d  -recognition-on-
-00002780: 6665 772d 6e65 7264 2d73 7570 2920 7573  few-nerd-sup) us
-00002790: 696e 6720 6062 6572 742d 6261 7365 602e  ing `bert-base`.
-000027a0: 204d 7920 7472 6169 6e69 6e67 2073 6372   My training scr
-000027b0: 6970 7420 7265 7365 6d62 6c65 7320 7468  ipt resembles th
-000027c0: 6520 6f6e 6520 7468 6174 2079 6f75 2063  e one that you c
-000027d0: 616e 2073 6565 2061 626f 7665 2e0d 0a20  an see above... 
-000027e0: 202a 2054 7279 2074 6865 206d 6f64 656c   * Try the model
-000027f0: 206f 7574 206f 6e6c 696e 6520 7573 696e   out online usin
-00002800: 6720 7468 6973 205b f09f a497 2053 7061  g this [.... Spa
-00002810: 6365 5d28 6874 7470 733a 2f2f 746f 6d61  ce](https://toma
-00002820: 6172 7365 6e2d 7370 616e 2d6d 6172 6b65  arsen-span-marke
-00002830: 722d 6265 7274 2d62 6173 652d 6665 776e  r-bert-base-fewn
-00002840: 6572 642d 6669 6e65 2d73 7570 6572 2e68  erd-fine-super.h
-00002850: 662e 7370 6163 652f 292e 0d0a 0d0a 2a20  f.space/).....* 
-00002860: 5b60 746f 6d61 6172 7365 6e2f 7370 616e  [`tomaarsen/span
-00002870: 2d6d 6172 6b65 722d 726f 6265 7274 612d  -marker-roberta-
-00002880: 6c61 7267 652d 6665 776e 6572 642d 6669  large-fewnerd-fi
-00002890: 6e65 2d73 7570 6572 605d 2868 7474 7073  ne-super`](https
-000028a0: 3a2f 2f68 7567 6769 6e67 6661 6365 2e63  ://huggingface.c
-000028b0: 6f2f 746f 6d61 6172 7365 6e2f 7370 616e  o/tomaarsen/span
-000028c0: 2d6d 6172 6b65 722d 726f 6265 7274 612d  -marker-roberta-
-000028d0: 6c61 7267 652d 6665 776e 6572 642d 6669  large-fewnerd-fi
-000028e0: 6e65 2d73 7570 6572 2920 7761 7320 7472  ne-super) was tr
-000028f0: 6169 6e65 6420 696e 2036 2068 6f75 7273  ained in 6 hours
-00002900: 206f 6e20 7468 6520 6669 6e65 6772 6169   on the finegrai
-00002910: 6e65 642c 2073 7570 6572 7669 7365 6420  ned, supervised 
-00002920: 5b46 6577 2d4e 4552 4420 6461 7461 7365  [Few-NERD datase
-00002930: 745d 2868 7474 7073 3a2f 2f68 7567 6769  t](https://huggi
-00002940: 6e67 6661 6365 2e63 6f2f 6461 7461 7365  ngface.co/datase
-00002950: 7473 2f44 464b 492d 534c 542f 6665 772d  ts/DFKI-SLT/few-
-00002960: 6e65 7264 2920 7573 696e 6720 6072 6f62  nerd) using `rob
-00002970: 6572 7461 2d6c 6172 6765 602e 2049 7420  erta-large`. It 
-00002980: 7265 6163 6865 6420 6120 302e 3731 3033  reached a 0.7103
-00002990: 2054 6573 7420 4631 2c20 7265 6163 6869   Test F1, reachi
-000029a0: 6e67 2061 206e 6577 2073 7461 7465 206f  ng a new state o
-000029b0: 6620 7468 6520 6172 7420 696e 2074 6865  f the art in the
-000029c0: 2061 6c6c 2d74 696d 6520 5b46 6577 2d4e   all-time [Few-N
-000029d0: 4552 4420 6c65 6164 6572 626f 6172 645d  ERD leaderboard]
-000029e0: 2868 7474 7073 3a2f 2f70 6170 6572 7377  (https://papersw
-000029f0: 6974 6863 6f64 652e 636f 6d2f 736f 7461  ithcode.com/sota
-00002a00: 2f6e 616d 6564 2d65 6e74 6974 792d 7265  /named-entity-re
-00002a10: 636f 676e 6974 696f 6e2d 6f6e 2d66 6577  cognition-on-few
-00002a20: 2d6e 6572 642d 7375 7029 2e0d 0a2a 205b  -nerd-sup)...* [
-00002a30: 6074 6f6d 6161 7273 656e 2f73 7061 6e2d  `tomaarsen/span-
-00002a40: 6d61 726b 6572 2d78 6c6d 2d72 6f62 6572  marker-xlm-rober
-00002a50: 7461 2d62 6173 652d 6665 776e 6572 642d  ta-base-fewnerd-
-00002a60: 6669 6e65 2d73 7570 6572 605d 2868 7474  fine-super`](htt
-00002a70: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
-00002a80: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
-00002a90: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
-00002aa0: 6265 7274 612d 6261 7365 2d66 6577 6e65  berta-base-fewne
-00002ab0: 7264 2d66 696e 652d 7375 7065 7229 2069  rd-fine-super) i
-00002ac0: 7320 6120 6d75 6c74 696c 696e 6775 616c  s a multilingual
-00002ad0: 206d 6f64 656c 2074 6861 7420 4920 6861   model that I ha
-00002ae0: 7665 2074 7261 696e 6564 2069 6e20 312e  ve trained in 1.
-00002af0: 3520 686f 7572 7320 6f6e 2074 6865 2066  5 hours on the f
-00002b00: 696e 6567 7261 696e 6564 2c20 7375 7065  inegrained, supe
-00002b10: 7276 6973 6564 205b 4665 772d 4e45 5244  rvised [Few-NERD
-00002b20: 2064 6174 6173 6574 5d28 6874 7470 733a   dataset](https:
-00002b30: 2f2f 6875 6767 696e 6766 6163 652e 636f  //huggingface.co
-00002b40: 2f64 6174 6173 6574 732f 4446 4b49 2d53  /datasets/DFKI-S
-00002b50: 4c54 2f66 6577 2d6e 6572 6429 2e20 4974  LT/few-nerd). It
-00002b60: 2072 6561 6368 6564 2061 2030 2e36 3836   reached a 0.686
-00002b70: 2054 6573 7420 4631 206f 6e20 456e 676c   Test F1 on Engl
-00002b80: 6973 682c 2061 6e64 2077 6f72 6b73 2077  ish, and works w
-00002b90: 656c 6c20 6f6e 206f 7468 6572 206c 616e  ell on other lan
-00002ba0: 6775 6167 6573 206c 696b 6520 5370 616e  guages like Span
-00002bb0: 6973 682c 2046 7265 6e63 682c 2047 6572  ish, French, Ger
-00002bc0: 6d61 6e2c 2052 7573 7369 616e 2c20 4475  man, Russian, Du
-00002bd0: 7463 682c 2050 6f6c 6973 682c 2049 6365  tch, Polish, Ice
-00002be0: 6c61 6e64 6963 2c20 4772 6565 6b20 616e  landic, Greek an
-00002bf0: 6420 6d61 6e79 206d 6f72 652e 0d0a 0d0a  d many more.....
-00002c00: 2323 2320 4f6e 746f 4e6f 7465 7320 7635  ### OntoNotes v5
-00002c10: 2e30 0d0a 2a20 5b60 746f 6d61 6172 7365  .0..* [`tomaarse
-00002c20: 6e2f 7370 616e 2d6d 6172 6b65 722d 726f  n/span-marker-ro
-00002c30: 6265 7274 612d 6c61 7267 652d 6f6e 746f  berta-large-onto
-00002c40: 6e6f 7465 7335 605d 2868 7474 7073 3a2f  notes5`](https:/
-00002c50: 2f68 7567 6769 6e67 6661 6365 2e63 6f2f  /huggingface.co/
-00002c60: 746f 6d61 6172 7365 6e2f 7370 616e 2d6d  tomaarsen/span-m
-00002c70: 6172 6b65 722d 726f 6265 7274 612d 6c61  arker-roberta-la
-00002c80: 7267 652d 6f6e 746f 6e6f 7465 7335 2920  rge-ontonotes5) 
-00002c90: 7761 7320 7472 6169 6e65 6420 696e 2033  was trained in 3
-00002ca0: 2068 6f75 7273 206f 6e20 7468 6520 4f6e   hours on the On
-00002cb0: 746f 4e6f 7465 7320 7635 2e30 2064 6174  toNotes v5.0 dat
-00002cc0: 6173 6574 2c20 7265 6163 6869 6e67 2061  aset, reaching a
-00002cd0: 2070 6572 666f 726d 616e 6365 206f 6620   performance of 
-00002ce0: 302e 3931 3534 2046 312e 2046 6f72 2072  0.9154 F1. For r
-00002cf0: 6566 6572 656e 6365 2c20 7468 6520 6375  eference, the cu
-00002d00: 7272 656e 7420 7374 726f 6e67 6573 7420  rrent strongest 
-00002d10: 7370 6143 7920 6d6f 6465 6c20 2860 656e  spaCy model (`en
-00002d20: 5f63 6f72 655f 7765 625f 7472 6660 2920  _core_web_trf`) 
-00002d30: 7265 6163 6865 7320 302e 3839 382e 2054  reaches 0.898. T
-00002d40: 6869 7320 5370 616e 4d61 726b 6572 206d  his SpanMarker m
-00002d50: 6f64 656c 2075 7365 7320 6120 6072 6f62  odel uses a `rob
-00002d60: 6572 7461 2d6c 6172 6765 6020 656e 636f  erta-large` enco
-00002d70: 6465 7220 756e 6465 7220 7468 6520 686f  der under the ho
-00002d80: 6f64 2e0d 0a0d 0a23 2323 2043 6f4e 4c4c  od.....### CoNLL
-00002d90: 3033 0d0a 2a20 5b60 746f 6d61 6172 7365  03..* [`tomaarse
-00002da0: 6e2f 7370 616e 2d6d 6172 6b65 722d 786c  n/span-marker-xl
-00002db0: 6d2d 726f 6265 7274 612d 6c61 7267 652d  m-roberta-large-
-00002dc0: 636f 6e6c 6c30 3360 5d28 6874 7470 733a  conll03`](https:
-00002dd0: 2f2f 6875 6767 696e 6766 6163 652e 636f  //huggingface.co
-00002de0: 2f74 6f6d 6161 7273 656e 2f73 7061 6e2d  /tomaarsen/span-
-00002df0: 6d61 726b 6572 2d78 6c6d 2d72 6f62 6572  marker-xlm-rober
-00002e00: 7461 2d6c 6172 6765 2d63 6f6e 6c6c 3033  ta-large-conll03
-00002e10: 2920 6973 2061 2053 7061 6e4d 6172 6b65  ) is a SpanMarke
-00002e20: 7220 6d6f 6465 6c20 7573 696e 6720 6078  r model using `x
-00002e30: 6c6d 2d72 6f62 6572 7461 2d6c 6172 6765  lm-roberta-large
-00002e40: 6020 7468 6174 2077 6173 2074 7261 696e  ` that was train
-00002e50: 6564 2069 6e20 3435 206d 696e 7574 6573  ed in 45 minutes
-00002e60: 2e20 4974 2072 6561 6368 6573 2061 2073  . It reaches a s
-00002e70: 7461 7465 206f 6620 7468 6520 6172 7420  tate of the art 
-00002e80: 302e 3933 3120 4631 206f 6e20 436f 4e4c  0.931 F1 on CoNL
-00002e90: 4c30 3320 7769 7468 6f75 7420 7573 696e  L03 without usin
-00002ea0: 6720 646f 6375 6d65 6e74 2d6c 6576 656c  g document-level
-00002eb0: 2063 6f6e 7465 7874 2e20 466f 7220 7265   context. For re
-00002ec0: 6665 7265 6e63 652c 2074 6865 2063 7572  ference, the cur
-00002ed0: 7265 6e74 2073 7472 6f6e 6765 7374 2073  rent strongest s
-00002ee0: 7061 4379 206d 6f64 656c 2028 6065 6e5f  paCy model (`en_
-00002ef0: 636f 7265 5f77 6562 5f74 7266 6029 2072  core_web_trf`) r
-00002f00: 6561 6368 6573 2039 312e 362e 0d0a 2a20  eaches 91.6...* 
-00002f10: 5b60 746f 6d61 6172 7365 6e2f 7370 616e  [`tomaarsen/span
-00002f20: 2d6d 6172 6b65 722d 786c 6d2d 726f 6265  -marker-xlm-robe
-00002f30: 7274 612d 6c61 7267 652d 636f 6e6c 6c30  rta-large-conll0
-00002f40: 332d 646f 632d 636f 6e74 6578 7460 5d28  3-doc-context`](
-00002f50: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
-00002f60: 6163 652e 636f 2f74 6f6d 6161 7273 656e  ace.co/tomaarsen
-00002f70: 2f73 7061 6e2d 6d61 726b 6572 2d78 6c6d  /span-marker-xlm
-00002f80: 2d72 6f62 6572 7461 2d6c 6172 6765 2d63  -roberta-large-c
-00002f90: 6f6e 6c6c 3033 2d64 6f63 2d63 6f6e 7465  onll03-doc-conte
-00002fa0: 7874 2920 6973 2061 6e6f 7468 6572 2053  xt) is another S
-00002fb0: 7061 6e4d 6172 6b65 7220 6d6f 6465 6c20  panMarker model 
-00002fc0: 7573 696e 6720 7468 6520 6078 6c6d 2d72  using the `xlm-r
-00002fd0: 6f62 6572 7461 2d6c 6172 6765 6020 656e  oberta-large` en
-00002fe0: 636f 6465 722e 2049 7420 7573 6573 205b  coder. It uses [
-00002ff0: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
-00003000: 6f6e 7465 7874 5d28 6874 7470 733a 2f2f  ontext](https://
-00003010: 746f 6d61 6172 7365 6e2e 6769 7468 7562  tomaarsen.github
-00003020: 2e69 6f2f 5370 616e 4d61 726b 6572 4e45  .io/SpanMarkerNE
-00003030: 522f 6e6f 7465 626f 6f6b 732f 646f 6375  R/notebooks/docu
-00003040: 6d65 6e74 5f6c 6576 656c 5f63 6f6e 7465  ment_level_conte
-00003050: 7874 2e68 746d 6c29 2074 6f20 7265 6163  xt.html) to reac
-00003060: 6820 6120 7374 6174 6520 6f66 2074 6865  h a state of the
-00003070: 2061 7274 2030 2e39 3434 2046 312e 2046   art 0.944 F1. F
-00003080: 6f72 2074 6865 2062 6573 7420 7065 7266  or the best perf
-00003090: 6f72 6d61 6e63 652c 2069 6e66 6572 656e  ormance, inferen
-000030a0: 6365 2073 686f 756c 6420 6265 2070 6572  ce should be per
-000030b0: 666f 726d 6564 2075 7369 6e67 2064 6f63  formed using doc
-000030c0: 756d 656e 742d 6c65 7665 6c20 636f 6e74  ument-level cont
-000030d0: 6578 7420 285b 646f 6373 5d28 6874 7470  ext ([docs](http
-000030e0: 733a 2f2f 746f 6d61 6172 7365 6e2e 6769  s://tomaarsen.gi
-000030f0: 7468 7562 2e69 6f2f 5370 616e 4d61 726b  thub.io/SpanMark
-00003100: 6572 4e45 522f 6e6f 7465 626f 6f6b 732f  erNER/notebooks/
-00003110: 646f 6375 6d65 6e74 5f6c 6576 656c 5f63  document_level_c
-00003120: 6f6e 7465 7874 2e68 746d 6c23 496e 6665  ontext.html#Infe
-00003130: 7265 6e63 6529 292e 2054 6869 7320 6d6f  rence)). This mo
-00003140: 6465 6c20 7761 7320 7472 6169 6e65 6420  del was trained 
-00003150: 696e 2031 2068 6f75 722e 0d0a 0d0a 2323  in 1 hour.....##
-00003160: 2320 436f 4e4c 4c2b 2b0d 0a2a 205b 6074  # CoNLL++..* [`t
-00003170: 6f6d 6161 7273 656e 2f73 7061 6e2d 6d61  omaarsen/span-ma
-00003180: 726b 6572 2d78 6c6d 2d72 6f62 6572 7461  rker-xlm-roberta
-00003190: 2d6c 6172 6765 2d63 6f6e 6c6c 7070 2d64  -large-conllpp-d
-000031a0: 6f63 2d63 6f6e 7465 7874 605d 2868 7474  oc-context`](htt
-000031b0: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
-000031c0: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
-000031d0: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
-000031e0: 6265 7274 612d 6c61 7267 652d 636f 6e6c  berta-large-conl
-000031f0: 6c70 702d 646f 632d 636f 6e74 6578 7429  lpp-doc-context)
-00003200: 2077 6173 2074 7261 696e 6564 2069 6e20   was trained in 
-00003210: 616e 2068 6f75 7220 7573 696e 6720 7468  an hour using th
-00003220: 6520 6078 6c6d 2d72 6f62 6572 7461 2d6c  e `xlm-roberta-l
-00003230: 6172 6765 6020 656e 636f 6465 7220 6f6e  arge` encoder on
-00003240: 2074 6865 2043 6f4e 4c4c 2b2b 2064 6174   the CoNLL++ dat
-00003250: 6173 6574 2e20 5573 696e 6720 5b64 6f63  aset. Using [doc
-00003260: 756d 656e 742d 6c65 7665 6c20 636f 6e74  ument-level cont
-00003270: 6578 745d 2868 7474 7073 3a2f 2f74 6f6d  ext](https://tom
-00003280: 6161 7273 656e 2e67 6974 6875 622e 696f  aarsen.github.io
-00003290: 2f53 7061 6e4d 6172 6b65 724e 4552 2f6e  /SpanMarkerNER/n
-000032a0: 6f74 6562 6f6f 6b73 2f64 6f63 756d 656e  otebooks/documen
-000032b0: 745f 6c65 7665 6c5f 636f 6e74 6578 742e  t_level_context.
-000032c0: 6874 6d6c 292c 2069 7420 7265 6163 6865  html), it reache
-000032d0: 7320 6120 7665 7279 2063 6f6d 7065 7469  s a very competi
-000032e0: 7469 7665 2030 2e39 3535 2046 312e 2046  tive 0.955 F1. F
-000032f0: 6f72 2074 6865 2062 6573 7420 7065 7266  or the best perf
-00003300: 6f72 6d61 6e63 652c 2069 6e66 6572 656e  ormance, inferen
-00003310: 6365 2073 686f 756c 6420 6265 2070 6572  ce should be per
-00003320: 666f 726d 6564 2075 7369 6e67 2064 6f63  formed using doc
-00003330: 756d 656e 742d 6c65 7665 6c20 636f 6e74  ument-level cont
-00003340: 6578 7420 285b 646f 6373 5d28 6874 7470  ext ([docs](http
-00003350: 733a 2f2f 746f 6d61 6172 7365 6e2e 6769  s://tomaarsen.gi
-00003360: 7468 7562 2e69 6f2f 5370 616e 4d61 726b  thub.io/SpanMark
-00003370: 6572 4e45 522f 6e6f 7465 626f 6f6b 732f  erNER/notebooks/
-00003380: 646f 6375 6d65 6e74 5f6c 6576 656c 5f63  document_level_c
-00003390: 6f6e 7465 7874 2e68 746d 6c23 496e 6665  ontext.html#Infe
-000033a0: 7265 6e63 6529 292e 0d0a 0d0a 2323 2055  rence)).....## U
-000033b0: 7369 6e67 2070 7265 7472 6169 6e65 6420  sing pretrained 
-000033c0: 5370 616e 4d61 726b 6572 206d 6f64 656c  SpanMarker model
-000033d0: 7320 7769 7468 2073 7061 4379 0d0a 416c  s with spaCy..Al
-000033e0: 6c20 5b53 7061 6e4d 6172 6b65 7220 6d6f  l [SpanMarker mo
-000033f0: 6465 6c73 206f 6e20 7468 6520 4875 6767  dels on the Hugg
-00003400: 696e 6720 4661 6365 2048 7562 5d28 6874  ing Face Hub](ht
-00003410: 7470 733a 2f2f 6875 6767 696e 6766 6163  tps://huggingfac
-00003420: 652e 636f 2f6d 6f64 656c 733f 6c69 6272  e.co/models?libr
-00003430: 6172 793d 7370 616e 2d6d 6172 6b65 7229  ary=span-marker)
-00003440: 2063 616e 2061 6c73 6f20 6265 2065 6173   can also be eas
-00003450: 696c 7920 7573 6564 2069 6e20 7370 6143  ily used in spaC
-00003460: 792e 2049 7427 7320 6173 2073 696d 706c  y. It's as simpl
-00003470: 6520 6173 2069 6e63 6c75 6469 6e67 2031  e as including 1
-00003480: 206c 696e 6520 746f 2061 6464 2074 6865   line to add the
-00003490: 2060 7370 616e 5f6d 6172 6b65 7260 2070   `span_marker` p
-000034a0: 6970 656c 696e 652e 2053 6565 2074 6865  ipeline. See the
-000034b0: 2044 6f63 756d 656e 7461 7469 6f6e 206f   Documentation o
-000034c0: 7220 4150 4920 5265 6665 7265 6e63 6520  r API Reference 
-000034d0: 666f 7220 6d6f 7265 2069 6e66 6f72 6d61  for more informa
-000034e0: 7469 6f6e 2e0d 0a60 6060 7079 7468 6f6e  tion...```python
-000034f0: 0d0a 696d 706f 7274 2073 7061 6379 0d0a  ..import spacy..
-00003500: 0d0a 2320 4c6f 6164 2074 6865 2073 7061  ..# Load the spa
-00003510: 4379 206d 6f64 656c 2077 6974 6820 7468  Cy model with th
-00003520: 6520 7370 616e 5f6d 6172 6b65 7220 7069  e span_marker pi
-00003530: 7065 6c69 6e65 2063 6f6d 706f 6e65 6e74  peline component
-00003540: 0d0a 6e6c 7020 3d20 7370 6163 792e 6c6f  ..nlp = spacy.lo
-00003550: 6164 2822 656e 5f63 6f72 655f 7765 625f  ad("en_core_web_
-00003560: 736d 222c 2064 6973 6162 6c65 3d5b 226e  sm", disable=["n
-00003570: 6572 225d 290d 0a6e 6c70 2e61 6464 5f70  er"])..nlp.add_p
-00003580: 6970 6528 2273 7061 6e5f 6d61 726b 6572  ipe("span_marker
-00003590: 222c 2063 6f6e 6669 673d 7b22 6d6f 6465  ", config={"mode
-000035a0: 6c22 3a20 2274 6f6d 6161 7273 656e 2f73  l": "tomaarsen/s
-000035b0: 7061 6e2d 6d61 726b 6572 2d72 6f62 6572  pan-marker-rober
-000035c0: 7461 2d6c 6172 6765 2d6f 6e74 6f6e 6f74  ta-large-ontonot
-000035d0: 6573 3522 7d29 0d0a 0d0a 2320 4665 6564  es5"})....# Feed
-000035e0: 2073 6f6d 6520 7465 7874 2074 6872 6f75   some text throu
-000035f0: 6768 2074 6865 206d 6f64 656c 2074 6f20  gh the model to 
-00003600: 6765 7420 6120 7370 6163 7920 446f 630d  get a spacy Doc.
-00003610: 0a74 6578 7420 3d20 2222 2243 6c65 6f70  .text = """Cleop
-00003620: 6174 7261 2056 4949 2c20 616c 736f 206b  atra VII, also k
-00003630: 6e6f 776e 2061 7320 436c 656f 7061 7472  nown as Cleopatr
-00003640: 6120 7468 6520 4772 6561 742c 2077 6173  a the Great, was
-00003650: 2074 6865 206c 6173 7420 6163 7469 7665   the last active
-00003660: 2072 756c 6572 206f 6620 7468 6520 5c0d   ruler of the \.
-00003670: 0a50 746f 6c65 6d61 6963 204b 696e 6764  .Ptolemaic Kingd
-00003680: 6f6d 206f 6620 4567 7970 742e 2053 6865  om of Egypt. She
-00003690: 2077 6173 2062 6f72 6e20 696e 2036 3920   was born in 69 
-000036a0: 4243 4520 616e 6420 7275 6c65 6420 4567  BCE and ruled Eg
-000036b0: 7970 7420 6672 6f6d 2035 3120 4243 4520  ypt from 51 BCE 
-000036c0: 756e 7469 6c20 6865 7220 5c0d 0a64 6561  until her \..dea
-000036d0: 7468 2069 6e20 3330 2042 4345 2e22 2222  th in 30 BCE."""
-000036e0: 0d0a 646f 6320 3d20 6e6c 7028 7465 7874  ..doc = nlp(text
-000036f0: 290d 0a0d 0a23 2041 6e64 206c 6f6f 6b20  )....# And look 
-00003700: 6174 2074 6865 2065 6e74 6974 6965 730d  at the entities.
-00003710: 0a70 7269 6e74 285b 2865 6e74 6974 792c  .print([(entity,
-00003720: 2065 6e74 6974 792e 6c61 6265 6c5f 2920   entity.label_) 
-00003730: 666f 7220 656e 7469 7479 2069 6e20 646f  for entity in do
-00003740: 632e 656e 7473 5d29 0d0a 2222 220d 0a5b  c.ents]).."""..[
-00003750: 2843 6c65 6f70 6174 7261 2056 4949 2c20  (Cleopatra VII, 
-00003760: 2250 4552 534f 4e22 292c 2028 436c 656f  "PERSON"), (Cleo
-00003770: 7061 7472 6120 7468 6520 4772 6561 742c  patra the Great,
-00003780: 2022 5045 5253 4f4e 2229 2c20 2874 6865   "PERSON"), (the
-00003790: 2050 746f 6c65 6d61 6963 204b 696e 6764   Ptolemaic Kingd
-000037a0: 6f6d 206f 6620 4567 7970 742c 2022 4750  om of Egypt, "GP
-000037b0: 4522 292c 0d0a 2836 3920 4243 452c 2022  E"),..(69 BCE, "
-000037c0: 4441 5445 2229 2c20 2845 6779 7074 2c20  DATE"), (Egypt, 
-000037d0: 2247 5045 2229 2c20 2835 3120 4243 452c  "GPE"), (51 BCE,
-000037e0: 2022 4441 5445 2229 2c20 2833 3020 4243   "DATE"), (30 BC
-000037f0: 452c 2022 4441 5445 2229 5d0d 0a22 2222  E, "DATE")].."""
-00003800: 0d0a 6060 600d 0a21 5b69 6d61 6765 5d28  ..```..![image](
-00003810: 6874 7470 733a 2f2f 7573 6572 2d69 6d61  https://user-ima
-00003820: 6765 732e 6769 7468 7562 7573 6572 636f  ges.githubuserco
-00003830: 6e74 656e 742e 636f 6d2f 3337 3632 3134  ntent.com/376214
-00003840: 3931 2f32 3436 3137 3036 3233 2d36 3335  91/246170623-635
-00003850: 3163 6237 652d 6262 6230 2d34 3437 322d  1cb7e-bbb0-4472-
-00003860: 6166 3136 2d39 6133 3531 6132 3533 6461  af16-9a351a253da
-00003870: 392e 706e 6729 0d0a 0d0a 2323 2043 6f6e  9.png)....## Con
-00003880: 7465 7874 0d0a 3c68 3120 616c 6967 6e3d  text..<h1 align=
-00003890: 2263 656e 7465 7222 3e0d 0a20 2020 203c  "center">..    <
-000038a0: 6120 6872 6566 3d22 6874 7470 733a 2f2f  a href="https://
-000038b0: 6769 7468 7562 2e63 6f6d 2f61 7267 696c  github.com/argil
-000038c0: 6c61 2d69 6f2f 6172 6769 6c6c 6122 3e0d  la-io/argilla">.
-000038d0: 0a20 2020 203c 696d 6720 7372 633d 2268  .    <img src="h
-000038e0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
-000038f0: 6d2f 6476 7372 6570 6f2f 696d 6773 2f72  m/dvsrepo/imgs/r
-00003900: 6177 2f6d 6169 6e2f 7267 2e73 7667 2220  aw/main/rg.svg" 
-00003910: 616c 743d 2241 7267 696c 6c61 2220 7769  alt="Argilla" wi
-00003920: 6474 683d 2231 3530 223e 0d0a 2020 2020  dth="150">..    
-00003930: 3c2f 613e 0d0a 3c2f 6831 3e0d 0a0d 0a49  </a>..</h1>....I
-00003940: 2068 6176 6520 6465 7665 6c6f 7065 6420   have developed 
-00003950: 7468 6973 206c 6962 7261 7279 2061 7320  this library as 
-00003960: 6120 7061 7274 206f 6620 6d79 2074 6865  a part of my the
-00003970: 7369 7320 776f 726b 2061 7420 5b41 7267  sis work at [Arg
-00003980: 696c 6c61 5d28 6874 7470 733a 2f2f 6769  illa](https://gi
-00003990: 7468 7562 2e63 6f6d 2f61 7267 696c 6c61  thub.com/argilla
-000039a0: 2d69 6f2f 6172 6769 6c6c 6129 2e0d 0a46  -io/argilla)...F
-000039b0: 6565 6c20 6672 6565 2074 6f20 e2ad 9020  eel free to ... 
-000039c0: 7374 6172 206f 7220 7761 7463 6820 7468  star or watch th
-000039d0: 6520 5370 616e 4d61 726b 6572 2072 6570  e SpanMarker rep
-000039e0: 6f73 6974 6f72 7920 746f 2067 6574 206e  ository to get n
-000039f0: 6f74 6966 6965 6420 7768 656e 206d 7920  otified when my 
-00003a00: 7468 6573 6973 2069 7320 7075 626c 6973  thesis is publis
-00003a10: 6865 642e 0d0a 0d0a 2323 2043 6861 6e67  hed.....## Chang
-00003a20: 656c 6f67 0d0a 5365 6520 5b43 4841 4e47  elog..See [CHANG
-00003a30: 454c 4f47 2e6d 645d 2843 4841 4e47 454c  ELOG.md](CHANGEL
-00003a40: 4f47 2e6d 6429 2066 6f72 206e 6577 7320  OG.md) for news 
-00003a50: 6f6e 2061 6c6c 2053 7061 6e4d 6172 6b65  on all SpanMarke
-00003a60: 7220 7665 7273 696f 6e73 2e0d 0a0d 0a23  r versions.....#
-00003a70: 2320 4c69 6365 6e73 650d 0a53 6565 205b  # License..See [
-00003a80: 4c49 4345 4e53 455d 284c 4943 454e 5345  LICENSE](LICENSE
-00003a90: 2e6d 6429 2066 6f72 2074 6865 2063 7572  .md) for the cur
-00003aa0: 7265 6e74 206c 6963 656e 7365 2e0d 0a    rent license...
+000013d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000013f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001400: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001410: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001420: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001430: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001440: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001450: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001460: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d7c 3a2d  -------------|:-
+00001470: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001480: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001490: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001500: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001510: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001520: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001530: 7c3a 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  |:--------------
+00001540: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001550: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001560: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001570: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001580: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001590: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000015f0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001600: 2d2d 2d2d 2d2d 2d2d 2d7c 0d0a 7c20 5b21  ---------|..| [!
+00001610: 5b4f 7065 6e20 496e 2043 6f6c 6162 5d28  [Open In Colab](
+00001620: 6874 7470 733a 2f2f 636f 6c61 622e 7265  https://colab.re
+00001630: 7365 6172 6368 2e67 6f6f 676c 652e 636f  search.google.co
+00001640: 6d2f 6173 7365 7473 2f63 6f6c 6162 2d62  m/assets/colab-b
+00001650: 6164 6765 2e73 7667 295d 2868 7474 7073  adge.svg)](https
+00001660: 3a2f 2f63 6f6c 6162 2e72 6573 6561 7263  ://colab.researc
+00001670: 682e 676f 6f67 6c65 2e63 6f6d 2f67 6974  h.google.com/git
+00001680: 6875 622f 746f 6d61 6172 7365 6e2f 5370  hub/tomaarsen/Sp
+00001690: 616e 4d61 726b 6572 4e45 522f 626c 6f62  anMarkerNER/blob
+000016a0: 2f6d 6169 6e2f 6e6f 7465 626f 6f6b 732f  /main/notebooks/
+000016b0: 6765 7474 696e 675f 7374 6172 7465 642e  getting_started.
+000016c0: 6970 796e 6229 2020 2020 2020 2020 2020  ipynb)          
+000016d0: 2020 2020 2020 2020 2020 2020 207c 205b               | [
+000016e0: 215b 4b61 6767 6c65 5d28 6874 7470 733a  ![Kaggle](https:
+000016f0: 2f2f 6b61 6767 6c65 2e63 6f6d 2f73 7461  //kaggle.com/sta
+00001700: 7469 632f 696d 6167 6573 2f6f 7065 6e2d  tic/images/open-
+00001710: 696e 2d6b 6167 676c 652e 7376 6729 5d28  in-kaggle.svg)](
+00001720: 6874 7470 733a 2f2f 6b61 6767 6c65 2e63  https://kaggle.c
+00001730: 6f6d 2f6b 6572 6e65 6c73 2f77 656c 636f  om/kernels/welco
+00001740: 6d65 3f73 7263 3d68 7474 7073 3a2f 2f67  me?src=https://g
+00001750: 6974 6875 622e 636f 6d2f 746f 6d61 6172  ithub.com/tomaar
+00001760: 7365 6e2f 5370 616e 4d61 726b 6572 4e45  sen/SpanMarkerNE
+00001770: 522f 626c 6f62 2f6d 6169 6e2f 6e6f 7465  R/blob/main/note
+00001780: 626f 6f6b 732f 6765 7474 696e 675f 7374  books/getting_st
+00001790: 6172 7465 642e 6970 796e 6229 2020 2020  arted.ipynb)    
+000017a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000017b0: 2020 207c 205b 215b 4772 6164 6965 6e74     | [![Gradient
+000017c0: 5d28 6874 7470 733a 2f2f 6173 7365 7473  ](https://assets
+000017d0: 2e70 6170 6572 7370 6163 652e 696f 2f69  .paperspace.io/i
+000017e0: 6d67 2f67 7261 6469 656e 742d 6261 6467  mg/gradient-badg
+000017f0: 652e 7376 6729 5d28 6874 7470 733a 2f2f  e.svg)](https://
+00001800: 636f 6e73 6f6c 652e 7061 7065 7273 7061  console.paperspa
+00001810: 6365 2e63 6f6d 2f67 6974 6875 622f 746f  ce.com/github/to
+00001820: 6d61 6172 7365 6e2f 5370 616e 4d61 726b  maarsen/SpanMark
+00001830: 6572 4e45 522f 626c 6f62 2f6d 6169 6e2f  erNER/blob/main/
+00001840: 6e6f 7465 626f 6f6b 732f 6765 7474 696e  notebooks/gettin
+00001850: 675f 7374 6172 7465 642e 6970 796e 6229  g_started.ipynb)
+00001860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001870: 2020 2020 2020 207c 205b 215b 4f70 656e         | [![Open
+00001880: 2049 6e20 5361 6765 4d61 6b65 7220 5374   In SageMaker St
+00001890: 7564 696f 204c 6162 5d28 6874 7470 733a  udio Lab](https:
+000018a0: 2f2f 7374 7564 696f 6c61 622e 7361 6765  //studiolab.sage
+000018b0: 6d61 6b65 722e 6177 732f 7374 7564 696f  maker.aws/studio
+000018c0: 6c61 622e 7376 6729 5d28 6874 7470 733a  lab.svg)](https:
+000018d0: 2f2f 7374 7564 696f 6c61 622e 7361 6765  //studiolab.sage
+000018e0: 6d61 6b65 722e 6177 732f 696d 706f 7274  maker.aws/import
+000018f0: 2f67 6974 6875 622f 746f 6d61 6172 7365  /github/tomaarse
+00001900: 6e2f 5370 616e 4d61 726b 6572 4e45 522f  n/SpanMarkerNER/
+00001910: 626c 6f62 2f6d 6169 6e2f 6e6f 7465 626f  blob/main/notebo
+00001920: 6f6b 732f 6765 7474 696e 675f 7374 6172  oks/getting_star
+00001930: 7465 642e 6970 796e 6229 2020 2020 2020  ted.ipynb)      
+00001940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001950: 207c 0d0a 0d0a 6060 6070 7974 686f 6e0d   |....```python.
+00001960: 0a66 726f 6d20 6461 7461 7365 7473 2069  .from datasets i
+00001970: 6d70 6f72 7420 6c6f 6164 5f64 6174 6173  mport load_datas
+00001980: 6574 0d0a 6672 6f6d 2074 7261 6e73 666f  et..from transfo
+00001990: 726d 6572 7320 696d 706f 7274 2054 7261  rmers import Tra
+000019a0: 696e 696e 6741 7267 756d 656e 7473 0d0a  iningArguments..
+000019b0: 6672 6f6d 2073 7061 6e5f 6d61 726b 6572  from span_marker
+000019c0: 2069 6d70 6f72 7420 5370 616e 4d61 726b   import SpanMark
+000019d0: 6572 4d6f 6465 6c2c 2054 7261 696e 6572  erModel, Trainer
+000019e0: 0d0a 0d0a 0d0a 6465 6620 6d61 696e 2829  ......def main()
+000019f0: 202d 3e20 4e6f 6e65 3a0d 0a20 2020 2023   -> None:..    #
+00001a00: 204c 6f61 6420 7468 6520 6461 7461 7365   Load the datase
+00001a10: 742c 2065 6e73 7572 6520 2274 6f6b 656e  t, ensure "token
+00001a20: 7322 2061 6e64 2022 6e65 725f 7461 6773  s" and "ner_tags
+00001a30: 2220 636f 6c75 6d6e 732c 2061 6e64 2067  " columns, and g
+00001a40: 6574 2061 206c 6973 7420 6f66 206c 6162  et a list of lab
+00001a50: 656c 730d 0a20 2020 2064 6174 6173 6574  els..    dataset
+00001a60: 203d 206c 6f61 645f 6461 7461 7365 7428   = load_dataset(
+00001a70: 2244 464b 492d 534c 542f 6665 772d 6e65  "DFKI-SLT/few-ne
+00001a80: 7264 222c 2022 7375 7065 7276 6973 6564  rd", "supervised
+00001a90: 2229 0d0a 2020 2020 6461 7461 7365 7420  ")..    dataset 
+00001aa0: 3d20 6461 7461 7365 742e 7265 6d6f 7665  = dataset.remove
+00001ab0: 5f63 6f6c 756d 6e73 2822 6e65 725f 7461  _columns("ner_ta
+00001ac0: 6773 2229 0d0a 2020 2020 6461 7461 7365  gs")..    datase
+00001ad0: 7420 3d20 6461 7461 7365 742e 7265 6e61  t = dataset.rena
+00001ae0: 6d65 5f63 6f6c 756d 6e28 2266 696e 655f  me_column("fine_
+00001af0: 6e65 725f 7461 6773 222c 2022 6e65 725f  ner_tags", "ner_
+00001b00: 7461 6773 2229 0d0a 2020 2020 6c61 6265  tags")..    labe
+00001b10: 6c73 203d 2064 6174 6173 6574 5b22 7472  ls = dataset["tr
+00001b20: 6169 6e22 5d2e 6665 6174 7572 6573 5b22  ain"].features["
+00001b30: 6e65 725f 7461 6773 225d 2e66 6561 7475  ner_tags"].featu
+00001b40: 7265 2e6e 616d 6573 0d0a 0d0a 2020 2020  re.names....    
+00001b50: 2320 496e 6974 6961 6c69 7a65 2061 2053  # Initialize a S
+00001b60: 7061 6e4d 6172 6b65 7220 6d6f 6465 6c20  panMarker model 
+00001b70: 7573 696e 6720 6120 7072 6574 7261 696e  using a pretrain
+00001b80: 6564 2042 4552 542d 7374 796c 6520 656e  ed BERT-style en
+00001b90: 636f 6465 720d 0a20 2020 206d 6f64 656c  coder..    model
+00001ba0: 5f6e 616d 6520 3d20 2262 6572 742d 6261  _name = "bert-ba
+00001bb0: 7365 2d63 6173 6564 220d 0a20 2020 206d  se-cased"..    m
+00001bc0: 6f64 656c 203d 2053 7061 6e4d 6172 6b65  odel = SpanMarke
+00001bd0: 724d 6f64 656c 2e66 726f 6d5f 7072 6574  rModel.from_pret
+00001be0: 7261 696e 6564 280d 0a20 2020 2020 2020  rained(..       
+00001bf0: 206d 6f64 656c 5f6e 616d 652c 0d0a 2020   model_name,..  
+00001c00: 2020 2020 2020 6c61 6265 6c73 3d6c 6162        labels=lab
+00001c10: 656c 732c 0d0a 2020 2020 2020 2020 2320  els,..        # 
+00001c20: 5370 616e 4d61 726b 6572 2068 7970 6572  SpanMarker hyper
+00001c30: 7061 7261 6d65 7465 7273 3a0d 0a20 2020  parameters:..   
+00001c40: 2020 2020 206d 6f64 656c 5f6d 6178 5f6c       model_max_l
+00001c50: 656e 6774 683d 3235 362c 0d0a 2020 2020  ength=256,..    
+00001c60: 2020 2020 6d61 726b 6572 5f6d 6178 5f6c      marker_max_l
+00001c70: 656e 6774 683d 3132 382c 0d0a 2020 2020  ength=128,..    
+00001c80: 2020 2020 656e 7469 7479 5f6d 6178 5f6c      entity_max_l
+00001c90: 656e 6774 683d 382c 0d0a 2020 2020 290d  ength=8,..    ).
+00001ca0: 0a0d 0a20 2020 2023 2050 7265 7061 7265  ...    # Prepare
+00001cb0: 2074 6865 20f0 9fa4 9720 7472 616e 7366   the .... transf
+00001cc0: 6f72 6d65 7273 2074 7261 696e 696e 6720  ormers training 
+00001cd0: 6172 6775 6d65 6e74 730d 0a20 2020 2061  arguments..    a
+00001ce0: 7267 7320 3d20 5472 6169 6e69 6e67 4172  rgs = TrainingAr
+00001cf0: 6775 6d65 6e74 7328 0d0a 2020 2020 2020  guments(..      
+00001d00: 2020 6f75 7470 7574 5f64 6972 3d22 6d6f    output_dir="mo
+00001d10: 6465 6c73 2f73 7061 6e5f 6d61 726b 6572  dels/span_marker
+00001d20: 5f62 6572 745f 6261 7365 5f63 6173 6564  _bert_base_cased
+00001d30: 5f66 6577 6e65 7264 5f66 696e 655f 7375  _fewnerd_fine_su
+00001d40: 7065 7222 2c0d 0a20 2020 2020 2020 2023  per",..        #
+00001d50: 2054 7261 696e 696e 6720 4879 7065 7270   Training Hyperp
+00001d60: 6172 616d 6574 6572 733a 0d0a 2020 2020  arameters:..    
+00001d70: 2020 2020 6c65 6172 6e69 6e67 5f72 6174      learning_rat
+00001d80: 653d 3565 2d35 2c0d 0a20 2020 2020 2020  e=5e-5,..       
+00001d90: 2070 6572 5f64 6576 6963 655f 7472 6169   per_device_trai
+00001da0: 6e5f 6261 7463 685f 7369 7a65 3d33 322c  n_batch_size=32,
+00001db0: 0d0a 2020 2020 2020 2020 7065 725f 6465  ..        per_de
+00001dc0: 7669 6365 5f65 7661 6c5f 6261 7463 685f  vice_eval_batch_
+00001dd0: 7369 7a65 3d33 322c 0d0a 2020 2020 2020  size=32,..      
+00001de0: 2020 6e75 6d5f 7472 6169 6e5f 6570 6f63    num_train_epoc
+00001df0: 6873 3d33 2c0d 0a20 2020 2020 2020 2077  hs=3,..        w
+00001e00: 6569 6768 745f 6465 6361 793d 302e 3031  eight_decay=0.01
+00001e10: 2c0d 0a20 2020 2020 2020 2077 6172 6d75  ,..        warmu
+00001e20: 705f 7261 7469 6f3d 302e 312c 0d0a 2020  p_ratio=0.1,..  
+00001e30: 2020 2020 2020 6266 3136 3d54 7275 652c        bf16=True,
+00001e40: 2020 2320 5265 706c 6163 6520 6062 6631    # Replace `bf1
+00001e50: 3660 2077 6974 6820 6066 7031 3660 2069  6` with `fp16` i
+00001e60: 6620 796f 7572 2068 6172 6477 6172 6520  f your hardware 
+00001e70: 6361 6e27 7420 7573 6520 6266 3136 2e0d  can't use bf16..
+00001e80: 0a20 2020 2020 2020 2023 204f 7468 6572  .        # Other
+00001e90: 2054 7261 696e 696e 6720 7061 7261 6d65   Training parame
+00001ea0: 7465 7273 0d0a 2020 2020 2020 2020 6c6f  ters..        lo
+00001eb0: 6767 696e 675f 6669 7273 745f 7374 6570  gging_first_step
+00001ec0: 3d54 7275 652c 0d0a 2020 2020 2020 2020  =True,..        
+00001ed0: 6c6f 6767 696e 675f 7374 6570 733d 3530  logging_steps=50
+00001ee0: 2c0d 0a20 2020 2020 2020 2065 7661 6c75  ,..        evalu
+00001ef0: 6174 696f 6e5f 7374 7261 7465 6779 3d22  ation_strategy="
+00001f00: 7374 6570 7322 2c0d 0a20 2020 2020 2020  steps",..       
+00001f10: 2073 6176 655f 7374 7261 7465 6779 3d22   save_strategy="
+00001f20: 7374 6570 7322 2c0d 0a20 2020 2020 2020  steps",..       
+00001f30: 2065 7661 6c5f 7374 6570 733d 3330 3030   eval_steps=3000
+00001f40: 2c0d 0a20 2020 2020 2020 2073 6176 655f  ,..        save_
+00001f50: 746f 7461 6c5f 6c69 6d69 743d 322c 0d0a  total_limit=2,..
+00001f60: 2020 2020 2020 2020 6461 7461 6c6f 6164          dataload
+00001f70: 6572 5f6e 756d 5f77 6f72 6b65 7273 3d32  er_num_workers=2
+00001f80: 2c0d 0a20 2020 2029 0d0a 0d0a 2020 2020  ,..    )....    
+00001f90: 2320 496e 6974 6961 6c69 7a65 2074 6865  # Initialize the
+00001fa0: 2074 7261 696e 6572 2075 7369 6e67 206f   trainer using o
+00001fb0: 7572 206d 6f64 656c 2c20 7472 6169 6e69  ur model, traini
+00001fc0: 6e67 2061 7267 7320 2620 6461 7461 7365  ng args & datase
+00001fd0: 742c 2061 6e64 2074 7261 696e 0d0a 2020  t, and train..  
+00001fe0: 2020 7472 6169 6e65 7220 3d20 5472 6169    trainer = Trai
+00001ff0: 6e65 7228 0d0a 2020 2020 2020 2020 6d6f  ner(..        mo
+00002000: 6465 6c3d 6d6f 6465 6c2c 0d0a 2020 2020  del=model,..    
+00002010: 2020 2020 6172 6773 3d61 7267 732c 0d0a      args=args,..
+00002020: 2020 2020 2020 2020 7472 6169 6e5f 6461          train_da
+00002030: 7461 7365 743d 6461 7461 7365 745b 2274  taset=dataset["t
+00002040: 7261 696e 225d 2c0d 0a20 2020 2020 2020  rain"],..       
+00002050: 2065 7661 6c5f 6461 7461 7365 743d 6461   eval_dataset=da
+00002060: 7461 7365 745b 2276 616c 6964 6174 696f  taset["validatio
+00002070: 6e22 5d2c 0d0a 2020 2020 290d 0a20 2020  n"],..    )..   
+00002080: 2074 7261 696e 6572 2e74 7261 696e 2829   trainer.train()
+00002090: 0d0a 2020 2020 7472 6169 6e65 722e 7361  ..    trainer.sa
+000020a0: 7665 5f6d 6f64 656c 2822 6d6f 6465 6c73  ve_model("models
+000020b0: 2f73 7061 6e5f 6d61 726b 6572 5f62 6572  /span_marker_ber
+000020c0: 745f 6261 7365 5f63 6173 6564 5f66 6577  t_base_cased_few
+000020d0: 6e65 7264 5f66 696e 655f 7375 7065 722f  nerd_fine_super/
+000020e0: 6368 6563 6b70 6f69 6e74 2d66 696e 616c  checkpoint-final
+000020f0: 2229 0d0a 0d0a 2020 2020 2320 436f 6d70  ")....    # Comp
+00002100: 7574 6520 2620 7361 7665 2074 6865 206d  ute & save the m
+00002110: 6574 7269 6373 206f 6e20 7468 6520 7465  etrics on the te
+00002120: 7374 2073 6574 0d0a 2020 2020 6d65 7472  st set..    metr
+00002130: 6963 7320 3d20 7472 6169 6e65 722e 6576  ics = trainer.ev
+00002140: 616c 7561 7465 2864 6174 6173 6574 5b22  aluate(dataset["
+00002150: 7465 7374 225d 2c20 6d65 7472 6963 5f6b  test"], metric_k
+00002160: 6579 5f70 7265 6669 783d 2274 6573 7422  ey_prefix="test"
+00002170: 290d 0a20 2020 2074 7261 696e 6572 2e73  )..    trainer.s
+00002180: 6176 655f 6d65 7472 6963 7328 2274 6573  ave_metrics("tes
+00002190: 7422 2c20 6d65 7472 6963 7329 0d0a 0d0a  t", metrics)....
+000021a0: 0d0a 6966 205f 5f6e 616d 655f 5f20 3d3d  ..if __name__ ==
+000021b0: 2022 5f5f 6d61 696e 5f5f 223a 0d0a 2020   "__main__":..  
+000021c0: 2020 6d61 696e 2829 0d0a 6060 600d 0a0d    main()..```...
+000021d0: 0a23 2323 2049 6e66 6572 656e 6365 0d0a  .### Inference..
+000021e0: 6060 6070 7974 686f 6e0d 0a66 726f 6d20  ```python..from 
+000021f0: 7370 616e 5f6d 6172 6b65 7220 696d 706f  span_marker impo
+00002200: 7274 2053 7061 6e4d 6172 6b65 724d 6f64  rt SpanMarkerMod
+00002210: 656c 0d0a 0d0a 2320 446f 776e 6c6f 6164  el....# Download
+00002220: 2066 726f 6d20 7468 6520 f09f a497 2048   from the .... H
+00002230: 7562 0d0a 6d6f 6465 6c20 3d20 5370 616e  ub..model = Span
+00002240: 4d61 726b 6572 4d6f 6465 6c2e 6672 6f6d  MarkerModel.from
+00002250: 5f70 7265 7472 6169 6e65 6428 2274 6f6d  _pretrained("tom
+00002260: 6161 7273 656e 2f73 7061 6e2d 6d61 726b  aarsen/span-mark
+00002270: 6572 2d62 6572 742d 6261 7365 2d66 6577  er-bert-base-few
+00002280: 6e65 7264 2d66 696e 652d 7375 7065 7222  nerd-fine-super"
+00002290: 290d 0a23 2052 756e 2069 6e66 6572 656e  )..# Run inferen
+000022a0: 6365 0d0a 656e 7469 7469 6573 203d 206d  ce..entities = m
+000022b0: 6f64 656c 2e70 7265 6469 6374 2822 416d  odel.predict("Am
+000022c0: 656c 6961 2045 6172 6861 7274 2066 6c65  elia Earhart fle
+000022d0: 7720 6865 7220 7369 6e67 6c65 2065 6e67  w her single eng
+000022e0: 696e 6520 4c6f 636b 6865 6564 2056 6567  ine Lockheed Veg
+000022f0: 6120 3542 2061 6372 6f73 7320 7468 6520  a 5B across the 
+00002300: 4174 6c61 6e74 6963 2074 6f20 5061 7269  Atlantic to Pari
+00002310: 732e 2229 0d0a 5b7b 2773 7061 6e27 3a20  s.")..[{'span': 
+00002320: 2741 6d65 6c69 6120 4561 7268 6172 7427  'Amelia Earhart'
+00002330: 2c20 276c 6162 656c 273a 2027 7065 7273  , 'label': 'pers
+00002340: 6f6e 2d6f 7468 6572 272c 2027 7363 6f72  on-other', 'scor
+00002350: 6527 3a20 302e 3736 3539 3539 3733 3936  e': 0.7659597396
+00002360: 3835 3035 3836 2c20 2763 6861 725f 7374  850586, 'char_st
+00002370: 6172 745f 696e 6465 7827 3a20 302c 2027  art_index': 0, '
+00002380: 6368 6172 5f65 6e64 5f69 6e64 6578 273a  char_end_index':
+00002390: 2031 347d 2c0d 0a20 7b27 7370 616e 273a   14},.. {'span':
+000023a0: 2027 4c6f 636b 6865 6564 2056 6567 6120   'Lockheed Vega 
+000023b0: 3542 272c 2027 6c61 6265 6c27 3a20 2770  5B', 'label': 'p
+000023c0: 726f 6475 6374 2d61 6972 706c 616e 6527  roduct-airplane'
+000023d0: 2c20 2773 636f 7265 273a 2030 2e39 3732  , 'score': 0.972
+000023e0: 3537 3835 3835 3134 3738 3537 372c 2027  5785851478577, '
+000023f0: 6368 6172 5f73 7461 7274 5f69 6e64 6578  char_start_index
+00002400: 273a 2033 382c 2027 6368 6172 5f65 6e64  ': 38, 'char_end
+00002410: 5f69 6e64 6578 273a 2035 347d 2c0d 0a20  _index': 54},.. 
+00002420: 7b27 7370 616e 273a 2027 4174 6c61 6e74  {'span': 'Atlant
+00002430: 6963 272c 2027 6c61 6265 6c27 3a20 276c  ic', 'label': 'l
+00002440: 6f63 6174 696f 6e2d 626f 6469 6573 6f66  ocation-bodiesof
+00002450: 7761 7465 7227 2c20 2773 636f 7265 273a  water', 'score':
+00002460: 2030 2e37 3538 3736 3739 3032 3835 3131   0.7587679028511
+00002470: 3034 372c 2027 6368 6172 5f73 7461 7274  047, 'char_start
+00002480: 5f69 6e64 6578 273a 2036 362c 2027 6368  _index': 66, 'ch
+00002490: 6172 5f65 6e64 5f69 6e64 6578 273a 2037  ar_end_index': 7
+000024a0: 347d 2c0d 0a20 7b27 7370 616e 273a 2027  4},.. {'span': '
+000024b0: 5061 7269 7327 2c20 276c 6162 656c 273a  Paris', 'label':
+000024c0: 2027 6c6f 6361 7469 6f6e 2d47 5045 272c   'location-GPE',
+000024d0: 2027 7363 6f72 6527 3a20 302e 3938 3932   'score': 0.9892
+000024e0: 3339 3039 3636 3431 3534 3035 2c20 2763  390966415405, 'c
+000024f0: 6861 725f 7374 6172 745f 696e 6465 7827  har_start_index'
+00002500: 3a20 3738 2c20 2763 6861 725f 656e 645f  : 78, 'char_end_
+00002510: 696e 6465 7827 3a20 3833 7d5d 0d0a 6060  index': 83}]..``
+00002520: 600d 0a0d 0a3c 212d 2d20 4265 6361 7573  `....<!-- Becaus
+00002530: 6520 7468 6973 2077 6f72 6b20 6973 2062  e this work is b
+00002540: 6173 6564 206f 6e20 5b50 4c2d 4d61 726b  ased on [PL-Mark
+00002550: 6572 5d28 6874 7470 733a 2f2f 6172 7869  er](https://arxi
+00002560: 762e 6f72 672f 7064 662f 3231 3039 2e30  v.org/pdf/2109.0
+00002570: 3630 3637 7635 2e70 6466 292c 2079 6f75  6067v5.pdf), you
+00002580: 206d 6179 2065 7870 6563 7420 7369 6d69   may expect simi
+00002590: 6c61 7220 7265 7375 6c74 7320 746f 2069  lar results to i
+000025a0: 7473 205b 5061 7065 7273 2077 6974 6820  ts [Papers with 
+000025b0: 436f 6465 204c 6561 6465 7262 6f61 7264  Code Leaderboard
+000025c0: 5d28 6874 7470 733a 2f2f 7061 7065 7273  ](https://papers
+000025d0: 7769 7468 636f 6465 2e63 6f6d 2f70 6170  withcode.com/pap
+000025e0: 6572 2f70 6163 6b2d 746f 6765 7468 6572  er/pack-together
+000025f0: 2d65 6e74 6974 792d 616e 642d 7265 6c61  -entity-and-rela
+00002600: 7469 6f6e 2d65 7874 7261 6374 696f 6e29  tion-extraction)
+00002610: 2072 6573 756c 7473 2e20 2d2d 3e0d 0a0d   results. -->...
+00002620: 0a23 2320 5072 6574 7261 696e 6564 204d  .## Pretrained M
+00002630: 6f64 656c 730d 0a0d 0a41 6c6c 206d 6f64  odels....All mod
+00002640: 656c 7320 696e 2074 6869 7320 6c69 7374  els in this list
+00002650: 2063 6f6e 7461 696e 2060 7472 6169 6e2e   contain `train.
+00002660: 7079 6020 6669 6c65 7320 7468 6174 2073  py` files that s
+00002670: 686f 7720 7468 6520 7472 6169 6e69 6e67  how the training
+00002680: 2073 6372 6970 7473 2075 7365 6420 746f   scripts used to
+00002690: 2067 656e 6572 6174 6520 7468 656d 2e20   generate them. 
+000026a0: 4164 6469 7469 6f6e 616c 6c79 2c20 616c  Additionally, al
+000026b0: 6c20 7472 6169 6e69 6e67 2073 6372 6970  l training scrip
+000026c0: 7473 2075 7365 6420 6172 6520 7374 6f72  ts used are stor
+000026d0: 6564 2069 6e20 7468 6520 5b74 7261 696e  ed in the [train
+000026e0: 696e 675f 7363 7269 7074 735d 2874 7261  ing_scripts](tra
+000026f0: 696e 696e 675f 7363 7269 7074 7329 2064  ining_scripts) d
+00002700: 6972 6563 746f 7279 2e0d 0a54 6865 7365  irectory...These
+00002710: 2074 7261 696e 6564 206d 6f64 656c 7320   trained models 
+00002720: 6861 7665 2048 6f73 7465 6420 496e 6665  have Hosted Infe
+00002730: 7265 6e63 6520 4150 4920 7769 6467 6574  rence API widget
+00002740: 7320 7468 6174 2079 6f75 2063 616e 2075  s that you can u
+00002750: 7365 2074 6f20 6578 7065 7269 6d65 6e74  se to experiment
+00002760: 2077 6974 6820 7468 6520 6d6f 6465 6c73   with the models
+00002770: 206f 6e20 7468 6569 7220 4875 6767 696e   on their Huggin
+00002780: 6720 4661 6365 206d 6f64 656c 2070 6167  g Face model pag
+00002790: 6573 2e20 4164 6469 7469 6f6e 616c 6c79  es. Additionally
+000027a0: 2c20 4875 6767 696e 6720 4661 6365 2070  , Hugging Face p
+000027b0: 726f 7669 6465 7320 6561 6368 206d 6f64  rovides each mod
+000027c0: 656c 2077 6974 6820 6120 6672 6565 2041  el with a free A
+000027d0: 5049 2028 6044 6570 6c6f 7960 203e 2060  PI (`Deploy` > `
+000027e0: 496e 6665 7265 6e63 6520 4150 4960 206f  Inference API` o
+000027f0: 6e20 7468 6520 6d6f 6465 6c20 7061 6765  n the model page
+00002800: 292e 0d0a 0d0a 2323 2320 4665 774e 4552  ).....### FewNER
+00002810: 440d 0a2a 205b 6074 6f6d 6161 7273 656e  D..* [`tomaarsen
+00002820: 2f73 7061 6e2d 6d61 726b 6572 2d62 6572  /span-marker-ber
+00002830: 742d 6261 7365 2d66 6577 6e65 7264 2d66  t-base-fewnerd-f
+00002840: 696e 652d 7375 7065 7260 5d28 6874 7470  ine-super`](http
+00002850: 733a 2f2f 6875 6767 696e 6766 6163 652e  s://huggingface.
+00002860: 636f 2f74 6f6d 6161 7273 656e 2f73 7061  co/tomaarsen/spa
+00002870: 6e2d 6d61 726b 6572 2d62 6572 742d 6261  n-marker-bert-ba
+00002880: 7365 2d66 6577 6e65 7264 2d66 696e 652d  se-fewnerd-fine-
+00002890: 7375 7065 7229 2069 7320 6120 6d6f 6465  super) is a mode
+000028a0: 6c20 7468 6174 2049 2068 6176 6520 7472  l that I have tr
+000028b0: 6169 6e65 6420 696e 2032 2068 6f75 7273  ained in 2 hours
+000028c0: 206f 6e20 7468 6520 6669 6e65 6772 6169   on the finegrai
+000028d0: 6e65 642c 2073 7570 6572 7669 7365 6420  ned, supervised 
+000028e0: 5b46 6577 2d4e 4552 4420 6461 7461 7365  [Few-NERD datase
+000028f0: 745d 2868 7474 7073 3a2f 2f68 7567 6769  t](https://huggi
+00002900: 6e67 6661 6365 2e63 6f2f 6461 7461 7365  ngface.co/datase
+00002910: 7473 2f44 464b 492d 534c 542f 6665 772d  ts/DFKI-SLT/few-
+00002920: 6e65 7264 292e 2049 7420 7265 6163 6865  nerd). It reache
+00002930: 6420 6120 302e 3730 3533 2054 6573 7420  d a 0.7053 Test 
+00002940: 4631 2c20 636f 6d70 6574 6974 6976 6520  F1, competitive 
+00002950: 696e 2074 6865 2061 6c6c 2d74 696d 6520  in the all-time 
+00002960: 5b46 6577 2d4e 4552 4420 6c65 6164 6572  [Few-NERD leader
+00002970: 626f 6172 645d 2868 7474 7073 3a2f 2f70  board](https://p
+00002980: 6170 6572 7377 6974 6863 6f64 652e 636f  aperswithcode.co
+00002990: 6d2f 736f 7461 2f6e 616d 6564 2d65 6e74  m/sota/named-ent
+000029a0: 6974 792d 7265 636f 676e 6974 696f 6e2d  ity-recognition-
+000029b0: 6f6e 2d66 6577 2d6e 6572 642d 7375 7029  on-few-nerd-sup)
+000029c0: 2075 7369 6e67 2060 6265 7274 2d62 6173   using `bert-bas
+000029d0: 6560 2e20 4d79 2074 7261 696e 696e 6720  e`. My training 
+000029e0: 7363 7269 7074 2072 6573 656d 626c 6573  script resembles
+000029f0: 2074 6865 206f 6e65 2074 6861 7420 796f   the one that yo
+00002a00: 7520 6361 6e20 7365 6520 6162 6f76 652e  u can see above.
+00002a10: 0d0a 2020 2a20 5472 7920 7468 6520 6d6f  ..  * Try the mo
+00002a20: 6465 6c20 6f75 7420 6f6e 6c69 6e65 2075  del out online u
+00002a30: 7369 6e67 2074 6869 7320 5bf0 9fa4 9720  sing this [.... 
+00002a40: 5370 6163 655d 2868 7474 7073 3a2f 2f74  Space](https://t
+00002a50: 6f6d 6161 7273 656e 2d73 7061 6e2d 6d61  omaarsen-span-ma
+00002a60: 726b 6572 2d62 6572 742d 6261 7365 2d66  rker-bert-base-f
+00002a70: 6577 6e65 7264 2d66 696e 652d 7375 7065  ewnerd-fine-supe
+00002a80: 722e 6866 2e73 7061 6365 2f29 2e0d 0a0d  r.hf.space/)....
+00002a90: 0a2a 205b 6074 6f6d 6161 7273 656e 2f73  .* [`tomaarsen/s
+00002aa0: 7061 6e2d 6d61 726b 6572 2d72 6f62 6572  pan-marker-rober
+00002ab0: 7461 2d6c 6172 6765 2d66 6577 6e65 7264  ta-large-fewnerd
+00002ac0: 2d66 696e 652d 7375 7065 7260 5d28 6874  -fine-super`](ht
+00002ad0: 7470 733a 2f2f 6875 6767 696e 6766 6163  tps://huggingfac
+00002ae0: 652e 636f 2f74 6f6d 6161 7273 656e 2f73  e.co/tomaarsen/s
+00002af0: 7061 6e2d 6d61 726b 6572 2d72 6f62 6572  pan-marker-rober
+00002b00: 7461 2d6c 6172 6765 2d66 6577 6e65 7264  ta-large-fewnerd
+00002b10: 2d66 696e 652d 7375 7065 7229 2077 6173  -fine-super) was
+00002b20: 2074 7261 696e 6564 2069 6e20 3620 686f   trained in 6 ho
+00002b30: 7572 7320 6f6e 2074 6865 2066 696e 6567  urs on the fineg
+00002b40: 7261 696e 6564 2c20 7375 7065 7276 6973  rained, supervis
+00002b50: 6564 205b 4665 772d 4e45 5244 2064 6174  ed [Few-NERD dat
+00002b60: 6173 6574 5d28 6874 7470 733a 2f2f 6875  aset](https://hu
+00002b70: 6767 696e 6766 6163 652e 636f 2f64 6174  ggingface.co/dat
+00002b80: 6173 6574 732f 4446 4b49 2d53 4c54 2f66  asets/DFKI-SLT/f
+00002b90: 6577 2d6e 6572 6429 2075 7369 6e67 2060  ew-nerd) using `
+00002ba0: 726f 6265 7274 612d 6c61 7267 6560 2e20  roberta-large`. 
+00002bb0: 4974 2072 6561 6368 6564 2061 2030 2e37  It reached a 0.7
+00002bc0: 3130 3320 5465 7374 2046 312c 2072 6561  103 Test F1, rea
+00002bd0: 6368 696e 6720 6120 6e65 7720 7374 6174  ching a new stat
+00002be0: 6520 6f66 2074 6865 2061 7274 2069 6e20  e of the art in 
+00002bf0: 7468 6520 616c 6c2d 7469 6d65 205b 4665  the all-time [Fe
+00002c00: 772d 4e45 5244 206c 6561 6465 7262 6f61  w-NERD leaderboa
+00002c10: 7264 5d28 6874 7470 733a 2f2f 7061 7065  rd](https://pape
+00002c20: 7273 7769 7468 636f 6465 2e63 6f6d 2f73  rswithcode.com/s
+00002c30: 6f74 612f 6e61 6d65 642d 656e 7469 7479  ota/named-entity
+00002c40: 2d72 6563 6f67 6e69 7469 6f6e 2d6f 6e2d  -recognition-on-
+00002c50: 6665 772d 6e65 7264 2d73 7570 292e 0d0a  few-nerd-sup)...
+00002c60: 2a20 5b60 746f 6d61 6172 7365 6e2f 7370  * [`tomaarsen/sp
+00002c70: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
+00002c80: 6265 7274 612d 6261 7365 2d66 6577 6e65  berta-base-fewne
+00002c90: 7264 2d66 696e 652d 7375 7065 7260 5d28  rd-fine-super`](
+00002ca0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
+00002cb0: 6163 652e 636f 2f74 6f6d 6161 7273 656e  ace.co/tomaarsen
+00002cc0: 2f73 7061 6e2d 6d61 726b 6572 2d78 6c6d  /span-marker-xlm
+00002cd0: 2d72 6f62 6572 7461 2d62 6173 652d 6665  -roberta-base-fe
+00002ce0: 776e 6572 642d 6669 6e65 2d73 7570 6572  wnerd-fine-super
+00002cf0: 2920 6973 2061 206d 756c 7469 6c69 6e67  ) is a multiling
+00002d00: 7561 6c20 6d6f 6465 6c20 7468 6174 2049  ual model that I
+00002d10: 2068 6176 6520 7472 6169 6e65 6420 696e   have trained in
+00002d20: 2031 2e35 2068 6f75 7273 206f 6e20 7468   1.5 hours on th
+00002d30: 6520 6669 6e65 6772 6169 6e65 642c 2073  e finegrained, s
+00002d40: 7570 6572 7669 7365 6420 5b46 6577 2d4e  upervised [Few-N
+00002d50: 4552 4420 6461 7461 7365 745d 2868 7474  ERD dataset](htt
+00002d60: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+00002d70: 2e63 6f2f 6461 7461 7365 7473 2f44 464b  .co/datasets/DFK
+00002d80: 492d 534c 542f 6665 772d 6e65 7264 292e  I-SLT/few-nerd).
+00002d90: 2049 7420 7265 6163 6865 6420 6120 302e   It reached a 0.
+00002da0: 3638 3620 5465 7374 2046 3120 6f6e 2045  686 Test F1 on E
+00002db0: 6e67 6c69 7368 2c20 616e 6420 776f 726b  nglish, and work
+00002dc0: 7320 7765 6c6c 206f 6e20 6f74 6865 7220  s well on other 
+00002dd0: 6c61 6e67 7561 6765 7320 6c69 6b65 2053  languages like S
+00002de0: 7061 6e69 7368 2c20 4672 656e 6368 2c20  panish, French, 
+00002df0: 4765 726d 616e 2c20 5275 7373 6961 6e2c  German, Russian,
+00002e00: 2044 7574 6368 2c20 506f 6c69 7368 2c20   Dutch, Polish, 
+00002e10: 4963 656c 616e 6469 632c 2047 7265 656b  Icelandic, Greek
+00002e20: 2061 6e64 206d 616e 7920 6d6f 7265 2e0d   and many more..
+00002e30: 0a0d 0a23 2323 204f 6e74 6f4e 6f74 6573  ...### OntoNotes
+00002e40: 2076 352e 300d 0a2a 205b 6074 6f6d 6161   v5.0..* [`tomaa
+00002e50: 7273 656e 2f73 7061 6e2d 6d61 726b 6572  rsen/span-marker
+00002e60: 2d72 6f62 6572 7461 2d6c 6172 6765 2d6f  -roberta-large-o
+00002e70: 6e74 6f6e 6f74 6573 3560 5d28 6874 7470  ntonotes5`](http
+00002e80: 733a 2f2f 6875 6767 696e 6766 6163 652e  s://huggingface.
+00002e90: 636f 2f74 6f6d 6161 7273 656e 2f73 7061  co/tomaarsen/spa
+00002ea0: 6e2d 6d61 726b 6572 2d72 6f62 6572 7461  n-marker-roberta
+00002eb0: 2d6c 6172 6765 2d6f 6e74 6f6e 6f74 6573  -large-ontonotes
+00002ec0: 3529 2077 6173 2074 7261 696e 6564 2069  5) was trained i
+00002ed0: 6e20 3320 686f 7572 7320 6f6e 2074 6865  n 3 hours on the
+00002ee0: 204f 6e74 6f4e 6f74 6573 2076 352e 3020   OntoNotes v5.0 
+00002ef0: 6461 7461 7365 742c 2072 6561 6368 696e  dataset, reachin
+00002f00: 6720 6120 7065 7266 6f72 6d61 6e63 6520  g a performance 
+00002f10: 6f66 2030 2e39 3135 3420 4631 2e20 466f  of 0.9154 F1. Fo
+00002f20: 7220 7265 6665 7265 6e63 652c 2074 6865  r reference, the
+00002f30: 2063 7572 7265 6e74 2073 7472 6f6e 6765   current stronge
+00002f40: 7374 2073 7061 4379 206d 6f64 656c 2028  st spaCy model (
+00002f50: 6065 6e5f 636f 7265 5f77 6562 5f74 7266  `en_core_web_trf
+00002f60: 6029 2072 6561 6368 6573 2030 2e38 3938  `) reaches 0.898
+00002f70: 2e20 5468 6973 2053 7061 6e4d 6172 6b65  . This SpanMarke
+00002f80: 7220 6d6f 6465 6c20 7573 6573 2061 2060  r model uses a `
+00002f90: 726f 6265 7274 612d 6c61 7267 6560 2065  roberta-large` e
+00002fa0: 6e63 6f64 6572 2075 6e64 6572 2074 6865  ncoder under the
+00002fb0: 2068 6f6f 642e 0d0a 0d0a 2323 2320 436f   hood.....### Co
+00002fc0: 4e4c 4c30 330d 0a2a 205b 6074 6f6d 6161  NLL03..* [`tomaa
+00002fd0: 7273 656e 2f73 7061 6e2d 6d61 726b 6572  rsen/span-marker
+00002fe0: 2d78 6c6d 2d72 6f62 6572 7461 2d6c 6172  -xlm-roberta-lar
+00002ff0: 6765 2d63 6f6e 6c6c 3033 605d 2868 7474  ge-conll03`](htt
+00003000: 7073 3a2f 2f68 7567 6769 6e67 6661 6365  ps://huggingface
+00003010: 2e63 6f2f 746f 6d61 6172 7365 6e2f 7370  .co/tomaarsen/sp
+00003020: 616e 2d6d 6172 6b65 722d 786c 6d2d 726f  an-marker-xlm-ro
+00003030: 6265 7274 612d 6c61 7267 652d 636f 6e6c  berta-large-conl
+00003040: 6c30 3329 2069 7320 6120 5370 616e 4d61  l03) is a SpanMa
+00003050: 726b 6572 206d 6f64 656c 2075 7369 6e67  rker model using
+00003060: 2060 786c 6d2d 726f 6265 7274 612d 6c61   `xlm-roberta-la
+00003070: 7267 6560 2074 6861 7420 7761 7320 7472  rge` that was tr
+00003080: 6169 6e65 6420 696e 2034 3520 6d69 6e75  ained in 45 minu
+00003090: 7465 732e 2049 7420 7265 6163 6865 7320  tes. It reaches 
+000030a0: 6120 7374 6174 6520 6f66 2074 6865 2061  a state of the a
+000030b0: 7274 2030 2e39 3331 2046 3120 6f6e 2043  rt 0.931 F1 on C
+000030c0: 6f4e 4c4c 3033 2077 6974 686f 7574 2075  oNLL03 without u
+000030d0: 7369 6e67 2064 6f63 756d 656e 742d 6c65  sing document-le
+000030e0: 7665 6c20 636f 6e74 6578 742e 2046 6f72  vel context. For
+000030f0: 2072 6566 6572 656e 6365 2c20 7468 6520   reference, the 
+00003100: 6375 7272 656e 7420 7374 726f 6e67 6573  current stronges
+00003110: 7420 7370 6143 7920 6d6f 6465 6c20 2860  t spaCy model (`
+00003120: 656e 5f63 6f72 655f 7765 625f 7472 6660  en_core_web_trf`
+00003130: 2920 7265 6163 6865 7320 3931 2e36 2e0d  ) reaches 91.6..
+00003140: 0a2a 205b 6074 6f6d 6161 7273 656e 2f73  .* [`tomaarsen/s
+00003150: 7061 6e2d 6d61 726b 6572 2d78 6c6d 2d72  pan-marker-xlm-r
+00003160: 6f62 6572 7461 2d6c 6172 6765 2d63 6f6e  oberta-large-con
+00003170: 6c6c 3033 2d64 6f63 2d63 6f6e 7465 7874  ll03-doc-context
+00003180: 605d 2868 7474 7073 3a2f 2f68 7567 6769  `](https://huggi
+00003190: 6e67 6661 6365 2e63 6f2f 746f 6d61 6172  ngface.co/tomaar
+000031a0: 7365 6e2f 7370 616e 2d6d 6172 6b65 722d  sen/span-marker-
+000031b0: 786c 6d2d 726f 6265 7274 612d 6c61 7267  xlm-roberta-larg
+000031c0: 652d 636f 6e6c 6c30 332d 646f 632d 636f  e-conll03-doc-co
+000031d0: 6e74 6578 7429 2069 7320 616e 6f74 6865  ntext) is anothe
+000031e0: 7220 5370 616e 4d61 726b 6572 206d 6f64  r SpanMarker mod
+000031f0: 656c 2075 7369 6e67 2074 6865 2060 786c  el using the `xl
+00003200: 6d2d 726f 6265 7274 612d 6c61 7267 6560  m-roberta-large`
+00003210: 2065 6e63 6f64 6572 2e20 4974 2075 7365   encoder. It use
+00003220: 7320 5b64 6f63 756d 656e 742d 6c65 7665  s [document-leve
+00003230: 6c20 636f 6e74 6578 745d 2868 7474 7073  l context](https
+00003240: 3a2f 2f74 6f6d 6161 7273 656e 2e67 6974  ://tomaarsen.git
+00003250: 6875 622e 696f 2f53 7061 6e4d 6172 6b65  hub.io/SpanMarke
+00003260: 724e 4552 2f6e 6f74 6562 6f6f 6b73 2f64  rNER/notebooks/d
+00003270: 6f63 756d 656e 745f 6c65 7665 6c5f 636f  ocument_level_co
+00003280: 6e74 6578 742e 6874 6d6c 2920 746f 2072  ntext.html) to r
+00003290: 6561 6368 2061 2073 7461 7465 206f 6620  each a state of 
+000032a0: 7468 6520 6172 7420 302e 3934 3420 4631  the art 0.944 F1
+000032b0: 2e20 466f 7220 7468 6520 6265 7374 2070  . For the best p
+000032c0: 6572 666f 726d 616e 6365 2c20 696e 6665  erformance, infe
+000032d0: 7265 6e63 6520 7368 6f75 6c64 2062 6520  rence should be 
+000032e0: 7065 7266 6f72 6d65 6420 7573 696e 6720  performed using 
+000032f0: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
+00003300: 6f6e 7465 7874 2028 5b64 6f63 735d 2868  ontext ([docs](h
+00003310: 7474 7073 3a2f 2f74 6f6d 6161 7273 656e  ttps://tomaarsen
+00003320: 2e67 6974 6875 622e 696f 2f53 7061 6e4d  .github.io/SpanM
+00003330: 6172 6b65 724e 4552 2f6e 6f74 6562 6f6f  arkerNER/noteboo
+00003340: 6b73 2f64 6f63 756d 656e 745f 6c65 7665  ks/document_leve
+00003350: 6c5f 636f 6e74 6578 742e 6874 6d6c 2349  l_context.html#I
+00003360: 6e66 6572 656e 6365 2929 2e20 5468 6973  nference)). This
+00003370: 206d 6f64 656c 2077 6173 2074 7261 696e   model was train
+00003380: 6564 2069 6e20 3120 686f 7572 2e0d 0a0d  ed in 1 hour....
+00003390: 0a23 2323 2043 6f4e 4c4c 2b2b 0d0a 2a20  .### CoNLL++..* 
+000033a0: 5b60 746f 6d61 6172 7365 6e2f 7370 616e  [`tomaarsen/span
+000033b0: 2d6d 6172 6b65 722d 786c 6d2d 726f 6265  -marker-xlm-robe
+000033c0: 7274 612d 6c61 7267 652d 636f 6e6c 6c70  rta-large-conllp
+000033d0: 702d 646f 632d 636f 6e74 6578 7460 5d28  p-doc-context`](
+000033e0: 6874 7470 733a 2f2f 6875 6767 696e 6766  https://huggingf
+000033f0: 6163 652e 636f 2f74 6f6d 6161 7273 656e  ace.co/tomaarsen
+00003400: 2f73 7061 6e2d 6d61 726b 6572 2d78 6c6d  /span-marker-xlm
+00003410: 2d72 6f62 6572 7461 2d6c 6172 6765 2d63  -roberta-large-c
+00003420: 6f6e 6c6c 7070 2d64 6f63 2d63 6f6e 7465  onllpp-doc-conte
+00003430: 7874 2920 7761 7320 7472 6169 6e65 6420  xt) was trained 
+00003440: 696e 2061 6e20 686f 7572 2075 7369 6e67  in an hour using
+00003450: 2074 6865 2060 786c 6d2d 726f 6265 7274   the `xlm-robert
+00003460: 612d 6c61 7267 6560 2065 6e63 6f64 6572  a-large` encoder
+00003470: 206f 6e20 7468 6520 436f 4e4c 4c2b 2b20   on the CoNLL++ 
+00003480: 6461 7461 7365 742e 2055 7369 6e67 205b  dataset. Using [
+00003490: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
+000034a0: 6f6e 7465 7874 5d28 6874 7470 733a 2f2f  ontext](https://
+000034b0: 746f 6d61 6172 7365 6e2e 6769 7468 7562  tomaarsen.github
+000034c0: 2e69 6f2f 5370 616e 4d61 726b 6572 4e45  .io/SpanMarkerNE
+000034d0: 522f 6e6f 7465 626f 6f6b 732f 646f 6375  R/notebooks/docu
+000034e0: 6d65 6e74 5f6c 6576 656c 5f63 6f6e 7465  ment_level_conte
+000034f0: 7874 2e68 746d 6c29 2c20 6974 2072 6561  xt.html), it rea
+00003500: 6368 6573 2061 2076 6572 7920 636f 6d70  ches a very comp
+00003510: 6574 6974 6976 6520 302e 3935 3520 4631  etitive 0.955 F1
+00003520: 2e20 466f 7220 7468 6520 6265 7374 2070  . For the best p
+00003530: 6572 666f 726d 616e 6365 2c20 696e 6665  erformance, infe
+00003540: 7265 6e63 6520 7368 6f75 6c64 2062 6520  rence should be 
+00003550: 7065 7266 6f72 6d65 6420 7573 696e 6720  performed using 
+00003560: 646f 6375 6d65 6e74 2d6c 6576 656c 2063  document-level c
+00003570: 6f6e 7465 7874 2028 5b64 6f63 735d 2868  ontext ([docs](h
+00003580: 7474 7073 3a2f 2f74 6f6d 6161 7273 656e  ttps://tomaarsen
+00003590: 2e67 6974 6875 622e 696f 2f53 7061 6e4d  .github.io/SpanM
+000035a0: 6172 6b65 724e 4552 2f6e 6f74 6562 6f6f  arkerNER/noteboo
+000035b0: 6b73 2f64 6f63 756d 656e 745f 6c65 7665  ks/document_leve
+000035c0: 6c5f 636f 6e74 6578 742e 6874 6d6c 2349  l_context.html#I
+000035d0: 6e66 6572 656e 6365 2929 2e0d 0a0d 0a23  nference)).....#
+000035e0: 2320 5573 696e 6720 7072 6574 7261 696e  # Using pretrain
+000035f0: 6564 2053 7061 6e4d 6172 6b65 7220 6d6f  ed SpanMarker mo
+00003600: 6465 6c73 2077 6974 6820 7370 6143 790d  dels with spaCy.
+00003610: 0a41 6c6c 205b 5370 616e 4d61 726b 6572  .All [SpanMarker
+00003620: 206d 6f64 656c 7320 6f6e 2074 6865 2048   models on the H
+00003630: 7567 6769 6e67 2046 6163 6520 4875 625d  ugging Face Hub]
+00003640: 2868 7474 7073 3a2f 2f68 7567 6769 6e67  (https://hugging
+00003650: 6661 6365 2e63 6f2f 6d6f 6465 6c73 3f6c  face.co/models?l
+00003660: 6962 7261 7279 3d73 7061 6e2d 6d61 726b  ibrary=span-mark
+00003670: 6572 2920 6361 6e20 616c 736f 2062 6520  er) can also be 
+00003680: 6561 7369 6c79 2075 7365 6420 696e 2073  easily used in s
+00003690: 7061 4379 2e20 4974 2773 2061 7320 7369  paCy. It's as si
+000036a0: 6d70 6c65 2061 7320 696e 636c 7564 696e  mple as includin
+000036b0: 6720 3120 6c69 6e65 2074 6f20 6164 6420  g 1 line to add 
+000036c0: 7468 6520 6073 7061 6e5f 6d61 726b 6572  the `span_marker
+000036d0: 6020 7069 7065 6c69 6e65 2e20 5365 6520  ` pipeline. See 
+000036e0: 7468 6520 446f 6375 6d65 6e74 6174 696f  the Documentatio
+000036f0: 6e20 6f72 2041 5049 2052 6566 6572 656e  n or API Referen
+00003700: 6365 2066 6f72 206d 6f72 6520 696e 666f  ce for more info
+00003710: 726d 6174 696f 6e2e 0d0a 6060 6070 7974  rmation...```pyt
+00003720: 686f 6e0d 0a69 6d70 6f72 7420 7370 6163  hon..import spac
+00003730: 790d 0a0d 0a23 204c 6f61 6420 7468 6520  y....# Load the 
+00003740: 7370 6143 7920 6d6f 6465 6c20 7769 7468  spaCy model with
+00003750: 2074 6865 2073 7061 6e5f 6d61 726b 6572   the span_marker
+00003760: 2070 6970 656c 696e 6520 636f 6d70 6f6e   pipeline compon
+00003770: 656e 740d 0a6e 6c70 203d 2073 7061 6379  ent..nlp = spacy
+00003780: 2e6c 6f61 6428 2265 6e5f 636f 7265 5f77  .load("en_core_w
+00003790: 6562 5f73 6d22 2c20 6469 7361 626c 653d  eb_sm", disable=
+000037a0: 5b22 6e65 7222 5d29 0d0a 6e6c 702e 6164  ["ner"])..nlp.ad
+000037b0: 645f 7069 7065 2822 7370 616e 5f6d 6172  d_pipe("span_mar
+000037c0: 6b65 7222 2c20 636f 6e66 6967 3d7b 226d  ker", config={"m
+000037d0: 6f64 656c 223a 2022 746f 6d61 6172 7365  odel": "tomaarse
+000037e0: 6e2f 7370 616e 2d6d 6172 6b65 722d 726f  n/span-marker-ro
+000037f0: 6265 7274 612d 6c61 7267 652d 6f6e 746f  berta-large-onto
+00003800: 6e6f 7465 7335 227d 290d 0a0d 0a23 2046  notes5"})....# F
+00003810: 6565 6420 736f 6d65 2074 6578 7420 7468  eed some text th
+00003820: 726f 7567 6820 7468 6520 6d6f 6465 6c20  rough the model 
+00003830: 746f 2067 6574 2061 2073 7061 6379 2044  to get a spacy D
+00003840: 6f63 0d0a 7465 7874 203d 2022 2222 436c  oc..text = """Cl
+00003850: 656f 7061 7472 6120 5649 492c 2061 6c73  eopatra VII, als
+00003860: 6f20 6b6e 6f77 6e20 6173 2043 6c65 6f70  o known as Cleop
+00003870: 6174 7261 2074 6865 2047 7265 6174 2c20  atra the Great, 
+00003880: 7761 7320 7468 6520 6c61 7374 2061 6374  was the last act
+00003890: 6976 6520 7275 6c65 7220 6f66 2074 6865  ive ruler of the
+000038a0: 205c 0d0a 5074 6f6c 656d 6169 6320 4b69   \..Ptolemaic Ki
+000038b0: 6e67 646f 6d20 6f66 2045 6779 7074 2e20  ngdom of Egypt. 
+000038c0: 5368 6520 7761 7320 626f 726e 2069 6e20  She was born in 
+000038d0: 3639 2042 4345 2061 6e64 2072 756c 6564  69 BCE and ruled
+000038e0: 2045 6779 7074 2066 726f 6d20 3531 2042   Egypt from 51 B
+000038f0: 4345 2075 6e74 696c 2068 6572 205c 0d0a  CE until her \..
+00003900: 6465 6174 6820 696e 2033 3020 4243 452e  death in 30 BCE.
+00003910: 2222 220d 0a64 6f63 203d 206e 6c70 2874  """..doc = nlp(t
+00003920: 6578 7429 0d0a 0d0a 2320 416e 6420 6c6f  ext)....# And lo
+00003930: 6f6b 2061 7420 7468 6520 656e 7469 7469  ok at the entiti
+00003940: 6573 0d0a 7072 696e 7428 5b28 656e 7469  es..print([(enti
+00003950: 7479 2c20 656e 7469 7479 2e6c 6162 656c  ty, entity.label
+00003960: 5f29 2066 6f72 2065 6e74 6974 7920 696e  _) for entity in
+00003970: 2064 6f63 2e65 6e74 735d 290d 0a22 2222   doc.ents]).."""
+00003980: 0d0a 5b28 436c 656f 7061 7472 6120 5649  ..[(Cleopatra VI
+00003990: 492c 2022 5045 5253 4f4e 2229 2c20 2843  I, "PERSON"), (C
+000039a0: 6c65 6f70 6174 7261 2074 6865 2047 7265  leopatra the Gre
+000039b0: 6174 2c20 2250 4552 534f 4e22 292c 2028  at, "PERSON"), (
+000039c0: 7468 6520 5074 6f6c 656d 6169 6320 4b69  the Ptolemaic Ki
+000039d0: 6e67 646f 6d20 6f66 2045 6779 7074 2c20  ngdom of Egypt, 
+000039e0: 2247 5045 2229 2c0d 0a28 3639 2042 4345  "GPE"),..(69 BCE
+000039f0: 2c20 2244 4154 4522 292c 2028 4567 7970  , "DATE"), (Egyp
+00003a00: 742c 2022 4750 4522 292c 2028 3531 2042  t, "GPE"), (51 B
+00003a10: 4345 2c20 2244 4154 4522 292c 2028 3330  CE, "DATE"), (30
+00003a20: 2042 4345 2c20 2244 4154 4522 295d 0d0a   BCE, "DATE")]..
+00003a30: 2222 220d 0a60 6060 0d0a 215b 696d 6167  """..```..![imag
+00003a40: 655d 2868 7474 7073 3a2f 2f75 7365 722d  e](https://user-
+00003a50: 696d 6167 6573 2e67 6974 6875 6275 7365  images.githubuse
+00003a60: 7263 6f6e 7465 6e74 2e63 6f6d 2f33 3736  rcontent.com/376
+00003a70: 3231 3439 312f 3234 3631 3730 3632 332d  21491/246170623-
+00003a80: 3633 3531 6362 3765 2d62 6262 302d 3434  6351cb7e-bbb0-44
+00003a90: 3732 2d61 6631 362d 3961 3335 3161 3235  72-af16-9a351a25
+00003aa0: 3364 6139 2e70 6e67 290d 0a0d 0a23 2320  3da9.png)....## 
+00003ab0: 436f 6e74 6578 740d 0a3c 6831 2061 6c69  Context..<h1 ali
+00003ac0: 676e 3d22 6365 6e74 6572 223e 0d0a 2020  gn="center">..  
+00003ad0: 2020 3c61 2068 7265 663d 2268 7474 7073    <a href="https
+00003ae0: 3a2f 2f67 6974 6875 622e 636f 6d2f 6172  ://github.com/ar
+00003af0: 6769 6c6c 612d 696f 2f61 7267 696c 6c61  gilla-io/argilla
+00003b00: 223e 0d0a 2020 2020 3c69 6d67 2073 7263  ">..    <img src
+00003b10: 3d22 6874 7470 733a 2f2f 6769 7468 7562  ="https://github
+00003b20: 2e63 6f6d 2f64 7673 7265 706f 2f69 6d67  .com/dvsrepo/img
+00003b30: 732f 7261 772f 6d61 696e 2f72 672e 7376  s/raw/main/rg.sv
+00003b40: 6722 2061 6c74 3d22 4172 6769 6c6c 6122  g" alt="Argilla"
+00003b50: 2077 6964 7468 3d22 3135 3022 3e0d 0a20   width="150">.. 
+00003b60: 2020 203c 2f61 3e0d 0a3c 2f68 313e 0d0a     </a>..</h1>..
+00003b70: 0d0a 4920 6861 7665 2064 6576 656c 6f70  ..I have develop
+00003b80: 6564 2074 6869 7320 6c69 6272 6172 7920  ed this library 
+00003b90: 6173 2061 2070 6172 7420 6f66 206d 7920  as a part of my 
+00003ba0: 7468 6573 6973 2077 6f72 6b20 6174 205b  thesis work at [
+00003bb0: 4172 6769 6c6c 615d 2868 7474 7073 3a2f  Argilla](https:/
+00003bc0: 2f67 6974 6875 622e 636f 6d2f 6172 6769  /github.com/argi
+00003bd0: 6c6c 612d 696f 2f61 7267 696c 6c61 292e  lla-io/argilla).
+00003be0: 0d0a 4665 656c 2066 7265 6520 746f 20e2  ..Feel free to .
+00003bf0: ad90 2073 7461 7220 6f72 2077 6174 6368  .. star or watch
+00003c00: 2074 6865 2053 7061 6e4d 6172 6b65 7220   the SpanMarker 
+00003c10: 7265 706f 7369 746f 7279 2074 6f20 6765  repository to ge
+00003c20: 7420 6e6f 7469 6669 6564 2077 6865 6e20  t notified when 
+00003c30: 6d79 2074 6865 7369 7320 6973 2070 7562  my thesis is pub
+00003c40: 6c69 7368 6564 2e0d 0a0d 0a23 2320 4368  lished.....## Ch
+00003c50: 616e 6765 6c6f 670d 0a53 6565 205b 4348  angelog..See [CH
+00003c60: 414e 4745 4c4f 472e 6d64 5d28 4348 414e  ANGELOG.md](CHAN
+00003c70: 4745 4c4f 472e 6d64 2920 666f 7220 6e65  GELOG.md) for ne
+00003c80: 7773 206f 6e20 616c 6c20 5370 616e 4d61  ws on all SpanMa
+00003c90: 726b 6572 2076 6572 7369 6f6e 732e 0d0a  rker versions...
+00003ca0: 0d0a 2323 204c 6963 656e 7365 0d0a 5365  ..## License..Se
+00003cb0: 6520 5b4c 4943 454e 5345 5d28 4c49 4345  e [LICENSE](LICE
+00003cc0: 4e53 452e 6d64 2920 666f 7220 7468 6520  NSE.md) for the 
+00003cd0: 6375 7272 656e 7420 6c69 6365 6e73 652e  current license.
+00003ce0: 0d0a                                     ..
```

### Comparing `span_marker-1.2.2/span_marker.egg-info/SOURCES.txt` & `span_marker-1.2.3/span_marker.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/tests/test_configuration.py` & `span_marker-1.2.3/tests/test_configuration.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/tests/test_model_card.py` & `span_marker-1.2.3/tests/test_model_card.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/tests/test_modeling.py` & `span_marker-1.2.3/tests/test_modeling.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/tests/test_spacy_integration.py` & `span_marker-1.2.3/tests/test_spacy_integration.py`

 * *Files identical despite different names*

### Comparing `span_marker-1.2.2/tests/test_trainer.py` & `span_marker-1.2.3/tests/test_trainer.py`

 * *Files identical despite different names*

